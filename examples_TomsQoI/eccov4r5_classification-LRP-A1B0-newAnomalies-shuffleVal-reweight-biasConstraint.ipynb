{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0bcbb0a-fdc9-49ae-b082-7624f8e5f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing to understand why some LRP blocks are blank\n",
    "### If same signs for a row in last layer, this happens, have to depend on a good random seed\n",
    "### Or fix bias at some high value in the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5950c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 21:24:39.983869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 21:24:41.427858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-04-17 21:24:41.430217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38221 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-04-17 21:24:41.432629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38221 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import metrics\n",
    "from keras.constraints import NonNeg\n",
    "import innvestigate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "import xmitgcm\n",
    "from xmitgcm import open_mdsdataset\n",
    "import ecco_v4_py as ecco\n",
    "\n",
    "import random\n",
    "\n",
    "# See if GPUs are available\n",
    "from keras import backend as K\n",
    "if bool(K._get_available_gpus()):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *\n",
    "\n",
    "### https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed ###\n",
    "### https://keras.io/examples/keras_recipes/reproducibility_recipes/ ###\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5b1ee2-570a-46e3-8b64-42843a17be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sverdrup \n",
    "# mainDir_r4  = '/scratch2/pillarh/eccov4r4'\n",
    "# mainDir_r5  = '/scratch2/pillarh/eccov4r5'\n",
    "# gridDir  = mainDir_r4 + '/GRID'\n",
    "# thetaDir = mainDir_r5 + '/V4r5/diags_daily/SST_day_mean'\n",
    "# thetaDir_ext = mainDir_r5 + '/V4r5_ext_2020_2023_Jun/diags_daily/SST_day_mean'\n",
    "\n",
    "## LS6\n",
    "mainDir_r4 = '/work/07665/shrey911/ls6/LRP_eccov4r4_data'\n",
    "mainDir_r5 = '/work/07665/shrey911/ls6/LRP_eccov4r5_data'\n",
    "gridDir  = mainDir_r5 + '/GRID'\n",
    "thetaDir = mainDir_r5 + '/SST_day_mean'\n",
    "thetaDir_ext = mainDir_r5 + '/SST_day_mean_ext_2020_2023_Jun'\n",
    "\n",
    "# For Sverdrup\n",
    "# ds_r4 = xr.open_dataset(f'/scratch2/shreyas/LRP_eccov4r4_data/thetaSurfECCOv4r4.nc')\n",
    "# For LS6\n",
    "ds_r4 = xr.open_dataset(mainDir_r4 + '/thetaSurfECCOv4r4.nc')\n",
    "\n",
    "# SSH has to be kept because someone used the SSH metadata for SST, \n",
    "# It's not a bug in this code but a hack to handle an existing bug.\n",
    "temp = xmitgcm.open_mdsdataset(data_dir = thetaDir,\n",
    "                             grid_dir = gridDir,\n",
    "                             extra_variables = dict(SSH = dict(dims=['k','j','i'],\n",
    "                                                               attrs = dict(standard_name=\"SST\",\n",
    "                                                                            long_name=\"Sea Surface Temperature\",\n",
    "                                                                            units=\"degC\"))))\n",
    "temp[\"SST\"] = temp[\"SSH\"]\n",
    "temp = temp.drop([\"SSH\"])\n",
    "    \n",
    "temp_ext = xmitgcm.open_mdsdataset(data_dir = thetaDir_ext,\n",
    "                                 grid_dir = gridDir,\n",
    "                                 extra_variables = dict(SST = dict(dims=['k','j','i'],\n",
    "                                                                   attrs = dict(standard_name=\"SST\",\n",
    "                                                                                long_name=\"Sea Surface Temperature\",\n",
    "                                                                                units=\"degC\"))))\n",
    "\n",
    "ds_r5 = xr.concat([temp, temp_ext], \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0039de40-13c0-42bc-a503-11942de7e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SST = xr.open_dataset(mainDir_r5+'/SST_all.nc')\n",
    "SST = ds_SST['SST'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccb3e1b-7581-4089-ad25-99bfbbce6b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_binary_array: loading file /work/07665/shrey911/ls6/LRP_eccov4r5_data/GRID/hFacC.data\n",
      "load_binary_array: data array shape  (1170, 90)\n",
      "load_binary_array: data array type  >f4\n",
      "llc_compact_to_faces: dims, llc  (1170, 90) 90\n",
      "llc_compact_to_faces: data_compact array type  >f4\n",
      "llc_faces_to_tiles: data_tiles shape  (13, 90, 90)\n",
      "llc_faces_to_tiles: data_tiles dtype  >f4\n"
     ]
    }
   ],
   "source": [
    "hFacC = ecco.read_llc_to_tiles(gridDir, 'hFacC.data')\n",
    "hFacC_mask = hFacC > 0\n",
    "hFacC_mask = hFacC_mask.astype(float)\n",
    "\n",
    "XC = ds_r4['XC'].data\n",
    "YC = ds_r4['YC'].data\n",
    "\n",
    "latMask = YC > -20.0\n",
    "latMask = latMask.astype(float)\n",
    "\n",
    "maskFinal = hFacC_mask * latMask\n",
    "NaNmaskFinal = np.copy(maskFinal)\n",
    "NaNmaskFinal[NaNmaskFinal == 0] = np.nan\n",
    "\n",
    "da_XC = xr.DataArray(\n",
    "    data=ds_r4['XC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"XC\"),\n",
    ")\n",
    "\n",
    "da_YC = xr.DataArray(\n",
    "    data=ds_r4['YC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"YC\"),\n",
    ")\n",
    "\n",
    "da_hFacC_mask = xr.DataArray(\n",
    "    data=hFacC_mask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"hFacC mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_latMask = xr.DataArray(\n",
    "    data=latMask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Latitude Mask 1 if > -20, else 0\"),\n",
    ")\n",
    "\n",
    "da_maskFinal = xr.DataArray(\n",
    "    data=maskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_NaNmaskFinal = xr.DataArray(\n",
    "    data=NaNmaskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D True if > 0, else NaN\"),\n",
    ")\n",
    "\n",
    "wetpoints = np.nonzero(maskFinal.data)\n",
    "da_wetpoints = xr.DataArray(\n",
    "    data=np.asarray(wetpoints),\n",
    "    dims=[\"wetpoints_dim\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        wetpoints_dim = np.arange(np.asarray(wetpoints).shape[0], dtype = int),\n",
    "        num_wetpoints = np.arange(np.asarray(wetpoints).shape[1], dtype = int),\n",
    "    ),\n",
    "    attrs=dict(description=\"indices of wetpoints in the order (tile, j, i) in the three rows\"),\n",
    ")\n",
    "\n",
    "da_SST = xr.DataArray(\n",
    "    data=SST,\n",
    "    dims=[\"time\", \"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        time = ds_r5['time'].data[:SST.shape[0]],\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"SST field in llc format\"),\n",
    ")\n",
    "\n",
    "ds = xr.Dataset()\n",
    "ds = ds.assign(XC           = da_XC,\n",
    "               YC           = da_YC,\n",
    "               hFacC_mask   = da_hFacC_mask,\n",
    "               latMask      = da_latMask,\n",
    "               maskFinal    = da_maskFinal,\n",
    "               NaNmaskFinal = da_NaNmaskFinal,\n",
    "               wetpoints    = da_wetpoints,\n",
    "               SST          = da_SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63f1c11-9c00-48fb-bf00-0afd6b5678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize_new(field, num_years = 31, first_leap_year_idx = 0):\n",
    "    \n",
    "    leap_yr_offsets_jan_feb   = np.array([0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8])\n",
    "    leap_yr_offsets_after_feb = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8])\n",
    "\n",
    "    if len(field.shape) > 1:\n",
    "        seasonal_trend = np.zeros((366, field.shape[1]))\n",
    "    else:\n",
    "        seasonal_trend = np.zeros((366,))\n",
    "\n",
    "    #### Calculate seasonal trend\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        same_cal_days_idx=[d+365*year+leap_yr_offsets_jan_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indexing\n",
    "    same_cal_days_idx=[365*year+59+int(year/4) for year in range(first_leap_year_idx,num_years,4)]\n",
    "    # Remove mean\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='constant', \n",
    "                                                    overwrite_data=False)\n",
    "    # Remove linear trend\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='linear', \n",
    "                                                    overwrite_data=False)\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        same_cal_days_idx=[d-1+365*year+leap_yr_offsets_after_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428e4223-fad9-483a-a905-629ce467174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds['SST'].data[:,wetpoints[0],wetpoints[1],wetpoints[2]].copy()\n",
    "X = anomalize_new(X)\n",
    "X_full = X.copy()\n",
    "X = X[30:-30]\n",
    "\n",
    "y = ds['SST'].isel(tile = 10, j = 1, i = 43).data.copy()\n",
    "y = anomalize_new(y)\n",
    "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "y = np.convolve(y, np.ones(61)/61, mode='valid')\n",
    "oneHotCost = np.zeros((y.shape[0], 2), dtype = int)\n",
    "oneHotCost[:,0] = y >= 0.0\n",
    "oneHotCost[:,1] = y <  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d23bff3-dea8-49fd-bade-db7a7cb87ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_X = xr.DataArray(\n",
    "    data=X,\n",
    "    dims=[\"time_allData\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data as matrix X; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_y = xr.DataArray(\n",
    "    data=y,\n",
    "    dims=[\"time_allData\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function y; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_X_full = xr.DataArray(\n",
    "    data=X_full,\n",
    "    dims=[\"time\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time          = ds['time'],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data without accounting for conv filter as matrix X_full; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_oneHotCost = xr.DataArray(\n",
    "    data=oneHotCost,\n",
    "    dims=[\"time_allData\", \"NN_output_dim\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        NN_output_dim = np.array([0,1]),\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function as one-hot vector.\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(X          = da_X,\n",
    "               y          = da_y,\n",
    "               X_full     = da_X_full,\n",
    "               oneHotCost = da_oneHotCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb797f76-7914-4d02-9ab5-f47c788dde7b",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b017273-5b15-4d62-a7ae-16381c5513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP(model, model_wo_softmax, X, y_true, lagSteps, lrp_methods, \n",
    "        suffix, normalizeDict, **kwargs):\n",
    "\n",
    "    result = {}\n",
    "    cost_NN = model.predict(X)\n",
    "    result[f'score_{suffix}'] = model.evaluate(X, y_true, verbose=0)\n",
    "\n",
    "    pred_NN = cost_NN.copy()\n",
    "    pred_NN[:,0] = pred_NN[:,0] > 0.5\n",
    "    pred_NN[:,1] = pred_NN[:,1] > 0.5\n",
    "\n",
    "    idx_NN_pos = []\n",
    "    idx_NN_neg = []\n",
    "\n",
    "    if lagSteps >= 0:\n",
    "        for i in range(len(y_true[lagSteps:,0])):\n",
    "            if y_true[lagSteps+i,0] == 1 and pred_NN[i,0] == 1:\n",
    "                idx_NN_pos.append(i)\n",
    "            if y_true[lagSteps+i,1] == 1 and pred_NN[i,1] == 1:\n",
    "                idx_NN_neg.append(i)\n",
    "    else:\n",
    "        for i in range(len(y_true[:lagSteps,0])):\n",
    "            if y_true[i,0] == 1 and pred_NN[i-lagSteps,0] == 1:\n",
    "                idx_NN_pos.append(i-lagSteps)\n",
    "            if y_true[i,1] == 1 and pred_NN[i-lagSteps,1] == 1:\n",
    "                idx_NN_neg.append(i-lagSteps)\n",
    "\n",
    "    result[f'idx_NN_pos'] = idx_NN_pos\n",
    "    result[f'idx_NN_neg'] = idx_NN_neg\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_pos), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_pos]\n",
    "    result[f'samples_correct_pos_{suffix}'] = rel\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_neg), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_neg]\n",
    "    result[f'samples_correct_neg_{suffix}'] = rel\n",
    "    \n",
    "    for method in lrp_methods:\n",
    "\n",
    "        title = method['title']\n",
    "        \n",
    "        print(f'Analyze using {title} for {suffix} data')\n",
    "        \n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_pos[:10]], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_pos_{suffix}'] = rel\n",
    "\n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_neg[:10]], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_neg_{suffix}'] = rel\n",
    "        \n",
    "    return result\n",
    "\n",
    "def quickSetup(X, y,\n",
    "               test_split_frac,\n",
    "               val_split_frac,\n",
    "               lrp_methods,\n",
    "               lagSteps,\n",
    "               decay_func = None,\n",
    "               **NNkwargs):\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    idx = int(X.shape[0]*(1-test_split_frac))\n",
    "    X_train = X[:idx]\n",
    "    oneHotCost_train = oneHotCost[:idx]\n",
    "    X_test = X[idx:]\n",
    "    oneHotCost_test = oneHotCost[idx:]\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    NNkwargs['optimizer'] = sgd\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    if lagSteps > 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[:-lagSteps], oneHotCost_train[lagSteps:], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    elif lagSteps == 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train, oneHotCost_train, \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    else:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[-lagSteps:], oneHotCost_train[:lagSteps], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "\n",
    "    K = TrainFullyConnectedNN(x_t, oneHotCost_t, validation_data = (x_v, oneHotCost_v), **NNkwargs)\n",
    "                   \n",
    "    best_model = K.quickTrain(decay_func)\n",
    "    best_model_wo_softmax = innvestigate.model_wo_softmax(best_model)\n",
    "                   \n",
    "    result['QoI_predict'] = best_model.predict(X)\n",
    "    result['QoI_predict_train'] = best_model.predict(X_train)\n",
    "    result['QoI_predict_test'] = best_model.predict(X_test)\n",
    "                   \n",
    "    normalizeDict = {'bool_': True, 'kind': 'MaxAbs'}\n",
    "    kwargs = {'y_ref': 0.00}\n",
    "\n",
    "    # LRP for all data\n",
    "    result_train = LRP(best_model, best_model_wo_softmax, \n",
    "                       X, oneHotCost, lagSteps, lrp_methods, \n",
    "                       suffix = 'all', normalizeDict = normalizeDict, **kwargs)\n",
    "    result.update(result_train)\n",
    "    \n",
    "    # # LRP for training data\n",
    "    # result_train = LRP(best_model, best_model_wo_softmax, \n",
    "    #                    X_train, oneHotCost_train, lagSteps, lrp_methods, \n",
    "    #                    suffix = 'train', normalizeDict = normalizeDict, **kwargs)\n",
    "    # result.update(result_train)\n",
    "\n",
    "    # # LRP for test data\n",
    "    # result_test = LRP(best_model, best_model_wo_softmax, \n",
    "    #                    X_test, oneHotCost_test, lagSteps, lrp_methods, \n",
    "    #                    suffix = 'test', normalizeDict = normalizeDict, **kwargs)\n",
    "    # result.update(result_test)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2171702a-a06c-4148-b84d-680a368626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = [{'size': X.shape[1], 'activation': None     , 'use_bias': None},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 10.0, 'l2_b_reg': 10.0},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01},\n",
    "          {'size': 2         , 'activation': 'softmax', 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01,\n",
    "           'bias_constraint': NonNeg()}]\n",
    "\n",
    "Losses = [{'kind': 'categorical_crossentropy', 'weight': 1.0}]\n",
    "\n",
    "LRPDict_theta = {}\n",
    "lagStepsList = [-60,-30,0]#,30,60,90,120,150,180]\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.70710678118 # 0.5^0.5\n",
    "    epochs_drop = 50\n",
    "    lrate = initial_lrate * drop**np.floor((1+epoch)/epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "methods = [# dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0', optParams = {}),\n",
    "           # dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0-W2', optParams = {'input_layer_rule':'WSquare'}),\n",
    "           dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0-B' , optParams = {'input_layer_rule':'Bounded'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64496c7-b56a-4305-9714-d50c3f6981dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.18692936088419, 1: 1.84251012145749}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_for_pos = len(oneHotCost[:-2161,0]) / np.sum(oneHotCost[:-2161,0])\n",
    "weight_for_neg = len(oneHotCost[:-2161,1]) / np.sum(oneHotCost[:-2161,1])\n",
    "class_weight = {0: weight_for_pos, 1: weight_for_neg}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6386a6-f5f7-4784-a556-83efa68ecfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag: -60 days, for Theta\n",
      "Train on 7233 samples, validate on 1809 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 21:27:01.057854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:01.059857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38221 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:01.061883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38221 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:01.080484: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-04-17 21:27:05.123275: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 3.09536, saving model to /home1/07665/shrey911/XAIRT/examples_TomsQoI/model-60_noL1.h5\n",
      "7233/7233 - 6s - loss: 9.6708 - metricF1: 0.7140 - accuracy: 0.7135 - categorical_crossentropy: 0.8069 - val_loss: 3.0954 - val_metricF1: 0.6504 - val_accuracy: 0.6506 - val_categorical_crossentropy: 1.0621 - lr: 0.0100 - 6s/epoch - 767us/sample\n",
      "Analyze using LRP-A1B0-B for all data\n",
      "Lag: -30 days, for Theta\n",
      "Train on 7257 samples, validate on 1815 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 21:27:13.755403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:13.757401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38221 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:13.759412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38221 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.93339, saving model to /home1/07665/shrey911/XAIRT/examples_TomsQoI/model-30_noL1.h5\n",
      "7257/7257 - 1s - loss: 9.9504 - metricF1: 0.7051 - accuracy: 0.7054 - categorical_crossentropy: 0.9220 - val_loss: 0.9334 - val_metricF1: 0.7482 - val_accuracy: 0.7488 - val_categorical_crossentropy: 0.6112 - lr: 0.0100 - 824ms/epoch - 114us/sample\n",
      "Analyze using LRP-A1B0-B for all data\n",
      "Lag: 0 days, for Theta\n",
      "Train on 7281 samples, validate on 1821 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 21:27:21.576081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:21.578083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38221 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-04-17 21:27:21.580098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38221 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.20418, saving model to /home1/07665/shrey911/XAIRT/examples_TomsQoI/model0_noL1.h5\n",
      "7281/7281 - 1s - loss: 9.8030 - metricF1: 0.7308 - accuracy: 0.7309 - categorical_crossentropy: 0.8720 - val_loss: 1.2042 - val_metricF1: 0.7455 - val_accuracy: 0.7392 - val_categorical_crossentropy: 0.5077 - lr: 0.0100 - 802ms/epoch - 110us/sample\n",
      "Analyze using LRP-A1B0-B for all data\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lagStepsList)):\n",
    "\n",
    "    print(f'Lag: {lagStepsList[i]} days, for Theta')\n",
    "    \n",
    "    NNkwargs = {'losses': Losses,\n",
    "                'metrics': [metricF1, # Custom f1 since metrics.F1Score(name='f1') is not available before tf v2.13\n",
    "                            'accuracy',\n",
    "                            'categorical_crossentropy',\n",
    "                            # metrics.CategoricalCrossentropy(name='cross_entropy'),  # (model's loss-L2 reg loss)\n",
    "                            # metrics.MeanSquaredError(name='brier_score'),\n",
    "                            # metrics.TruePositives(name='tp'),\n",
    "                            # metrics.FalsePositives(name='fp'),\n",
    "                            # metrics.TrueNegatives(name='tn'),\n",
    "                            # metrics.FalseNegatives(name='fn'), \n",
    "                            # metrics.BinaryAccuracy(name='accuracy'),\n",
    "                            # metrics.Precision(name='precision'),\n",
    "                            # metrics.Recall(name='recall'),\n",
    "                            # metrics.AUC(name='auc'),\n",
    "                            # metrics.AUC(name='prc', curve='PR'), # precision-recall curve],\n",
    "                           ],\n",
    "                'batch_size': 128, 'epochs': 1, #'validation_split': 0.1,\n",
    "                'filename': f'model{lagStepsList[i]}_noL1', 'dirname': os.path.abspath(''),\n",
    "                'random_nn_seed': 42, 'class_weight': class_weight,\n",
    "                'custom_objects': {'metricF1': metricF1}, 'verbose': 2}\n",
    "    \n",
    "    LRPDict_theta[f'LRP{lagStepsList[i]}'] = quickSetup(X, y, test_split_frac = 2161.0/11263.0,\n",
    "                                                        val_split_frac = 0.2,\n",
    "                                                        lrp_methods = methods,\n",
    "                                                        lagSteps = lagStepsList[i],\n",
    "                                                        decay_func = step_decay,\n",
    "                                                        layers = Layers, **NNkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a372860-af14-44ec-803f-9456edea77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from innvestigate.analyzer.base import AnalyzerBase\n",
    "\n",
    "model = keras.models.load_model('model0_noL1.h5', custom_objects={'metricF1': metricF1})\n",
    "model_wo_softmax = innvestigate.model_wo_softmax(model)\n",
    "Analyze = innvestigate.create_analyzer('lrp.alpha_1_beta_0', model_wo_softmax, input_layer_rule='Bounded')\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_neg_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c763017-b45b-4374-ab7a-8ac0942fe10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36320633, -0.21332543],\n",
       "       [-0.43081477, -0.1625068 ],\n",
       "       [-0.50695264, -0.11905544],\n",
       "       ...,\n",
       "       [-0.13323672,  0.53053445],\n",
       "       [-0.12375152,  0.5035467 ],\n",
       "       [-0.12355828,  0.49237347]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.predict(LRPDict_theta['LRP0']['samples_correct_neg_all'][:,wetpoints[0],wetpoints[1],wetpoints[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d2a138a-26c3-42a6-9a0b-c40bb59b2568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0727483"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyze = innvestigate.create_analyzer('lrp.alpha_1_beta_0', model_wo_softmax, input_layer_rule='Bounded')\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_neg_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89aeb6ee-ab72-4403-a7ae-61f77a33cf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5121174"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyze = innvestigate.create_analyzer('lrp.z', model_wo_softmax)\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_neg_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9de240-8e36-4785-a645-16df5f3e6515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8836373 , -0.6698701 ],\n",
       "       [ 0.91328305, -0.6360027 ],\n",
       "       [ 0.91239804, -0.64911586],\n",
       "       ...,\n",
       "       [ 0.30899364,  0.11315773],\n",
       "       [ 0.28144538,  0.16191523],\n",
       "       [ 0.28097343,  0.23456167]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.predict(LRPDict_theta['LRP0']['samples_correct_pos_all'][:,wetpoints[0],wetpoints[1],wetpoints[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d868d99c-a90d-4bfc-bdf8-d8241fcb1749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.015388"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyze = innvestigate.create_analyzer('lrp.alpha_1_beta_0', model_wo_softmax, input_layer_rule='Bounded')\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_pos_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f53198-fbfb-4112-95d2-d825e8310022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2893748"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyze = innvestigate.create_analyzer('lrp.z', model_wo_softmax)\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_pos_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e894ac-9d4b-4b54-bae4-1239c18862b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.13878407, -0.07667916],\n",
       "        [-0.3684209 ,  0.03289204],\n",
       "        [-0.04403416, -0.19840208],\n",
       "        [ 0.17202522, -0.3319991 ],\n",
       "        [ 0.1678728 , -0.3053772 ],\n",
       "        [ 0.35115594,  0.72690094],\n",
       "        [ 0.6358902 ,  0.23027413],\n",
       "        [ 0.12554084,  0.1512276 ]], dtype=float32),\n",
       " array([-0.        ,  0.10137586], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.layers[-1].get_weights()[0], model_wo_softmax.layers[-1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1f8a1f3-d353-469b-b6e8-bfa600270449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.15695263, -0.1890395 , -0.38048807, -0.19095354, -0.29115474,\n",
       "         -0.01246644, -0.51051366,  0.23851246],\n",
       "        [ 0.16469038, -0.35451564,  0.24644446,  0.28855434,  0.85127294,\n",
       "          0.11622895, -0.6371932 ,  0.4977694 ],\n",
       "        [-0.0668036 ,  0.5177149 , -0.09532668, -0.29096928, -0.0727115 ,\n",
       "         -0.32019994, -0.2376931 , -0.4644658 ],\n",
       "        [-0.03370003,  1.0580934 , -0.33147657,  0.37784266,  0.05481944,\n",
       "          0.42534202,  0.0287399 ,  0.10487936],\n",
       "        [-0.09779887,  0.05178871,  0.3458608 , -0.43983147,  0.5959899 ,\n",
       "         -0.28019264, -0.03330053, -0.7696547 ],\n",
       "        [-0.28288943, -0.3029981 ,  0.20015028,  0.3683088 , -0.41913405,\n",
       "         -0.08374354,  0.79560924, -0.36694312],\n",
       "        [ 0.08402578, -0.07843378,  0.17449471, -0.58836085,  0.37387958,\n",
       "          0.1981437 , -0.3537311 , -0.14975516],\n",
       "        [ 0.32105803, -0.19068165, -0.39748433,  0.31145713,  0.12014196,\n",
       "         -0.41334882, -0.4318624 , -0.24736986]], dtype=float32),\n",
       " array([-0.15690665, -0.05464773, -0.35066965, -0.13055637, -0.29506597,\n",
       "         0.08830209, -0.5512205 ,  0.17379509], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.layers[-2].get_weights()[0], model_wo_softmax.layers[-2].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c0a9b5d-97e2-4ded-a12b-a7c587b4092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from innvestigate.analyzer.base import AnalyzerBase\n",
    "\n",
    "model = keras.models.load_model('model-30_noL1.h5', custom_objects={'metricF1': metricF1})\n",
    "model_wo_softmax = innvestigate.model_wo_softmax(model)\n",
    "Analyze = innvestigate.create_analyzer('lrp.alpha_1_beta_0', model_wo_softmax, input_layer_rule='Bounded')\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_neg_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84e7a997-faee-4045-8180-aebed0c4400d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07533138,  0.13981977],\n",
       "       [ 0.07316314,  0.13468629],\n",
       "       [ 0.07221177,  0.13720539],\n",
       "       ...,\n",
       "       [-0.08523329, -0.01531784],\n",
       "       [-0.08992133, -0.02229644],\n",
       "       [-0.09892866, -0.03854377]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.predict(LRPDict_theta['LRP0']['samples_correct_neg_all'][:,wetpoints[0],wetpoints[1],wetpoints[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7b26487-23da-4fc4-b1c0-77d864d280ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.10096561, -0.11449764],\n",
       "        [-0.07321362, -0.2623151 ],\n",
       "        [-0.2345912 , -0.00784506],\n",
       "        [-0.08942901, -0.07054525],\n",
       "        [-0.00458508, -0.13291907],\n",
       "        [ 0.42924574,  0.64881086],\n",
       "        [ 0.5474082 ,  0.3187558 ],\n",
       "        [ 0.1481839 ,  0.12858473]], dtype=float32),\n",
       " array([0.00531946, 0.09743632], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.layers[-1].get_weights()[0], model_wo_softmax.layers[-1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a1df162-c259-4f35-9a44-0604914c8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from innvestigate.analyzer.base import AnalyzerBase\n",
    "\n",
    "model = keras.models.load_model('model-60_noL1.h5', custom_objects={'metricF1': metricF1})\n",
    "model_wo_softmax = innvestigate.model_wo_softmax(model)\n",
    "Analyze = innvestigate.create_analyzer('lrp.alpha_1_beta_0', model_wo_softmax, input_layer_rule='Bounded')\n",
    "sample = LRPDict_theta['LRP0']['samples_correct_neg_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "a = Analyze.analyze(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f848e74-4d9a-4fdf-b812-943a544e5823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5828423 , -1.26056   ],\n",
       "       [ 0.5975521 , -1.3044808 ],\n",
       "       [ 0.6133962 , -1.3482869 ],\n",
       "       ...,\n",
       "       [-2.7397432 ,  0.10793108],\n",
       "       [-2.795439  ,  0.10581087],\n",
       "       [-2.9008088 ,  0.10179965]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.predict(LRPDict_theta['LRP0']['samples_correct_neg_all'][:,wetpoints[0],wetpoints[1],wetpoints[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9497f237-77d4-4c19-917d-17235e68efd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.4818206e-04, -2.1491505e-01],\n",
       "        [-3.2322422e-01, -1.2304485e-02],\n",
       "        [-1.3072477e-01, -1.1171138e-01],\n",
       "        [ 6.9510862e-02, -2.2948444e-01],\n",
       "        [ 1.0696639e-01, -2.4447083e-01],\n",
       "        [ 4.1766283e-01,  6.6039354e-01],\n",
       "        [ 5.7215530e-01,  2.9400894e-01],\n",
       "        [ 7.0390165e-02,  2.0637855e-01]], dtype=float32),\n",
       " array([0.00093038, 0.21226293], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.layers[-1].get_weights()[0], model_wo_softmax.layers[-1].get_weights()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
