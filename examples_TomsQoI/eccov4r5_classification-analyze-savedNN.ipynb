{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5950c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:25:59.260855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:03.491125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:03.493767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:03.496945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 20:26:03.502399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:03.504831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:03.507240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:08.097430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:08.100037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:08.102098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:26:08.104208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    }
   ],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import metrics\n",
    "import innvestigate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "import xmitgcm\n",
    "from xmitgcm import open_mdsdataset\n",
    "import ecco_v4_py as ecco\n",
    "\n",
    "import random\n",
    "\n",
    "# See if GPUs are available\n",
    "from keras import backend as K\n",
    "if bool(K._get_available_gpus()):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *\n",
    "\n",
    "### https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed ###\n",
    "### https://keras.io/examples/keras_recipes/reproducibility_recipes/ ###\n",
    "SEED = 1997\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5b1ee2-570a-46e3-8b64-42843a17be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sverdrup \n",
    "# mainDir_r4  = '/scratch2/pillarh/eccov4r4'\n",
    "# mainDir_r5  = '/scratch2/pillarh/eccov4r5'\n",
    "# gridDir  = mainDir_r4 + '/GRID'\n",
    "# thetaDir = mainDir_r5 + '/V4r5/diags_daily/SST_day_mean'\n",
    "# thetaDir_ext = mainDir_r5 + '/V4r5_ext_2020_2023_Jun/diags_daily/SST_day_mean'\n",
    "\n",
    "## LS6\n",
    "mainDir_r4 = '/work/07665/shrey911/ls6/LRP_eccov4r4_data'\n",
    "mainDir_r5 = '/work/07665/shrey911/ls6/LRP_eccov4r5_data'\n",
    "gridDir  = mainDir_r5 + '/GRID'\n",
    "thetaDir = mainDir_r5 + '/SST_day_mean'\n",
    "thetaDir_ext = mainDir_r5 + '/SST_day_mean_ext_2020_2023_Jun'\n",
    "\n",
    "# For Sverdrup\n",
    "# ds_r4 = xr.open_dataset(f'/scratch2/shreyas/LRP_eccov4r4_data/thetaSurfECCOv4r4.nc')\n",
    "# For LS6\n",
    "ds_r4 = xr.open_dataset(mainDir_r4 + '/thetaSurfECCOv4r4.nc')\n",
    "\n",
    "# SSH has to be kept because someone used the SSH metadata for SST, \n",
    "# It's not a bug in this code but a hack to handle an existing bug.\n",
    "temp = xmitgcm.open_mdsdataset(data_dir = thetaDir,\n",
    "                             grid_dir = gridDir,\n",
    "                             extra_variables = dict(SSH = dict(dims=['k','j','i'],\n",
    "                                                               attrs = dict(standard_name=\"SST\",\n",
    "                                                                            long_name=\"Sea Surface Temperature\",\n",
    "                                                                            units=\"degC\"))))\n",
    "temp[\"SST\"] = temp[\"SSH\"]\n",
    "temp = temp.drop([\"SSH\"])\n",
    "    \n",
    "temp_ext = xmitgcm.open_mdsdataset(data_dir = thetaDir_ext,\n",
    "                                 grid_dir = gridDir,\n",
    "                                 extra_variables = dict(SST = dict(dims=['k','j','i'],\n",
    "                                                                   attrs = dict(standard_name=\"SST\",\n",
    "                                                                                long_name=\"Sea Surface Temperature\",\n",
    "                                                                                units=\"degC\"))))\n",
    "\n",
    "ds_r5 = xr.concat([temp, temp_ext], \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0039de40-13c0-42bc-a503-11942de7e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SST = xr.open_dataset(mainDir_r5+'/SST_all.nc')\n",
    "SST = ds_SST['SST'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccb3e1b-7581-4089-ad25-99bfbbce6b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_binary_array: loading file /work/07665/shrey911/ls6/LRP_eccov4r5_data/GRID/hFacC.data\n",
      "load_binary_array: data array shape  (1170, 90)\n",
      "load_binary_array: data array type  >f4\n",
      "llc_compact_to_faces: dims, llc  (1170, 90) 90\n",
      "llc_compact_to_faces: data_compact array type  >f4\n",
      "llc_faces_to_tiles: data_tiles shape  (13, 90, 90)\n",
      "llc_faces_to_tiles: data_tiles dtype  >f4\n"
     ]
    }
   ],
   "source": [
    "hFacC = ecco.read_llc_to_tiles(gridDir, 'hFacC.data')\n",
    "hFacC_mask = hFacC > 0\n",
    "hFacC_mask = hFacC_mask.astype(float)\n",
    "\n",
    "XC = ds_r4['XC'].data\n",
    "YC = ds_r4['YC'].data\n",
    "\n",
    "latMask = YC > -20.0\n",
    "latMask = latMask.astype(float)\n",
    "\n",
    "maskFinal = hFacC_mask * latMask\n",
    "NaNmaskFinal = np.copy(maskFinal)\n",
    "NaNmaskFinal[NaNmaskFinal == 0] = np.nan\n",
    "\n",
    "da_XC = xr.DataArray(\n",
    "    data=ds_r4['XC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"XC\"),\n",
    ")\n",
    "\n",
    "da_YC = xr.DataArray(\n",
    "    data=ds_r4['YC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"YC\"),\n",
    ")\n",
    "\n",
    "da_hFacC_mask = xr.DataArray(\n",
    "    data=hFacC_mask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"hFacC mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_latMask = xr.DataArray(\n",
    "    data=latMask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Latitude Mask 1 if > -20, else 0\"),\n",
    ")\n",
    "\n",
    "da_maskFinal = xr.DataArray(\n",
    "    data=maskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_NaNmaskFinal = xr.DataArray(\n",
    "    data=NaNmaskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D True if > 0, else NaN\"),\n",
    ")\n",
    "\n",
    "wetpoints = np.nonzero(maskFinal.data)\n",
    "da_wetpoints = xr.DataArray(\n",
    "    data=np.asarray(wetpoints),\n",
    "    dims=[\"wetpoints_dim\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        wetpoints_dim = np.arange(np.asarray(wetpoints).shape[0], dtype = int),\n",
    "        num_wetpoints = np.arange(np.asarray(wetpoints).shape[1], dtype = int),\n",
    "    ),\n",
    "    attrs=dict(description=\"indices of wetpoints in the order (tile, j, i) in the three rows\"),\n",
    ")\n",
    "\n",
    "da_SST = xr.DataArray(\n",
    "    data=SST,\n",
    "    dims=[\"time\", \"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        time = ds_r5['time'].data[:SST.shape[0]],\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"SST field in llc format\"),\n",
    ")\n",
    "\n",
    "ds = xr.Dataset()\n",
    "ds = ds.assign(XC           = da_XC,\n",
    "               YC           = da_YC,\n",
    "               hFacC_mask   = da_hFacC_mask,\n",
    "               latMask      = da_latMask,\n",
    "               maskFinal    = da_maskFinal,\n",
    "               NaNmaskFinal = da_NaNmaskFinal,\n",
    "               wetpoints    = da_wetpoints,\n",
    "               SST          = da_SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63f1c11-9c00-48fb-bf00-0afd6b5678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize_new(field, num_years = 31, first_leap_year_idx = 0):\n",
    "    \n",
    "    leap_yr_offsets_jan_feb   = np.array([0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8])\n",
    "    leap_yr_offsets_after_feb = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8])\n",
    "\n",
    "    if len(field.shape) > 1:\n",
    "        seasonal_trend = np.zeros((366, field.shape[1]))\n",
    "    else:\n",
    "        seasonal_trend = np.zeros((366,))\n",
    "\n",
    "    #### Calculate seasonal trend\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        same_cal_days_idx=[d+365*year+leap_yr_offsets_jan_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indexing\n",
    "    same_cal_days_idx=[365*year+59+int(year/4) for year in range(first_leap_year_idx,num_years,4)]\n",
    "    # Remove mean\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='constant', \n",
    "                                                    overwrite_data=False)\n",
    "    # Remove linear trend\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='linear', \n",
    "                                                    overwrite_data=False)\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        same_cal_days_idx=[d-1+365*year+leap_yr_offsets_after_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "428e4223-fad9-483a-a905-629ce467174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds['SST'].data[:,wetpoints[0],wetpoints[1],wetpoints[2]].copy()\n",
    "X = anomalize_new(X)\n",
    "X_full = X.copy()\n",
    "X = X[30:-30]\n",
    "\n",
    "y = ds['SST'].isel(tile = 10, j = 1, i = 43).data.copy()\n",
    "y = anomalize_new(y)\n",
    "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "y = np.convolve(y, np.ones(61)/61, mode='valid')\n",
    "oneHotCost = np.zeros((y.shape[0], 2), dtype = int)\n",
    "oneHotCost[:,0] = y >= 0.0\n",
    "oneHotCost[:,1] = y <  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d23bff3-dea8-49fd-bade-db7a7cb87ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_X = xr.DataArray(\n",
    "    data=X,\n",
    "    dims=[\"time_allData\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data as matrix X; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_y = xr.DataArray(\n",
    "    data=y,\n",
    "    dims=[\"time_allData\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function y; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_X_full = xr.DataArray(\n",
    "    data=X_full,\n",
    "    dims=[\"time\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time          = ds['time'],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data without accounting for conv filter as matrix X_full; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_oneHotCost = xr.DataArray(\n",
    "    data=oneHotCost,\n",
    "    dims=[\"time_allData\", \"NN_output_dim\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        NN_output_dim = np.array([0,1]),\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function as one-hot vector.\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(X          = da_X,\n",
    "               y          = da_y,\n",
    "               X_full     = da_X_full,\n",
    "               oneHotCost = da_oneHotCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c15b0-3436-46fa-aa1c-a7894bb0b6c0",
   "metadata": {},
   "source": [
    "## Optimal Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b017273-5b15-4d62-a7ae-16381c5513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickSetup(X, y,\n",
    "               test_split_frac,\n",
    "               val_split_frac,\n",
    "               lagSteps):\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    idx = int(X.shape[0]*(1-test_split_frac))\n",
    "    X_train = X[:idx]\n",
    "    oneHotCost_train = oneHotCost[:idx]\n",
    "    X_test = X[idx:]\n",
    "    oneHotCost_test = oneHotCost[idx:]\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    if lagSteps > 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[:-lagSteps], oneHotCost_train[lagSteps:], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    elif lagSteps == 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train, oneHotCost_train, \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    else:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[-lagSteps:], oneHotCost_train[:lagSteps], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "\n",
    "    best_model = keras.models.load_model(f'LRP_output_forHelen/saved_models/model{lagSteps}_noL1.h5', \n",
    "                                         custom_objects={'metricF1': metricF1})\n",
    "\n",
    "    result['QoI_true_full'] = np.argmax(oneHotCost, axis=1)\n",
    "                   \n",
    "    if lagSteps > 0:\n",
    "        QoI_predict = np.argmax(best_model.predict(X[:-lagSteps]), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost[lagSteps:], axis=1)\n",
    "    elif lagSteps == 0:\n",
    "        QoI_predict = np.argmax(best_model.predict(X), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost, axis=1)\n",
    "    else:\n",
    "        QoI_predict = np.argmax(best_model.predict(X[-lagSteps:]), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost[:lagSteps], axis=1)\n",
    "\n",
    "    result['QoI_true_trunc'] = QoI_true.copy()\n",
    "    result['QoI_predict_trunc'] = QoI_predict.copy()\n",
    "                   \n",
    "    result['QoI_success_predict_flag'] = QoI_true == QoI_predict\n",
    "    result['Accuracy'] = accuracy_score(QoI_true, QoI_predict)\n",
    "    result['F1_Score'] = f1_score(QoI_true, QoI_predict, average='binary')\n",
    "\n",
    "    if lagSteps > 0:\n",
    "        QoI_predict = np.argmax(best_model.predict(X_train[:-lagSteps]), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost_train[lagSteps:], axis=1)\n",
    "    elif lagSteps == 0:\n",
    "        QoI_predict = np.argmax(best_model.predict(X_train), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost_train, axis=1)\n",
    "    else:\n",
    "        QoI_predict = np.argmax(best_model.predict(X_train[-lagSteps:]), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost_train[:lagSteps], axis=1)\n",
    "\n",
    "    result['QoI_true_train_trunc'] = QoI_true.copy()\n",
    "    result['QoI_predict_train_trunc'] = QoI_predict.copy()\n",
    "                   \n",
    "    result['QoI_success_predict_flag_train'] = QoI_true == QoI_predict\n",
    "    result['Accuracy_train'] = accuracy_score(QoI_true, QoI_predict)\n",
    "    result['F1_Score_train'] = f1_score(QoI_true, QoI_predict, average='binary')\n",
    "\n",
    "    if lagSteps > 0:\n",
    "        QoI_predict = np.argmax(best_model.predict(X_test[:-lagSteps]), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost_test[lagSteps:], axis=1)\n",
    "    elif lagSteps == 0:\n",
    "        QoI_predict = np.argmax(best_model.predict(X_test), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost_test, axis=1)\n",
    "    else:\n",
    "        QoI_predict = np.argmax(best_model.predict(X_test[-lagSteps:]), axis=1)\n",
    "        QoI_true = np.argmax(oneHotCost_test[:lagSteps], axis=1)\n",
    "\n",
    "    result['QoI_true_test_trunc'] = QoI_true.copy()\n",
    "    result['QoI_predict_test_trunc'] = QoI_predict.copy()\n",
    "                   \n",
    "    result['QoI_success_predict_flag_test'] = QoI_true == QoI_predict\n",
    "    result['Accuracy_test'] = accuracy_score(QoI_true, QoI_predict)\n",
    "    result['F1_Score_test'] = f1_score(QoI_true, QoI_predict, average='binary')\n",
    "\n",
    "    print(f\"Lag {lagSteps}, Accuracy {result['Accuracy']*100:2.2f}, F1 Score {result['F1_Score']:2.2f}, Accuracy Train {result['Accuracy_train']*100:2.2f}, F1 Score Train {result['F1_Score_train']:2.2f}, Accuracy Test {result['Accuracy_test']*100:2.2f}, F1 Score Test {result['F1_Score_test']:2.2f}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982bb350-90e2-418c-965a-d84033051192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag: -60 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:30:55.642210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:30:55.644549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:30:55.646629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:30:55.648751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:30:55.650848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:30:55.652855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n",
      "2024-12-09 20:30:55.660269: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-12-09 20:31:11.919378: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag -60, Accuracy 91.87, F1 Score 0.93, Accuracy Train 99.47, F1 Score Train 1.00, Accuracy Test 61.78, F1 Score Test 0.69\n",
      "Lag: -30 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:13.360757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:13.363227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:13.365350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:13.367475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:13.369551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:13.371552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag -30, Accuracy 91.34, F1 Score 0.92, Accuracy Train 99.50, F1 Score Train 1.00, Accuracy Test 56.45, F1 Score Test 0.53\n",
      "Lag: 0 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:14.903486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:14.905859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:14.907945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:14.910108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:14.912229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:14.914254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 0, Accuracy 93.25, F1 Score 0.94, Accuracy Train 99.37, F1 Score Train 0.99, Accuracy Test 67.47, F1 Score Test 0.71\n",
      "Lag: 30 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:16.511434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:16.513952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:16.516075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:16.518193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:16.520258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:16.522256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 30, Accuracy 93.39, F1 Score 0.94, Accuracy Train 99.63, F1 Score Train 1.00, Accuracy Test 66.78, F1 Score Test 0.72\n",
      "Lag: 60 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:18.152752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:18.155142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:18.157223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:18.159350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:18.161416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:18.163409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 60, Accuracy 92.91, F1 Score 0.94, Accuracy Train 99.42, F1 Score Train 0.99, Accuracy Test 67.11, F1 Score Test 0.72\n",
      "Lag: 90 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:19.759915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:19.762297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:19.764375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:19.766504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:19.768581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:19.770571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 90, Accuracy 91.18, F1 Score 0.92, Accuracy Train 99.48, F1 Score Train 1.00, Accuracy Test 58.04, F1 Score Test 0.66\n",
      "Lag: 120 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:21.281175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:21.283521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:21.285616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:21.287740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:21.289808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:21.291802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 120, Accuracy 92.25, F1 Score 0.93, Accuracy Train 99.39, F1 Score Train 0.99, Accuracy Test 62.22, F1 Score Test 0.64\n",
      "Lag: 150 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:22.708042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:22.710388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:22.712464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:22.714599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:22.716672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:22.718679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 150, Accuracy 94.52, F1 Score 0.95, Accuracy Train 99.46, F1 Score Train 1.00, Accuracy Test 72.10, F1 Score Test 0.74\n",
      "Lag: 180 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 20:31:24.146511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:24.148842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:24.150918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:24.153073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:24.155148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-09 20:31:24.157142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 180, Accuracy 93.77, F1 Score 0.94, Accuracy Train 99.38, F1 Score Train 0.99, Accuracy Test 67.90, F1 Score Test 0.72\n"
     ]
    }
   ],
   "source": [
    "lagStepsList = [-60,-30,0,30,60,90,120,150,180]\n",
    "result_dict = {}\n",
    "for i in range(len(lagStepsList)):\n",
    "\n",
    "    print(f'Lag: {lagStepsList[i]} days, for Theta')\n",
    "    # 365*6-30+1 = 2161\n",
    "    result_dict[f'lag{lagStepsList[i]}'] = quickSetup(X, y, 2161.0/11263.0,\n",
    "                                             0.2,\n",
    "                                             lagStepsList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51ef4b7-ae19-405f-a80f-68d090f50444",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset()\n",
    "ds_1 = xr.Dataset()\n",
    "\n",
    "da_QoI_true_full_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_true_full'].copy(), dims=(\"minus60_full\"))\n",
    "da_QoI_true_trunc_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_true_trunc'].copy(), dims=(\"minus60_trunc\"))\n",
    "da_QoI_predict_trunc_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_predict_trunc'].copy(), dims=(\"minus60_trunc\"))\n",
    "da_QoI_true_train_trunc_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_true_train_trunc'].copy(), dims=(\"minus60_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_predict_train_trunc'].copy(), dims=(\"minus60_train_trunc\"))\n",
    "da_QoI_true_test_trunc_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_true_test_trunc'].copy(), dims=(\"minus60_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_minus60 = xr.DataArray(result_dict[f'lag-60']['QoI_predict_test_trunc'].copy(), dims=(\"minus60_test_trunc\"))\n",
    "da_accuracy_minus60 = xr.DataArray(result_dict[f'lag-60']['Accuracy'].copy())\n",
    "da_f1score_minus60 = xr.DataArray(result_dict[f'lag-60']['F1_Score'].copy())\n",
    "da_accuracy_train_minus60 = xr.DataArray(result_dict[f'lag-60']['Accuracy_train'].copy())\n",
    "da_f1score_train_minus60 = xr.DataArray(result_dict[f'lag-60']['F1_Score_train'].copy())\n",
    "da_accuracy_test_minus60 = xr.DataArray(result_dict[f'lag-60']['Accuracy_test'].copy())\n",
    "da_f1score_test_minus60 = xr.DataArray(result_dict[f'lag-60']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_minus60 = da_QoI_true_full_minus60,\n",
    "               QoI_true_trunc_minus60 = da_QoI_true_trunc_minus60,\n",
    "               QoI_predict_trunc_minus60 = da_QoI_predict_trunc_minus60,\n",
    "               QoI_true_train_trunc_minus60 = da_QoI_true_train_trunc_minus60,\n",
    "               QoI_predict_train_trunc_minus60 = da_QoI_predict_train_trunc_minus60,\n",
    "               QoI_true_test_trunc_minus60 = da_QoI_true_test_trunc_minus60,\n",
    "               QoI_predict_test_trunc_minus60 = da_QoI_predict_test_trunc_minus60)\n",
    "ds_1 = ds_1.assign(accuracy_minus60 = da_accuracy_minus60,\n",
    "                   f1score_minus60 = da_f1score_minus60,\n",
    "                   accuracy_train_minus60 = da_accuracy_train_minus60,\n",
    "                   f1score_train_minus60 = da_f1score_train_minus60,\n",
    "                   accuracy_test_minus60 = da_accuracy_test_minus60,\n",
    "                   f1score_test_minus60 = da_f1score_test_minus60)\n",
    "\n",
    "da_QoI_true_full_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_true_full'].copy(), dims=(\"minus30_full\"))\n",
    "da_QoI_true_trunc_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_true_trunc'].copy(), dims=(\"minus30_trunc\"))\n",
    "da_QoI_predict_trunc_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_predict_trunc'].copy(), dims=(\"minus30_trunc\"))\n",
    "da_QoI_true_train_trunc_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_true_train_trunc'].copy(), dims=(\"minus30_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_predict_train_trunc'].copy(), dims=(\"minus30_train_trunc\"))\n",
    "da_QoI_true_test_trunc_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_true_test_trunc'].copy(), dims=(\"minus30_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_minus30 = xr.DataArray(result_dict[f'lag-30']['QoI_predict_test_trunc'].copy(), dims=(\"minus30_test_trunc\"))\n",
    "da_accuracy_minus30 = xr.DataArray(result_dict[f'lag-30']['Accuracy'].copy())\n",
    "da_f1score_minus30 = xr.DataArray(result_dict[f'lag-30']['F1_Score'].copy())\n",
    "da_accuracy_train_minus30 = xr.DataArray(result_dict[f'lag-30']['Accuracy_train'].copy())\n",
    "da_f1score_train_minus30 = xr.DataArray(result_dict[f'lag-30']['F1_Score_train'].copy())\n",
    "da_accuracy_test_minus30 = xr.DataArray(result_dict[f'lag-30']['Accuracy_test'].copy())\n",
    "da_f1score_test_minus30 = xr.DataArray(result_dict[f'lag-30']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_minus30 = da_QoI_true_full_minus30,\n",
    "               QoI_true_trunc_minus30 = da_QoI_true_trunc_minus30,\n",
    "               QoI_predict_trunc_minus30 = da_QoI_predict_trunc_minus30,\n",
    "               QoI_true_train_trunc_minus30 = da_QoI_true_train_trunc_minus30,\n",
    "               QoI_predict_train_trunc_minus30 = da_QoI_predict_train_trunc_minus30,\n",
    "               QoI_true_test_trunc_minus30 = da_QoI_true_test_trunc_minus30,\n",
    "               QoI_predict_test_trunc_minus30 = da_QoI_predict_test_trunc_minus30)\n",
    "ds_1 = ds_1.assign(accuracy_minus30 = da_accuracy_minus30,\n",
    "                   f1score_minus30 = da_f1score_minus30,\n",
    "                   accuracy_train_minus30 = da_accuracy_train_minus30,\n",
    "                   f1score_train_minus30 = da_f1score_train_minus30,\n",
    "                   accuracy_test_minus30 = da_accuracy_test_minus30,\n",
    "                   f1score_test_minus30 = da_f1score_test_minus30)\n",
    "\n",
    "da_QoI_true_full_0 = xr.DataArray(result_dict[f'lag0']['QoI_true_full'].copy(), dims=(\"0_full\"))\n",
    "da_QoI_true_trunc_0 = xr.DataArray(result_dict[f'lag0']['QoI_true_trunc'].copy(), dims=(\"0_trunc\"))\n",
    "da_QoI_predict_trunc_0 = xr.DataArray(result_dict[f'lag0']['QoI_predict_trunc'].copy(), dims=(\"0_trunc\"))\n",
    "da_QoI_true_train_trunc_0 = xr.DataArray(result_dict[f'lag0']['QoI_true_train_trunc'].copy(), dims=(\"0_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_0 = xr.DataArray(result_dict[f'lag0']['QoI_predict_train_trunc'].copy(), dims=(\"0_train_trunc\"))\n",
    "da_QoI_true_test_trunc_0 = xr.DataArray(result_dict[f'lag0']['QoI_true_test_trunc'].copy(), dims=(\"0_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_0 = xr.DataArray(result_dict[f'lag0']['QoI_predict_test_trunc'].copy(), dims=(\"0_test_trunc\"))\n",
    "da_accuracy_0 = xr.DataArray(result_dict[f'lag0']['Accuracy'].copy())\n",
    "da_f1score_0 = xr.DataArray(result_dict[f'lag0']['F1_Score'].copy())\n",
    "da_accuracy_train_0 = xr.DataArray(result_dict[f'lag0']['Accuracy_train'].copy())\n",
    "da_f1score_train_0 = xr.DataArray(result_dict[f'lag0']['F1_Score_train'].copy())\n",
    "da_accuracy_test_0 = xr.DataArray(result_dict[f'lag0']['Accuracy_test'].copy())\n",
    "da_f1score_test_0 = xr.DataArray(result_dict[f'lag0']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_0 = da_QoI_true_full_0,\n",
    "               QoI_true_trunc_0 = da_QoI_true_trunc_0,\n",
    "               QoI_predict_trunc_0 = da_QoI_predict_trunc_0,\n",
    "               QoI_true_train_trunc_0 = da_QoI_true_train_trunc_0,\n",
    "               QoI_predict_train_trunc_0 = da_QoI_predict_train_trunc_0,\n",
    "               QoI_true_test_trunc_0 = da_QoI_true_test_trunc_0,\n",
    "               QoI_predict_test_trunc_0 = da_QoI_predict_test_trunc_0)\n",
    "ds_1 = ds_1.assign(accuracy_0 = da_accuracy_0,\n",
    "                   f1score_0 = da_f1score_0,\n",
    "                   accuracy_train_0 = da_accuracy_train_0,\n",
    "                   f1score_train_0 = da_f1score_train_0,\n",
    "                   accuracy_test_0 = da_accuracy_test_0,\n",
    "                   f1score_test_0 = da_f1score_test_0)\n",
    "\n",
    "da_QoI_true_full_30 = xr.DataArray(result_dict[f'lag30']['QoI_true_full'].copy(), dims=(\"30_full\"))\n",
    "da_QoI_true_trunc_30 = xr.DataArray(result_dict[f'lag30']['QoI_true_trunc'].copy(), dims=(\"30_trunc\"))\n",
    "da_QoI_predict_trunc_30 = xr.DataArray(result_dict[f'lag30']['QoI_predict_trunc'].copy(), dims=(\"30_trunc\"))\n",
    "da_QoI_true_train_trunc_30 = xr.DataArray(result_dict[f'lag30']['QoI_true_train_trunc'].copy(), dims=(\"30_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_30 = xr.DataArray(result_dict[f'lag30']['QoI_predict_train_trunc'].copy(), dims=(\"30_train_trunc\"))\n",
    "da_QoI_true_test_trunc_30 = xr.DataArray(result_dict[f'lag30']['QoI_true_test_trunc'].copy(), dims=(\"30_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_30 = xr.DataArray(result_dict[f'lag30']['QoI_predict_test_trunc'].copy(), dims=(\"30_test_trunc\"))\n",
    "da_accuracy_30 = xr.DataArray(result_dict[f'lag30']['Accuracy'].copy())\n",
    "da_f1score_30 = xr.DataArray(result_dict[f'lag30']['F1_Score'].copy())\n",
    "da_accuracy_train_30 = xr.DataArray(result_dict[f'lag30']['Accuracy_train'].copy())\n",
    "da_f1score_train_30 = xr.DataArray(result_dict[f'lag30']['F1_Score_train'].copy())\n",
    "da_accuracy_test_30 = xr.DataArray(result_dict[f'lag30']['Accuracy_test'].copy())\n",
    "da_f1score_test_30 = xr.DataArray(result_dict[f'lag30']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_30 = da_QoI_true_full_30,\n",
    "               QoI_true_trunc_30 = da_QoI_true_trunc_30,\n",
    "               QoI_predict_trunc_30 = da_QoI_predict_trunc_30,\n",
    "               QoI_true_train_trunc_30 = da_QoI_true_train_trunc_30,\n",
    "               QoI_predict_train_trunc_30 = da_QoI_predict_train_trunc_30,\n",
    "               QoI_true_test_trunc_30 = da_QoI_true_test_trunc_30,\n",
    "               QoI_predict_test_trunc_30 = da_QoI_predict_test_trunc_30)\n",
    "ds_1 = ds_1.assign(accuracy_30 = da_accuracy_30,\n",
    "                   f1score_30 = da_f1score_30,\n",
    "                   accuracy_train_30 = da_accuracy_train_30,\n",
    "                   f1score_train_30 = da_f1score_train_30,\n",
    "                   accuracy_test_30 = da_accuracy_test_30,\n",
    "                   f1score_test_30 = da_f1score_test_30)\n",
    "\n",
    "da_QoI_true_full_60 = xr.DataArray(result_dict[f'lag60']['QoI_true_full'].copy(), dims=(\"60_full\"))\n",
    "da_QoI_true_trunc_60 = xr.DataArray(result_dict[f'lag60']['QoI_true_trunc'].copy(), dims=(\"60_trunc\"))\n",
    "da_QoI_predict_trunc_60 = xr.DataArray(result_dict[f'lag60']['QoI_predict_trunc'].copy(), dims=(\"60_trunc\"))\n",
    "da_QoI_true_train_trunc_60 = xr.DataArray(result_dict[f'lag60']['QoI_true_train_trunc'].copy(), dims=(\"60_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_60 = xr.DataArray(result_dict[f'lag60']['QoI_predict_train_trunc'].copy(), dims=(\"60_train_trunc\"))\n",
    "da_QoI_true_test_trunc_60 = xr.DataArray(result_dict[f'lag60']['QoI_true_test_trunc'].copy(), dims=(\"60_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_60 = xr.DataArray(result_dict[f'lag60']['QoI_predict_test_trunc'].copy(), dims=(\"60_test_trunc\"))\n",
    "da_accuracy_60 = xr.DataArray(result_dict[f'lag60']['Accuracy'].copy())\n",
    "da_f1score_60 = xr.DataArray(result_dict[f'lag60']['F1_Score'].copy())\n",
    "da_accuracy_train_60 = xr.DataArray(result_dict[f'lag60']['Accuracy_train'].copy())\n",
    "da_f1score_train_60 = xr.DataArray(result_dict[f'lag60']['F1_Score_train'].copy())\n",
    "da_accuracy_test_60 = xr.DataArray(result_dict[f'lag60']['Accuracy_test'].copy())\n",
    "da_f1score_test_60 = xr.DataArray(result_dict[f'lag60']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_60 = da_QoI_true_full_60,\n",
    "               QoI_true_trunc_60 = da_QoI_true_trunc_60,\n",
    "               QoI_predict_trunc_60 = da_QoI_predict_trunc_60,\n",
    "               QoI_true_train_trunc_60 = da_QoI_true_train_trunc_60,\n",
    "               QoI_predict_train_trunc_60 = da_QoI_predict_train_trunc_60,\n",
    "               QoI_true_test_trunc_60 = da_QoI_true_test_trunc_60,\n",
    "               QoI_predict_test_trunc_60 = da_QoI_predict_test_trunc_60)\n",
    "ds_1 = ds_1.assign(accuracy_60 = da_accuracy_60,\n",
    "                   f1score_60 = da_f1score_60,\n",
    "                   accuracy_train_60 = da_accuracy_train_60,\n",
    "                   f1score_train_60 = da_f1score_train_60,\n",
    "                   accuracy_test_60 = da_accuracy_test_60,\n",
    "                   f1score_test_60 = da_f1score_test_60)\n",
    "\n",
    "da_QoI_true_full_90 = xr.DataArray(result_dict[f'lag90']['QoI_true_full'].copy(), dims=(\"90_full\"))\n",
    "da_QoI_true_trunc_90 = xr.DataArray(result_dict[f'lag90']['QoI_true_trunc'].copy(), dims=(\"90_trunc\"))\n",
    "da_QoI_predict_trunc_90 = xr.DataArray(result_dict[f'lag90']['QoI_predict_trunc'].copy(), dims=(\"90_trunc\"))\n",
    "da_QoI_true_train_trunc_90 = xr.DataArray(result_dict[f'lag90']['QoI_true_train_trunc'].copy(), dims=(\"90_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_90 = xr.DataArray(result_dict[f'lag90']['QoI_predict_train_trunc'].copy(), dims=(\"90_train_trunc\"))\n",
    "da_QoI_true_test_trunc_90 = xr.DataArray(result_dict[f'lag90']['QoI_true_test_trunc'].copy(), dims=(\"90_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_90 = xr.DataArray(result_dict[f'lag90']['QoI_predict_test_trunc'].copy(), dims=(\"90_test_trunc\"))\n",
    "da_accuracy_90 = xr.DataArray(result_dict[f'lag90']['Accuracy'].copy())\n",
    "da_f1score_90 = xr.DataArray(result_dict[f'lag90']['F1_Score'].copy())\n",
    "da_accuracy_train_90 = xr.DataArray(result_dict[f'lag90']['Accuracy_train'].copy())\n",
    "da_f1score_train_90 = xr.DataArray(result_dict[f'lag90']['F1_Score_train'].copy())\n",
    "da_accuracy_test_90 = xr.DataArray(result_dict[f'lag90']['Accuracy_test'].copy())\n",
    "da_f1score_test_90 = xr.DataArray(result_dict[f'lag90']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_90 = da_QoI_true_full_90,\n",
    "               QoI_true_trunc_90 = da_QoI_true_trunc_90,\n",
    "               QoI_predict_trunc_90 = da_QoI_predict_trunc_90,\n",
    "               QoI_true_train_trunc_90 = da_QoI_true_train_trunc_90,\n",
    "               QoI_predict_train_trunc_90 = da_QoI_predict_train_trunc_90,\n",
    "               QoI_true_test_trunc_90 = da_QoI_true_test_trunc_90,\n",
    "               QoI_predict_test_trunc_90 = da_QoI_predict_test_trunc_90)\n",
    "ds_1 = ds_1.assign(accuracy_90 = da_accuracy_90,\n",
    "                   f1score_90 = da_f1score_90,\n",
    "                   accuracy_train_90 = da_accuracy_train_90,\n",
    "                   f1score_train_90 = da_f1score_train_90,\n",
    "                   accuracy_test_90 = da_accuracy_test_90,\n",
    "                   f1score_test_90 = da_f1score_test_90)\n",
    "\n",
    "da_QoI_true_full_120 = xr.DataArray(result_dict[f'lag120']['QoI_true_full'].copy(), dims=(\"120_full\"))\n",
    "da_QoI_true_trunc_120 = xr.DataArray(result_dict[f'lag120']['QoI_true_trunc'].copy(), dims=(\"120_trunc\"))\n",
    "da_QoI_predict_trunc_120 = xr.DataArray(result_dict[f'lag120']['QoI_predict_trunc'].copy(), dims=(\"120_trunc\"))\n",
    "da_QoI_true_train_trunc_120 = xr.DataArray(result_dict[f'lag120']['QoI_true_train_trunc'].copy(), dims=(\"120_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_120 = xr.DataArray(result_dict[f'lag120']['QoI_predict_train_trunc'].copy(), dims=(\"120_train_trunc\"))\n",
    "da_QoI_true_test_trunc_120 = xr.DataArray(result_dict[f'lag120']['QoI_true_test_trunc'].copy(), dims=(\"120_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_120 = xr.DataArray(result_dict[f'lag120']['QoI_predict_test_trunc'].copy(), dims=(\"120_test_trunc\"))\n",
    "da_accuracy_120 = xr.DataArray(result_dict[f'lag120']['Accuracy'].copy())\n",
    "da_f1score_120 = xr.DataArray(result_dict[f'lag120']['F1_Score'].copy())\n",
    "da_accuracy_train_120 = xr.DataArray(result_dict[f'lag120']['Accuracy_train'].copy())\n",
    "da_f1score_train_120 = xr.DataArray(result_dict[f'lag120']['F1_Score_train'].copy())\n",
    "da_accuracy_test_120 = xr.DataArray(result_dict[f'lag120']['Accuracy_test'].copy())\n",
    "da_f1score_test_120 = xr.DataArray(result_dict[f'lag120']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_120 = da_QoI_true_full_120,\n",
    "               QoI_true_trunc_120 = da_QoI_true_trunc_120,\n",
    "               QoI_predict_trunc_120 = da_QoI_predict_trunc_120,\n",
    "               QoI_true_train_trunc_120 = da_QoI_true_train_trunc_120,\n",
    "               QoI_predict_train_trunc_120 = da_QoI_predict_train_trunc_120,\n",
    "               QoI_true_test_trunc_120 = da_QoI_true_test_trunc_120,\n",
    "               QoI_predict_test_trunc_120 = da_QoI_predict_test_trunc_120)\n",
    "ds_1 = ds_1.assign(accuracy_120 = da_accuracy_120,\n",
    "                   f1score_120 = da_f1score_120,\n",
    "                   accuracy_train_120 = da_accuracy_train_120,\n",
    "                   f1score_train_120 = da_f1score_train_120,\n",
    "                   accuracy_test_120 = da_accuracy_test_120,\n",
    "                   f1score_test_120 = da_f1score_test_120)\n",
    "\n",
    "da_QoI_true_full_150 = xr.DataArray(result_dict[f'lag150']['QoI_true_full'].copy(), dims=(\"150_full\"))\n",
    "da_QoI_true_trunc_150 = xr.DataArray(result_dict[f'lag150']['QoI_true_trunc'].copy(), dims=(\"150_trunc\"))\n",
    "da_QoI_predict_trunc_150 = xr.DataArray(result_dict[f'lag150']['QoI_predict_trunc'].copy(), dims=(\"150_trunc\"))\n",
    "da_QoI_true_train_trunc_150 = xr.DataArray(result_dict[f'lag150']['QoI_true_train_trunc'].copy(), dims=(\"150_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_150 = xr.DataArray(result_dict[f'lag150']['QoI_predict_train_trunc'].copy(), dims=(\"150_train_trunc\"))\n",
    "da_QoI_true_test_trunc_150 = xr.DataArray(result_dict[f'lag150']['QoI_true_test_trunc'].copy(), dims=(\"150_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_150 = xr.DataArray(result_dict[f'lag150']['QoI_predict_test_trunc'].copy(), dims=(\"150_test_trunc\"))\n",
    "da_accuracy_150 = xr.DataArray(result_dict[f'lag150']['Accuracy'].copy())\n",
    "da_f1score_150 = xr.DataArray(result_dict[f'lag150']['F1_Score'].copy())\n",
    "da_accuracy_train_150 = xr.DataArray(result_dict[f'lag150']['Accuracy_train'].copy())\n",
    "da_f1score_train_150 = xr.DataArray(result_dict[f'lag150']['F1_Score_train'].copy())\n",
    "da_accuracy_test_150 = xr.DataArray(result_dict[f'lag150']['Accuracy_test'].copy())\n",
    "da_f1score_test_150 = xr.DataArray(result_dict[f'lag150']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_150 = da_QoI_true_full_150,\n",
    "               QoI_true_trunc_150 = da_QoI_true_trunc_150,\n",
    "               QoI_predict_trunc_150 = da_QoI_predict_trunc_150,\n",
    "               QoI_true_train_trunc_150 = da_QoI_true_train_trunc_150,\n",
    "               QoI_predict_train_trunc_150 = da_QoI_predict_train_trunc_150,\n",
    "               QoI_true_test_trunc_150 = da_QoI_true_test_trunc_150,\n",
    "               QoI_predict_test_trunc_150 = da_QoI_predict_test_trunc_150)\n",
    "ds_1 = ds_1.assign(accuracy_150 = da_accuracy_150,\n",
    "                   f1score_150 = da_f1score_150,\n",
    "                   accuracy_train_150 = da_accuracy_train_150,\n",
    "                   f1score_train_150 = da_f1score_train_150,\n",
    "                   accuracy_test_150 = da_accuracy_test_150,\n",
    "                   f1score_test_150 = da_f1score_test_150)\n",
    "\n",
    "da_QoI_true_full_180 = xr.DataArray(result_dict[f'lag180']['QoI_true_full'].copy(), dims=(\"180_full\"))\n",
    "da_QoI_true_trunc_180 = xr.DataArray(result_dict[f'lag180']['QoI_true_trunc'].copy(), dims=(\"180_trunc\"))\n",
    "da_QoI_predict_trunc_180 = xr.DataArray(result_dict[f'lag180']['QoI_predict_trunc'].copy(), dims=(\"180_trunc\"))\n",
    "da_QoI_true_train_trunc_180 = xr.DataArray(result_dict[f'lag180']['QoI_true_train_trunc'].copy(), dims=(\"180_train_trunc\"))\n",
    "da_QoI_predict_train_trunc_180 = xr.DataArray(result_dict[f'lag180']['QoI_predict_train_trunc'].copy(), dims=(\"180_train_trunc\"))\n",
    "da_QoI_true_test_trunc_180 = xr.DataArray(result_dict[f'lag180']['QoI_true_test_trunc'].copy(), dims=(\"180_test_trunc\"))\n",
    "da_QoI_predict_test_trunc_180 = xr.DataArray(result_dict[f'lag180']['QoI_predict_test_trunc'].copy(), dims=(\"180_test_trunc\"))\n",
    "da_accuracy_180 = xr.DataArray(result_dict[f'lag180']['Accuracy'].copy())\n",
    "da_f1score_180 = xr.DataArray(result_dict[f'lag180']['F1_Score'].copy())\n",
    "da_accuracy_train_180 = xr.DataArray(result_dict[f'lag180']['Accuracy_train'].copy())\n",
    "da_f1score_train_180 = xr.DataArray(result_dict[f'lag180']['F1_Score_train'].copy())\n",
    "da_accuracy_test_180 = xr.DataArray(result_dict[f'lag180']['Accuracy_test'].copy())\n",
    "da_f1score_test_180 = xr.DataArray(result_dict[f'lag180']['F1_Score_test'].copy())\n",
    "\n",
    "ds = ds.assign(QoI_true_full_180 = da_QoI_true_full_180,\n",
    "               QoI_true_trunc_180 = da_QoI_true_trunc_180,\n",
    "               QoI_predict_trunc_180 = da_QoI_predict_trunc_180,\n",
    "               QoI_true_train_trunc_180 = da_QoI_true_train_trunc_180,\n",
    "               QoI_predict_train_trunc_180 = da_QoI_predict_train_trunc_180,\n",
    "               QoI_true_test_trunc_180 = da_QoI_true_test_trunc_180,\n",
    "               QoI_predict_test_trunc_180 = da_QoI_predict_test_trunc_180)\n",
    "ds_1 = ds_1.assign(accuracy_180 = da_accuracy_180,\n",
    "                   f1score_180 = da_f1score_180,\n",
    "                   accuracy_train_180 = da_accuracy_train_180,\n",
    "                   f1score_train_180 = da_f1score_train_180,\n",
    "                   accuracy_test_180 = da_accuracy_test_180,\n",
    "                   f1score_test_180 = da_f1score_test_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0fd57e5-9ef0-49b2-a0b7-b2626837cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('LRP_output_forHelen/qoi_pred.nc')\n",
    "ds_1.to_netcdf('LRP_output_forHelen/accuracy_f1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cd2b4-3c84-4b5f-89ae-0ef8d58114c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
