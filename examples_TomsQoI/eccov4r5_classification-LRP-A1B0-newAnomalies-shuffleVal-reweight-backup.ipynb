{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5950c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 19:16:37.839982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 19:16:48.944060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38221 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-04-17 19:16:48.946648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38221 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-04-17 19:16:48.949167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38221 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import metrics\n",
    "import innvestigate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "import xmitgcm\n",
    "from xmitgcm import open_mdsdataset\n",
    "import ecco_v4_py as ecco\n",
    "\n",
    "import random\n",
    "\n",
    "# See if GPUs are available\n",
    "from keras import backend as K\n",
    "if bool(K._get_available_gpus()):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *\n",
    "\n",
    "### https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed ###\n",
    "### https://keras.io/examples/keras_recipes/reproducibility_recipes/ ###\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5b1ee2-570a-46e3-8b64-42843a17be60",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xmitgcm/mds_store.py:202\u001b[0m, in \u001b[0;36mopen_mdsdataset\u001b[0;34m(data_dir, grid_dir, iters, prefix, read_grid, delta_t, ref_date, calendar, levels, geometry, grid_vars_to_coords, swap_dims, endian, chunks, ignore_unknown_vars, default_dtype, nx, ny, nz, llc_method, extra_metadata, extra_variables)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     iternum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# if not we probably have some kind of list\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m ds_r4 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(mainDir_r4 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/thetaSurfECCOv4r4.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# SSH has to be kept because someone used the SSH metadata for SST, \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# It's not a bug in this code but a hack to handle an existing bug.\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mxmitgcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mdsdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthetaDir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mgrid_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgridDir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mextra_variables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSSH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                               \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstandard_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                                            \u001b[49m\u001b[43mlong_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSea Surface Temperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                                            \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdegC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSST\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m temp \u001b[38;5;241m=\u001b[39m temp\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSH\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xmitgcm/mds_store.py:239\u001b[0m, in \u001b[0;36mopen_mdsdataset\u001b[0;34m(data_dir, grid_dir, iters, prefix, read_grid, delta_t, ref_date, calendar, levels, geometry, grid_vars_to_coords, swap_dims, endian, chunks, ignore_unknown_vars, default_dtype, nx, ny, nz, llc_method, extra_metadata, extra_variables)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# recursively open each dataset at a time\u001b[39;00m\n\u001b[1;32m    228\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    229\u001b[0m     grid_dir\u001b[38;5;241m=\u001b[39mgrid_dir, delta_t\u001b[38;5;241m=\u001b[39mdelta_t, swap_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix, ref_date\u001b[38;5;241m=\u001b[39mref_date, calendar\u001b[38;5;241m=\u001b[39mcalendar,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels, extra_metadata\u001b[38;5;241m=\u001b[39mextra_metadata,\n\u001b[1;32m    238\u001b[0m     extra_variables\u001b[38;5;241m=\u001b[39mextra_variables)\n\u001b[0;32m--> 239\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [open_mdsdataset(\n\u001b[1;32m    240\u001b[0m         data_dir, iters\u001b[38;5;241m=\u001b[39miternum, read_grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iternum \u001b[38;5;129;01min\u001b[39;00m iters]\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# now add the grid\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_grid:\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xmitgcm/mds_store.py:239\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# recursively open each dataset at a time\u001b[39;00m\n\u001b[1;32m    228\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    229\u001b[0m     grid_dir\u001b[38;5;241m=\u001b[39mgrid_dir, delta_t\u001b[38;5;241m=\u001b[39mdelta_t, swap_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix, ref_date\u001b[38;5;241m=\u001b[39mref_date, calendar\u001b[38;5;241m=\u001b[39mcalendar,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels, extra_metadata\u001b[38;5;241m=\u001b[39mextra_metadata,\n\u001b[1;32m    238\u001b[0m     extra_variables\u001b[38;5;241m=\u001b[39mextra_variables)\n\u001b[0;32m--> 239\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_mdsdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miternum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iternum \u001b[38;5;129;01min\u001b[39;00m iters]\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# now add the grid\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_grid:\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xmitgcm/mds_store.py:282\u001b[0m, in \u001b[0;36mopen_mdsdataset\u001b[0;34m(data_dir, grid_dir, iters, prefix, read_grid, delta_t, ref_date, calendar, levels, geometry, grid_vars_to_coords, swap_dims, endian, chunks, ignore_unknown_vars, default_dtype, nx, ny, nz, llc_method, extra_metadata, extra_variables)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ds\n\u001b[1;32m    273\u001b[0m store \u001b[38;5;241m=\u001b[39m _MDSDataStore(data_dir, grid_dir, iternum, delta_t, read_grid,\n\u001b[1;32m    274\u001b[0m                       prefix, ref_date, calendar,\n\u001b[1;32m    275\u001b[0m                       geometry, endian,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m                       levels\u001b[38;5;241m=\u001b[39mlevels, extra_metadata\u001b[38;5;241m=\u001b[39mextra_metadata,\n\u001b[1;32m    280\u001b[0m                      extra_variables\u001b[38;5;241m=\u001b[39mextra_variables)\n\u001b[0;32m--> 282\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swap_dims:\n\u001b[1;32m    284\u001b[0m     ds \u001b[38;5;241m=\u001b[39m _swap_dimensions(ds, geometry)\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/dataset.py:672\u001b[0m, in \u001b[0;36mDataset.load_store\u001b[0;34m(cls, store, decoder)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder:\n\u001b[1;32m    671\u001b[0m     variables, attributes \u001b[38;5;241m=\u001b[39m decoder(variables, attributes)\n\u001b[0;32m--> 672\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m obj\u001b[38;5;241m.\u001b[39mset_close(store\u001b[38;5;241m.\u001b[39mclose)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/dataset.py:652\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    650\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39m_variables\n\u001b[0;32m--> 652\u001b[0m variables, coord_names, dims, indexes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_data_and_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbroadcast_equals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    654\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs) \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/merge.py:569\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data_vars, coords, compat, join)\u001b[0m\n\u001b[1;32m    567\u001b[0m objects \u001b[38;5;241m=\u001b[39m [data_vars, coords]\n\u001b[1;32m    568\u001b[0m explicit_coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplicit_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIndexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/merge.py:755\u001b[0m, in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value)\u001b[0m\n\u001b[1;32m    751\u001b[0m coerced \u001b[38;5;241m=\u001b[39m coerce_pandas_values(objects)\n\u001b[1;32m    752\u001b[0m aligned \u001b[38;5;241m=\u001b[39m deep_align(\n\u001b[1;32m    753\u001b[0m     coerced, join\u001b[38;5;241m=\u001b[39mjoin, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indexes\u001b[38;5;241m=\u001b[39mindexes, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    754\u001b[0m )\n\u001b[0;32m--> 755\u001b[0m collected \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_variables_and_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m prioritized \u001b[38;5;241m=\u001b[39m _get_priority_vars_and_indexes(aligned, priority_arg, compat\u001b[38;5;241m=\u001b[39mcompat)\n\u001b[1;32m    757\u001b[0m variables, out_indexes \u001b[38;5;241m=\u001b[39m merge_collected(\n\u001b[1;32m    758\u001b[0m     collected, prioritized, compat\u001b[38;5;241m=\u001b[39mcompat, combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs\n\u001b[1;32m    759\u001b[0m )\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/merge.py:358\u001b[0m, in \u001b[0;36mcollect_variables_and_indexes\u001b[0;34m(list_of_mappings, indexes)\u001b[0m\n\u001b[1;32m    356\u001b[0m     append(name, variable, indexes[name])\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m==\u001b[39m (name,):\n\u001b[0;32m--> 358\u001b[0m     idx, idx_vars \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_default_index_implicit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     append_all(idx_vars, {k: idx \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m idx_vars})\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/indexes.py:1091\u001b[0m, in \u001b[0;36mcreate_default_index_implicit\u001b[0;34m(dim_variable, all_variables)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     dim_var \u001b[38;5;241m=\u001b[39m {name: dim_variable}\n\u001b[1;32m   1090\u001b[0m     index \u001b[38;5;241m=\u001b[39m PandasIndex\u001b[38;5;241m.\u001b[39mfrom_variables(dim_var, options\u001b[38;5;241m=\u001b[39m{})\n\u001b[0;32m-> 1091\u001b[0m     index_vars \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, index_vars\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/indexes.py:418\u001b[0m, in \u001b[0;36mPandasIndex.create_variables\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    415\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    417\u001b[0m data \u001b[38;5;241m=\u001b[39m PandasIndexingAdapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_dtype)\n\u001b[0;32m--> 418\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[43mIndexVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: var}\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/variable.py:2904\u001b[0m, in \u001b[0;36mIndexVariable.__init__\u001b[0;34m(self, dims, data, attrs, encoding, fastpath)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dims, data, attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2906\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m objects must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/variable.py:373\u001b[0m, in \u001b[0;36mVariable.__init__\u001b[0;34m(self, dims, data, attrs, encoding, fastpath)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs \u001b[38;5;241m=\u001b[39m attrs\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m \u001b[38;5;241m=\u001b[39m encoding\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/xarray/core/variable.py:1000\u001b[0m, in \u001b[0;36mVariable.encoding\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;129m@encoding\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoding\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding must be castable to a dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Sverdrup \n",
    "# mainDir_r4  = '/scratch2/pillarh/eccov4r4'\n",
    "# mainDir_r5  = '/scratch2/pillarh/eccov4r5'\n",
    "# gridDir  = mainDir_r4 + '/GRID'\n",
    "# thetaDir = mainDir_r5 + '/V4r5/diags_daily/SST_day_mean'\n",
    "# thetaDir_ext = mainDir_r5 + '/V4r5_ext_2020_2023_Jun/diags_daily/SST_day_mean'\n",
    "\n",
    "## LS6\n",
    "mainDir_r4 = '/work/07665/shrey911/ls6/LRP_eccov4r4_data'\n",
    "mainDir_r5 = '/work/07665/shrey911/ls6/LRP_eccov4r5_data'\n",
    "gridDir  = mainDir_r5 + '/GRID'\n",
    "thetaDir = mainDir_r5 + '/SST_day_mean'\n",
    "thetaDir_ext = mainDir_r5 + '/SST_day_mean_ext_2020_2023_Jun'\n",
    "\n",
    "# For Sverdrup\n",
    "# ds_r4 = xr.open_dataset(f'/scratch2/shreyas/LRP_eccov4r4_data/thetaSurfECCOv4r4.nc')\n",
    "# For LS6\n",
    "ds_r4 = xr.open_dataset(mainDir_r4 + '/thetaSurfECCOv4r4.nc')\n",
    "\n",
    "# SSH has to be kept because someone used the SSH metadata for SST, \n",
    "# It's not a bug in this code but a hack to handle an existing bug.\n",
    "temp = xmitgcm.open_mdsdataset(data_dir = thetaDir,\n",
    "                             grid_dir = gridDir,\n",
    "                             extra_variables = dict(SSH = dict(dims=['k','j','i'],\n",
    "                                                               attrs = dict(standard_name=\"SST\",\n",
    "                                                                            long_name=\"Sea Surface Temperature\",\n",
    "                                                                            units=\"degC\"))))\n",
    "temp[\"SST\"] = temp[\"SSH\"]\n",
    "temp = temp.drop([\"SSH\"])\n",
    "    \n",
    "temp_ext = xmitgcm.open_mdsdataset(data_dir = thetaDir_ext,\n",
    "                                 grid_dir = gridDir,\n",
    "                                 extra_variables = dict(SST = dict(dims=['k','j','i'],\n",
    "                                                                   attrs = dict(standard_name=\"SST\",\n",
    "                                                                                long_name=\"Sea Surface Temperature\",\n",
    "                                                                                units=\"degC\"))))\n",
    "\n",
    "ds_r5 = xr.concat([temp, temp_ext], \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039de40-13c0-42bc-a503-11942de7e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SST = xr.open_dataset(mainDir_r5+'/SST_all.nc')\n",
    "SST = ds_SST['SST'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb3e1b-7581-4089-ad25-99bfbbce6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hFacC = ecco.read_llc_to_tiles(gridDir, 'hFacC.data')\n",
    "hFacC_mask = hFacC > 0\n",
    "hFacC_mask = hFacC_mask.astype(float)\n",
    "\n",
    "XC = ds_r4['XC'].data\n",
    "YC = ds_r4['YC'].data\n",
    "\n",
    "latMask = YC > -20.0\n",
    "latMask = latMask.astype(float)\n",
    "\n",
    "maskFinal = hFacC_mask * latMask\n",
    "NaNmaskFinal = np.copy(maskFinal)\n",
    "NaNmaskFinal[NaNmaskFinal == 0] = np.nan\n",
    "\n",
    "da_XC = xr.DataArray(\n",
    "    data=ds_r4['XC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"XC\"),\n",
    ")\n",
    "\n",
    "da_YC = xr.DataArray(\n",
    "    data=ds_r4['YC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"YC\"),\n",
    ")\n",
    "\n",
    "da_hFacC_mask = xr.DataArray(\n",
    "    data=hFacC_mask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"hFacC mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_latMask = xr.DataArray(\n",
    "    data=latMask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Latitude Mask 1 if > -20, else 0\"),\n",
    ")\n",
    "\n",
    "da_maskFinal = xr.DataArray(\n",
    "    data=maskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_NaNmaskFinal = xr.DataArray(\n",
    "    data=NaNmaskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D True if > 0, else NaN\"),\n",
    ")\n",
    "\n",
    "wetpoints = np.nonzero(maskFinal.data)\n",
    "da_wetpoints = xr.DataArray(\n",
    "    data=np.asarray(wetpoints),\n",
    "    dims=[\"wetpoints_dim\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        wetpoints_dim = np.arange(np.asarray(wetpoints).shape[0], dtype = int),\n",
    "        num_wetpoints = np.arange(np.asarray(wetpoints).shape[1], dtype = int),\n",
    "    ),\n",
    "    attrs=dict(description=\"indices of wetpoints in the order (tile, j, i) in the three rows\"),\n",
    ")\n",
    "\n",
    "da_SST = xr.DataArray(\n",
    "    data=SST,\n",
    "    dims=[\"time\", \"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        time = ds_r5['time'].data[:SST.shape[0]],\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"SST field in llc format\"),\n",
    ")\n",
    "\n",
    "ds = xr.Dataset()\n",
    "ds = ds.assign(XC           = da_XC,\n",
    "               YC           = da_YC,\n",
    "               hFacC_mask   = da_hFacC_mask,\n",
    "               latMask      = da_latMask,\n",
    "               maskFinal    = da_maskFinal,\n",
    "               NaNmaskFinal = da_NaNmaskFinal,\n",
    "               wetpoints    = da_wetpoints,\n",
    "               SST          = da_SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f1c11-9c00-48fb-bf00-0afd6b5678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize_new(field, num_years = 31, first_leap_year_idx = 0):\n",
    "    \n",
    "    leap_yr_offsets_jan_feb   = np.array([0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8])\n",
    "    leap_yr_offsets_after_feb = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8])\n",
    "\n",
    "    if len(field.shape) > 1:\n",
    "        seasonal_trend = np.zeros((366, field.shape[1]))\n",
    "    else:\n",
    "        seasonal_trend = np.zeros((366,))\n",
    "\n",
    "    #### Calculate seasonal trend\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        same_cal_days_idx=[d+365*year+leap_yr_offsets_jan_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indexing\n",
    "    same_cal_days_idx=[365*year+59+int(year/4) for year in range(first_leap_year_idx,num_years,4)]\n",
    "    # Remove mean\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='constant', \n",
    "                                                    overwrite_data=False)\n",
    "    # Remove linear trend\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='linear', \n",
    "                                                    overwrite_data=False)\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        same_cal_days_idx=[d-1+365*year+leap_yr_offsets_after_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e4223-fad9-483a-a905-629ce467174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds['SST'].data[:,wetpoints[0],wetpoints[1],wetpoints[2]].copy()\n",
    "X = anomalize_new(X)\n",
    "X_full = X.copy()\n",
    "X = X[30:-30]\n",
    "\n",
    "y = ds['SST'].isel(tile = 10, j = 1, i = 43).data.copy()\n",
    "y = anomalize_new(y)\n",
    "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "y = np.convolve(y, np.ones(61)/61, mode='valid')\n",
    "oneHotCost = np.zeros((y.shape[0], 2), dtype = int)\n",
    "oneHotCost[:,0] = y >= 0.0\n",
    "oneHotCost[:,1] = y <  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23bff3-dea8-49fd-bade-db7a7cb87ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_X = xr.DataArray(\n",
    "    data=X,\n",
    "    dims=[\"time_allData\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data as matrix X; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_y = xr.DataArray(\n",
    "    data=y,\n",
    "    dims=[\"time_allData\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function y; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_X_full = xr.DataArray(\n",
    "    data=X_full,\n",
    "    dims=[\"time\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time          = ds['time'],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data without accounting for conv filter as matrix X_full; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_oneHotCost = xr.DataArray(\n",
    "    data=oneHotCost,\n",
    "    dims=[\"time_allData\", \"NN_output_dim\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        NN_output_dim = np.array([0,1]),\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function as one-hot vector.\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(X          = da_X,\n",
    "               y          = da_y,\n",
    "               X_full     = da_X_full,\n",
    "               oneHotCost = da_oneHotCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb797f76-7914-4d02-9ab5-f47c788dde7b",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b017273-5b15-4d62-a7ae-16381c5513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP(model, model_wo_softmax, X, y_true, lagSteps, lrp_methods, \n",
    "        suffix, normalizeDict, **kwargs):\n",
    "\n",
    "    result = {}\n",
    "    cost_NN = model.predict(X)\n",
    "    result[f'score_{suffix}'] = model.evaluate(X, y_true, verbose=0)\n",
    "\n",
    "    pred_NN = cost_NN.copy()\n",
    "    pred_NN[:,0] = pred_NN[:,0] > 0.5\n",
    "    pred_NN[:,1] = pred_NN[:,1] > 0.5\n",
    "\n",
    "    idx_NN_pos = []\n",
    "    idx_NN_neg = []\n",
    "\n",
    "    if lagSteps >= 0:\n",
    "        for i in range(len(y_true[lagSteps:,0])):\n",
    "            if y_true[lagSteps+i,0] == 1 and pred_NN[i,0] == 1:\n",
    "                idx_NN_pos.append(i)\n",
    "            if y_true[lagSteps+i,1] == 1 and pred_NN[i,1] == 1:\n",
    "                idx_NN_neg.append(i)\n",
    "    else:\n",
    "        for i in range(len(y_true[:lagSteps,0])):\n",
    "            if y_true[i,0] == 1 and pred_NN[i-lagSteps,0] == 1:\n",
    "                idx_NN_pos.append(i-lagSteps)\n",
    "            if y_true[i,1] == 1 and pred_NN[i-lagSteps,1] == 1:\n",
    "                idx_NN_neg.append(i-lagSteps)\n",
    "\n",
    "    result[f'idx_NN_pos'] = idx_NN_pos\n",
    "    result[f'idx_NN_neg'] = idx_NN_neg\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_pos), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_pos]\n",
    "    result[f'samples_correct_pos_{suffix}'] = rel\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_neg), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_neg]\n",
    "    result[f'samples_correct_neg_{suffix}'] = rel\n",
    "    \n",
    "    for method in lrp_methods:\n",
    "\n",
    "        title = method['title']\n",
    "        \n",
    "        print(f'Analyze using {title} for {suffix} data')\n",
    "        \n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_pos], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_pos_{suffix}'] = rel\n",
    "\n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_neg], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_neg_{suffix}'] = rel\n",
    "        \n",
    "    return result\n",
    "\n",
    "def quickSetup(X, y,\n",
    "               test_split_frac,\n",
    "               val_split_frac,\n",
    "               lrp_methods,\n",
    "               lagSteps,\n",
    "               decay_func = None,\n",
    "               **NNkwargs):\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    idx = int(X.shape[0]*(1-test_split_frac))\n",
    "    X_train = X[:idx]\n",
    "    oneHotCost_train = oneHotCost[:idx]\n",
    "    X_test = X[idx:]\n",
    "    oneHotCost_test = oneHotCost[idx:]\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    NNkwargs['optimizer'] = sgd\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    if lagSteps > 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[:-lagSteps], oneHotCost_train[lagSteps:], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    elif lagSteps == 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train, oneHotCost_train, \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    else:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[-lagSteps:], oneHotCost_train[:lagSteps], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "\n",
    "    K = TrainFullyConnectedNN(x_t, oneHotCost_t, validation_data = (x_v, oneHotCost_v), **NNkwargs)\n",
    "                   \n",
    "    best_model = K.quickTrain(decay_func)\n",
    "    best_model_wo_softmax = innvestigate.model_wo_softmax(best_model)\n",
    "                   \n",
    "    result['QoI_predict'] = best_model.predict(X)\n",
    "    result['QoI_predict_train'] = best_model.predict(X_train)\n",
    "    result['QoI_predict_test'] = best_model.predict(X_test)\n",
    "                   \n",
    "    normalizeDict = {'bool_': True, 'kind': 'MaxAbs'}\n",
    "    kwargs = {'y_ref': 0.00}\n",
    "\n",
    "    # LRP for all data\n",
    "    result_train = LRP(best_model, best_model_wo_softmax, \n",
    "                       X, oneHotCost, lagSteps, lrp_methods, \n",
    "                       suffix = 'all', normalizeDict = normalizeDict, **kwargs)\n",
    "    result.update(result_train)\n",
    "    \n",
    "    # # LRP for training data\n",
    "    # result_train = LRP(best_model, best_model_wo_softmax, \n",
    "    #                    X_train, oneHotCost_train, lagSteps, lrp_methods, \n",
    "    #                    suffix = 'train', normalizeDict = normalizeDict, **kwargs)\n",
    "    # result.update(result_train)\n",
    "\n",
    "    # # LRP for test data\n",
    "    # result_test = LRP(best_model, best_model_wo_softmax, \n",
    "    #                    X_test, oneHotCost_test, lagSteps, lrp_methods, \n",
    "    #                    suffix = 'test', normalizeDict = normalizeDict, **kwargs)\n",
    "    # result.update(result_test)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171702a-a06c-4148-b84d-680a368626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = [{'size': X.shape[1], 'activation': None     , 'use_bias': None},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 10.0, 'l2_b_reg': 10.0},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01},\n",
    "          {'size': 2         , 'activation': 'softmax', 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01}]\n",
    "\n",
    "Losses = [{'kind': 'categorical_crossentropy', 'weight': 1.0}]\n",
    "\n",
    "LRPDict_theta = {}\n",
    "lagStepsList = [-60,-30,0]#,30,60,90,120,150,180]\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.70710678118 # 0.5^0.5\n",
    "    epochs_drop = 50\n",
    "    lrate = initial_lrate * drop**np.floor((1+epoch)/epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "methods = [# dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0', optParams = {}),\n",
    "           # dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0-W2', optParams = {'input_layer_rule':'WSquare'}),\n",
    "           dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0-B' , optParams = {'input_layer_rule':'Bounded'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64496c7-b56a-4305-9714-d50c3f6981dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_pos = len(oneHotCost[:-2161,0]) / np.sum(oneHotCost[:-2161,0])\n",
    "weight_for_neg = len(oneHotCost[:-2161,1]) / np.sum(oneHotCost[:-2161,1])\n",
    "class_weight = {0: weight_for_pos, 1: weight_for_neg}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6386a6-f5f7-4784-a556-83efa68ecfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lagStepsList)):\n",
    "\n",
    "    print(f'Lag: {lagStepsList[i]} days, for Theta')\n",
    "    \n",
    "    NNkwargs = {'losses': Losses,\n",
    "                'metrics': [metricF1, # Custom f1 since metrics.F1Score(name='f1') is not available before tf v2.13\n",
    "                            'accuracy',\n",
    "                            'categorical_crossentropy',\n",
    "                            # metrics.CategoricalCrossentropy(name='cross_entropy'),  # (model's loss-L2 reg loss)\n",
    "                            # metrics.MeanSquaredError(name='brier_score'),\n",
    "                            # metrics.TruePositives(name='tp'),\n",
    "                            # metrics.FalsePositives(name='fp'),\n",
    "                            # metrics.TrueNegatives(name='tn'),\n",
    "                            # metrics.FalseNegatives(name='fn'), \n",
    "                            # metrics.BinaryAccuracy(name='accuracy'),\n",
    "                            # metrics.Precision(name='precision'),\n",
    "                            # metrics.Recall(name='recall'),\n",
    "                            # metrics.AUC(name='auc'),\n",
    "                            # metrics.AUC(name='prc', curve='PR'), # precision-recall curve],\n",
    "                           ],\n",
    "                'batch_size': 128, 'epochs': 1000, #'validation_split': 0.1,\n",
    "                'filename': f'model{lagStepsList[i]}_noL1', 'dirname': os.path.abspath(''),\n",
    "                'random_nn_seed': 42, 'class_weight': class_weight,\n",
    "                'custom_objects': {'metricF1': metricF1}, 'verbose': 2}\n",
    "    \n",
    "    LRPDict_theta[f'LRP{lagStepsList[i]}'] = quickSetup(X, y, test_split_frac = 2161.0/11263.0,\n",
    "                                                        val_split_frac = 0.2,\n",
    "                                                        lrp_methods = methods,\n",
    "                                                        lagSteps = lagStepsList[i],\n",
    "                                                        decay_func = step_decay,\n",
    "                                                        layers = Layers, **NNkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebc6a6-95a8-4cf0-9bf1-264466266f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP_normalize(field):\n",
    "    # Max for each sample is already 1 so no real need to divide by max\n",
    "    field = field / np.nanmax(field, axis = (1,2,3))[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "    field = np.nanmean(field, axis=0)\n",
    "    return xr.DataArray(field)\n",
    "\n",
    "lrp_minus60_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_minus60_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_minus30_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_minus30_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_0_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_0_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_30_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_30_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_60_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_60_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_90_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_90_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_120_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_120_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_150_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_150_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_180_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_180_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-B_neg_all'].copy())\n",
    "\n",
    "ds_lrp = xr.Dataset()\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_minus60_a1b0_b_pos_all = lrp_minus60_a1b0_b_pos_all_da,\n",
    "                       lrp_minus60_a1b0_b_neg_all = lrp_minus60_a1b0_b_neg_all_da,\n",
    "                       lrp_minus30_a1b0_b_pos_all = lrp_minus30_a1b0_b_pos_all_da,\n",
    "                       lrp_minus30_a1b0_b_neg_all = lrp_minus30_a1b0_b_neg_all_da,\n",
    "                       lrp_0_a1b0_b_pos_all = lrp_0_a1b0_b_pos_all_da,\n",
    "                       lrp_0_a1b0_b_neg_all = lrp_0_a1b0_b_neg_all_da,\n",
    "                       lrp_30_a1b0_b_pos_all = lrp_30_a1b0_b_pos_all_da,\n",
    "                       lrp_30_a1b0_b_neg_all = lrp_30_a1b0_b_neg_all_da,\n",
    "                       lrp_60_a1b0_b_pos_all = lrp_60_a1b0_b_pos_all_da,\n",
    "                       lrp_60_a1b0_b_neg_all = lrp_60_a1b0_b_neg_all_da,\n",
    "                       lrp_90_a1b0_b_pos_all = lrp_90_a1b0_b_pos_all_da,\n",
    "                       lrp_90_a1b0_b_neg_all = lrp_90_a1b0_b_neg_all_da,\n",
    "                       lrp_120_a1b0_b_pos_all = lrp_120_a1b0_b_pos_all_da,\n",
    "                       lrp_120_a1b0_b_neg_all = lrp_120_a1b0_b_neg_all_da,\n",
    "                       lrp_150_a1b0_b_pos_all = lrp_150_a1b0_b_pos_all_da,\n",
    "                       lrp_150_a1b0_b_neg_all = lrp_150_a1b0_b_neg_all_da,\n",
    "                       lrp_180_a1b0_b_pos_all = lrp_180_a1b0_b_pos_all_da,\n",
    "                       lrp_180_a1b0_b_neg_all = lrp_180_a1b0_b_neg_all_da)\n",
    "\n",
    "ds_lrp.to_netcdf('LRP_output_forHelen/LRP_A1B0_newAnomalies_shuffleVal_reweight.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc13f6-ef66-4c0f-ad71-4e63f4bf33b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_pos_all'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -1, cmax = 1,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (pos_all) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (positive_all)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb1965-1e27-4716-9cbc-6dcc77836282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_neg_all'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -1, cmax = 1,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (neg_all) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (negative_all)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc297c5-e82e-414e-9dc9-5f14167dbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_pos_all'].copy()\n",
    "        # Max for each sample is already 1 so no real need to divide by max\n",
    "        field = field / np.nanmax(field, axis = (1,2,3))[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True, \n",
    "                                      cmap = 'jet', \n",
    "                                      cmin = 0.0, \n",
    "                                      cmax = 0.5,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "        P[1].set_title(f\"{method['title']}_all lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "\n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_all for pos QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbedfe39-a3d2-49f1-b28b-ae3c88865aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_neg_all'].copy()\n",
    "        # Max for each sample is already 1 so no real need to divide by max\n",
    "        field = field / np.nanmax(field, axis = (1,2,3))[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True,\n",
    "                                      cmap = 'jet', \n",
    "                                      cmin = 0.0, \n",
    "                                      cmax = 0.5,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "        \n",
    "        P[1].set_title(f\"{method['title']}_all lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "        \n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_all for neg QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a372860-af14-44ec-803f-9456edea77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import innvestigate\n",
    "# import innvestigate.utils as iutils\n",
    "# from innvestigate.analyzer.base import AnalyzerBase\n",
    "\n",
    "# model = keras.models.load_model('model0_noL1.h5', custom_objects={'metricF1': metricF1})\n",
    "# model_wo_softmax = innvestigate.model_wo_softmax(model)\n",
    "# Analyze = innvestigate.create_analyzer('lrp.alpha_1_beta_0', model_wo_softmax, input_layer_rule='Bounded')\n",
    "# sample = LRPDict_theta['LRP0']['samples_correct_neg_all'][0,wetpoints[0],wetpoints[1],wetpoints[2]][np.newaxis,:]\n",
    "\n",
    "# a = Analyze.analyze(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
