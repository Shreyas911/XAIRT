{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5950c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import metrics\n",
    "from keras.constraints import NonNeg\n",
    "import innvestigate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "import xmitgcm\n",
    "from xmitgcm import open_mdsdataset\n",
    "import ecco_v4_py as ecco\n",
    "\n",
    "import random\n",
    "\n",
    "# See if GPUs are available\n",
    "from keras import backend as K\n",
    "if bool(K._get_available_gpus()):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *\n",
    "\n",
    "### https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed ###\n",
    "### https://keras.io/examples/keras_recipes/reproducibility_recipes/ ###\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b1ee2-570a-46e3-8b64-42843a17be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sverdrup \n",
    "# mainDir_r4  = '/scratch2/pillarh/eccov4r4'\n",
    "# mainDir_r5  = '/scratch2/pillarh/eccov4r5'\n",
    "# gridDir  = mainDir_r4 + '/GRID'\n",
    "# thetaDir = mainDir_r5 + '/V4r5/diags_daily/SST_day_mean'\n",
    "# thetaDir_ext = mainDir_r5 + '/V4r5_ext_2020_2023_Jun/diags_daily/SST_day_mean'\n",
    "\n",
    "## LS6\n",
    "mainDir_r4 = '/work/07665/shrey911/ls6/LRP_eccov4r4_data'\n",
    "mainDir_r5 = '/work/07665/shrey911/ls6/LRP_eccov4r5_data'\n",
    "gridDir  = mainDir_r5 + '/GRID'\n",
    "thetaDir = mainDir_r5 + '/SST_day_mean'\n",
    "thetaDir_ext = mainDir_r5 + '/SST_day_mean_ext_2020_2023_Jun'\n",
    "\n",
    "# For Sverdrup\n",
    "# ds_r4 = xr.open_dataset(f'/scratch2/shreyas/LRP_eccov4r4_data/thetaSurfECCOv4r4.nc')\n",
    "# For LS6\n",
    "ds_r4 = xr.open_dataset(mainDir_r4 + '/thetaSurfECCOv4r4.nc')\n",
    "\n",
    "# SSH has to be kept because someone used the SSH metadata for SST, \n",
    "# It's not a bug in this code but a hack to handle an existing bug.\n",
    "temp = xmitgcm.open_mdsdataset(data_dir = thetaDir,\n",
    "                             grid_dir = gridDir,\n",
    "                             extra_variables = dict(SSH = dict(dims=['k','j','i'],\n",
    "                                                               attrs = dict(standard_name=\"SST\",\n",
    "                                                                            long_name=\"Sea Surface Temperature\",\n",
    "                                                                            units=\"degC\"))))\n",
    "temp[\"SST\"] = temp[\"SSH\"]\n",
    "temp = temp.drop([\"SSH\"])\n",
    "    \n",
    "temp_ext = xmitgcm.open_mdsdataset(data_dir = thetaDir_ext,\n",
    "                                 grid_dir = gridDir,\n",
    "                                 extra_variables = dict(SST = dict(dims=['k','j','i'],\n",
    "                                                                   attrs = dict(standard_name=\"SST\",\n",
    "                                                                                long_name=\"Sea Surface Temperature\",\n",
    "                                                                                units=\"degC\"))))\n",
    "\n",
    "ds_r5 = xr.concat([temp, temp_ext], \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039de40-13c0-42bc-a503-11942de7e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SST = xr.open_dataset(mainDir_r5+'/SST_all.nc')\n",
    "SST = ds_SST['SST'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb3e1b-7581-4089-ad25-99bfbbce6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hFacC = ecco.read_llc_to_tiles(gridDir, 'hFacC.data')\n",
    "hFacC_mask = hFacC > 0\n",
    "hFacC_mask = hFacC_mask.astype(float)\n",
    "\n",
    "XC = ds_r4['XC'].data\n",
    "YC = ds_r4['YC'].data\n",
    "\n",
    "latMask = YC > -20.0\n",
    "latMask = latMask.astype(float)\n",
    "\n",
    "maskFinal = hFacC_mask * latMask\n",
    "NaNmaskFinal = np.copy(maskFinal)\n",
    "NaNmaskFinal[NaNmaskFinal == 0] = np.nan\n",
    "\n",
    "da_XC = xr.DataArray(\n",
    "    data=ds_r4['XC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"XC\"),\n",
    ")\n",
    "\n",
    "da_YC = xr.DataArray(\n",
    "    data=ds_r4['YC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"YC\"),\n",
    ")\n",
    "\n",
    "da_hFacC_mask = xr.DataArray(\n",
    "    data=hFacC_mask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"hFacC mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_latMask = xr.DataArray(\n",
    "    data=latMask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Latitude Mask 1 if > -20, else 0\"),\n",
    ")\n",
    "\n",
    "da_maskFinal = xr.DataArray(\n",
    "    data=maskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_NaNmaskFinal = xr.DataArray(\n",
    "    data=NaNmaskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D True if > 0, else NaN\"),\n",
    ")\n",
    "\n",
    "wetpoints = np.nonzero(maskFinal.data)\n",
    "da_wetpoints = xr.DataArray(\n",
    "    data=np.asarray(wetpoints),\n",
    "    dims=[\"wetpoints_dim\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        wetpoints_dim = np.arange(np.asarray(wetpoints).shape[0], dtype = int),\n",
    "        num_wetpoints = np.arange(np.asarray(wetpoints).shape[1], dtype = int),\n",
    "    ),\n",
    "    attrs=dict(description=\"indices of wetpoints in the order (tile, j, i) in the three rows\"),\n",
    ")\n",
    "\n",
    "da_SST = xr.DataArray(\n",
    "    data=SST,\n",
    "    dims=[\"time\", \"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        time = ds_r5['time'].data[:SST.shape[0]],\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"SST field in llc format\"),\n",
    ")\n",
    "\n",
    "ds = xr.Dataset()\n",
    "ds = ds.assign(XC           = da_XC,\n",
    "               YC           = da_YC,\n",
    "               hFacC_mask   = da_hFacC_mask,\n",
    "               latMask      = da_latMask,\n",
    "               maskFinal    = da_maskFinal,\n",
    "               NaNmaskFinal = da_NaNmaskFinal,\n",
    "               wetpoints    = da_wetpoints,\n",
    "               SST          = da_SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f1c11-9c00-48fb-bf00-0afd6b5678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize_new(field, num_years = 31, first_leap_year_idx = 0):\n",
    "    \n",
    "    leap_yr_offsets_jan_feb   = np.array([0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8])\n",
    "    leap_yr_offsets_after_feb = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8])\n",
    "\n",
    "    if len(field.shape) > 1:\n",
    "        seasonal_trend = np.zeros((366, field.shape[1]))\n",
    "    else:\n",
    "        seasonal_trend = np.zeros((366,))\n",
    "\n",
    "    #### Calculate seasonal trend\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        same_cal_days_idx=[d+365*year+leap_yr_offsets_jan_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indexing\n",
    "    same_cal_days_idx=[365*year+59+int(year/4) for year in range(first_leap_year_idx,num_years,4)]\n",
    "    # Remove mean\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='constant', \n",
    "                                                    overwrite_data=False)\n",
    "    # Remove linear trend\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='linear', \n",
    "                                                    overwrite_data=False)\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        same_cal_days_idx=[d-1+365*year+leap_yr_offsets_after_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e4223-fad9-483a-a905-629ce467174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds['SST'].data[:,wetpoints[0],wetpoints[1],wetpoints[2]].copy()\n",
    "X = anomalize_new(X)\n",
    "X_full = X.copy()\n",
    "X = X[30:-30]\n",
    "\n",
    "y = ds['SST'].isel(tile = 10, j = 1, i = 43).data.copy()\n",
    "y = anomalize_new(y)\n",
    "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "y = np.convolve(y, np.ones(61)/61, mode='valid')\n",
    "oneHotCost = np.zeros((y.shape[0], 2), dtype = int)\n",
    "oneHotCost[:,0] = y >= 0.0\n",
    "oneHotCost[:,1] = y <  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23bff3-dea8-49fd-bade-db7a7cb87ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_X = xr.DataArray(\n",
    "    data=X,\n",
    "    dims=[\"time_allData\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data as matrix X; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_y = xr.DataArray(\n",
    "    data=y,\n",
    "    dims=[\"time_allData\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function y; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_X_full = xr.DataArray(\n",
    "    data=X_full,\n",
    "    dims=[\"time\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time          = ds['time'],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data without accounting for conv filter as matrix X_full; Mean removed and delinearized for each calendar day.\"),\n",
    ")\n",
    "\n",
    "da_oneHotCost = xr.DataArray(\n",
    "    data=oneHotCost,\n",
    "    dims=[\"time_allData\", \"NN_output_dim\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        NN_output_dim = np.array([0,1]),\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function as one-hot vector.\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(X          = da_X,\n",
    "               y          = da_y,\n",
    "               X_full     = da_X_full,\n",
    "               oneHotCost = da_oneHotCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb797f76-7914-4d02-9ab5-f47c788dde7b",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b017273-5b15-4d62-a7ae-16381c5513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP(model, model_wo_softmax, X, y_true, lagSteps, lrp_methods, \n",
    "        suffix, normalizeDict, **kwargs):\n",
    "\n",
    "    result = {}\n",
    "    cost_NN = model.predict(X)\n",
    "    result[f'score_{suffix}'] = model.evaluate(X, y_true, verbose=0)\n",
    "\n",
    "    pred_NN = cost_NN.copy()\n",
    "    pred_NN[:,0] = pred_NN[:,0] > 0.5\n",
    "    pred_NN[:,1] = pred_NN[:,1] > 0.5\n",
    "\n",
    "    idx_NN_pos = []\n",
    "    idx_NN_neg = []\n",
    "\n",
    "    if lagSteps >= 0:\n",
    "        for i in range(len(y_true[lagSteps:,0])):\n",
    "            if y_true[lagSteps+i,0] == 1 and pred_NN[i,0] == 1:\n",
    "                idx_NN_pos.append(i)\n",
    "            if y_true[lagSteps+i,1] == 1 and pred_NN[i,1] == 1:\n",
    "                idx_NN_neg.append(i)\n",
    "    else:\n",
    "        for i in range(len(y_true[:lagSteps,0])):\n",
    "            if y_true[i,0] == 1 and pred_NN[i-lagSteps,0] == 1:\n",
    "                idx_NN_pos.append(i-lagSteps)\n",
    "            if y_true[i,1] == 1 and pred_NN[i-lagSteps,1] == 1:\n",
    "                idx_NN_neg.append(i-lagSteps)\n",
    "\n",
    "    result[f'idx_NN_pos'] = idx_NN_pos\n",
    "    result[f'idx_NN_neg'] = idx_NN_neg\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_pos), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_pos]\n",
    "    result[f'samples_correct_pos_{suffix}'] = rel\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_neg), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_neg]\n",
    "    result[f'samples_correct_neg_{suffix}'] = rel\n",
    "    \n",
    "    for method in lrp_methods:\n",
    "\n",
    "        title = method['title']\n",
    "        \n",
    "        print(f'Analyze using {title} for {suffix} data')\n",
    "        \n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_pos], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_pos_{suffix}'] = rel\n",
    "\n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_neg], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_neg_{suffix}'] = rel\n",
    "        \n",
    "    return result\n",
    "\n",
    "def quickSetup(X, y,\n",
    "               test_split_frac,\n",
    "               val_split_frac,\n",
    "               lrp_methods,\n",
    "               lagSteps,\n",
    "               decay_func = None,\n",
    "               **NNkwargs):\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    idx = int(X.shape[0]*(1-test_split_frac))\n",
    "    X_train = X[:idx]\n",
    "    oneHotCost_train = oneHotCost[:idx]\n",
    "    X_test = X[idx:]\n",
    "    oneHotCost_test = oneHotCost[idx:]\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    NNkwargs['optimizer'] = sgd\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    if lagSteps > 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[:-lagSteps], oneHotCost_train[lagSteps:], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    elif lagSteps == 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train, oneHotCost_train, \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    else:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[-lagSteps:], oneHotCost_train[:lagSteps], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "\n",
    "    K = TrainFullyConnectedNN(x_t, oneHotCost_t, validation_data = (x_v, oneHotCost_v), **NNkwargs)\n",
    "                   \n",
    "    best_model = K.quickTrain(decay_func)\n",
    "    best_model_wo_softmax = innvestigate.model_wo_softmax(best_model)\n",
    "                   \n",
    "    result['QoI_predict'] = best_model.predict(X)\n",
    "    result['QoI_predict_train'] = best_model.predict(X_train)\n",
    "    result['QoI_predict_test'] = best_model.predict(X_test)\n",
    "                   \n",
    "    normalizeDict = {'bool_': True, 'kind': 'MaxAbs'}\n",
    "    kwargs = {'y_ref': 0.00}\n",
    "\n",
    "    # LRP for all data\n",
    "    result_train = LRP(best_model, best_model_wo_softmax, \n",
    "                       X, oneHotCost, lagSteps, lrp_methods, \n",
    "                       suffix = 'all', normalizeDict = normalizeDict, **kwargs)\n",
    "    result.update(result_train)\n",
    "    \n",
    "    # # LRP for training data\n",
    "    # result_train = LRP(best_model, best_model_wo_softmax, \n",
    "    #                    X_train, oneHotCost_train, lagSteps, lrp_methods, \n",
    "    #                    suffix = 'train', normalizeDict = normalizeDict, **kwargs)\n",
    "    # result.update(result_train)\n",
    "\n",
    "    # # LRP for test data\n",
    "    # result_test = LRP(best_model, best_model_wo_softmax, \n",
    "    #                    X_test, oneHotCost_test, lagSteps, lrp_methods, \n",
    "    #                    suffix = 'test', normalizeDict = normalizeDict, **kwargs)\n",
    "    # result.update(result_test)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171702a-a06c-4148-b84d-680a368626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = [{'size': X.shape[1], 'activation': None     , 'use_bias': None},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 10.0, 'l2_b_reg': 10.0},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01},\n",
    "          {'size': 2         , 'activation': 'softmax', 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01, 'bias_constraint': None}]\n",
    "\n",
    "Losses = [{'kind': 'categorical_crossentropy', 'weight': 1.0}]\n",
    "\n",
    "LRPDict_theta = {}\n",
    "lagStepsList = [-60,-30,0,30,60,90,120,150,180]\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 50\n",
    "    lrate = initial_lrate * drop**np.floor((1+epoch)/epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "methods = [dict(name='lrp.alpha_1_beta_0_IB', title = 'LRP-A1B0-B' , optParams = {'input_layer_rule':'Bounded'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64496c7-b56a-4305-9714-d50c3f6981dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_pos = len(oneHotCost[:-2161,0]) / np.sum(oneHotCost[:-2161,0])\n",
    "weight_for_neg = len(oneHotCost[:-2161,1]) / np.sum(oneHotCost[:-2161,1])\n",
    "class_weight = {0: weight_for_pos, 1: weight_for_neg}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6386a6-f5f7-4784-a556-83efa68ecfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lagStepsList)):\n",
    "\n",
    "    print(f'Lag: {lagStepsList[i]} days, for Theta')\n",
    "    \n",
    "    NNkwargs = {'losses': Losses,\n",
    "                'metrics': [metricF1, # Custom f1 since metrics.F1Score(name='f1') is not available before tf v2.13\n",
    "                            'accuracy',\n",
    "                            'categorical_crossentropy',\n",
    "                            # metrics.CategoricalCrossentropy(name='cross_entropy'),  # (model's loss-L2 reg loss)\n",
    "                            # metrics.MeanSquaredError(name='brier_score'),\n",
    "                            # metrics.TruePositives(name='tp'),\n",
    "                            # metrics.FalsePositives(name='fp'),\n",
    "                            # metrics.TrueNegatives(name='tn'),\n",
    "                            # metrics.FalseNegatives(name='fn'), \n",
    "                            # metrics.BinaryAccuracy(name='accuracy'),\n",
    "                            # metrics.Precision(name='precision'),\n",
    "                            # metrics.Recall(name='recall'),\n",
    "                            # metrics.AUC(name='auc'),\n",
    "                            # metrics.AUC(name='prc', curve='PR'), # precision-recall curve],\n",
    "                           ],\n",
    "                'batch_size': 128, 'epochs': 500, #'validation_split': 0.1,\n",
    "                'filename': f'model{lagStepsList[i]}_noL1', 'dirname': os.path.abspath(''),\n",
    "                'random_nn_seed': 42, 'class_weight': class_weight,\n",
    "                'custom_objects': {'metricF1': metricF1}, 'verbose': 2}\n",
    "    \n",
    "    LRPDict_theta[f'LRP{lagStepsList[i]}'] = quickSetup(X, y, test_split_frac = 2161.0/11263.0,\n",
    "                                                        val_split_frac = 0.2,\n",
    "                                                        lrp_methods = methods,\n",
    "                                                        lagSteps = lagStepsList[i],\n",
    "                                                        decay_func = step_decay,\n",
    "                                                        layers = Layers, **NNkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebc6a6-95a8-4cf0-9bf1-264466266f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP_normalize(field):\n",
    "    # Max for each sample is already 1 so no real need to divide by max\n",
    "    field = field / np.nanmax(field, axis = (1,2,3))[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "    field = np.nanmean(field, axis=0)\n",
    "    return xr.DataArray(field)\n",
    "\n",
    "lrp_minus60_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_minus60_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_minus30_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_minus30_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_0_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_0_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_30_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_30_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_60_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_60_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_90_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_90_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_120_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_120_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_150_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_150_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-B_neg_all'].copy())\n",
    "lrp_180_a1b0_b_pos_all_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-B_pos_all'].copy())\n",
    "lrp_180_a1b0_b_neg_all_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-B_neg_all'].copy())\n",
    "\n",
    "ds_lrp = xr.Dataset()\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_minus60_a1b0_b_pos_all = lrp_minus60_a1b0_b_pos_all_da,\n",
    "                       lrp_minus60_a1b0_b_neg_all = lrp_minus60_a1b0_b_neg_all_da,\n",
    "                       lrp_minus30_a1b0_b_pos_all = lrp_minus30_a1b0_b_pos_all_da,\n",
    "                       lrp_minus30_a1b0_b_neg_all = lrp_minus30_a1b0_b_neg_all_da,\n",
    "                       lrp_0_a1b0_b_pos_all = lrp_0_a1b0_b_pos_all_da,\n",
    "                       lrp_0_a1b0_b_neg_all = lrp_0_a1b0_b_neg_all_da,\n",
    "                       lrp_30_a1b0_b_pos_all = lrp_30_a1b0_b_pos_all_da,\n",
    "                       lrp_30_a1b0_b_neg_all = lrp_30_a1b0_b_neg_all_da,\n",
    "                       lrp_60_a1b0_b_pos_all = lrp_60_a1b0_b_pos_all_da,\n",
    "                       lrp_60_a1b0_b_neg_all = lrp_60_a1b0_b_neg_all_da,\n",
    "                       lrp_90_a1b0_b_pos_all = lrp_90_a1b0_b_pos_all_da,\n",
    "                       lrp_90_a1b0_b_neg_all = lrp_90_a1b0_b_neg_all_da,\n",
    "                       lrp_120_a1b0_b_pos_all = lrp_120_a1b0_b_pos_all_da,\n",
    "                       lrp_120_a1b0_b_neg_all = lrp_120_a1b0_b_neg_all_da,\n",
    "                       lrp_150_a1b0_b_pos_all = lrp_150_a1b0_b_pos_all_da,\n",
    "                       lrp_150_a1b0_b_neg_all = lrp_150_a1b0_b_neg_all_da,\n",
    "                       lrp_180_a1b0_b_pos_all = lrp_180_a1b0_b_pos_all_da,\n",
    "                       lrp_180_a1b0_b_neg_all = lrp_180_a1b0_b_neg_all_da)\n",
    "\n",
    "ds_lrp.to_netcdf('LRP_output_forHelen/LRP_A1B0_IB_B_newAnomalies_shuffleVal_reweight_posBiasFinLayer.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc13f6-ef66-4c0f-ad71-4e63f4bf33b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_pos_all'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -1, cmax = 1,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (pos_all) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (positive_all)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb1965-1e27-4716-9cbc-6dcc77836282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_neg_all'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -1, cmax = 1,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (neg_all) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (negative_all)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc297c5-e82e-414e-9dc9-5f14167dbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_pos_all'].copy()\n",
    "        # Max for each sample is already 1 so no real need to divide by max\n",
    "        field = field / np.nanmax(field, axis = (1,2,3))[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True, \n",
    "                                      cmap = 'jet', \n",
    "                                      cmin = 0.0, \n",
    "                                      cmax = 0.3,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "        P[1].set_title(f\"{method['title']}_all lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "\n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_all for pos QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbedfe39-a3d2-49f1-b28b-ae3c88865aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_neg_all'].copy()\n",
    "        # Max for each sample is already 1 so no real need to divide by max\n",
    "        field = field / np.nanmax(field, axis = (1,2,3))[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True,\n",
    "                                      cmap = 'jet', \n",
    "                                      cmin = 0.0, \n",
    "                                      cmax = 0.3,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "        \n",
    "        P[1].set_title(f\"{method['title']}_all lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "        \n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_all for neg QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
