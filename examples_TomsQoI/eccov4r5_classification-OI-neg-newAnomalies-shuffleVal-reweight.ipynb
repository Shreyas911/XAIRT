{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5950c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import metrics\n",
    "import innvestigate\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "import xmitgcm\n",
    "from xmitgcm import open_mdsdataset\n",
    "import ecco_v4_py as ecco\n",
    "\n",
    "import random\n",
    "\n",
    "# See if GPUs are available\n",
    "from keras import backend as K\n",
    "if bool(K._get_available_gpus()):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *\n",
    "\n",
    "### https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed ###\n",
    "### https://keras.io/examples/keras_recipes/reproducibility_recipes/ ###\n",
    "SEED = 1997\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b1ee2-570a-46e3-8b64-42843a17be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sverdrup \n",
    "# mainDir_r4  = '/scratch2/pillarh/eccov4r4'\n",
    "# mainDir_r5  = '/scratch2/pillarh/eccov4r5'\n",
    "# gridDir  = mainDir_r4 + '/GRID'\n",
    "# thetaDir = mainDir_r5 + '/V4r5/diags_daily/SST_day_mean'\n",
    "# thetaDir_ext = mainDir_r5 + '/V4r5_ext_2020_2023_Jun/diags_daily/SST_day_mean'\n",
    "\n",
    "## LS6\n",
    "mainDir_r4 = '/work/07665/shrey911/ls6/LRP_eccov4r4_data'\n",
    "mainDir_r5 = '/work/07665/shrey911/ls6/LRP_eccov4r5_data'\n",
    "gridDir  = mainDir_r5 + '/GRID'\n",
    "thetaDir = mainDir_r5 + '/SST_day_mean'\n",
    "thetaDir_ext = mainDir_r5 + '/SST_day_mean_ext_2020_2023_Jun'\n",
    "\n",
    "# For Sverdrup\n",
    "# ds_r4 = xr.open_dataset(f'/scratch2/shreyas/LRP_eccov4r4_data/thetaSurfECCOv4r4.nc')\n",
    "# For LS6\n",
    "ds_r4 = xr.open_dataset(mainDir_r4 + '/thetaSurfECCOv4r4.nc')\n",
    "\n",
    "# SSH has to be kept because someone used the SSH metadata for SST, \n",
    "# It's not a bug in this code but a hack to handle an existing bug.\n",
    "temp = xmitgcm.open_mdsdataset(data_dir = thetaDir,\n",
    "                             grid_dir = gridDir,\n",
    "                             extra_variables = dict(SSH = dict(dims=['k','j','i'],\n",
    "                                                               attrs = dict(standard_name=\"SST\",\n",
    "                                                                            long_name=\"Sea Surface Temperature\",\n",
    "                                                                            units=\"degC\"))))\n",
    "temp[\"SST\"] = temp[\"SSH\"]\n",
    "temp = temp.drop([\"SSH\"])\n",
    "    \n",
    "temp_ext = xmitgcm.open_mdsdataset(data_dir = thetaDir_ext,\n",
    "                                 grid_dir = gridDir,\n",
    "                                 extra_variables = dict(SST = dict(dims=['k','j','i'],\n",
    "                                                                   attrs = dict(standard_name=\"SST\",\n",
    "                                                                                long_name=\"Sea Surface Temperature\",\n",
    "                                                                                units=\"degC\"))))\n",
    "\n",
    "ds_r5 = xr.concat([temp, temp_ext], \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039de40-13c0-42bc-a503-11942de7e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SST = xr.open_dataset(mainDir_r5+'/SST_all.nc')\n",
    "SST = ds_SST['SST'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb3e1b-7581-4089-ad25-99bfbbce6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hFacC = ecco.read_llc_to_tiles(gridDir, 'hFacC.data')\n",
    "hFacC_mask = hFacC > 0\n",
    "hFacC_mask = hFacC_mask.astype(float)\n",
    "\n",
    "XC = ds_r4['XC'].data\n",
    "YC = ds_r4['YC'].data\n",
    "\n",
    "latMask = YC > -20.0\n",
    "latMask = latMask.astype(float)\n",
    "\n",
    "maskFinal = hFacC_mask * latMask\n",
    "NaNmaskFinal = np.copy(maskFinal)\n",
    "NaNmaskFinal[NaNmaskFinal == 0] = np.nan\n",
    "\n",
    "da_XC = xr.DataArray(\n",
    "    data=ds_r4['XC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"XC\"),\n",
    ")\n",
    "\n",
    "da_YC = xr.DataArray(\n",
    "    data=ds_r4['YC'].data,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"YC\"),\n",
    ")\n",
    "\n",
    "da_hFacC_mask = xr.DataArray(\n",
    "    data=hFacC_mask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"hFacC mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_latMask = xr.DataArray(\n",
    "    data=latMask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Latitude Mask 1 if > -20, else 0\"),\n",
    ")\n",
    "\n",
    "da_maskFinal = xr.DataArray(\n",
    "    data=maskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_NaNmaskFinal = xr.DataArray(\n",
    "    data=NaNmaskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D True if > 0, else NaN\"),\n",
    ")\n",
    "\n",
    "wetpoints = np.nonzero(maskFinal.data)\n",
    "da_wetpoints = xr.DataArray(\n",
    "    data=np.asarray(wetpoints),\n",
    "    dims=[\"wetpoints_dim\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        wetpoints_dim = np.arange(np.asarray(wetpoints).shape[0], dtype = int),\n",
    "        num_wetpoints = np.arange(np.asarray(wetpoints).shape[1], dtype = int),\n",
    "    ),\n",
    "    attrs=dict(description=\"indices of wetpoints in the order (tile, j, i) in the three rows\"),\n",
    ")\n",
    "\n",
    "da_SST = xr.DataArray(\n",
    "    data=SST,\n",
    "    dims=[\"time\", \"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        time = ds_r5['time'].data[:SST.shape[0]],\n",
    "        tile = ds_r4['tile'].data,\n",
    "        j    = ds_r4['j'].data,\n",
    "        i    = ds_r4['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"SST field in llc format\"),\n",
    ")\n",
    "\n",
    "ds = xr.Dataset()\n",
    "ds = ds.assign(XC           = da_XC,\n",
    "               YC           = da_YC,\n",
    "               hFacC_mask   = da_hFacC_mask,\n",
    "               latMask      = da_latMask,\n",
    "               maskFinal    = da_maskFinal,\n",
    "               NaNmaskFinal = da_NaNmaskFinal,\n",
    "               wetpoints    = da_wetpoints,\n",
    "               SST          = da_SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f1c11-9c00-48fb-bf00-0afd6b5678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize_new(field, num_years = 31, first_leap_year_idx = 0):\n",
    "    \n",
    "    leap_yr_offsets_jan_feb   = np.array([0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8])\n",
    "    leap_yr_offsets_after_feb = np.array([1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8])\n",
    "\n",
    "    if len(field.shape) > 1:\n",
    "        seasonal_trend = np.zeros((366, field.shape[1]))\n",
    "    else:\n",
    "        seasonal_trend = np.zeros((366,))\n",
    "\n",
    "    #### Calculate seasonal trend\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        same_cal_days_idx=[d+365*year+leap_yr_offsets_jan_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indexing\n",
    "    same_cal_days_idx=[365*year+59+int(year/4) for year in range(first_leap_year_idx,num_years,4)]\n",
    "    # Remove mean\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='constant', \n",
    "                                                    overwrite_data=False)\n",
    "    # Remove linear trend\n",
    "    field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                    axis=0, \n",
    "                                                    type='linear', \n",
    "                                                    overwrite_data=False)\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        same_cal_days_idx=[d-1+365*year+leap_yr_offsets_after_feb[year] for year in range(num_years)]\n",
    "        # Remove mean\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='constant', \n",
    "                                                        overwrite_data=False)\n",
    "        # Remove linear trend\n",
    "        field[same_cal_days_idx] = scipy.signal.detrend(field[same_cal_days_idx], \n",
    "                                                        axis=0, \n",
    "                                                        type='linear', \n",
    "                                                        overwrite_data=False)\n",
    "\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e4223-fad9-483a-a905-629ce467174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds['SST'].data[:,wetpoints[0],wetpoints[1],wetpoints[2]].copy()\n",
    "X = anomalize_new(X)\n",
    "X_full = X.copy()\n",
    "X = X[30:-30]\n",
    "\n",
    "y = ds['SST'].isel(tile = 10, j = 1, i = 43).data.copy()\n",
    "y = anomalize_new(y)\n",
    "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "y = np.convolve(y, np.ones(61)/61, mode='valid')\n",
    "oneHotCost = np.zeros((y.shape[0], 2), dtype = int)\n",
    "oneHotCost[:,0] = y >= 0.0\n",
    "oneHotCost[:,1] = y <  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23bff3-dea8-49fd-bade-db7a7cb87ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_X = xr.DataArray(\n",
    "    data=X,\n",
    "    dims=[\"time_allData\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data as matrix X; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_y = xr.DataArray(\n",
    "    data=y,\n",
    "    dims=[\"time_allData\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function y; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_X_full = xr.DataArray(\n",
    "    data=X_full,\n",
    "    dims=[\"time\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time          = ds['time'],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data without accounting for conv filter as matrix X_full; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_oneHotCost = xr.DataArray(\n",
    "    data=oneHotCost,\n",
    "    dims=[\"time_allData\", \"NN_output_dim\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        NN_output_dim = np.array([0,1]),\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function as one-hot vector.\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(X          = da_X,\n",
    "               y          = da_y,\n",
    "               X_full     = da_X_full,\n",
    "               oneHotCost = da_oneHotCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c15b0-3436-46fa-aa1c-a7894bb0b6c0",
   "metadata": {},
   "source": [
    "## Optimal Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b017273-5b15-4d62-a7ae-16381c5513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickSetup(X, y,\n",
    "               test_split_frac,\n",
    "               val_split_frac,\n",
    "               lagSteps,\n",
    "               desired_labels,\n",
    "               compute_loss,\n",
    "               OI_eta,\n",
    "               OI_epochs,\n",
    "               OI_print_freq,\n",
    "               decay_func = None,\n",
    "               **NNkwargs):\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    idx = int(X.shape[0]*(1-test_split_frac))\n",
    "    X_train = X[:idx]\n",
    "    oneHotCost_train = oneHotCost[:idx]\n",
    "    X_test = X[idx:]\n",
    "    oneHotCost_test = oneHotCost[idx:]\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    NNkwargs['optimizer'] = sgd\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    if lagSteps > 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[:-lagSteps], oneHotCost_train[lagSteps:], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    elif lagSteps == 0:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train, oneHotCost_train, \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "    else:\n",
    "        x_t, x_v, oneHotCost_t, oneHotCost_v = train_test_split(X_train[-lagSteps:], oneHotCost_train[:lagSteps], \n",
    "                                                                test_size=val_split_frac, shuffle= True, random_state=42)\n",
    "\n",
    "    K = TrainFullyConnectedNN(x_t, oneHotCost_t, validation_data = (x_v, oneHotCost_v), **NNkwargs)\n",
    "                   \n",
    "    best_model = K.quickTrain(decay_func)\n",
    "    \n",
    "    oi = OI(best_model, desired_labels, \n",
    "          compute_loss, OI_eta, OI_epochs, OI_print_freq)\n",
    "    \n",
    "    return oi\n",
    "\n",
    "def compute_loss(desired_labels, pred):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    return bce(desired_labels, pred)\n",
    "\n",
    "def OI(model, desired_labels_numpy, compute_loss, eta, max_iters, print_freq):\n",
    "    \n",
    "    inp_numpy = np.zeros((1, X.shape[1]), dtype = float)\n",
    "\n",
    "    if len(desired_labels_numpy.shape) == 1:\n",
    "        desired_labels_numpy = desired_labels_numpy[np.newaxis, :]\n",
    "        \n",
    "    desired_labels = tf.convert_to_tensor(desired_labels_numpy)\n",
    "    print(f\"Desired label : {desired_labels_numpy}\")\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        inp = tf.convert_to_tensor(inp_numpy)\n",
    "        grads = GradientDescent_useGradientTape(model, inp, desired_labels, compute_loss)\n",
    "        grads_numpy = np.squeeze(tf_to_numpy(grads))\n",
    "        inp_numpy[0,:] = inp_numpy[0,:] - eta*grads_numpy\n",
    "        if (i+1)%print_freq == 0:\n",
    "            print(f\"Iter {i+1}, Prediction {tf_to_numpy(model.predict(inp_numpy))}\")\n",
    "\n",
    "    optimal_input = np.zeros((13,90,90), dtype = float)\n",
    "    optimal_input[wetpoints[0], wetpoints[1], wetpoints[2]] = inp_numpy[0]\n",
    "    \n",
    "    return optimal_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171702a-a06c-4148-b84d-680a368626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = [{'size': X.shape[1], 'activation': None     , 'use_bias': None},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 10.0, 'l2_b_reg': 10.0},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01},\n",
    "          {'size': 2         , 'activation': 'softmax', 'use_bias': True, \n",
    "           'l1_w_reg': 0.0, 'l1_b_reg': 0.0, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01, 'bias_constraint': None}]\n",
    "\n",
    "Losses = [{'kind': 'categorical_crossentropy', 'weight': 1.0}]\n",
    "lagStepsList = [-60,-30,0,30,60,90,120,150,180]\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 50\n",
    "    lrate = initial_lrate * drop**np.floor((1+epoch)/epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "OI_dict = {}\n",
    "OI_eta = 0.9999\n",
    "OI_epochs = 8000\n",
    "OI_print_freq = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6386a6-f5f7-4784-a556-83efa68ecfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lagStepsList)):\n",
    "\n",
    "    print(f'Lag: {lagStepsList[i]} days, for Theta')\n",
    "    \n",
    "    NNkwargs = {'losses': Losses,\n",
    "                'metrics': [metricF1, # Custom f1 since metrics.F1Score(name='f1') is not available before tf v2.13\n",
    "                            'accuracy',\n",
    "                            'categorical_crossentropy',\n",
    "                            # metrics.CategoricalCrossentropy(name='cross_entropy'),  # (model's loss-L2 reg loss)\n",
    "                            # metrics.MeanSquaredError(name='brier_score'),\n",
    "                            # metrics.TruePositives(name='tp'),\n",
    "                            # metrics.FalsePositives(name='fp'),\n",
    "                            # metrics.TrueNegatives(name='tn'),\n",
    "                            # metrics.FalseNegatives(name='fn'), \n",
    "                            # metrics.BinaryAccuracy(name='accuracy'),\n",
    "                            # metrics.Precision(name='precision'),\n",
    "                            # metrics.Recall(name='recall'),\n",
    "                            # metrics.AUC(name='auc'),\n",
    "                            # metrics.AUC(name='prc', curve='PR'), # precision-recall curve],\n",
    "                           ],\n",
    "                'batch_size': 128, 'epochs': 500, #'validation_split': 0.1,\n",
    "                'filename': f'model{lagStepsList[i]}_OI_neg', 'dirname': os.path.abspath(''),\n",
    "                'random_nn_seed': 42, 'class_weight': class_weight,\n",
    "                'custom_objects': {'metricF1': metricF1}, 'verbose': 2}\n",
    "    \n",
    "    OI_dict[f'lag{lagStepsList[i]}'] = quickSetup(X, y, 2161.0/11263.0,\n",
    "                                             lagStepsList[i],\n",
    "                                             np.array([[0.0,1.0]]),\n",
    "                                             compute_loss,\n",
    "                                             OI_eta,\n",
    "                                             OI_epochs,\n",
    "                                             OI_print_freq,\n",
    "                                             step_decay,\n",
    "                                             layers = Layers, **NNkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33280d31-790f-434b-b5f5-fb696ddd1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      OI_dict[f'lag{lag}'],\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap='RdBu_r', \n",
    "                                      cmin = -2, cmax = 2,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"OI_neg lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Optimal Inputs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d702dec-c39c-4d03-a46e-479cfc2506df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_oi = xr.Dataset()\n",
    "\n",
    "ds_oi = ds_oi.assign(OI_minus60 = xr.DataArray(OI_dict['lag-60']),\n",
    "                     OI_minus30 = xr.DataArray(OI_dict['lag-30']),\n",
    "                     OI_0  = xr.DataArray(OI_dict['lag0']),\n",
    "                     OI_30 = xr.DataArray(OI_dict['lag30']),\n",
    "                     OI_60 = xr.DataArray(OI_dict['lag60']),\n",
    "                     OI_90 = xr.DataArray(OI_dict['lag90']),\n",
    "                     OI_120 = xr.DataArray(OI_dict['lag120']),\n",
    "                     OI_150 = xr.DataArray(OI_dict['lag150']),\n",
    "                     OI_180 = xr.DataArray(OI_dict['lag180']))\n",
    "\n",
    "ds_oi.to_netcdf('LRP_output_forHelen/OI_v4r5_neg_newAnomalies_shuffleVal_reweight.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
