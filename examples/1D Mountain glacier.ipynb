{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd7382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82c18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1000\r"
     ]
    }
   ],
   "source": [
    "def basal_topology_func(x):\n",
    "    b = 1.0 - 0.1*x\n",
    "    return b\n",
    "\n",
    "def solution(nx, nt, L, T, M, basal_topology_func):\n",
    "\n",
    "    if len(M) != nx + 1:\n",
    "        raise ValueError('M specified but len(M) != nx + 1')\n",
    "        \n",
    "    dx = L/nx\n",
    "    dt = T/nt\n",
    "    x = np.linspace(0,L,nx+1)\n",
    "    t = np.linspace(0,T,nt+1)\n",
    "\n",
    "    b = basal_topology_func(x)\n",
    "\n",
    "    A = 1e-16\n",
    "    rho = 920.0\n",
    "    g = 9.2 \n",
    "    n = 3\n",
    "\n",
    "    C = 2*A/(n+2) * (rho*g)**n * (1e3)**n\n",
    "\n",
    "    h = np.zeros((nx+1,nt+1))\n",
    "    H = np.zeros((nx+1,nt+1))\n",
    "    h[:,0] = b\n",
    "    h[0,:] = b[0]\n",
    "    h[-1,:] = b[-1]\n",
    "\n",
    "    H[:,0] = h[:,0] - b\n",
    "    H[0,:] = h[0,:] - b[0]\n",
    "    H[-1,:] = h[-1,:] - b[-1]\n",
    "\n",
    "    for i in range(1,len(t)):\n",
    "\n",
    "        D = C *((H[1:,i-1]+H[:nx,i-1])/2.0)**(n+2) * ((h[1:,i-1] - h[:nx,i-1])/dx)**(n-1)\n",
    "\n",
    "        phi = -D*(h[1:,i-1]-h[:nx,i-1])/dx\n",
    "\n",
    "        h[1:nx,i] = h[1:nx,i-1] + M[1:nx]*dt - dt/dx * (phi[1:]-phi[:nx-1])\n",
    "        h[1:nx,i] = (h[1:nx,i] < b[1:nx]) * b[1:nx] + (h[1:nx,i] >= b[1:nx]) * h[1:nx,i]\n",
    "        H[:,i] = np.maximum(h[:,i] - b, 0.)\n",
    "\n",
    "        if not np.any(H[:,i]>=0.0):\n",
    "            raise Exception(\"Something went wrong.\")\n",
    "            \n",
    "    Volume = np.sum(H)*dx\n",
    "    \n",
    "    return H[int(nx/2),-1], h[int(nx/2),-1], Volume\n",
    "\n",
    "L = 30.\n",
    "T = 10.\n",
    "nx = 300\n",
    "nt = 12000\n",
    "samples = 1000\n",
    "\n",
    "M_samples = 0.01*np.random.rand(samples, nx+1)\n",
    "H_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "Volume_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "\n",
    "for sample in range(samples):\n",
    "    if (sample+1) % 100 == 0:\n",
    "        print(f'Sample #{sample+1}', end='\\r')\n",
    "    H_samples[sample], _, Volume_samples[sample] = solution(nx, nt, L, T, M_samples[sample], basal_topology_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9844b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 20:01:25.351376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/gnu7/openmpi/netcdf/4.5.0/lib:/opt/ohpc/pub/libs/gnu7/openmpi/netcdf-fortran/4.4.4/lib:/opt/ohpc/pub/libs/gnu7/openmpi/hdf5/1.10.1/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64:/home/shreyas/lis-2.1.3/installation/lib:/share/jdk-16.0.1/lib::\n",
      "2023-07-28 20:01:25.351439: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-07-28 20:01:25.351477: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c5-3): /proc/driver/nvidia/version does not exist\n",
      "2023-07-28 20:01:25.352101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-28 20:01:25.377377: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/800 [=======================>......] - ETA: 0s - loss: 7.3425e-04 - mae: 0.0220\n",
      "Epoch 1: val_loss improved from inf to 0.00050, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 405us/sample - loss: 6.9064e-04 - mae: 0.0215 - val_loss: 4.9767e-04 - val_mae: 0.0186\n",
      "Epoch 2/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 4.8464e-04 - mae: 0.0190\n",
      "Epoch 2: val_loss improved from 0.00050 to 0.00048, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 4.8181e-04 - mae: 0.0190 - val_loss: 4.7527e-04 - val_mae: 0.0184\n",
      "Epoch 3/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 4.7855e-04 - mae: 0.0188\n",
      "Epoch 3: val_loss improved from 0.00048 to 0.00046, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 4.7767e-04 - mae: 0.0188 - val_loss: 4.5597e-04 - val_mae: 0.0182\n",
      "Epoch 4/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 4.6394e-04 - mae: 0.0186\n",
      "Epoch 4: val_loss did not improve from 0.00046\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 4.6324e-04 - mae: 0.0186 - val_loss: 4.7594e-04 - val_mae: 0.0182\n",
      "Epoch 5/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 4.5443e-04 - mae: 0.0183\n",
      "Epoch 5: val_loss improved from 0.00046 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 4.5049e-04 - mae: 0.0182 - val_loss: 4.4054e-04 - val_mae: 0.0179\n",
      "Epoch 6/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 4.1792e-04 - mae: 0.0176\n",
      "Epoch 6: val_loss improved from 0.00044 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 4.4355e-04 - mae: 0.0182 - val_loss: 4.3999e-04 - val_mae: 0.0180\n",
      "Epoch 7/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 4.1279e-04 - mae: 0.0176\n",
      "Epoch 7: val_loss improved from 0.00044 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 4.2739e-04 - mae: 0.0179 - val_loss: 4.3815e-04 - val_mae: 0.0175\n",
      "Epoch 8/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 4.0951e-04 - mae: 0.0172\n",
      "Epoch 8: val_loss improved from 0.00044 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 4.1032e-04 - mae: 0.0173 - val_loss: 4.1088e-04 - val_mae: 0.0172\n",
      "Epoch 9/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 3.8788e-04 - mae: 0.0168\n",
      "Epoch 9: val_loss did not improve from 0.00041\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 3.9093e-04 - mae: 0.0168 - val_loss: 4.2332e-04 - val_mae: 0.0172\n",
      "Epoch 10/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 3.8630e-04 - mae: 0.0169\n",
      "Epoch 10: val_loss improved from 0.00041 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 3.8776e-04 - mae: 0.0169 - val_loss: 3.9678e-04 - val_mae: 0.0170\n",
      "Epoch 11/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.6490e-04 - mae: 0.0164\n",
      "Epoch 11: val_loss improved from 0.00040 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 3.6486e-04 - mae: 0.0164 - val_loss: 3.9038e-04 - val_mae: 0.0169\n",
      "Epoch 12/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.4671e-04 - mae: 0.0160\n",
      "Epoch 12: val_loss improved from 0.00039 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 3.4326e-04 - mae: 0.0159 - val_loss: 3.6245e-04 - val_mae: 0.0160\n",
      "Epoch 13/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 3.2390e-04 - mae: 0.0156\n",
      "Epoch 13: val_loss improved from 0.00036 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 3.2856e-04 - mae: 0.0155 - val_loss: 3.5792e-04 - val_mae: 0.0158\n",
      "Epoch 14/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 3.0180e-04 - mae: 0.0148\n",
      "Epoch 14: val_loss improved from 0.00036 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 201us/sample - loss: 3.0250e-04 - mae: 0.0148 - val_loss: 3.3349e-04 - val_mae: 0.0153\n",
      "Epoch 15/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.8070e-04 - mae: 0.0143\n",
      "Epoch 15: val_loss improved from 0.00033 to 0.00032, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 2.7838e-04 - mae: 0.0143 - val_loss: 3.1852e-04 - val_mae: 0.0151\n",
      "Epoch 16/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.6094e-04 - mae: 0.0138\n",
      "Epoch 16: val_loss improved from 0.00032 to 0.00030, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 2.5822e-04 - mae: 0.0137 - val_loss: 3.0253e-04 - val_mae: 0.0147\n",
      "Epoch 17/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.4273e-04 - mae: 0.0132\n",
      "Epoch 17: val_loss improved from 0.00030 to 0.00029, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 201us/sample - loss: 2.3897e-04 - mae: 0.0131 - val_loss: 2.9274e-04 - val_mae: 0.0141\n",
      "Epoch 18/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.2549e-04 - mae: 0.0127\n",
      "Epoch 18: val_loss improved from 0.00029 to 0.00028, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 200us/sample - loss: 2.2168e-04 - mae: 0.0125 - val_loss: 2.8114e-04 - val_mae: 0.0138\n",
      "Epoch 19/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9716e-04 - mae: 0.0118\n",
      "Epoch 19: val_loss improved from 0.00028 to 0.00026, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.9441e-04 - mae: 0.0117 - val_loss: 2.6023e-04 - val_mae: 0.0133\n",
      "Epoch 20/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7898e-04 - mae: 0.0111\n",
      "Epoch 20: val_loss improved from 0.00026 to 0.00025, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.7923e-04 - mae: 0.0111 - val_loss: 2.5370e-04 - val_mae: 0.0130\n",
      "Epoch 21/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.6246e-04 - mae: 0.0106\n",
      "Epoch 21: val_loss improved from 0.00025 to 0.00023, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 1.6107e-04 - mae: 0.0105 - val_loss: 2.2629e-04 - val_mae: 0.0125\n",
      "Epoch 22/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.4730e-04 - mae: 0.0100\n",
      "Epoch 22: val_loss improved from 0.00023 to 0.00022, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 198us/sample - loss: 1.4595e-04 - mae: 0.0100 - val_loss: 2.1749e-04 - val_mae: 0.0120\n",
      "Epoch 23/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.2721e-04 - mae: 0.0093\n",
      "Epoch 23: val_loss did not improve from 0.00022\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.2770e-04 - mae: 0.0093 - val_loss: 2.1978e-04 - val_mae: 0.0124\n",
      "Epoch 24/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.2611e-04 - mae: 0.0093\n",
      "Epoch 24: val_loss improved from 0.00022 to 0.00018, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.2179e-04 - mae: 0.0090 - val_loss: 1.8470e-04 - val_mae: 0.0111\n",
      "Epoch 25/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.0161e-04 - mae: 0.0082\n",
      "Epoch 25: val_loss did not improve from 0.00018\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.0077e-04 - mae: 0.0082 - val_loss: 2.0373e-04 - val_mae: 0.0119\n",
      "Epoch 26/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/800 [===========================>..] - ETA: 0s - loss: 9.6415e-05 - mae: 0.0080\n",
      "Epoch 26: val_loss improved from 0.00018 to 0.00018, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 9.5982e-05 - mae: 0.0080 - val_loss: 1.7593e-04 - val_mae: 0.0109\n",
      "Epoch 27/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 8.8084e-05 - mae: 0.0076\n",
      "Epoch 27: val_loss improved from 0.00018 to 0.00016, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 202us/sample - loss: 8.8879e-05 - mae: 0.0077 - val_loss: 1.6000e-04 - val_mae: 0.0104\n",
      "Epoch 28/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 8.2216e-05 - mae: 0.0073\n",
      "Epoch 28: val_loss improved from 0.00016 to 0.00015, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 8.2655e-05 - mae: 0.0074 - val_loss: 1.4674e-04 - val_mae: 0.0098\n",
      "Epoch 29/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 7.8339e-05 - mae: 0.0072\n",
      "Epoch 29: val_loss improved from 0.00015 to 0.00014, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 7.7552e-05 - mae: 0.0071 - val_loss: 1.3798e-04 - val_mae: 0.0095\n",
      "Epoch 30/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 6.6747e-05 - mae: 0.0065\n",
      "Epoch 30: val_loss improved from 0.00014 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 6.6675e-05 - mae: 0.0065 - val_loss: 1.3290e-04 - val_mae: 0.0094\n",
      "Epoch 31/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 6.5707e-05 - mae: 0.0065\n",
      "Epoch 31: val_loss improved from 0.00013 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 6.4798e-05 - mae: 0.0065 - val_loss: 1.2553e-04 - val_mae: 0.0090\n",
      "Epoch 32/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 5.8620e-05 - mae: 0.0061\n",
      "Epoch 32: val_loss improved from 0.00013 to 0.00012, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 5.9679e-05 - mae: 0.0062 - val_loss: 1.1889e-04 - val_mae: 0.0088\n",
      "Epoch 33/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 5.9011e-05 - mae: 0.0061\n",
      "Epoch 33: val_loss improved from 0.00012 to 0.00011, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 5.8599e-05 - mae: 0.0061 - val_loss: 1.1468e-04 - val_mae: 0.0086\n",
      "Epoch 34/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 5.0773e-05 - mae: 0.0058\n",
      "Epoch 34: val_loss improved from 0.00011 to 0.00011, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 5.0105e-05 - mae: 0.0058 - val_loss: 1.0723e-04 - val_mae: 0.0084\n",
      "Epoch 35/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 4.3712e-05 - mae: 0.0054\n",
      "Epoch 35: val_loss improved from 0.00011 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 4.5974e-05 - mae: 0.0054 - val_loss: 9.9547e-05 - val_mae: 0.0080\n",
      "Epoch 36/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 4.5720e-05 - mae: 0.0054\n",
      "Epoch 36: val_loss improved from 0.00010 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 4.6299e-05 - mae: 0.0055 - val_loss: 9.5333e-05 - val_mae: 0.0078\n",
      "Epoch 37/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 4.2536e-05 - mae: 0.0052\n",
      "Epoch 37: val_loss improved from 0.00010 to 0.00009, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 4.3083e-05 - mae: 0.0053 - val_loss: 9.0707e-05 - val_mae: 0.0077\n",
      "Epoch 38/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 4.4401e-05 - mae: 0.0053\n",
      "Epoch 38: val_loss did not improve from 0.00009\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 4.4101e-05 - mae: 0.0053 - val_loss: 9.7382e-05 - val_mae: 0.0081\n",
      "Epoch 39/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 3.9782e-05 - mae: 0.0051\n",
      "Epoch 39: val_loss improved from 0.00009 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 3.9454e-05 - mae: 0.0050 - val_loss: 8.3473e-05 - val_mae: 0.0074\n",
      "Epoch 40/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.6105e-05 - mae: 0.0048\n",
      "Epoch 40: val_loss did not improve from 0.00008\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 3.7923e-05 - mae: 0.0049 - val_loss: 9.8743e-05 - val_mae: 0.0082\n",
      "Epoch 41/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 4.2494e-05 - mae: 0.0052\n",
      "Epoch 41: val_loss improved from 0.00008 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 4.2619e-05 - mae: 0.0052 - val_loss: 7.6826e-05 - val_mae: 0.0070\n",
      "Epoch 42/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 3.6305e-05 - mae: 0.0048\n",
      "Epoch 42: val_loss did not improve from 0.00008\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 3.6731e-05 - mae: 0.0048 - val_loss: 8.2357e-05 - val_mae: 0.0074\n",
      "Epoch 43/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 3.4053e-05 - mae: 0.0047\n",
      "Epoch 43: val_loss improved from 0.00008 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 3.4064e-05 - mae: 0.0047 - val_loss: 7.4949e-05 - val_mae: 0.0071\n",
      "Epoch 44/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 3.2206e-05 - mae: 0.0046\n",
      "Epoch 44: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 196us/sample - loss: 3.2728e-05 - mae: 0.0046 - val_loss: 7.3326e-05 - val_mae: 0.0068\n",
      "Epoch 45/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 3.1865e-05 - mae: 0.0045\n",
      "Epoch 45: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 220us/sample - loss: 3.1723e-05 - mae: 0.0045 - val_loss: 6.5482e-05 - val_mae: 0.0065\n",
      "Epoch 46/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.8311e-05 - mae: 0.0042\n",
      "Epoch 46: val_loss did not improve from 0.00007\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.8370e-05 - mae: 0.0042 - val_loss: 7.2119e-05 - val_mae: 0.0070\n",
      "Epoch 47/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.8649e-05 - mae: 0.0043\n",
      "Epoch 47: val_loss did not improve from 0.00007\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.8412e-05 - mae: 0.0042 - val_loss: 6.6857e-05 - val_mae: 0.0067\n",
      "Epoch 48/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 3.1172e-05 - mae: 0.0045\n",
      "Epoch 48: val_loss improved from 0.00007 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 200us/sample - loss: 2.9898e-05 - mae: 0.0044 - val_loss: 6.0498e-05 - val_mae: 0.0063\n",
      "Epoch 49/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.5644e-05 - mae: 0.0041\n",
      "Epoch 49: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.5647e-05 - mae: 0.0040 - val_loss: 7.0634e-05 - val_mae: 0.0069\n",
      "Epoch 50/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.4892e-05 - mae: 0.0040\n",
      "Epoch 50: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.4973e-05 - mae: 0.0040 - val_loss: 6.2113e-05 - val_mae: 0.0065\n",
      "Epoch 51/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.4004e-05 - mae: 0.0039\n",
      "Epoch 51: val_loss improved from 0.00006 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 171us/sample - loss: 2.4716e-05 - mae: 0.0040 - val_loss: 5.4152e-05 - val_mae: 0.0059\n",
      "Epoch 52/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.7401e-05 - mae: 0.0042\n",
      "Epoch 52: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 2.7289e-05 - mae: 0.0042 - val_loss: 5.3344e-05 - val_mae: 0.0059\n",
      "Epoch 53/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.5135e-05 - mae: 0.0040\n",
      "Epoch 53: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 2.5265e-05 - mae: 0.0040 - val_loss: 5.1720e-05 - val_mae: 0.0058\n",
      "Epoch 54/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.5446e-05 - mae: 0.0040\n",
      "Epoch 54: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.5828e-05 - mae: 0.0040 - val_loss: 5.3500e-05 - val_mae: 0.0059\n",
      "Epoch 55/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.2192e-05 - mae: 0.0037\n",
      "Epoch 55: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 2.3018e-05 - mae: 0.0038 - val_loss: 4.9683e-05 - val_mae: 0.0057\n",
      "Epoch 56/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.3827e-05 - mae: 0.0039\n",
      "Epoch 56: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 2.3827e-05 - mae: 0.0039 - val_loss: 4.9170e-05 - val_mae: 0.0057\n",
      "Epoch 57/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9624e-05 - mae: 0.0035\n",
      "Epoch 57: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.2601e-05 - mae: 0.0038 - val_loss: 6.0584e-05 - val_mae: 0.0064\n",
      "Epoch 58/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.1609e-05 - mae: 0.0037\n",
      "Epoch 58: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.2534e-05 - mae: 0.0038 - val_loss: 5.3018e-05 - val_mae: 0.0060\n",
      "Epoch 59/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.1703e-05 - mae: 0.0037\n",
      "Epoch 59: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 2.1592e-05 - mae: 0.0037 - val_loss: 4.6073e-05 - val_mae: 0.0055\n",
      "Epoch 60/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.2111e-05 - mae: 0.0038\n",
      "Epoch 60: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.1894e-05 - mae: 0.0038 - val_loss: 5.0053e-05 - val_mae: 0.0058\n",
      "Epoch 61/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.1036e-05 - mae: 0.0036\n",
      "Epoch 61: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 2.0685e-05 - mae: 0.0036 - val_loss: 4.6059e-05 - val_mae: 0.0055\n",
      "Epoch 62/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.2335e-05 - mae: 0.0037\n",
      "Epoch 62: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 2.2454e-05 - mae: 0.0038 - val_loss: 4.5245e-05 - val_mae: 0.0055\n",
      "Epoch 63/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.3694e-05 - mae: 0.0038\n",
      "Epoch 63: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.3742e-05 - mae: 0.0039 - val_loss: 5.0183e-05 - val_mae: 0.0059\n",
      "Epoch 64/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.2129e-05 - mae: 0.0038\n",
      "Epoch 64: val_loss improved from 0.00005 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 2.1998e-05 - mae: 0.0038 - val_loss: 4.2882e-05 - val_mae: 0.0053\n",
      "Epoch 65/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.0452e-05 - mae: 0.0036\n",
      "Epoch 65: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 2.0118e-05 - mae: 0.0036 - val_loss: 4.8053e-05 - val_mae: 0.0057\n",
      "Epoch 66/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0110e-05 - mae: 0.0036\n",
      "Epoch 66: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9995e-05 - mae: 0.0036 - val_loss: 4.2944e-05 - val_mae: 0.0054\n",
      "Epoch 67/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.4110e-05 - mae: 0.0039\n",
      "Epoch 67: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 2.3978e-05 - mae: 0.0039 - val_loss: 4.0948e-05 - val_mae: 0.0053\n",
      "Epoch 68/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9575e-05 - mae: 0.0035\n",
      "Epoch 68: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.2061e-05 - mae: 0.0037 - val_loss: 4.7394e-05 - val_mae: 0.0057\n",
      "Epoch 69/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9405e-05 - mae: 0.0035\n",
      "Epoch 69: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 2.0774e-05 - mae: 0.0036 - val_loss: 4.2108e-05 - val_mae: 0.0053\n",
      "Epoch 70/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.4484e-05 - mae: 0.0039\n",
      "Epoch 70: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.5025e-05 - mae: 0.0040 - val_loss: 5.6669e-05 - val_mae: 0.0060\n",
      "Epoch 71/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.4080e-05 - mae: 0.0039\n",
      "Epoch 71: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.3634e-05 - mae: 0.0038 - val_loss: 4.3905e-05 - val_mae: 0.0055\n",
      "Epoch 72/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.8934e-05 - mae: 0.0035\n",
      "Epoch 72: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.9920e-05 - mae: 0.0036 - val_loss: 5.4621e-05 - val_mae: 0.0061\n",
      "Epoch 73/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9870e-05 - mae: 0.0036\n",
      "Epoch 73: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.0483e-05 - mae: 0.0036 - val_loss: 4.5610e-05 - val_mae: 0.0056\n",
      "Epoch 74/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 2.0853e-05 - mae: 0.0036\n",
      "Epoch 74: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 2.0827e-05 - mae: 0.0036 - val_loss: 4.2830e-05 - val_mae: 0.0053\n",
      "Epoch 75/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 2.1969e-05 - mae: 0.0037\n",
      "Epoch 75: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 2.1888e-05 - mae: 0.0037 - val_loss: 4.1316e-05 - val_mae: 0.0053\n",
      "Epoch 76/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.0546e-05 - mae: 0.0036\n",
      "Epoch 76: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 201us/sample - loss: 2.0800e-05 - mae: 0.0036 - val_loss: 3.9779e-05 - val_mae: 0.0052\n",
      "Epoch 77/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9755e-05 - mae: 0.0035\n",
      "Epoch 77: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9795e-05 - mae: 0.0036 - val_loss: 5.6938e-05 - val_mae: 0.0062\n",
      "Epoch 78/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0681e-05 - mae: 0.0036\n",
      "Epoch 78: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0932e-05 - mae: 0.0036 - val_loss: 4.0416e-05 - val_mae: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7928e-05 - mae: 0.0034\n",
      "Epoch 79: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8626e-05 - mae: 0.0034 - val_loss: 4.3173e-05 - val_mae: 0.0054\n",
      "Epoch 80/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8857e-05 - mae: 0.0034\n",
      "Epoch 80: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9468e-05 - mae: 0.0035 - val_loss: 4.4289e-05 - val_mae: 0.0055\n",
      "Epoch 81/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9533e-05 - mae: 0.0035\n",
      "Epoch 81: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 1.9187e-05 - mae: 0.0035 - val_loss: 3.8939e-05 - val_mae: 0.0051\n",
      "Epoch 82/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.0515e-05 - mae: 0.0036\n",
      "Epoch 82: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 2.0088e-05 - mae: 0.0035 - val_loss: 3.9732e-05 - val_mae: 0.0052\n",
      "Epoch 83/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8788e-05 - mae: 0.0034\n",
      "Epoch 83: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.8453e-05 - mae: 0.0034 - val_loss: 4.0345e-05 - val_mae: 0.0052\n",
      "Epoch 84/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.0266e-05 - mae: 0.0036\n",
      "Epoch 84: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.0648e-05 - mae: 0.0036 - val_loss: 4.1816e-05 - val_mae: 0.0053\n",
      "Epoch 85/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8595e-05 - mae: 0.0034\n",
      "Epoch 85: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0531e-05 - mae: 0.0035 - val_loss: 4.1029e-05 - val_mae: 0.0052\n",
      "Epoch 86/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8799e-05 - mae: 0.0035\n",
      "Epoch 86: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.1052e-05 - mae: 0.0036 - val_loss: 3.8737e-05 - val_mae: 0.0051\n",
      "Epoch 87/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9684e-05 - mae: 0.0035\n",
      "Epoch 87: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9541e-05 - mae: 0.0035 - val_loss: 3.9860e-05 - val_mae: 0.0052\n",
      "Epoch 88/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.3852e-05 - mae: 0.0038\n",
      "Epoch 88: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.3170e-05 - mae: 0.0038 - val_loss: 3.8728e-05 - val_mae: 0.0051\n",
      "Epoch 89/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9163e-05 - mae: 0.0035\n",
      "Epoch 89: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.9288e-05 - mae: 0.0035 - val_loss: 4.1219e-05 - val_mae: 0.0053\n",
      "Epoch 90/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8082e-05 - mae: 0.0034\n",
      "Epoch 90: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.9258e-05 - mae: 0.0035 - val_loss: 3.8844e-05 - val_mae: 0.0051\n",
      "Epoch 91/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0203e-05 - mae: 0.0036\n",
      "Epoch 91: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0068e-05 - mae: 0.0036 - val_loss: 4.3471e-05 - val_mae: 0.0054\n",
      "Epoch 92/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9130e-05 - mae: 0.0035\n",
      "Epoch 92: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9130e-05 - mae: 0.0035 - val_loss: 3.9646e-05 - val_mae: 0.0052\n",
      "Epoch 93/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9076e-05 - mae: 0.0035\n",
      "Epoch 93: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9076e-05 - mae: 0.0035 - val_loss: 4.7122e-05 - val_mae: 0.0056\n",
      "Epoch 94/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8539e-05 - mae: 0.0034\n",
      "Epoch 94: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8288e-05 - mae: 0.0034 - val_loss: 4.2120e-05 - val_mae: 0.0054\n",
      "Epoch 95/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9194e-05 - mae: 0.0035\n",
      "Epoch 95: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9202e-05 - mae: 0.0035 - val_loss: 4.4444e-05 - val_mae: 0.0055\n",
      "Epoch 96/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8533e-05 - mae: 0.0034\n",
      "Epoch 96: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8621e-05 - mae: 0.0034 - val_loss: 3.8949e-05 - val_mae: 0.0052\n",
      "Epoch 97/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7722e-05 - mae: 0.0034\n",
      "Epoch 97: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9486e-05 - mae: 0.0035 - val_loss: 3.8828e-05 - val_mae: 0.0051\n",
      "Epoch 98/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7431e-05 - mae: 0.0033\n",
      "Epoch 98: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8792e-05 - mae: 0.0035 - val_loss: 4.6177e-05 - val_mae: 0.0056\n",
      "Epoch 99/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0318e-05 - mae: 0.0036\n",
      "Epoch 99: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 2.0200e-05 - mae: 0.0036 - val_loss: 3.8461e-05 - val_mae: 0.0051\n",
      "Epoch 100/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9501e-05 - mae: 0.0035\n",
      "Epoch 100: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.9491e-05 - mae: 0.0035 - val_loss: 3.8048e-05 - val_mae: 0.0051\n",
      "Epoch 101/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8522e-05 - mae: 0.0034\n",
      "Epoch 101: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 216us/sample - loss: 1.9070e-05 - mae: 0.0035 - val_loss: 3.8019e-05 - val_mae: 0.0051\n",
      "Epoch 102/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.0036e-05 - mae: 0.0036\n",
      "Epoch 102: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 2.0346e-05 - mae: 0.0036 - val_loss: 4.4779e-05 - val_mae: 0.0055\n",
      "Epoch 103/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0523e-05 - mae: 0.0036\n",
      "Epoch 103: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.0423e-05 - mae: 0.0036 - val_loss: 5.7057e-05 - val_mae: 0.0062\n",
      "Epoch 104/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.8460e-05 - mae: 0.0034\n",
      "Epoch 104: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9642e-05 - mae: 0.0035 - val_loss: 3.8163e-05 - val_mae: 0.0051\n",
      "Epoch 105/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9519e-05 - mae: 0.0035\n",
      "Epoch 105: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9519e-05 - mae: 0.0035 - val_loss: 3.8452e-05 - val_mae: 0.0051\n",
      "Epoch 106/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9915e-05 - mae: 0.0036\n",
      "Epoch 106: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0274e-05 - mae: 0.0036 - val_loss: 4.3667e-05 - val_mae: 0.0054\n",
      "Epoch 107/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9274e-05 - mae: 0.0035\n",
      "Epoch 107: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9121e-05 - mae: 0.0035 - val_loss: 4.0022e-05 - val_mae: 0.0052\n",
      "Epoch 108/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9436e-05 - mae: 0.0035\n",
      "Epoch 108: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0392e-05 - mae: 0.0036 - val_loss: 3.9107e-05 - val_mae: 0.0052\n",
      "Epoch 109/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8466e-05 - mae: 0.0034\n",
      "Epoch 109: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8628e-05 - mae: 0.0034 - val_loss: 3.9751e-05 - val_mae: 0.0052\n",
      "Epoch 110/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8463e-05 - mae: 0.0034\n",
      "Epoch 110: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0112e-05 - mae: 0.0035 - val_loss: 3.8728e-05 - val_mae: 0.0051\n",
      "Epoch 111/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9130e-05 - mae: 0.0035\n",
      "Epoch 111: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9045e-05 - mae: 0.0035 - val_loss: 3.9752e-05 - val_mae: 0.0052\n",
      "Epoch 112/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9668e-05 - mae: 0.0035\n",
      "Epoch 112: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9668e-05 - mae: 0.0035 - val_loss: 3.8854e-05 - val_mae: 0.0051\n",
      "Epoch 113/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9966e-05 - mae: 0.0035\n",
      "Epoch 113: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9560e-05 - mae: 0.0035 - val_loss: 3.9501e-05 - val_mae: 0.0052\n",
      "Epoch 114/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8984e-05 - mae: 0.0035\n",
      "Epoch 114: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8838e-05 - mae: 0.0034 - val_loss: 4.5234e-05 - val_mae: 0.0055\n",
      "Epoch 115/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9082e-05 - mae: 0.0034\n",
      "Epoch 115: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8294e-05 - mae: 0.0034 - val_loss: 3.8671e-05 - val_mae: 0.0051\n",
      "Epoch 116/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8711e-05 - mae: 0.0034\n",
      "Epoch 116: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8680e-05 - mae: 0.0034 - val_loss: 4.1779e-05 - val_mae: 0.0053\n",
      "Epoch 117/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8554e-05 - mae: 0.0034\n",
      "Epoch 117: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9279e-05 - mae: 0.0035 - val_loss: 4.5940e-05 - val_mae: 0.0056\n",
      "Epoch 118/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7739e-05 - mae: 0.0033\n",
      "Epoch 118: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7712e-05 - mae: 0.0033 - val_loss: 3.8706e-05 - val_mae: 0.0051\n",
      "Epoch 119/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1712e-05 - mae: 0.0037\n",
      "Epoch 119: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1937e-05 - mae: 0.0037 - val_loss: 4.6465e-05 - val_mae: 0.0056\n",
      "Epoch 120/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9678e-05 - mae: 0.0035\n",
      "Epoch 120: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0325e-05 - mae: 0.0036 - val_loss: 3.8426e-05 - val_mae: 0.0051\n",
      "Epoch 121/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8530e-05 - mae: 0.0034\n",
      "Epoch 121: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8499e-05 - mae: 0.0034 - val_loss: 4.2381e-05 - val_mae: 0.0052\n",
      "Epoch 122/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0588e-05 - mae: 0.0036\n",
      "Epoch 122: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.0450e-05 - mae: 0.0035 - val_loss: 4.3940e-05 - val_mae: 0.0053\n",
      "Epoch 123/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.4323e-05 - mae: 0.0040\n",
      "Epoch 123: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.3450e-05 - mae: 0.0039 - val_loss: 3.9697e-05 - val_mae: 0.0051\n",
      "Epoch 124/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9937e-05 - mae: 0.0036\n",
      "Epoch 124: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0366e-05 - mae: 0.0036 - val_loss: 4.4011e-05 - val_mae: 0.0054\n",
      "Epoch 125/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8880e-05 - mae: 0.0035\n",
      "Epoch 125: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9276e-05 - mae: 0.0035 - val_loss: 4.0763e-05 - val_mae: 0.0052\n",
      "Epoch 126/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2871e-05 - mae: 0.0039\n",
      "Epoch 126: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.1660e-05 - mae: 0.0037 - val_loss: 3.8645e-05 - val_mae: 0.0051\n",
      "Epoch 127/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8026e-05 - mae: 0.0033\n",
      "Epoch 127: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.2589e-05 - mae: 0.0037 - val_loss: 5.2938e-05 - val_mae: 0.0059\n",
      "Epoch 128/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9759e-05 - mae: 0.0036\n",
      "Epoch 128: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0887e-05 - mae: 0.0037 - val_loss: 3.9417e-05 - val_mae: 0.0052\n",
      "Epoch 129/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0088e-05 - mae: 0.0035\n",
      "Epoch 129: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1063e-05 - mae: 0.0036 - val_loss: 3.9008e-05 - val_mae: 0.0051\n",
      "Epoch 130/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7063e-05 - mae: 0.0033\n",
      "Epoch 130: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8166e-05 - mae: 0.0034 - val_loss: 3.9194e-05 - val_mae: 0.0052\n",
      "Epoch 131/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9430e-05 - mae: 0.0035\n",
      "Epoch 131: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8429e-05 - mae: 0.0034 - val_loss: 3.9276e-05 - val_mae: 0.0052\n",
      "Epoch 132/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9751e-05 - mae: 0.0035\n",
      "Epoch 132: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9751e-05 - mae: 0.0035 - val_loss: 3.9510e-05 - val_mae: 0.0052\n",
      "Epoch 133/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8630e-05 - mae: 0.0035\n",
      "Epoch 133: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8951e-05 - mae: 0.0035 - val_loss: 4.0143e-05 - val_mae: 0.0052\n",
      "Epoch 134/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1643e-05 - mae: 0.0037\n",
      "Epoch 134: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.1573e-05 - mae: 0.0037 - val_loss: 3.9921e-05 - val_mae: 0.0052\n",
      "Epoch 135/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.8318e-05 - mae: 0.0034\n",
      "Epoch 135: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 1.8567e-05 - mae: 0.0034 - val_loss: 4.5301e-05 - val_mae: 0.0055\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/800 [=======================>......] - ETA: 0s - loss: 1.8011e-05 - mae: 0.0033\n",
      "Epoch 136: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.8267e-05 - mae: 0.0034 - val_loss: 3.9155e-05 - val_mae: 0.0052\n",
      "Epoch 137/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.0494e-05 - mae: 0.0036\n",
      "Epoch 137: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 2.0526e-05 - mae: 0.0036 - val_loss: 4.2154e-05 - val_mae: 0.0053\n",
      "Epoch 138/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.8546e-05 - mae: 0.0034\n",
      "Epoch 138: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 1.8672e-05 - mae: 0.0034 - val_loss: 3.9675e-05 - val_mae: 0.0052\n",
      "Epoch 139/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9786e-05 - mae: 0.0035\n",
      "Epoch 139: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 2.0240e-05 - mae: 0.0036 - val_loss: 4.3854e-05 - val_mae: 0.0053\n",
      "Epoch 140/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0786e-05 - mae: 0.0036\n",
      "Epoch 140: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9834e-05 - mae: 0.0035 - val_loss: 4.2738e-05 - val_mae: 0.0054\n",
      "Epoch 141/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8710e-05 - mae: 0.0034\n",
      "Epoch 141: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8904e-05 - mae: 0.0035 - val_loss: 3.8361e-05 - val_mae: 0.0051\n",
      "Epoch 142/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9514e-05 - mae: 0.0035\n",
      "Epoch 142: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9934e-05 - mae: 0.0035 - val_loss: 5.6869e-05 - val_mae: 0.0062\n",
      "Epoch 143/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9842e-05 - mae: 0.0035\n",
      "Epoch 143: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.1266e-05 - mae: 0.0036 - val_loss: 3.8526e-05 - val_mae: 0.0051\n",
      "Epoch 144/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6730e-05 - mae: 0.0032\n",
      "Epoch 144: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8130e-05 - mae: 0.0033 - val_loss: 3.9688e-05 - val_mae: 0.0052\n",
      "Epoch 145/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8949e-05 - mae: 0.0035\n",
      "Epoch 145: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9188e-05 - mae: 0.0035 - val_loss: 4.1251e-05 - val_mae: 0.0053\n",
      "Epoch 146/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8540e-05 - mae: 0.0034\n",
      "Epoch 146: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8922e-05 - mae: 0.0034 - val_loss: 3.9154e-05 - val_mae: 0.0051\n",
      "Epoch 147/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8087e-05 - mae: 0.0034\n",
      "Epoch 147: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7838e-05 - mae: 0.0033 - val_loss: 3.9329e-05 - val_mae: 0.0052\n",
      "Epoch 148/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7689e-05 - mae: 0.0034\n",
      "Epoch 148: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0201e-05 - mae: 0.0036 - val_loss: 4.0654e-05 - val_mae: 0.0053\n",
      "Epoch 149/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1948e-05 - mae: 0.0037\n",
      "Epoch 149: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.1948e-05 - mae: 0.0037 - val_loss: 3.9128e-05 - val_mae: 0.0051\n",
      "Epoch 150/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0647e-05 - mae: 0.0036\n",
      "Epoch 150: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0413e-05 - mae: 0.0036 - val_loss: 4.1332e-05 - val_mae: 0.0052\n",
      "Epoch 151/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7163e-05 - mae: 0.0034\n",
      "Epoch 151: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9743e-05 - mae: 0.0036 - val_loss: 3.9524e-05 - val_mae: 0.0052\n",
      "Epoch 152/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8824e-05 - mae: 0.0034\n",
      "Epoch 152: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8239e-05 - mae: 0.0034 - val_loss: 3.8943e-05 - val_mae: 0.0051\n",
      "Epoch 153/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9028e-05 - mae: 0.0035\n",
      "Epoch 153: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9223e-05 - mae: 0.0035 - val_loss: 3.9463e-05 - val_mae: 0.0052\n",
      "Epoch 154/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6684e-05 - mae: 0.0032\n",
      "Epoch 154: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8430e-05 - mae: 0.0034 - val_loss: 4.0401e-05 - val_mae: 0.0052\n",
      "Epoch 155/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8356e-05 - mae: 0.0034\n",
      "Epoch 155: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0364e-05 - mae: 0.0036 - val_loss: 6.0811e-05 - val_mae: 0.0063\n",
      "Epoch 156/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1028e-05 - mae: 0.0036\n",
      "Epoch 156: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.1030e-05 - mae: 0.0036 - val_loss: 4.1249e-05 - val_mae: 0.0053\n",
      "Epoch 157/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1162e-05 - mae: 0.0036\n",
      "Epoch 157: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0091e-05 - mae: 0.0036 - val_loss: 5.0584e-05 - val_mae: 0.0057\n",
      "Epoch 158/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8150e-05 - mae: 0.0034\n",
      "Epoch 158: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8320e-05 - mae: 0.0034 - val_loss: 3.9195e-05 - val_mae: 0.0052\n",
      "Epoch 159/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1112e-05 - mae: 0.0037\n",
      "Epoch 159: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.1012e-05 - mae: 0.0037 - val_loss: 4.0172e-05 - val_mae: 0.0052\n",
      "Epoch 160/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8255e-05 - mae: 0.0033\n",
      "Epoch 160: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8001e-05 - mae: 0.0033 - val_loss: 4.1694e-05 - val_mae: 0.0053\n",
      "Epoch 161/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0130e-05 - mae: 0.0035\n",
      "Epoch 161: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0802e-05 - mae: 0.0036 - val_loss: 4.0608e-05 - val_mae: 0.0053\n",
      "Epoch 162/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1335e-05 - mae: 0.0037\n",
      "Epoch 162: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0972e-05 - mae: 0.0036 - val_loss: 5.7612e-05 - val_mae: 0.0062\n",
      "Epoch 163/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9000e-05 - mae: 0.0034\n",
      "Epoch 163: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.9457e-05 - mae: 0.0035 - val_loss: 4.2642e-05 - val_mae: 0.0054\n",
      "Epoch 164/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9066e-05 - mae: 0.0035\n",
      "Epoch 164: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8842e-05 - mae: 0.0034 - val_loss: 4.0267e-05 - val_mae: 0.0052\n",
      "Epoch 165/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/800 [==========================>...] - ETA: 0s - loss: 2.2464e-05 - mae: 0.0039\n",
      "Epoch 165: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.2467e-05 - mae: 0.0039 - val_loss: 4.4718e-05 - val_mae: 0.0055\n",
      "Epoch 166/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8581e-05 - mae: 0.0034\n",
      "Epoch 166: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9216e-05 - mae: 0.0035 - val_loss: 3.9489e-05 - val_mae: 0.0052\n",
      "Epoch 167/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0404e-05 - mae: 0.0036\n",
      "Epoch 167: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.0139e-05 - mae: 0.0036 - val_loss: 4.6787e-05 - val_mae: 0.0056\n",
      "Epoch 168/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.9005e-05 - mae: 0.0035\n",
      "Epoch 168: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.9511e-05 - mae: 0.0035 - val_loss: 3.9059e-05 - val_mae: 0.0052\n",
      "Epoch 169/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9847e-05 - mae: 0.0035\n",
      "Epoch 169: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.0340e-05 - mae: 0.0036 - val_loss: 4.4775e-05 - val_mae: 0.0055\n",
      "Epoch 170/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9076e-05 - mae: 0.0034\n",
      "Epoch 170: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9894e-05 - mae: 0.0035 - val_loss: 4.0079e-05 - val_mae: 0.0052\n",
      "Epoch 171/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9624e-05 - mae: 0.0035\n",
      "Epoch 171: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9366e-05 - mae: 0.0035 - val_loss: 3.9027e-05 - val_mae: 0.0051\n",
      "Epoch 172/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8220e-05 - mae: 0.0033\n",
      "Epoch 172: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8154e-05 - mae: 0.0034 - val_loss: 3.9491e-05 - val_mae: 0.0052\n",
      "Epoch 173/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8585e-05 - mae: 0.0034\n",
      "Epoch 173: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.9063e-05 - mae: 0.0034 - val_loss: 5.3470e-05 - val_mae: 0.0060\n",
      "Epoch 174/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8943e-05 - mae: 0.0035\n",
      "Epoch 174: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8775e-05 - mae: 0.0035 - val_loss: 4.2580e-05 - val_mae: 0.0054\n",
      "Epoch 175/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8501e-05 - mae: 0.0034\n",
      "Epoch 175: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8501e-05 - mae: 0.0034 - val_loss: 4.2165e-05 - val_mae: 0.0054\n",
      "Epoch 176/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0833e-05 - mae: 0.0036\n",
      "Epoch 176: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9210e-05 - mae: 0.0035 - val_loss: 4.1912e-05 - val_mae: 0.0053\n",
      "Epoch 177/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8916e-05 - mae: 0.0034\n",
      "Epoch 177: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8863e-05 - mae: 0.0034 - val_loss: 4.2237e-05 - val_mae: 0.0053\n",
      "Epoch 178/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1780e-05 - mae: 0.0037\n",
      "Epoch 178: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0116e-05 - mae: 0.0035 - val_loss: 4.8497e-05 - val_mae: 0.0057\n",
      "Epoch 179/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6152e-05 - mae: 0.0032\n",
      "Epoch 179: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8450e-05 - mae: 0.0034 - val_loss: 4.1063e-05 - val_mae: 0.0053\n",
      "Epoch 180/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7818e-05 - mae: 0.0034\n",
      "Epoch 180: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7818e-05 - mae: 0.0034 - val_loss: 3.9555e-05 - val_mae: 0.0052\n",
      "Epoch 181/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9379e-05 - mae: 0.0035\n",
      "Epoch 181: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9272e-05 - mae: 0.0035 - val_loss: 4.2176e-05 - val_mae: 0.0053\n",
      "Epoch 182/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8975e-05 - mae: 0.0035\n",
      "Epoch 182: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8919e-05 - mae: 0.0034 - val_loss: 4.2178e-05 - val_mae: 0.0053\n",
      "Epoch 183/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8817e-05 - mae: 0.0034\n",
      "Epoch 183: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8842e-05 - mae: 0.0034 - val_loss: 4.3721e-05 - val_mae: 0.0054\n",
      "Epoch 184/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0597e-05 - mae: 0.0036\n",
      "Epoch 184: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9346e-05 - mae: 0.0035 - val_loss: 3.9529e-05 - val_mae: 0.0052\n",
      "Epoch 185/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6918e-05 - mae: 0.0032\n",
      "Epoch 185: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8196e-05 - mae: 0.0034 - val_loss: 3.9863e-05 - val_mae: 0.0052\n",
      "Epoch 186/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7732e-05 - mae: 0.0034\n",
      "Epoch 186: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8154e-05 - mae: 0.0034 - val_loss: 3.9091e-05 - val_mae: 0.0051\n",
      "Epoch 187/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0353e-05 - mae: 0.0036\n",
      "Epoch 187: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1332e-05 - mae: 0.0036 - val_loss: 3.9123e-05 - val_mae: 0.0052\n",
      "Epoch 188/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7625e-05 - mae: 0.0034\n",
      "Epoch 188: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9542e-05 - mae: 0.0035 - val_loss: 3.9130e-05 - val_mae: 0.0052\n",
      "Epoch 189/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9617e-05 - mae: 0.0036\n",
      "Epoch 189: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0732e-05 - mae: 0.0036 - val_loss: 4.4165e-05 - val_mae: 0.0054\n",
      "Epoch 190/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9525e-05 - mae: 0.0035\n",
      "Epoch 190: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0048e-05 - mae: 0.0035 - val_loss: 3.8993e-05 - val_mae: 0.0051\n",
      "Epoch 191/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7135e-05 - mae: 0.0033\n",
      "Epoch 191: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0047e-05 - mae: 0.0035 - val_loss: 4.0634e-05 - val_mae: 0.0052\n",
      "Epoch 192/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8876e-05 - mae: 0.0035\n",
      "Epoch 192: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0038e-05 - mae: 0.0036 - val_loss: 4.7260e-05 - val_mae: 0.0056\n",
      "Epoch 193/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8914e-05 - mae: 0.0034\n",
      "Epoch 193: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7921e-05 - mae: 0.0033 - val_loss: 4.2875e-05 - val_mae: 0.0054\n",
      "Epoch 194/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5572e-05 - mae: 0.0031\n",
      "Epoch 194: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9845e-05 - mae: 0.0035 - val_loss: 3.8872e-05 - val_mae: 0.0051\n",
      "Epoch 195/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6917e-05 - mae: 0.0033\n",
      "Epoch 195: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7939e-05 - mae: 0.0033 - val_loss: 3.8805e-05 - val_mae: 0.0051\n",
      "Epoch 196/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7910e-05 - mae: 0.0034\n",
      "Epoch 196: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8392e-05 - mae: 0.0034 - val_loss: 4.0666e-05 - val_mae: 0.0052\n",
      "Epoch 197/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0023e-05 - mae: 0.0035\n",
      "Epoch 197: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9524e-05 - mae: 0.0035 - val_loss: 3.8555e-05 - val_mae: 0.0051\n",
      "Epoch 198/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9066e-05 - mae: 0.0034\n",
      "Epoch 198: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1164e-05 - mae: 0.0036 - val_loss: 3.9177e-05 - val_mae: 0.0051\n",
      "Epoch 199/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8742e-05 - mae: 0.0033\n",
      "Epoch 199: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8871e-05 - mae: 0.0034 - val_loss: 4.3975e-05 - val_mae: 0.0054\n",
      "Epoch 200/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9103e-05 - mae: 0.0034\n",
      "Epoch 200: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9024e-05 - mae: 0.0034 - val_loss: 3.9288e-05 - val_mae: 0.0052\n",
      "Epoch 201/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7772e-05 - mae: 0.0033\n",
      "Epoch 201: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8304e-05 - mae: 0.0034 - val_loss: 3.9704e-05 - val_mae: 0.0052\n",
      "Epoch 202/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7970e-05 - mae: 0.0034\n",
      "Epoch 202: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9576e-05 - mae: 0.0035 - val_loss: 3.9037e-05 - val_mae: 0.0051\n",
      "Epoch 203/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9638e-05 - mae: 0.0035\n",
      "Epoch 203: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9445e-05 - mae: 0.0035 - val_loss: 3.9706e-05 - val_mae: 0.0051\n",
      "Epoch 204/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0457e-05 - mae: 0.0037\n",
      "Epoch 204: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9539e-05 - mae: 0.0035 - val_loss: 4.3181e-05 - val_mae: 0.0054\n",
      "Epoch 205/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0428e-05 - mae: 0.0036\n",
      "Epoch 205: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0087e-05 - mae: 0.0036 - val_loss: 3.8515e-05 - val_mae: 0.0051\n",
      "Epoch 206/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7860e-05 - mae: 0.0034\n",
      "Epoch 206: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9128e-05 - mae: 0.0035 - val_loss: 4.0340e-05 - val_mae: 0.0052\n",
      "Epoch 207/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8471e-05 - mae: 0.0034\n",
      "Epoch 207: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8779e-05 - mae: 0.0034 - val_loss: 3.9022e-05 - val_mae: 0.0051\n",
      "Epoch 208/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0260e-05 - mae: 0.0036\n",
      "Epoch 208: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9865e-05 - mae: 0.0035 - val_loss: 3.8959e-05 - val_mae: 0.0051\n",
      "Epoch 209/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7756e-05 - mae: 0.0033\n",
      "Epoch 209: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9295e-05 - mae: 0.0035 - val_loss: 3.8301e-05 - val_mae: 0.0051\n",
      "Epoch 210/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7495e-05 - mae: 0.0033\n",
      "Epoch 210: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8801e-05 - mae: 0.0034 - val_loss: 3.8299e-05 - val_mae: 0.0051\n",
      "Epoch 211/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1228e-05 - mae: 0.0037\n",
      "Epoch 211: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0058e-05 - mae: 0.0036 - val_loss: 4.5005e-05 - val_mae: 0.0055\n",
      "Epoch 212/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8874e-05 - mae: 0.0034\n",
      "Epoch 212: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9981e-05 - mae: 0.0035 - val_loss: 4.0214e-05 - val_mae: 0.0052\n",
      "Epoch 213/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7902e-05 - mae: 0.0034\n",
      "Epoch 213: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8989e-05 - mae: 0.0035 - val_loss: 4.0962e-05 - val_mae: 0.0052\n",
      "Epoch 214/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9759e-05 - mae: 0.0034\n",
      "Epoch 214: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9775e-05 - mae: 0.0034 - val_loss: 3.9599e-05 - val_mae: 0.0052\n",
      "Epoch 215/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9437e-05 - mae: 0.0035\n",
      "Epoch 215: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8509e-05 - mae: 0.0034 - val_loss: 3.8695e-05 - val_mae: 0.0051\n",
      "Epoch 216/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8256e-05 - mae: 0.0034\n",
      "Epoch 216: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8498e-05 - mae: 0.0034 - val_loss: 4.0474e-05 - val_mae: 0.0052\n",
      "Epoch 217/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7787e-05 - mae: 0.0033\n",
      "Epoch 217: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9341e-05 - mae: 0.0035 - val_loss: 3.9234e-05 - val_mae: 0.0051\n",
      "Epoch 218/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8353e-05 - mae: 0.0034\n",
      "Epoch 218: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7452e-05 - mae: 0.0033 - val_loss: 3.8754e-05 - val_mae: 0.0051\n",
      "Epoch 219/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9392e-05 - mae: 0.0035\n",
      "Epoch 219: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9346e-05 - mae: 0.0035 - val_loss: 3.8356e-05 - val_mae: 0.0051\n",
      "Epoch 220/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8817e-05 - mae: 0.0035\n",
      "Epoch 220: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8870e-05 - mae: 0.0035 - val_loss: 4.1108e-05 - val_mae: 0.0052\n",
      "Epoch 221/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0611e-05 - mae: 0.0036\n",
      "Epoch 221: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 2.0757e-05 - mae: 0.0036 - val_loss: 3.9391e-05 - val_mae: 0.0052\n",
      "Epoch 222/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7037e-05 - mae: 0.0033\n",
      "Epoch 222: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7972e-05 - mae: 0.0033 - val_loss: 4.1020e-05 - val_mae: 0.0053\n",
      "Epoch 223/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 2.3457e-05 - mae: 0.0039\n",
      "Epoch 223: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1318e-05 - mae: 0.0037 - val_loss: 3.8881e-05 - val_mae: 0.0051\n",
      "Epoch 224/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8051e-05 - mae: 0.0034\n",
      "Epoch 224: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8749e-05 - mae: 0.0034 - val_loss: 3.9248e-05 - val_mae: 0.0052\n",
      "Epoch 225/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1043e-05 - mae: 0.0036\n",
      "Epoch 225: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.1112e-05 - mae: 0.0037 - val_loss: 4.0117e-05 - val_mae: 0.0052\n",
      "Epoch 226/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9685e-05 - mae: 0.0035\n",
      "Epoch 226: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0989e-05 - mae: 0.0036 - val_loss: 3.9020e-05 - val_mae: 0.0051\n",
      "Epoch 227/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8998e-05 - mae: 0.0034\n",
      "Epoch 227: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7891e-05 - mae: 0.0033 - val_loss: 4.0115e-05 - val_mae: 0.0052\n",
      "Epoch 228/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.2097e-05 - mae: 0.0038\n",
      "Epoch 228: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0809e-05 - mae: 0.0036 - val_loss: 3.9399e-05 - val_mae: 0.0052\n",
      "Epoch 229/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9122e-05 - mae: 0.0034\n",
      "Epoch 229: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 2.0501e-05 - mae: 0.0036 - val_loss: 3.9109e-05 - val_mae: 0.0051\n",
      "Epoch 230/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7318e-05 - mae: 0.0033\n",
      "Epoch 230: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8323e-05 - mae: 0.0034 - val_loss: 3.9315e-05 - val_mae: 0.0052\n",
      "Epoch 231/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8251e-05 - mae: 0.0034\n",
      "Epoch 231: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0571e-05 - mae: 0.0036 - val_loss: 3.8845e-05 - val_mae: 0.0051\n",
      "Epoch 232/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6906e-05 - mae: 0.0032\n",
      "Epoch 232: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8178e-05 - mae: 0.0033 - val_loss: 3.9143e-05 - val_mae: 0.0051\n",
      "Epoch 233/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7690e-05 - mae: 0.0033\n",
      "Epoch 233: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8314e-05 - mae: 0.0034 - val_loss: 3.8721e-05 - val_mae: 0.0051\n",
      "Epoch 234/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7830e-05 - mae: 0.0033\n",
      "Epoch 234: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8471e-05 - mae: 0.0034 - val_loss: 3.9145e-05 - val_mae: 0.0051\n",
      "Epoch 235/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.9164e-05 - mae: 0.0034\n",
      "Epoch 235: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.9180e-05 - mae: 0.0034 - val_loss: 3.9982e-05 - val_mae: 0.0051\n",
      "Epoch 236/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0579e-05 - mae: 0.0036\n",
      "Epoch 236: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 2.0891e-05 - mae: 0.0036 - val_loss: 3.8662e-05 - val_mae: 0.0051\n",
      "Epoch 237/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9643e-05 - mae: 0.0035\n",
      "Epoch 237: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9672e-05 - mae: 0.0035 - val_loss: 4.3527e-05 - val_mae: 0.0054\n",
      "Epoch 238/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9540e-05 - mae: 0.0035\n",
      "Epoch 238: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9488e-05 - mae: 0.0035 - val_loss: 4.0131e-05 - val_mae: 0.0052\n",
      "Epoch 239/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8100e-05 - mae: 0.0034\n",
      "Epoch 239: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8100e-05 - mae: 0.0034 - val_loss: 3.9765e-05 - val_mae: 0.0052\n",
      "Epoch 240/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7332e-05 - mae: 0.0033\n",
      "Epoch 240: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9804e-05 - mae: 0.0035 - val_loss: 4.5452e-05 - val_mae: 0.0055\n",
      "Epoch 241/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9904e-05 - mae: 0.0035\n",
      "Epoch 241: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9087e-05 - mae: 0.0035 - val_loss: 3.8723e-05 - val_mae: 0.0052\n",
      "Epoch 242/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8001e-05 - mae: 0.0033\n",
      "Epoch 242: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7717e-05 - mae: 0.0033 - val_loss: 3.8559e-05 - val_mae: 0.0051\n",
      "Epoch 243/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8649e-05 - mae: 0.0034\n",
      "Epoch 243: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8642e-05 - mae: 0.0034 - val_loss: 4.0509e-05 - val_mae: 0.0052\n",
      "Epoch 244/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8039e-05 - mae: 0.0034\n",
      "Epoch 244: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8225e-05 - mae: 0.0034 - val_loss: 3.9577e-05 - val_mae: 0.0052\n",
      "Epoch 245/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9655e-05 - mae: 0.0035\n",
      "Epoch 245: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9464e-05 - mae: 0.0035 - val_loss: 4.0941e-05 - val_mae: 0.0053\n",
      "Epoch 246/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9207e-05 - mae: 0.0035\n",
      "Epoch 246: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9207e-05 - mae: 0.0035 - val_loss: 4.3839e-05 - val_mae: 0.0054\n",
      "Epoch 247/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7893e-05 - mae: 0.0033\n",
      "Epoch 247: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8057e-05 - mae: 0.0033 - val_loss: 4.0061e-05 - val_mae: 0.0052\n",
      "Epoch 248/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9838e-05 - mae: 0.0036\n",
      "Epoch 248: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.1918e-05 - mae: 0.0037 - val_loss: 3.8595e-05 - val_mae: 0.0051\n",
      "Epoch 249/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8711e-05 - mae: 0.0034\n",
      "Epoch 249: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8599e-05 - mae: 0.0034 - val_loss: 3.9480e-05 - val_mae: 0.0051\n",
      "Epoch 250/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7336e-05 - mae: 0.0032\n",
      "Epoch 250: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8051e-05 - mae: 0.0033 - val_loss: 4.4112e-05 - val_mae: 0.0055\n",
      "Epoch 251/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8563e-05 - mae: 0.0034\n",
      "Epoch 251: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9865e-05 - mae: 0.0035 - val_loss: 3.9075e-05 - val_mae: 0.0052\n",
      "Epoch 252/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5644e-05 - mae: 0.0032\n",
      "Epoch 252: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7484e-05 - mae: 0.0033 - val_loss: 3.9221e-05 - val_mae: 0.0052\n",
      "Epoch 253/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8775e-05 - mae: 0.0034\n",
      "Epoch 253: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8226e-05 - mae: 0.0034 - val_loss: 4.3624e-05 - val_mae: 0.0054\n",
      "Epoch 254/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9422e-05 - mae: 0.0035\n",
      "Epoch 254: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8993e-05 - mae: 0.0034 - val_loss: 3.9798e-05 - val_mae: 0.0051\n",
      "Epoch 255/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1771e-05 - mae: 0.0037\n",
      "Epoch 255: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0606e-05 - mae: 0.0036 - val_loss: 3.9027e-05 - val_mae: 0.0051\n",
      "Epoch 256/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9684e-05 - mae: 0.0035\n",
      "Epoch 256: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8856e-05 - mae: 0.0035 - val_loss: 4.2727e-05 - val_mae: 0.0053\n",
      "Epoch 257/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 3.0682e-05 - mae: 0.0045\n",
      "Epoch 257: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.5200e-05 - mae: 0.0040 - val_loss: 3.8944e-05 - val_mae: 0.0051\n",
      "Epoch 258/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9170e-05 - mae: 0.0034\n",
      "Epoch 258: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0232e-05 - mae: 0.0036 - val_loss: 3.9122e-05 - val_mae: 0.0051\n",
      "Epoch 259/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8645e-05 - mae: 0.0034\n",
      "Epoch 259: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8645e-05 - mae: 0.0034 - val_loss: 3.9986e-05 - val_mae: 0.0052\n",
      "Epoch 260/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9093e-05 - mae: 0.0035\n",
      "Epoch 260: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9522e-05 - mae: 0.0035 - val_loss: 4.4068e-05 - val_mae: 0.0055\n",
      "Epoch 261/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9248e-05 - mae: 0.0034\n",
      "Epoch 261: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9462e-05 - mae: 0.0035 - val_loss: 4.5639e-05 - val_mae: 0.0054\n",
      "Epoch 262/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8501e-05 - mae: 0.0034\n",
      "Epoch 262: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9710e-05 - mae: 0.0035 - val_loss: 4.2466e-05 - val_mae: 0.0054\n",
      "Epoch 263/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.2556e-05 - mae: 0.0038\n",
      "Epoch 263: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1181e-05 - mae: 0.0037 - val_loss: 4.7373e-05 - val_mae: 0.0056\n",
      "Epoch 264/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9628e-05 - mae: 0.0035\n",
      "Epoch 264: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9335e-05 - mae: 0.0035 - val_loss: 4.5804e-05 - val_mae: 0.0055\n",
      "Epoch 265/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8918e-05 - mae: 0.0034\n",
      "Epoch 265: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8441e-05 - mae: 0.0034 - val_loss: 3.8890e-05 - val_mae: 0.0051\n",
      "Epoch 266/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7811e-05 - mae: 0.0033\n",
      "Epoch 266: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8460e-05 - mae: 0.0034 - val_loss: 4.0091e-05 - val_mae: 0.0051\n",
      "Epoch 267/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9053e-05 - mae: 0.0035\n",
      "Epoch 267: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8393e-05 - mae: 0.0034 - val_loss: 3.9226e-05 - val_mae: 0.0051\n",
      "Epoch 268/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7971e-05 - mae: 0.0033\n",
      "Epoch 268: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7668e-05 - mae: 0.0033 - val_loss: 5.4410e-05 - val_mae: 0.0060\n",
      "Epoch 269/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7106e-05 - mae: 0.0033\n",
      "Epoch 269: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8347e-05 - mae: 0.0034 - val_loss: 4.1173e-05 - val_mae: 0.0053\n",
      "Epoch 270/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8295e-05 - mae: 0.0034\n",
      "Epoch 270: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9831e-05 - mae: 0.0035 - val_loss: 4.0447e-05 - val_mae: 0.0052\n",
      "Epoch 271/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9877e-05 - mae: 0.0036\n",
      "Epoch 271: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8807e-05 - mae: 0.0034 - val_loss: 3.9212e-05 - val_mae: 0.0052\n",
      "Epoch 272/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9963e-05 - mae: 0.0035\n",
      "Epoch 272: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9732e-05 - mae: 0.0035 - val_loss: 4.4333e-05 - val_mae: 0.0053\n",
      "Epoch 273/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7965e-05 - mae: 0.0034\n",
      "Epoch 273: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9292e-05 - mae: 0.0035 - val_loss: 4.0677e-05 - val_mae: 0.0052\n",
      "Epoch 274/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9856e-05 - mae: 0.0035\n",
      "Epoch 274: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9856e-05 - mae: 0.0035 - val_loss: 3.9058e-05 - val_mae: 0.0051\n",
      "Epoch 275/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8557e-05 - mae: 0.0034\n",
      "Epoch 275: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8489e-05 - mae: 0.0034 - val_loss: 4.1622e-05 - val_mae: 0.0053\n",
      "Epoch 276/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8752e-05 - mae: 0.0034\n",
      "Epoch 276: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8993e-05 - mae: 0.0034 - val_loss: 4.0754e-05 - val_mae: 0.0052\n",
      "Epoch 277/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7289e-05 - mae: 0.0033\n",
      "Epoch 277: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9010e-05 - mae: 0.0035 - val_loss: 4.4452e-05 - val_mae: 0.0055\n",
      "Epoch 278/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3759e-05 - mae: 0.0039\n",
      "Epoch 278: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.1313e-05 - mae: 0.0037 - val_loss: 4.2397e-05 - val_mae: 0.0054\n",
      "Epoch 279/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8434e-05 - mae: 0.0033\n",
      "Epoch 279: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9190e-05 - mae: 0.0035 - val_loss: 3.9312e-05 - val_mae: 0.0051\n",
      "Epoch 280/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8659e-05 - mae: 0.0034\n",
      "Epoch 280: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0270e-05 - mae: 0.0036 - val_loss: 3.8909e-05 - val_mae: 0.0051\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/800 [================>.............] - ETA: 0s - loss: 1.6980e-05 - mae: 0.0032\n",
      "Epoch 281: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9367e-05 - mae: 0.0035 - val_loss: 3.8898e-05 - val_mae: 0.0051\n",
      "Epoch 282/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8032e-05 - mae: 0.0034\n",
      "Epoch 282: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8032e-05 - mae: 0.0034 - val_loss: 3.9154e-05 - val_mae: 0.0051\n",
      "Epoch 283/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5954e-05 - mae: 0.0031\n",
      "Epoch 283: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7513e-05 - mae: 0.0033 - val_loss: 3.9943e-05 - val_mae: 0.0052\n",
      "Epoch 284/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8347e-05 - mae: 0.0034\n",
      "Epoch 284: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.9218e-05 - mae: 0.0035 - val_loss: 4.3933e-05 - val_mae: 0.0054\n",
      "Epoch 285/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9259e-05 - mae: 0.0035\n",
      "Epoch 285: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9754e-05 - mae: 0.0035 - val_loss: 4.1955e-05 - val_mae: 0.0053\n",
      "Epoch 286/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8714e-05 - mae: 0.0034\n",
      "Epoch 286: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9493e-05 - mae: 0.0035 - val_loss: 4.0461e-05 - val_mae: 0.0052\n",
      "Epoch 287/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8975e-05 - mae: 0.0035\n",
      "Epoch 287: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8889e-05 - mae: 0.0035 - val_loss: 4.0475e-05 - val_mae: 0.0053\n",
      "Epoch 288/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7342e-05 - mae: 0.0033\n",
      "Epoch 288: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9138e-05 - mae: 0.0035 - val_loss: 4.2765e-05 - val_mae: 0.0054\n",
      "Epoch 289/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7372e-05 - mae: 0.0033\n",
      "Epoch 289: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8400e-05 - mae: 0.0034 - val_loss: 4.2886e-05 - val_mae: 0.0053\n",
      "Epoch 290/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9243e-05 - mae: 0.0034\n",
      "Epoch 290: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.8985e-05 - mae: 0.0034 - val_loss: 4.3881e-05 - val_mae: 0.0054\n",
      "Epoch 291/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7935e-05 - mae: 0.0034\n",
      "Epoch 291: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8163e-05 - mae: 0.0034 - val_loss: 3.9404e-05 - val_mae: 0.0052\n",
      "Epoch 292/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6825e-05 - mae: 0.0032\n",
      "Epoch 292: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8465e-05 - mae: 0.0034 - val_loss: 3.9129e-05 - val_mae: 0.0052\n",
      "Epoch 293/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6972e-05 - mae: 0.0032\n",
      "Epoch 293: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8330e-05 - mae: 0.0034 - val_loss: 3.9241e-05 - val_mae: 0.0051\n",
      "Epoch 294/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9336e-05 - mae: 0.0035\n",
      "Epoch 294: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9688e-05 - mae: 0.0035 - val_loss: 3.9534e-05 - val_mae: 0.0052\n",
      "Epoch 295/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0039e-05 - mae: 0.0035\n",
      "Epoch 295: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9139e-05 - mae: 0.0035 - val_loss: 4.2070e-05 - val_mae: 0.0054\n",
      "Epoch 296/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6234e-05 - mae: 0.0032\n",
      "Epoch 296: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8232e-05 - mae: 0.0034 - val_loss: 4.0182e-05 - val_mae: 0.0052\n",
      "Epoch 297/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8368e-05 - mae: 0.0033\n",
      "Epoch 297: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8978e-05 - mae: 0.0034 - val_loss: 3.8694e-05 - val_mae: 0.0051\n",
      "Epoch 298/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8990e-05 - mae: 0.0034\n",
      "Epoch 298: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8990e-05 - mae: 0.0034 - val_loss: 4.8452e-05 - val_mae: 0.0057\n",
      "Epoch 299/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7930e-05 - mae: 0.0033\n",
      "Epoch 299: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9167e-05 - mae: 0.0034 - val_loss: 3.9564e-05 - val_mae: 0.0052\n",
      "Epoch 300/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0236e-05 - mae: 0.0036\n",
      "Epoch 300: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0236e-05 - mae: 0.0036 - val_loss: 4.5715e-05 - val_mae: 0.0054\n",
      "Epoch 301/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.4966e-05 - mae: 0.0040\n",
      "Epoch 301: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.1599e-05 - mae: 0.0037 - val_loss: 3.8727e-05 - val_mae: 0.0051\n",
      "Epoch 302/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.7582e-05 - mae: 0.0034\n",
      "Epoch 302: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 1.8512e-05 - mae: 0.0034 - val_loss: 3.9720e-05 - val_mae: 0.0051\n",
      "Epoch 303/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9896e-05 - mae: 0.0035\n",
      "Epoch 303: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.0273e-05 - mae: 0.0036 - val_loss: 4.3560e-05 - val_mae: 0.0054\n",
      "Epoch 304/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9627e-05 - mae: 0.0035\n",
      "Epoch 304: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9670e-05 - mae: 0.0035 - val_loss: 4.2932e-05 - val_mae: 0.0054\n",
      "Epoch 305/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9536e-05 - mae: 0.0035\n",
      "Epoch 305: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.9587e-05 - mae: 0.0035 - val_loss: 3.8945e-05 - val_mae: 0.0051\n",
      "Epoch 306/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.9149e-05 - mae: 0.0034\n",
      "Epoch 306: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 1.9338e-05 - mae: 0.0035 - val_loss: 3.9076e-05 - val_mae: 0.0051\n",
      "Epoch 307/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.7639e-05 - mae: 0.0033\n",
      "Epoch 307: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 196us/sample - loss: 1.7694e-05 - mae: 0.0033 - val_loss: 4.8936e-05 - val_mae: 0.0057\n",
      "Epoch 308/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1031e-05 - mae: 0.0036\n",
      "Epoch 308: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.1031e-05 - mae: 0.0036 - val_loss: 4.2261e-05 - val_mae: 0.0052\n",
      "Epoch 309/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.5297e-05 - mae: 0.0040\n",
      "Epoch 309: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.5765e-05 - mae: 0.0041 - val_loss: 4.4368e-05 - val_mae: 0.0054\n",
      "Epoch 310/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8295e-05 - mae: 0.0034\n",
      "Epoch 310: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8355e-05 - mae: 0.0034 - val_loss: 4.0596e-05 - val_mae: 0.0053\n",
      "Epoch 311/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8684e-05 - mae: 0.0034\n",
      "Epoch 311: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8322e-05 - mae: 0.0034 - val_loss: 3.8567e-05 - val_mae: 0.0051\n",
      "Epoch 312/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0338e-05 - mae: 0.0036\n",
      "Epoch 312: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0256e-05 - mae: 0.0036 - val_loss: 4.0031e-05 - val_mae: 0.0052\n",
      "Epoch 313/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7566e-05 - mae: 0.0033\n",
      "Epoch 313: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.7608e-05 - mae: 0.0033 - val_loss: 4.2219e-05 - val_mae: 0.0054\n",
      "Epoch 314/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7773e-05 - mae: 0.0034\n",
      "Epoch 314: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7773e-05 - mae: 0.0034 - val_loss: 3.9088e-05 - val_mae: 0.0052\n",
      "Epoch 315/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7615e-05 - mae: 0.0033\n",
      "Epoch 315: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7985e-05 - mae: 0.0033 - val_loss: 4.6574e-05 - val_mae: 0.0056\n",
      "Epoch 316/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8462e-05 - mae: 0.0034\n",
      "Epoch 316: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.8621e-05 - mae: 0.0034 - val_loss: 4.2747e-05 - val_mae: 0.0054\n",
      "Epoch 317/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.0663e-05 - mae: 0.0036\n",
      "Epoch 317: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 2.0498e-05 - mae: 0.0036 - val_loss: 4.4218e-05 - val_mae: 0.0055\n",
      "Epoch 318/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6537e-05 - mae: 0.0032\n",
      "Epoch 318: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9262e-05 - mae: 0.0035 - val_loss: 3.9221e-05 - val_mae: 0.0052\n",
      "Epoch 319/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7461e-05 - mae: 0.0033\n",
      "Epoch 319: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7795e-05 - mae: 0.0033 - val_loss: 4.1769e-05 - val_mae: 0.0053\n",
      "Epoch 320/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7135e-05 - mae: 0.0032\n",
      "Epoch 320: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7979e-05 - mae: 0.0033 - val_loss: 3.9526e-05 - val_mae: 0.0052\n",
      "Epoch 321/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9809e-05 - mae: 0.0035\n",
      "Epoch 321: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0337e-05 - mae: 0.0036 - val_loss: 3.9865e-05 - val_mae: 0.0052\n",
      "Epoch 322/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0423e-05 - mae: 0.0036\n",
      "Epoch 322: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0297e-05 - mae: 0.0036 - val_loss: 4.1433e-05 - val_mae: 0.0053\n",
      "Epoch 323/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7813e-05 - mae: 0.0034\n",
      "Epoch 323: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8995e-05 - mae: 0.0035 - val_loss: 3.8898e-05 - val_mae: 0.0052\n",
      "Epoch 324/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6830e-05 - mae: 0.0033\n",
      "Epoch 324: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9464e-05 - mae: 0.0035 - val_loss: 3.8960e-05 - val_mae: 0.0052\n",
      "Epoch 325/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8042e-05 - mae: 0.0034\n",
      "Epoch 325: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9385e-05 - mae: 0.0035 - val_loss: 3.9173e-05 - val_mae: 0.0052\n",
      "Epoch 326/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.8177e-05 - mae: 0.0034\n",
      "Epoch 326: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.1115e-05 - mae: 0.0036 - val_loss: 3.9273e-05 - val_mae: 0.0052\n",
      "Epoch 327/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8138e-05 - mae: 0.0034\n",
      "Epoch 327: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8335e-05 - mae: 0.0034 - val_loss: 3.9829e-05 - val_mae: 0.0052\n",
      "Epoch 328/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5452e-05 - mae: 0.0031\n",
      "Epoch 328: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1376e-05 - mae: 0.0037 - val_loss: 4.7608e-05 - val_mae: 0.0056\n",
      "Epoch 329/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.2008e-05 - mae: 0.0037\n",
      "Epoch 329: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0780e-05 - mae: 0.0036 - val_loss: 3.9231e-05 - val_mae: 0.0052\n",
      "Epoch 330/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7687e-05 - mae: 0.0034\n",
      "Epoch 330: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8100e-05 - mae: 0.0034 - val_loss: 3.9697e-05 - val_mae: 0.0052\n",
      "Epoch 331/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0006e-05 - mae: 0.0035\n",
      "Epoch 331: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0006e-05 - mae: 0.0035 - val_loss: 4.0955e-05 - val_mae: 0.0053\n",
      "Epoch 332/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8073e-05 - mae: 0.0034\n",
      "Epoch 332: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8183e-05 - mae: 0.0034 - val_loss: 3.8955e-05 - val_mae: 0.0051\n",
      "Epoch 333/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6311e-05 - mae: 0.0032\n",
      "Epoch 333: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8585e-05 - mae: 0.0034 - val_loss: 4.8794e-05 - val_mae: 0.0057\n",
      "Epoch 334/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1574e-05 - mae: 0.0037\n",
      "Epoch 334: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9705e-05 - mae: 0.0035 - val_loss: 3.9445e-05 - val_mae: 0.0051\n",
      "Epoch 335/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9404e-05 - mae: 0.0035\n",
      "Epoch 335: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9359e-05 - mae: 0.0035 - val_loss: 4.1093e-05 - val_mae: 0.0053\n",
      "Epoch 336/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.7816e-05 - mae: 0.0034\n",
      "Epoch 336: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 1.8064e-05 - mae: 0.0034 - val_loss: 3.9403e-05 - val_mae: 0.0052\n",
      "Epoch 337/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7881e-05 - mae: 0.0033\n",
      "Epoch 337: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.7924e-05 - mae: 0.0033 - val_loss: 3.9716e-05 - val_mae: 0.0052\n",
      "Epoch 338/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7910e-05 - mae: 0.0033\n",
      "Epoch 338: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.7809e-05 - mae: 0.0033 - val_loss: 3.9739e-05 - val_mae: 0.0052\n",
      "Epoch 339/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8315e-05 - mae: 0.0034\n",
      "Epoch 339: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9237e-05 - mae: 0.0034 - val_loss: 4.9379e-05 - val_mae: 0.0057\n",
      "Epoch 340/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7528e-05 - mae: 0.0034\n",
      "Epoch 340: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8278e-05 - mae: 0.0034 - val_loss: 4.2994e-05 - val_mae: 0.0054\n",
      "Epoch 341/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7840e-05 - mae: 0.0033\n",
      "Epoch 341: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9816e-05 - mae: 0.0035 - val_loss: 4.7506e-05 - val_mae: 0.0056\n",
      "Epoch 342/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8892e-05 - mae: 0.0035\n",
      "Epoch 342: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9443e-05 - mae: 0.0035 - val_loss: 3.8930e-05 - val_mae: 0.0051\n",
      "Epoch 343/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0596e-05 - mae: 0.0036\n",
      "Epoch 343: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.9575e-05 - mae: 0.0035 - val_loss: 4.0384e-05 - val_mae: 0.0052\n",
      "Epoch 344/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6178e-05 - mae: 0.0032\n",
      "Epoch 344: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7707e-05 - mae: 0.0033 - val_loss: 4.8831e-05 - val_mae: 0.0057\n",
      "Epoch 345/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.2727e-05 - mae: 0.0038\n",
      "Epoch 345: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.1294e-05 - mae: 0.0036 - val_loss: 3.8708e-05 - val_mae: 0.0051\n",
      "Epoch 346/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8491e-05 - mae: 0.0034\n",
      "Epoch 346: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9424e-05 - mae: 0.0035 - val_loss: 5.0815e-05 - val_mae: 0.0058\n",
      "Epoch 347/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8859e-05 - mae: 0.0034\n",
      "Epoch 347: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.9175e-05 - mae: 0.0035 - val_loss: 3.9483e-05 - val_mae: 0.0051\n",
      "Epoch 348/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7552e-05 - mae: 0.0033\n",
      "Epoch 348: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7488e-05 - mae: 0.0033 - val_loss: 3.8831e-05 - val_mae: 0.0051\n",
      "Epoch 349/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7063e-05 - mae: 0.0032\n",
      "Epoch 349: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7304e-05 - mae: 0.0033 - val_loss: 3.9671e-05 - val_mae: 0.0052\n",
      "Epoch 350/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8262e-05 - mae: 0.0034\n",
      "Epoch 350: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7855e-05 - mae: 0.0034 - val_loss: 4.1578e-05 - val_mae: 0.0053\n",
      "Epoch 351/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7277e-05 - mae: 0.0033\n",
      "Epoch 351: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8207e-05 - mae: 0.0034 - val_loss: 4.4096e-05 - val_mae: 0.0054\n",
      "Epoch 352/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8798e-05 - mae: 0.0034\n",
      "Epoch 352: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9387e-05 - mae: 0.0035 - val_loss: 3.9475e-05 - val_mae: 0.0052\n",
      "Epoch 353/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8286e-05 - mae: 0.0033\n",
      "Epoch 353: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8465e-05 - mae: 0.0034 - val_loss: 3.9195e-05 - val_mae: 0.0052\n",
      "Epoch 354/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8353e-05 - mae: 0.0034\n",
      "Epoch 354: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9049e-05 - mae: 0.0035 - val_loss: 3.9018e-05 - val_mae: 0.0052\n",
      "Epoch 355/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7493e-05 - mae: 0.0032\n",
      "Epoch 355: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0298e-05 - mae: 0.0035 - val_loss: 4.2081e-05 - val_mae: 0.0052\n",
      "Epoch 356/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6846e-05 - mae: 0.0032\n",
      "Epoch 356: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8552e-05 - mae: 0.0034 - val_loss: 4.1932e-05 - val_mae: 0.0053\n",
      "Epoch 357/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9375e-05 - mae: 0.0035\n",
      "Epoch 357: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 2.0267e-05 - mae: 0.0036 - val_loss: 4.0788e-05 - val_mae: 0.0053\n",
      "Epoch 358/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1440e-05 - mae: 0.0037\n",
      "Epoch 358: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9845e-05 - mae: 0.0036 - val_loss: 3.8785e-05 - val_mae: 0.0051\n",
      "Epoch 359/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8120e-05 - mae: 0.0034\n",
      "Epoch 359: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8783e-05 - mae: 0.0034 - val_loss: 4.5929e-05 - val_mae: 0.0055\n",
      "Epoch 360/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8310e-05 - mae: 0.0034\n",
      "Epoch 360: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9566e-05 - mae: 0.0035 - val_loss: 3.9802e-05 - val_mae: 0.0052\n",
      "Epoch 361/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7672e-05 - mae: 0.0034\n",
      "Epoch 361: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8280e-05 - mae: 0.0034 - val_loss: 4.0602e-05 - val_mae: 0.0052\n",
      "Epoch 362/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9695e-05 - mae: 0.0036\n",
      "Epoch 362: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9598e-05 - mae: 0.0035 - val_loss: 3.9076e-05 - val_mae: 0.0051\n",
      "Epoch 363/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.5688e-05 - mae: 0.0040\n",
      "Epoch 363: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 2.1966e-05 - mae: 0.0037 - val_loss: 4.0120e-05 - val_mae: 0.0052\n",
      "Epoch 364/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.0780e-05 - mae: 0.0036\n",
      "Epoch 364: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 2.0603e-05 - mae: 0.0036 - val_loss: 3.9434e-05 - val_mae: 0.0052\n",
      "Epoch 365/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9156e-05 - mae: 0.0035\n",
      "Epoch 365: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9306e-05 - mae: 0.0035 - val_loss: 4.1042e-05 - val_mae: 0.0053\n",
      "Epoch 366/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9615e-05 - mae: 0.0035\n",
      "Epoch 366: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.9566e-05 - mae: 0.0035 - val_loss: 4.8475e-05 - val_mae: 0.0055\n",
      "Epoch 367/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8771e-05 - mae: 0.0035\n",
      "Epoch 367: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8418e-05 - mae: 0.0034 - val_loss: 3.8729e-05 - val_mae: 0.0051\n",
      "Epoch 368/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9289e-05 - mae: 0.0035\n",
      "Epoch 368: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9432e-05 - mae: 0.0035 - val_loss: 3.9289e-05 - val_mae: 0.0051\n",
      "Epoch 369/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9501e-05 - mae: 0.0035\n",
      "Epoch 369: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9513e-05 - mae: 0.0035 - val_loss: 3.9352e-05 - val_mae: 0.0052\n",
      "Epoch 370/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7013e-05 - mae: 0.0032\n",
      "Epoch 370: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7286e-05 - mae: 0.0033 - val_loss: 3.8860e-05 - val_mae: 0.0051\n",
      "Epoch 371/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7759e-05 - mae: 0.0033\n",
      "Epoch 371: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7646e-05 - mae: 0.0033 - val_loss: 4.4171e-05 - val_mae: 0.0053\n",
      "Epoch 372/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9089e-05 - mae: 0.0034\n",
      "Epoch 372: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9287e-05 - mae: 0.0035 - val_loss: 4.2675e-05 - val_mae: 0.0054\n",
      "Epoch 373/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.6649e-05 - mae: 0.0033\n",
      "Epoch 373: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8622e-05 - mae: 0.0034 - val_loss: 3.8649e-05 - val_mae: 0.0051\n",
      "Epoch 374/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9437e-05 - mae: 0.0035\n",
      "Epoch 374: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9308e-05 - mae: 0.0035 - val_loss: 4.5717e-05 - val_mae: 0.0055\n",
      "Epoch 375/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9779e-05 - mae: 0.0036\n",
      "Epoch 375: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9706e-05 - mae: 0.0036 - val_loss: 4.0163e-05 - val_mae: 0.0051\n",
      "Epoch 376/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6091e-05 - mae: 0.0032\n",
      "Epoch 376: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9464e-05 - mae: 0.0035 - val_loss: 4.7846e-05 - val_mae: 0.0055\n",
      "Epoch 377/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.3481e-05 - mae: 0.0039\n",
      "Epoch 377: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.1527e-05 - mae: 0.0037 - val_loss: 3.8712e-05 - val_mae: 0.0051\n",
      "Epoch 378/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0079e-05 - mae: 0.0035\n",
      "Epoch 378: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9328e-05 - mae: 0.0035 - val_loss: 4.1747e-05 - val_mae: 0.0053\n",
      "Epoch 379/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9289e-05 - mae: 0.0035\n",
      "Epoch 379: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9761e-05 - mae: 0.0035 - val_loss: 5.0257e-05 - val_mae: 0.0058\n",
      "Epoch 380/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0136e-05 - mae: 0.0036\n",
      "Epoch 380: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9820e-05 - mae: 0.0035 - val_loss: 3.8815e-05 - val_mae: 0.0051\n",
      "Epoch 381/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5933e-05 - mae: 0.0031\n",
      "Epoch 381: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7722e-05 - mae: 0.0033 - val_loss: 3.8809e-05 - val_mae: 0.0051\n",
      "Epoch 382/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5337e-05 - mae: 0.0031\n",
      "Epoch 382: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7584e-05 - mae: 0.0033 - val_loss: 4.2857e-05 - val_mae: 0.0054\n",
      "Epoch 383/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.0460e-05 - mae: 0.0036\n",
      "Epoch 383: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.9134e-05 - mae: 0.0035 - val_loss: 3.9893e-05 - val_mae: 0.0052\n",
      "Epoch 384/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7132e-05 - mae: 0.0032\n",
      "Epoch 384: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8152e-05 - mae: 0.0034 - val_loss: 4.5356e-05 - val_mae: 0.0055\n",
      "Epoch 385/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0044e-05 - mae: 0.0035\n",
      "Epoch 385: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8734e-05 - mae: 0.0034 - val_loss: 4.0125e-05 - val_mae: 0.0052\n",
      "Epoch 386/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1949e-05 - mae: 0.0037\n",
      "Epoch 386: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9571e-05 - mae: 0.0035 - val_loss: 3.8929e-05 - val_mae: 0.0051\n",
      "Epoch 387/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6885e-05 - mae: 0.0032\n",
      "Epoch 387: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8126e-05 - mae: 0.0034 - val_loss: 4.1565e-05 - val_mae: 0.0053\n",
      "Epoch 388/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6216e-05 - mae: 0.0032\n",
      "Epoch 388: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8158e-05 - mae: 0.0034 - val_loss: 3.9279e-05 - val_mae: 0.0051\n",
      "Epoch 389/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7288e-05 - mae: 0.0033\n",
      "Epoch 389: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8343e-05 - mae: 0.0034 - val_loss: 3.9361e-05 - val_mae: 0.0052\n",
      "Epoch 390/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8008e-05 - mae: 0.0034\n",
      "Epoch 390: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7862e-05 - mae: 0.0034 - val_loss: 3.9277e-05 - val_mae: 0.0051\n",
      "Epoch 391/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8915e-05 - mae: 0.0034\n",
      "Epoch 391: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9829e-05 - mae: 0.0035 - val_loss: 4.8414e-05 - val_mae: 0.0057\n",
      "Epoch 392/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8584e-05 - mae: 0.0034\n",
      "Epoch 392: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9289e-05 - mae: 0.0035 - val_loss: 3.8662e-05 - val_mae: 0.0051\n",
      "Epoch 393/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9364e-05 - mae: 0.0034\n",
      "Epoch 393: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.9318e-05 - mae: 0.0034 - val_loss: 3.9554e-05 - val_mae: 0.0051\n",
      "Epoch 394/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6859e-05 - mae: 0.0033\n",
      "Epoch 394: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7979e-05 - mae: 0.0033 - val_loss: 3.9997e-05 - val_mae: 0.0052\n",
      "Epoch 395/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9289e-05 - mae: 0.0034\n",
      "Epoch 395: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9163e-05 - mae: 0.0034 - val_loss: 3.9320e-05 - val_mae: 0.0052\n",
      "Epoch 396/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7603e-05 - mae: 0.0034\n",
      "Epoch 396: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8933e-05 - mae: 0.0034 - val_loss: 3.9218e-05 - val_mae: 0.0052\n",
      "Epoch 397/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8839e-05 - mae: 0.0034\n",
      "Epoch 397: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1246e-05 - mae: 0.0037 - val_loss: 4.3344e-05 - val_mae: 0.0053\n",
      "Epoch 398/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9432e-05 - mae: 0.0035\n",
      "Epoch 398: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0327e-05 - mae: 0.0035 - val_loss: 4.0692e-05 - val_mae: 0.0052\n",
      "Epoch 399/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6622e-05 - mae: 0.0032\n",
      "Epoch 399: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7691e-05 - mae: 0.0033 - val_loss: 3.9283e-05 - val_mae: 0.0052\n",
      "Epoch 400/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8916e-05 - mae: 0.0034\n",
      "Epoch 400: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7785e-05 - mae: 0.0033 - val_loss: 3.8929e-05 - val_mae: 0.0051\n",
      "Epoch 401/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9782e-05 - mae: 0.0035\n",
      "Epoch 401: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7682e-05 - mae: 0.0033 - val_loss: 3.9071e-05 - val_mae: 0.0051\n",
      "Epoch 402/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0666e-05 - mae: 0.0036\n",
      "Epoch 402: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0460e-05 - mae: 0.0036 - val_loss: 4.9409e-05 - val_mae: 0.0057\n",
      "Epoch 403/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9361e-05 - mae: 0.0035\n",
      "Epoch 403: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.9339e-05 - mae: 0.0035 - val_loss: 3.9284e-05 - val_mae: 0.0052\n",
      "Epoch 404/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.8120e-05 - mae: 0.0034\n",
      "Epoch 404: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.8230e-05 - mae: 0.0034 - val_loss: 4.1070e-05 - val_mae: 0.0052\n",
      "Epoch 405/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8594e-05 - mae: 0.0034\n",
      "Epoch 405: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8405e-05 - mae: 0.0034 - val_loss: 4.0404e-05 - val_mae: 0.0052\n",
      "Epoch 406/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8952e-05 - mae: 0.0035\n",
      "Epoch 406: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7708e-05 - mae: 0.0033 - val_loss: 3.8655e-05 - val_mae: 0.0051\n",
      "Epoch 407/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9285e-05 - mae: 0.0035\n",
      "Epoch 407: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9159e-05 - mae: 0.0035 - val_loss: 4.8339e-05 - val_mae: 0.0057\n",
      "Epoch 408/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9162e-05 - mae: 0.0035\n",
      "Epoch 408: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9314e-05 - mae: 0.0035 - val_loss: 5.2893e-05 - val_mae: 0.0059\n",
      "Epoch 409/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0237e-05 - mae: 0.0035\n",
      "Epoch 409: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0167e-05 - mae: 0.0036 - val_loss: 3.8462e-05 - val_mae: 0.0051\n",
      "Epoch 410/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9815e-05 - mae: 0.0035\n",
      "Epoch 410: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9702e-05 - mae: 0.0035 - val_loss: 3.8849e-05 - val_mae: 0.0051\n",
      "Epoch 411/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7791e-05 - mae: 0.0033\n",
      "Epoch 411: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8076e-05 - mae: 0.0033 - val_loss: 4.1608e-05 - val_mae: 0.0053\n",
      "Epoch 412/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8810e-05 - mae: 0.0034\n",
      "Epoch 412: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8869e-05 - mae: 0.0034 - val_loss: 3.8676e-05 - val_mae: 0.0051\n",
      "Epoch 413/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6763e-05 - mae: 0.0032\n",
      "Epoch 413: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7332e-05 - mae: 0.0032 - val_loss: 3.9351e-05 - val_mae: 0.0051\n",
      "Epoch 414/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1685e-05 - mae: 0.0037\n",
      "Epoch 414: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0489e-05 - mae: 0.0036 - val_loss: 3.8760e-05 - val_mae: 0.0051\n",
      "Epoch 415/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6847e-05 - mae: 0.0033\n",
      "Epoch 415: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8963e-05 - mae: 0.0034 - val_loss: 4.3935e-05 - val_mae: 0.0054\n",
      "Epoch 416/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9033e-05 - mae: 0.0035\n",
      "Epoch 416: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0197e-05 - mae: 0.0036 - val_loss: 4.0326e-05 - val_mae: 0.0052\n",
      "Epoch 417/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7923e-05 - mae: 0.0034\n",
      "Epoch 417: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8040e-05 - mae: 0.0034 - val_loss: 3.8713e-05 - val_mae: 0.0051\n",
      "Epoch 418/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8043e-05 - mae: 0.0034\n",
      "Epoch 418: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8043e-05 - mae: 0.0034 - val_loss: 3.9748e-05 - val_mae: 0.0051\n",
      "Epoch 419/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8577e-05 - mae: 0.0035\n",
      "Epoch 419: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8737e-05 - mae: 0.0034 - val_loss: 3.8476e-05 - val_mae: 0.0051\n",
      "Epoch 420/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9608e-05 - mae: 0.0034\n",
      "Epoch 420: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9473e-05 - mae: 0.0034 - val_loss: 4.4326e-05 - val_mae: 0.0053\n",
      "Epoch 421/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7391e-05 - mae: 0.0033\n",
      "Epoch 421: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8434e-05 - mae: 0.0034 - val_loss: 3.9262e-05 - val_mae: 0.0052\n",
      "Epoch 422/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7967e-05 - mae: 0.0034\n",
      "Epoch 422: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9050e-05 - mae: 0.0035 - val_loss: 3.8888e-05 - val_mae: 0.0051\n",
      "Epoch 423/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8037e-05 - mae: 0.0034\n",
      "Epoch 423: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8115e-05 - mae: 0.0034 - val_loss: 4.2768e-05 - val_mae: 0.0054\n",
      "Epoch 424/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8376e-05 - mae: 0.0034\n",
      "Epoch 424: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8266e-05 - mae: 0.0034 - val_loss: 3.8727e-05 - val_mae: 0.0051\n",
      "Epoch 425/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5822e-05 - mae: 0.0032\n",
      "Epoch 425: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8230e-05 - mae: 0.0034 - val_loss: 3.9187e-05 - val_mae: 0.0052\n",
      "Epoch 426/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.7682e-05 - mae: 0.0034\n",
      "Epoch 426: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9136e-05 - mae: 0.0035 - val_loss: 3.8845e-05 - val_mae: 0.0051\n",
      "Epoch 427/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9143e-05 - mae: 0.0034\n",
      "Epoch 427: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8903e-05 - mae: 0.0034 - val_loss: 4.4749e-05 - val_mae: 0.0054\n",
      "Epoch 428/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8616e-05 - mae: 0.0034\n",
      "Epoch 428: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0257e-05 - mae: 0.0036 - val_loss: 6.7072e-05 - val_mae: 0.0067\n",
      "Epoch 429/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9663e-05 - mae: 0.0035\n",
      "Epoch 429: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8662e-05 - mae: 0.0034 - val_loss: 4.6746e-05 - val_mae: 0.0056\n",
      "Epoch 430/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9349e-05 - mae: 0.0035\n",
      "Epoch 430: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9816e-05 - mae: 0.0036 - val_loss: 4.1345e-05 - val_mae: 0.0053\n",
      "Epoch 431/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6293e-05 - mae: 0.0032\n",
      "Epoch 431: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7743e-05 - mae: 0.0033 - val_loss: 3.9311e-05 - val_mae: 0.0051\n",
      "Epoch 432/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6980e-05 - mae: 0.0033\n",
      "Epoch 432: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8312e-05 - mae: 0.0034 - val_loss: 4.0776e-05 - val_mae: 0.0053\n",
      "Epoch 433/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8139e-05 - mae: 0.0034\n",
      "Epoch 433: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8139e-05 - mae: 0.0034 - val_loss: 4.0812e-05 - val_mae: 0.0053\n",
      "Epoch 434/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7668e-05 - mae: 0.0033\n",
      "Epoch 434: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8111e-05 - mae: 0.0033 - val_loss: 3.8848e-05 - val_mae: 0.0051\n",
      "Epoch 435/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7540e-05 - mae: 0.0033\n",
      "Epoch 435: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9944e-05 - mae: 0.0036 - val_loss: 3.9255e-05 - val_mae: 0.0051\n",
      "Epoch 436/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8208e-05 - mae: 0.0033\n",
      "Epoch 436: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9383e-05 - mae: 0.0034 - val_loss: 4.6679e-05 - val_mae: 0.0056\n",
      "Epoch 437/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7961e-05 - mae: 0.0033\n",
      "Epoch 437: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8131e-05 - mae: 0.0033 - val_loss: 4.1994e-05 - val_mae: 0.0053\n",
      "Epoch 438/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9747e-05 - mae: 0.0035\n",
      "Epoch 438: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8940e-05 - mae: 0.0035 - val_loss: 4.4505e-05 - val_mae: 0.0055\n",
      "Epoch 439/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8338e-05 - mae: 0.0034\n",
      "Epoch 439: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9667e-05 - mae: 0.0035 - val_loss: 5.5931e-05 - val_mae: 0.0061\n",
      "Epoch 440/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9845e-05 - mae: 0.0036\n",
      "Epoch 440: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.0083e-05 - mae: 0.0036 - val_loss: 3.9828e-05 - val_mae: 0.0052\n",
      "Epoch 441/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5962e-05 - mae: 0.0032\n",
      "Epoch 441: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7389e-05 - mae: 0.0033 - val_loss: 4.1559e-05 - val_mae: 0.0053\n",
      "Epoch 442/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7978e-05 - mae: 0.0034\n",
      "Epoch 442: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8184e-05 - mae: 0.0034 - val_loss: 4.0814e-05 - val_mae: 0.0052\n",
      "Epoch 443/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9287e-05 - mae: 0.0035\n",
      "Epoch 443: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9082e-05 - mae: 0.0035 - val_loss: 4.0245e-05 - val_mae: 0.0052\n",
      "Epoch 444/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6383e-05 - mae: 0.0032\n",
      "Epoch 444: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7705e-05 - mae: 0.0034 - val_loss: 3.8982e-05 - val_mae: 0.0052\n",
      "Epoch 445/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0741e-05 - mae: 0.0036\n",
      "Epoch 445: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9490e-05 - mae: 0.0035 - val_loss: 4.3717e-05 - val_mae: 0.0054\n",
      "Epoch 446/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8612e-05 - mae: 0.0034\n",
      "Epoch 446: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9593e-05 - mae: 0.0035 - val_loss: 3.9329e-05 - val_mae: 0.0052\n",
      "Epoch 447/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9181e-05 - mae: 0.0034\n",
      "Epoch 447: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.1588e-05 - mae: 0.0036 - val_loss: 4.0853e-05 - val_mae: 0.0052\n",
      "Epoch 448/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7710e-05 - mae: 0.0033\n",
      "Epoch 448: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8322e-05 - mae: 0.0033 - val_loss: 4.6639e-05 - val_mae: 0.0055\n",
      "Epoch 449/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9090e-05 - mae: 0.0035\n",
      "Epoch 449: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8167e-05 - mae: 0.0034 - val_loss: 4.4164e-05 - val_mae: 0.0054\n",
      "Epoch 450/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8881e-05 - mae: 0.0035\n",
      "Epoch 450: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8580e-05 - mae: 0.0034 - val_loss: 4.3939e-05 - val_mae: 0.0054\n",
      "Epoch 451/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1414e-05 - mae: 0.0036\n",
      "Epoch 451: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0364e-05 - mae: 0.0036 - val_loss: 4.7698e-05 - val_mae: 0.0055\n",
      "Epoch 452/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9475e-05 - mae: 0.0035\n",
      "Epoch 452: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 2.0631e-05 - mae: 0.0036 - val_loss: 5.4729e-05 - val_mae: 0.0060\n",
      "Epoch 453/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7599e-05 - mae: 0.0034\n",
      "Epoch 453: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8724e-05 - mae: 0.0035 - val_loss: 3.8502e-05 - val_mae: 0.0051\n",
      "Epoch 454/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7709e-05 - mae: 0.0033\n",
      "Epoch 454: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8358e-05 - mae: 0.0034 - val_loss: 3.8289e-05 - val_mae: 0.0051\n",
      "Epoch 455/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7719e-05 - mae: 0.0034\n",
      "Epoch 455: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8295e-05 - mae: 0.0034 - val_loss: 4.8679e-05 - val_mae: 0.0057\n",
      "Epoch 456/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9244e-05 - mae: 0.0034\n",
      "Epoch 456: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9204e-05 - mae: 0.0034 - val_loss: 3.9942e-05 - val_mae: 0.0052\n",
      "Epoch 457/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9215e-05 - mae: 0.0034\n",
      "Epoch 457: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8578e-05 - mae: 0.0034 - val_loss: 3.8519e-05 - val_mae: 0.0051\n",
      "Epoch 458/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7985e-05 - mae: 0.0033\n",
      "Epoch 458: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8507e-05 - mae: 0.0034 - val_loss: 3.9917e-05 - val_mae: 0.0052\n",
      "Epoch 459/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8952e-05 - mae: 0.0035\n",
      "Epoch 459: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7950e-05 - mae: 0.0034 - val_loss: 4.0076e-05 - val_mae: 0.0052\n",
      "Epoch 460/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8683e-05 - mae: 0.0035\n",
      "Epoch 460: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9208e-05 - mae: 0.0035 - val_loss: 5.1213e-05 - val_mae: 0.0058\n",
      "Epoch 461/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1939e-05 - mae: 0.0038\n",
      "Epoch 461: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.3475e-05 - mae: 0.0039 - val_loss: 3.9544e-05 - val_mae: 0.0051\n",
      "Epoch 462/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8991e-05 - mae: 0.0035\n",
      "Epoch 462: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8754e-05 - mae: 0.0035 - val_loss: 3.9089e-05 - val_mae: 0.0052\n",
      "Epoch 463/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8805e-05 - mae: 0.0034\n",
      "Epoch 463: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.7776e-05 - mae: 0.0033 - val_loss: 3.9491e-05 - val_mae: 0.0052\n",
      "Epoch 464/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7476e-05 - mae: 0.0033\n",
      "Epoch 464: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8186e-05 - mae: 0.0034 - val_loss: 4.7225e-05 - val_mae: 0.0056\n",
      "Epoch 465/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8160e-05 - mae: 0.0034\n",
      "Epoch 465: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8215e-05 - mae: 0.0034 - val_loss: 3.9529e-05 - val_mae: 0.0052\n",
      "Epoch 466/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8486e-05 - mae: 0.0034\n",
      "Epoch 466: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8387e-05 - mae: 0.0034 - val_loss: 4.0096e-05 - val_mae: 0.0052\n",
      "Epoch 467/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5966e-05 - mae: 0.0032\n",
      "Epoch 467: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8930e-05 - mae: 0.0034 - val_loss: 4.3929e-05 - val_mae: 0.0053\n",
      "Epoch 468/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.2066e-05 - mae: 0.0038\n",
      "Epoch 468: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0337e-05 - mae: 0.0036 - val_loss: 3.9316e-05 - val_mae: 0.0051\n",
      "Epoch 469/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8212e-05 - mae: 0.0034\n",
      "Epoch 469: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8561e-05 - mae: 0.0034 - val_loss: 3.9318e-05 - val_mae: 0.0052\n",
      "Epoch 470/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7213e-05 - mae: 0.0032\n",
      "Epoch 470: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8112e-05 - mae: 0.0034 - val_loss: 5.2070e-05 - val_mae: 0.0059\n",
      "Epoch 471/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1581e-05 - mae: 0.0036\n",
      "Epoch 471: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.3128e-05 - mae: 0.0038 - val_loss: 4.3194e-05 - val_mae: 0.0054\n",
      "Epoch 472/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9380e-05 - mae: 0.0035\n",
      "Epoch 472: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9285e-05 - mae: 0.0035 - val_loss: 4.0943e-05 - val_mae: 0.0053\n",
      "Epoch 473/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.8603e-05 - mae: 0.0034\n",
      "Epoch 473: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.8370e-05 - mae: 0.0034 - val_loss: 3.9770e-05 - val_mae: 0.0052\n",
      "Epoch 474/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8469e-05 - mae: 0.0034\n",
      "Epoch 474: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.8898e-05 - mae: 0.0035 - val_loss: 4.2376e-05 - val_mae: 0.0054\n",
      "Epoch 475/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9850e-05 - mae: 0.0035\n",
      "Epoch 475: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.9660e-05 - mae: 0.0035 - val_loss: 3.8619e-05 - val_mae: 0.0051\n",
      "Epoch 476/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.8874e-05 - mae: 0.0035\n",
      "Epoch 476: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.9103e-05 - mae: 0.0035 - val_loss: 3.9968e-05 - val_mae: 0.0051\n",
      "Epoch 477/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7395e-05 - mae: 0.0033\n",
      "Epoch 477: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.7369e-05 - mae: 0.0033 - val_loss: 3.9890e-05 - val_mae: 0.0052\n",
      "Epoch 478/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7845e-05 - mae: 0.0034\n",
      "Epoch 478: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.8163e-05 - mae: 0.0034 - val_loss: 4.1560e-05 - val_mae: 0.0052\n",
      "Epoch 479/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7512e-05 - mae: 0.0033\n",
      "Epoch 479: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7723e-05 - mae: 0.0033 - val_loss: 3.8660e-05 - val_mae: 0.0051\n",
      "Epoch 480/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9020e-05 - mae: 0.0035\n",
      "Epoch 480: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.0500e-05 - mae: 0.0036 - val_loss: 3.9303e-05 - val_mae: 0.0052\n",
      "Epoch 481/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9690e-05 - mae: 0.0035\n",
      "Epoch 481: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7717e-05 - mae: 0.0033 - val_loss: 3.9104e-05 - val_mae: 0.0051\n",
      "Epoch 482/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9349e-05 - mae: 0.0035\n",
      "Epoch 482: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9100e-05 - mae: 0.0035 - val_loss: 3.9290e-05 - val_mae: 0.0051\n",
      "Epoch 483/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8637e-05 - mae: 0.0033\n",
      "Epoch 483: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8614e-05 - mae: 0.0034 - val_loss: 4.2626e-05 - val_mae: 0.0052\n",
      "Epoch 484/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7880e-05 - mae: 0.0034\n",
      "Epoch 484: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.7627e-05 - mae: 0.0033 - val_loss: 4.2471e-05 - val_mae: 0.0054\n",
      "Epoch 485/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.2179e-05 - mae: 0.0038\n",
      "Epoch 485: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.2179e-05 - mae: 0.0038 - val_loss: 4.0221e-05 - val_mae: 0.0051\n",
      "Epoch 486/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0422e-05 - mae: 0.0036\n",
      "Epoch 486: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9559e-05 - mae: 0.0035 - val_loss: 4.3585e-05 - val_mae: 0.0054\n",
      "Epoch 487/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9523e-05 - mae: 0.0034\n",
      "Epoch 487: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9605e-05 - mae: 0.0035 - val_loss: 4.7457e-05 - val_mae: 0.0055\n",
      "Epoch 488/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8934e-05 - mae: 0.0035\n",
      "Epoch 488: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8493e-05 - mae: 0.0034 - val_loss: 3.9241e-05 - val_mae: 0.0052\n",
      "Epoch 489/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6878e-05 - mae: 0.0033\n",
      "Epoch 489: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7972e-05 - mae: 0.0034 - val_loss: 3.9965e-05 - val_mae: 0.0052\n",
      "Epoch 490/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9096e-05 - mae: 0.0035\n",
      "Epoch 490: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9138e-05 - mae: 0.0035 - val_loss: 4.2376e-05 - val_mae: 0.0054\n",
      "Epoch 491/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.0422e-05 - mae: 0.0036\n",
      "Epoch 491: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 2.0323e-05 - mae: 0.0036 - val_loss: 3.9284e-05 - val_mae: 0.0052\n",
      "Epoch 492/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7353e-05 - mae: 0.0032\n",
      "Epoch 492: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.7565e-05 - mae: 0.0033 - val_loss: 3.9887e-05 - val_mae: 0.0052\n",
      "Epoch 493/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8175e-05 - mae: 0.0033\n",
      "Epoch 493: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8316e-05 - mae: 0.0034 - val_loss: 3.8603e-05 - val_mae: 0.0051\n",
      "Epoch 494/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8914e-05 - mae: 0.0034\n",
      "Epoch 494: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8740e-05 - mae: 0.0034 - val_loss: 3.9745e-05 - val_mae: 0.0052\n",
      "Epoch 495/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8486e-05 - mae: 0.0034\n",
      "Epoch 495: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8135e-05 - mae: 0.0034 - val_loss: 4.6268e-05 - val_mae: 0.0055\n",
      "Epoch 496/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6684e-05 - mae: 0.0033\n",
      "Epoch 496: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8346e-05 - mae: 0.0034 - val_loss: 3.9142e-05 - val_mae: 0.0052\n",
      "Epoch 497/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8792e-05 - mae: 0.0034\n",
      "Epoch 497: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7988e-05 - mae: 0.0034 - val_loss: 3.8882e-05 - val_mae: 0.0051\n",
      "Epoch 498/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7123e-05 - mae: 0.0033\n",
      "Epoch 498: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.7523e-05 - mae: 0.0033 - val_loss: 3.8729e-05 - val_mae: 0.0051\n",
      "Epoch 499/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8258e-05 - mae: 0.0033\n",
      "Epoch 499: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8197e-05 - mae: 0.0034 - val_loss: 4.1849e-05 - val_mae: 0.0053\n",
      "Epoch 500/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8551e-05 - mae: 0.0034\n",
      "Epoch 500: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7624e-05 - mae: 0.0033 - val_loss: 3.8750e-05 - val_mae: 0.0051\n",
      "Epoch 501/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8694e-05 - mae: 0.0035\n",
      "Epoch 501: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8299e-05 - mae: 0.0034 - val_loss: 3.9264e-05 - val_mae: 0.0051\n",
      "Epoch 502/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8401e-05 - mae: 0.0034\n",
      "Epoch 502: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0251e-05 - mae: 0.0036 - val_loss: 4.0152e-05 - val_mae: 0.0052\n",
      "Epoch 503/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5810e-05 - mae: 0.0031\n",
      "Epoch 503: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7759e-05 - mae: 0.0033 - val_loss: 3.9572e-05 - val_mae: 0.0052\n",
      "Epoch 504/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9350e-05 - mae: 0.0035\n",
      "Epoch 504: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9090e-05 - mae: 0.0035 - val_loss: 5.3985e-05 - val_mae: 0.0060\n",
      "Epoch 505/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8792e-05 - mae: 0.0034\n",
      "Epoch 505: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8553e-05 - mae: 0.0034 - val_loss: 3.9805e-05 - val_mae: 0.0052\n",
      "Epoch 506/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.1269e-05 - mae: 0.0037\n",
      "Epoch 506: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.1167e-05 - mae: 0.0036 - val_loss: 3.8366e-05 - val_mae: 0.0051\n",
      "Epoch 507/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8824e-05 - mae: 0.0034\n",
      "Epoch 507: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8634e-05 - mae: 0.0034 - val_loss: 4.2815e-05 - val_mae: 0.0054\n",
      "Epoch 508/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1114e-05 - mae: 0.0037\n",
      "Epoch 508: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9285e-05 - mae: 0.0035 - val_loss: 3.8957e-05 - val_mae: 0.0051\n",
      "Epoch 509/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7154e-05 - mae: 0.0033\n",
      "Epoch 509: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7235e-05 - mae: 0.0033 - val_loss: 4.5821e-05 - val_mae: 0.0055\n",
      "Epoch 510/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7961e-05 - mae: 0.0034\n",
      "Epoch 510: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7613e-05 - mae: 0.0033 - val_loss: 4.7865e-05 - val_mae: 0.0056\n",
      "Epoch 511/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7792e-05 - mae: 0.0034\n",
      "Epoch 511: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0457e-05 - mae: 0.0036 - val_loss: 4.3072e-05 - val_mae: 0.0053\n",
      "Epoch 512/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8919e-05 - mae: 0.0034\n",
      "Epoch 512: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9660e-05 - mae: 0.0035 - val_loss: 4.1482e-05 - val_mae: 0.0053\n",
      "Epoch 513/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8103e-05 - mae: 0.0033\n",
      "Epoch 513: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8462e-05 - mae: 0.0034 - val_loss: 3.9782e-05 - val_mae: 0.0051\n",
      "Epoch 514/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8075e-05 - mae: 0.0034\n",
      "Epoch 514: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7770e-05 - mae: 0.0033 - val_loss: 4.4299e-05 - val_mae: 0.0055\n",
      "Epoch 515/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9936e-05 - mae: 0.0035\n",
      "Epoch 515: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8368e-05 - mae: 0.0034 - val_loss: 4.1759e-05 - val_mae: 0.0052\n",
      "Epoch 516/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1427e-05 - mae: 0.0036\n",
      "Epoch 516: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1269e-05 - mae: 0.0036 - val_loss: 3.9400e-05 - val_mae: 0.0052\n",
      "Epoch 517/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1359e-05 - mae: 0.0037\n",
      "Epoch 517: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9919e-05 - mae: 0.0035 - val_loss: 4.1128e-05 - val_mae: 0.0053\n",
      "Epoch 518/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.9150e-05 - mae: 0.0035\n",
      "Epoch 518: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8501e-05 - mae: 0.0034 - val_loss: 3.9093e-05 - val_mae: 0.0052\n",
      "Epoch 519/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9559e-05 - mae: 0.0035\n",
      "Epoch 519: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8664e-05 - mae: 0.0034 - val_loss: 3.9984e-05 - val_mae: 0.0052\n",
      "Epoch 520/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7519e-05 - mae: 0.0034\n",
      "Epoch 520: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8312e-05 - mae: 0.0034 - val_loss: 3.9508e-05 - val_mae: 0.0052\n",
      "Epoch 521/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8709e-05 - mae: 0.0034\n",
      "Epoch 521: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8661e-05 - mae: 0.0034 - val_loss: 3.9203e-05 - val_mae: 0.0052\n",
      "Epoch 522/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7975e-05 - mae: 0.0034\n",
      "Epoch 522: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8282e-05 - mae: 0.0034 - val_loss: 3.9865e-05 - val_mae: 0.0051\n",
      "Epoch 523/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7629e-05 - mae: 0.0033\n",
      "Epoch 523: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9076e-05 - mae: 0.0034 - val_loss: 4.0321e-05 - val_mae: 0.0051\n",
      "Epoch 524/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8661e-05 - mae: 0.0034\n",
      "Epoch 524: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8661e-05 - mae: 0.0034 - val_loss: 3.9334e-05 - val_mae: 0.0052\n",
      "Epoch 525/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7627e-05 - mae: 0.0033\n",
      "Epoch 525: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8175e-05 - mae: 0.0034 - val_loss: 4.0776e-05 - val_mae: 0.0053\n",
      "Epoch 526/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 2.0449e-05 - mae: 0.0036\n",
      "Epoch 526: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.9605e-05 - mae: 0.0035 - val_loss: 4.0189e-05 - val_mae: 0.0052\n",
      "Epoch 527/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9294e-05 - mae: 0.0035\n",
      "Epoch 527: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8301e-05 - mae: 0.0034 - val_loss: 3.8587e-05 - val_mae: 0.0051\n",
      "Epoch 528/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7244e-05 - mae: 0.0033\n",
      "Epoch 528: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7477e-05 - mae: 0.0033 - val_loss: 3.9304e-05 - val_mae: 0.0052\n",
      "Epoch 529/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7112e-05 - mae: 0.0032\n",
      "Epoch 529: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7951e-05 - mae: 0.0033 - val_loss: 3.9839e-05 - val_mae: 0.0052\n",
      "Epoch 530/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7267e-05 - mae: 0.0033\n",
      "Epoch 530: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8937e-05 - mae: 0.0034 - val_loss: 4.0100e-05 - val_mae: 0.0051\n",
      "Epoch 531/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6988e-05 - mae: 0.0033\n",
      "Epoch 531: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8114e-05 - mae: 0.0034 - val_loss: 3.8465e-05 - val_mae: 0.0051\n",
      "Epoch 532/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8710e-05 - mae: 0.0034\n",
      "Epoch 532: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7656e-05 - mae: 0.0033 - val_loss: 4.1484e-05 - val_mae: 0.0053\n",
      "Epoch 533/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8879e-05 - mae: 0.0035\n",
      "Epoch 533: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8872e-05 - mae: 0.0035 - val_loss: 3.8465e-05 - val_mae: 0.0051\n",
      "Epoch 534/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1966e-05 - mae: 0.0038\n",
      "Epoch 534: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0646e-05 - mae: 0.0037 - val_loss: 3.9469e-05 - val_mae: 0.0051\n",
      "Epoch 535/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7542e-05 - mae: 0.0033\n",
      "Epoch 535: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7568e-05 - mae: 0.0033 - val_loss: 4.3237e-05 - val_mae: 0.0054\n",
      "Epoch 536/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8608e-05 - mae: 0.0034\n",
      "Epoch 536: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8394e-05 - mae: 0.0034 - val_loss: 3.9289e-05 - val_mae: 0.0052\n",
      "Epoch 537/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7190e-05 - mae: 0.0033\n",
      "Epoch 537: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7509e-05 - mae: 0.0033 - val_loss: 4.2153e-05 - val_mae: 0.0053\n",
      "Epoch 538/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0049e-05 - mae: 0.0035\n",
      "Epoch 538: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8874e-05 - mae: 0.0035 - val_loss: 4.0259e-05 - val_mae: 0.0052\n",
      "Epoch 539/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8697e-05 - mae: 0.0035\n",
      "Epoch 539: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0539e-05 - mae: 0.0036 - val_loss: 4.4717e-05 - val_mae: 0.0055\n",
      "Epoch 540/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7542e-05 - mae: 0.0033\n",
      "Epoch 540: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7540e-05 - mae: 0.0033 - val_loss: 5.3138e-05 - val_mae: 0.0059\n",
      "Epoch 541/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1389e-05 - mae: 0.0036\n",
      "Epoch 541: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0654e-05 - mae: 0.0036 - val_loss: 4.2578e-05 - val_mae: 0.0054\n",
      "Epoch 542/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8731e-05 - mae: 0.0034\n",
      "Epoch 542: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8794e-05 - mae: 0.0034 - val_loss: 3.9173e-05 - val_mae: 0.0052\n",
      "Epoch 543/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6792e-05 - mae: 0.0033\n",
      "Epoch 543: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7235e-05 - mae: 0.0033 - val_loss: 4.0224e-05 - val_mae: 0.0052\n",
      "Epoch 544/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0105e-05 - mae: 0.0036\n",
      "Epoch 544: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8824e-05 - mae: 0.0035 - val_loss: 3.9447e-05 - val_mae: 0.0051\n",
      "Epoch 545/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7830e-05 - mae: 0.0033\n",
      "Epoch 545: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.8121e-05 - mae: 0.0033 - val_loss: 3.9627e-05 - val_mae: 0.0051\n",
      "Epoch 546/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.6611e-05 - mae: 0.0032\n",
      "Epoch 546: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.7103e-05 - mae: 0.0033 - val_loss: 3.9755e-05 - val_mae: 0.0052\n",
      "Epoch 547/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7312e-05 - mae: 0.0032\n",
      "Epoch 547: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.7478e-05 - mae: 0.0033 - val_loss: 3.8823e-05 - val_mae: 0.0051\n",
      "Epoch 548/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8104e-05 - mae: 0.0033\n",
      "Epoch 548: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.7810e-05 - mae: 0.0033 - val_loss: 4.1262e-05 - val_mae: 0.0053\n",
      "Epoch 549/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6543e-05 - mae: 0.0032\n",
      "Epoch 549: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7901e-05 - mae: 0.0033 - val_loss: 4.1136e-05 - val_mae: 0.0053\n",
      "Epoch 550/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8702e-05 - mae: 0.0034\n",
      "Epoch 550: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8892e-05 - mae: 0.0034 - val_loss: 4.5524e-05 - val_mae: 0.0055\n",
      "Epoch 551/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8681e-05 - mae: 0.0034\n",
      "Epoch 551: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.8875e-05 - mae: 0.0034 - val_loss: 4.4042e-05 - val_mae: 0.0054\n",
      "Epoch 552/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7867e-05 - mae: 0.0033\n",
      "Epoch 552: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7964e-05 - mae: 0.0034 - val_loss: 4.1309e-05 - val_mae: 0.0053\n",
      "Epoch 553/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7540e-05 - mae: 0.0033\n",
      "Epoch 553: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8806e-05 - mae: 0.0034 - val_loss: 3.8509e-05 - val_mae: 0.0051\n",
      "Epoch 554/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8288e-05 - mae: 0.0033\n",
      "Epoch 554: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0145e-05 - mae: 0.0035 - val_loss: 4.5502e-05 - val_mae: 0.0055\n",
      "Epoch 555/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2179e-05 - mae: 0.0038\n",
      "Epoch 555: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0540e-05 - mae: 0.0036 - val_loss: 3.9936e-05 - val_mae: 0.0051\n",
      "Epoch 556/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6563e-05 - mae: 0.0031\n",
      "Epoch 556: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8250e-05 - mae: 0.0034 - val_loss: 4.0933e-05 - val_mae: 0.0052\n",
      "Epoch 557/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0423e-05 - mae: 0.0036\n",
      "Epoch 557: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9311e-05 - mae: 0.0035 - val_loss: 3.8645e-05 - val_mae: 0.0051\n",
      "Epoch 558/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7886e-05 - mae: 0.0033\n",
      "Epoch 558: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7837e-05 - mae: 0.0033 - val_loss: 3.8327e-05 - val_mae: 0.0051\n",
      "Epoch 559/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8073e-05 - mae: 0.0034\n",
      "Epoch 559: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7672e-05 - mae: 0.0033 - val_loss: 3.9829e-05 - val_mae: 0.0052\n",
      "Epoch 560/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7169e-05 - mae: 0.0033\n",
      "Epoch 560: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8480e-05 - mae: 0.0034 - val_loss: 3.8181e-05 - val_mae: 0.0051\n",
      "Epoch 561/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9332e-05 - mae: 0.0035\n",
      "Epoch 561: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8818e-05 - mae: 0.0034 - val_loss: 4.2828e-05 - val_mae: 0.0054\n",
      "Epoch 562/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7369e-05 - mae: 0.0033\n",
      "Epoch 562: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8400e-05 - mae: 0.0034 - val_loss: 3.8721e-05 - val_mae: 0.0051\n",
      "Epoch 563/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1930e-05 - mae: 0.0037\n",
      "Epoch 563: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1270e-05 - mae: 0.0036 - val_loss: 4.3836e-05 - val_mae: 0.0054\n",
      "Epoch 564/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8885e-05 - mae: 0.0035\n",
      "Epoch 564: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8523e-05 - mae: 0.0034 - val_loss: 3.9536e-05 - val_mae: 0.0051\n",
      "Epoch 565/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9254e-05 - mae: 0.0035\n",
      "Epoch 565: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8472e-05 - mae: 0.0034 - val_loss: 4.0519e-05 - val_mae: 0.0053\n",
      "Epoch 566/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8415e-05 - mae: 0.0034\n",
      "Epoch 566: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8569e-05 - mae: 0.0034 - val_loss: 4.3021e-05 - val_mae: 0.0054\n",
      "Epoch 567/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6685e-05 - mae: 0.0033\n",
      "Epoch 567: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8317e-05 - mae: 0.0034 - val_loss: 4.5591e-05 - val_mae: 0.0055\n",
      "Epoch 568/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6913e-05 - mae: 0.0033\n",
      "Epoch 568: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8807e-05 - mae: 0.0034 - val_loss: 3.8702e-05 - val_mae: 0.0051\n",
      "Epoch 569/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8235e-05 - mae: 0.0033\n",
      "Epoch 569: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7999e-05 - mae: 0.0033 - val_loss: 3.9944e-05 - val_mae: 0.0052\n",
      "Epoch 570/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8065e-05 - mae: 0.0034\n",
      "Epoch 570: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7675e-05 - mae: 0.0033 - val_loss: 4.7234e-05 - val_mae: 0.0056\n",
      "Epoch 571/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7686e-05 - mae: 0.0033\n",
      "Epoch 571: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7657e-05 - mae: 0.0033 - val_loss: 3.8389e-05 - val_mae: 0.0051\n",
      "Epoch 572/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6662e-05 - mae: 0.0032\n",
      "Epoch 572: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7803e-05 - mae: 0.0034 - val_loss: 3.9410e-05 - val_mae: 0.0052\n",
      "Epoch 573/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8344e-05 - mae: 0.0034\n",
      "Epoch 573: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8786e-05 - mae: 0.0034 - val_loss: 5.4866e-05 - val_mae: 0.0060\n",
      "Epoch 574/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9551e-05 - mae: 0.0035\n",
      "Epoch 574: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9172e-05 - mae: 0.0034 - val_loss: 4.2260e-05 - val_mae: 0.0054\n",
      "Epoch 575/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9158e-05 - mae: 0.0035\n",
      "Epoch 575: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8630e-05 - mae: 0.0034 - val_loss: 3.9198e-05 - val_mae: 0.0051\n",
      "Epoch 576/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9078e-05 - mae: 0.0035\n",
      "Epoch 576: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9406e-05 - mae: 0.0035 - val_loss: 4.7059e-05 - val_mae: 0.0056\n",
      "Epoch 577/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9783e-05 - mae: 0.0035\n",
      "Epoch 577: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8729e-05 - mae: 0.0034 - val_loss: 4.0597e-05 - val_mae: 0.0052\n",
      "Epoch 578/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7849e-05 - mae: 0.0034\n",
      "Epoch 578: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.8160e-05 - mae: 0.0034 - val_loss: 4.2420e-05 - val_mae: 0.0053\n",
      "Epoch 579/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9312e-05 - mae: 0.0035\n",
      "Epoch 579: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9431e-05 - mae: 0.0035 - val_loss: 3.9019e-05 - val_mae: 0.0051\n",
      "Epoch 580/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7572e-05 - mae: 0.0033\n",
      "Epoch 580: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7483e-05 - mae: 0.0033 - val_loss: 3.9406e-05 - val_mae: 0.0051\n",
      "Epoch 581/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6900e-05 - mae: 0.0033\n",
      "Epoch 581: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8059e-05 - mae: 0.0034 - val_loss: 3.9428e-05 - val_mae: 0.0052\n",
      "Epoch 582/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8223e-05 - mae: 0.0034\n",
      "Epoch 582: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8564e-05 - mae: 0.0034 - val_loss: 4.0899e-05 - val_mae: 0.0053\n",
      "Epoch 583/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7165e-05 - mae: 0.0033\n",
      "Epoch 583: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8414e-05 - mae: 0.0034 - val_loss: 3.8975e-05 - val_mae: 0.0051\n",
      "Epoch 584/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7727e-05 - mae: 0.0033\n",
      "Epoch 584: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7527e-05 - mae: 0.0033 - val_loss: 3.9919e-05 - val_mae: 0.0052\n",
      "Epoch 585/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7877e-05 - mae: 0.0033\n",
      "Epoch 585: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7536e-05 - mae: 0.0033 - val_loss: 3.9320e-05 - val_mae: 0.0051\n",
      "Epoch 586/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9421e-05 - mae: 0.0034\n",
      "Epoch 586: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9005e-05 - mae: 0.0034 - val_loss: 4.7780e-05 - val_mae: 0.0056\n",
      "Epoch 587/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2039e-05 - mae: 0.0038\n",
      "Epoch 587: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0710e-05 - mae: 0.0036 - val_loss: 3.8733e-05 - val_mae: 0.0051\n",
      "Epoch 588/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9551e-05 - mae: 0.0035\n",
      "Epoch 588: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9611e-05 - mae: 0.0035 - val_loss: 4.0208e-05 - val_mae: 0.0052\n",
      "Epoch 589/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8194e-05 - mae: 0.0034\n",
      "Epoch 589: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7920e-05 - mae: 0.0034 - val_loss: 3.8607e-05 - val_mae: 0.0051\n",
      "Epoch 590/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9497e-05 - mae: 0.0035\n",
      "Epoch 590: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8802e-05 - mae: 0.0034 - val_loss: 3.8878e-05 - val_mae: 0.0051\n",
      "Epoch 591/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9791e-05 - mae: 0.0035\n",
      "Epoch 591: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8681e-05 - mae: 0.0034 - val_loss: 4.0048e-05 - val_mae: 0.0052\n",
      "Epoch 592/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7986e-05 - mae: 0.0033\n",
      "Epoch 592: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8649e-05 - mae: 0.0034 - val_loss: 4.0172e-05 - val_mae: 0.0052\n",
      "Epoch 593/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8200e-05 - mae: 0.0034\n",
      "Epoch 593: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8322e-05 - mae: 0.0034 - val_loss: 4.0224e-05 - val_mae: 0.0052\n",
      "Epoch 594/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6890e-05 - mae: 0.0032\n",
      "Epoch 594: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8204e-05 - mae: 0.0034 - val_loss: 4.1407e-05 - val_mae: 0.0053\n",
      "Epoch 595/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8037e-05 - mae: 0.0033\n",
      "Epoch 595: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7505e-05 - mae: 0.0033 - val_loss: 4.1283e-05 - val_mae: 0.0053\n",
      "Epoch 596/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8159e-05 - mae: 0.0033\n",
      "Epoch 596: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8031e-05 - mae: 0.0033 - val_loss: 3.8619e-05 - val_mae: 0.0051\n",
      "Epoch 597/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7225e-05 - mae: 0.0033\n",
      "Epoch 597: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7030e-05 - mae: 0.0033 - val_loss: 4.6098e-05 - val_mae: 0.0054\n",
      "Epoch 598/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9271e-05 - mae: 0.0035\n",
      "Epoch 598: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9271e-05 - mae: 0.0035 - val_loss: 3.8846e-05 - val_mae: 0.0051\n",
      "Epoch 599/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8037e-05 - mae: 0.0033\n",
      "Epoch 599: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7778e-05 - mae: 0.0033 - val_loss: 3.9379e-05 - val_mae: 0.0052\n",
      "Epoch 600/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6713e-05 - mae: 0.0033\n",
      "Epoch 600: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8198e-05 - mae: 0.0034 - val_loss: 3.9317e-05 - val_mae: 0.0052\n",
      "Epoch 601/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7995e-05 - mae: 0.0034\n",
      "Epoch 601: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9079e-05 - mae: 0.0035 - val_loss: 3.9745e-05 - val_mae: 0.0052\n",
      "Epoch 602/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6861e-05 - mae: 0.0033\n",
      "Epoch 602: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8535e-05 - mae: 0.0034 - val_loss: 3.9038e-05 - val_mae: 0.0051\n",
      "Epoch 603/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7682e-05 - mae: 0.0033\n",
      "Epoch 603: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7745e-05 - mae: 0.0034 - val_loss: 3.9102e-05 - val_mae: 0.0052\n",
      "Epoch 604/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7931e-05 - mae: 0.0034\n",
      "Epoch 604: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8293e-05 - mae: 0.0034 - val_loss: 3.9574e-05 - val_mae: 0.0052\n",
      "Epoch 605/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8768e-05 - mae: 0.0034\n",
      "Epoch 605: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9005e-05 - mae: 0.0034 - val_loss: 5.7841e-05 - val_mae: 0.0062\n",
      "Epoch 606/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7830e-05 - mae: 0.0033\n",
      "Epoch 606: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7946e-05 - mae: 0.0033 - val_loss: 4.1741e-05 - val_mae: 0.0053\n",
      "Epoch 607/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7883e-05 - mae: 0.0033\n",
      "Epoch 607: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7769e-05 - mae: 0.0033 - val_loss: 4.0708e-05 - val_mae: 0.0053\n",
      "Epoch 608/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8879e-05 - mae: 0.0034\n",
      "Epoch 608: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8990e-05 - mae: 0.0034 - val_loss: 5.4520e-05 - val_mae: 0.0060\n",
      "Epoch 609/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8796e-05 - mae: 0.0034\n",
      "Epoch 609: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9109e-05 - mae: 0.0034 - val_loss: 7.2466e-05 - val_mae: 0.0069\n",
      "Epoch 610/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9501e-05 - mae: 0.0035\n",
      "Epoch 610: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.9391e-05 - mae: 0.0035 - val_loss: 3.9593e-05 - val_mae: 0.0052\n",
      "Epoch 611/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9224e-05 - mae: 0.0034\n",
      "Epoch 611: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8931e-05 - mae: 0.0034 - val_loss: 4.1653e-05 - val_mae: 0.0052\n",
      "Epoch 612/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8126e-05 - mae: 0.0034\n",
      "Epoch 612: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7738e-05 - mae: 0.0034 - val_loss: 4.1182e-05 - val_mae: 0.0053\n",
      "Epoch 613/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 2.0000e-05 - mae: 0.0035\n",
      "Epoch 613: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8778e-05 - mae: 0.0034 - val_loss: 4.3796e-05 - val_mae: 0.0054\n",
      "Epoch 614/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6395e-05 - mae: 0.0032\n",
      "Epoch 614: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7973e-05 - mae: 0.0034 - val_loss: 4.1537e-05 - val_mae: 0.0053\n",
      "Epoch 615/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9215e-05 - mae: 0.0035\n",
      "Epoch 615: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8954e-05 - mae: 0.0034 - val_loss: 3.9532e-05 - val_mae: 0.0052\n",
      "Epoch 616/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0173e-05 - mae: 0.0036\n",
      "Epoch 616: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9111e-05 - mae: 0.0035 - val_loss: 4.1755e-05 - val_mae: 0.0053\n",
      "Epoch 617/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6853e-05 - mae: 0.0033\n",
      "Epoch 617: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8333e-05 - mae: 0.0034 - val_loss: 4.4328e-05 - val_mae: 0.0055\n",
      "Epoch 618/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9366e-05 - mae: 0.0035\n",
      "Epoch 618: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7738e-05 - mae: 0.0033 - val_loss: 3.9011e-05 - val_mae: 0.0052\n",
      "Epoch 619/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6499e-05 - mae: 0.0031\n",
      "Epoch 619: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7561e-05 - mae: 0.0033 - val_loss: 4.0681e-05 - val_mae: 0.0053\n",
      "Epoch 620/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7868e-05 - mae: 0.0034\n",
      "Epoch 620: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9404e-05 - mae: 0.0035 - val_loss: 4.7516e-05 - val_mae: 0.0056\n",
      "Epoch 621/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7445e-05 - mae: 0.0033\n",
      "Epoch 621: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7113e-05 - mae: 0.0033 - val_loss: 3.8699e-05 - val_mae: 0.0051\n",
      "Epoch 622/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 2.0514e-05 - mae: 0.0036\n",
      "Epoch 622: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0084e-05 - mae: 0.0035 - val_loss: 3.9012e-05 - val_mae: 0.0051\n",
      "Epoch 623/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6287e-05 - mae: 0.0032\n",
      "Epoch 623: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7342e-05 - mae: 0.0033 - val_loss: 3.8547e-05 - val_mae: 0.0051\n",
      "Epoch 624/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8050e-05 - mae: 0.0034\n",
      "Epoch 624: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7944e-05 - mae: 0.0034 - val_loss: 3.8895e-05 - val_mae: 0.0051\n",
      "Epoch 625/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7179e-05 - mae: 0.0033\n",
      "Epoch 625: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7623e-05 - mae: 0.0033 - val_loss: 3.8895e-05 - val_mae: 0.0051\n",
      "Epoch 626/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7055e-05 - mae: 0.0033\n",
      "Epoch 626: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8366e-05 - mae: 0.0034 - val_loss: 4.4297e-05 - val_mae: 0.0054\n",
      "Epoch 627/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9146e-05 - mae: 0.0035\n",
      "Epoch 627: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8524e-05 - mae: 0.0034 - val_loss: 3.8419e-05 - val_mae: 0.0051\n",
      "Epoch 628/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7610e-05 - mae: 0.0033\n",
      "Epoch 628: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7622e-05 - mae: 0.0033 - val_loss: 4.6467e-05 - val_mae: 0.0056\n",
      "Epoch 629/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5752e-05 - mae: 0.0031\n",
      "Epoch 629: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8263e-05 - mae: 0.0034 - val_loss: 4.3284e-05 - val_mae: 0.0054\n",
      "Epoch 630/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9108e-05 - mae: 0.0035\n",
      "Epoch 630: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9490e-05 - mae: 0.0035 - val_loss: 3.8883e-05 - val_mae: 0.0051\n",
      "Epoch 631/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5500e-05 - mae: 0.0032\n",
      "Epoch 631: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7235e-05 - mae: 0.0033 - val_loss: 4.1802e-05 - val_mae: 0.0052\n",
      "Epoch 632/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8423e-05 - mae: 0.0033\n",
      "Epoch 632: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8736e-05 - mae: 0.0034 - val_loss: 4.4774e-05 - val_mae: 0.0055\n",
      "Epoch 633/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7513e-05 - mae: 0.0033\n",
      "Epoch 633: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7539e-05 - mae: 0.0033 - val_loss: 3.8898e-05 - val_mae: 0.0051\n",
      "Epoch 634/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7271e-05 - mae: 0.0033\n",
      "Epoch 634: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8247e-05 - mae: 0.0034 - val_loss: 3.8730e-05 - val_mae: 0.0051\n",
      "Epoch 635/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1119e-05 - mae: 0.0036\n",
      "Epoch 635: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0199e-05 - mae: 0.0035 - val_loss: 4.1360e-05 - val_mae: 0.0053\n",
      "Epoch 636/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7256e-05 - mae: 0.0033\n",
      "Epoch 636: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6915e-05 - mae: 0.0032 - val_loss: 3.8726e-05 - val_mae: 0.0051\n",
      "Epoch 637/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7850e-05 - mae: 0.0034\n",
      "Epoch 637: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8267e-05 - mae: 0.0034 - val_loss: 4.0830e-05 - val_mae: 0.0053\n",
      "Epoch 638/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9528e-05 - mae: 0.0035\n",
      "Epoch 638: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0735e-05 - mae: 0.0036 - val_loss: 4.2228e-05 - val_mae: 0.0052\n",
      "Epoch 639/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6356e-05 - mae: 0.0032\n",
      "Epoch 639: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8344e-05 - mae: 0.0033 - val_loss: 3.9666e-05 - val_mae: 0.0051\n",
      "Epoch 640/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8886e-05 - mae: 0.0035\n",
      "Epoch 640: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8768e-05 - mae: 0.0034 - val_loss: 3.8787e-05 - val_mae: 0.0051\n",
      "Epoch 641/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8335e-05 - mae: 0.0034\n",
      "Epoch 641: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7728e-05 - mae: 0.0033 - val_loss: 4.0414e-05 - val_mae: 0.0052\n",
      "Epoch 642/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7503e-05 - mae: 0.0032\n",
      "Epoch 642: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7797e-05 - mae: 0.0033 - val_loss: 3.8712e-05 - val_mae: 0.0051\n",
      "Epoch 643/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7460e-05 - mae: 0.0033\n",
      "Epoch 643: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.7325e-05 - mae: 0.0033 - val_loss: 3.8719e-05 - val_mae: 0.0051\n",
      "Epoch 644/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8531e-05 - mae: 0.0034\n",
      "Epoch 644: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8322e-05 - mae: 0.0034 - val_loss: 3.9498e-05 - val_mae: 0.0052\n",
      "Epoch 645/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5536e-05 - mae: 0.0031\n",
      "Epoch 645: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7407e-05 - mae: 0.0033 - val_loss: 3.9621e-05 - val_mae: 0.0051\n",
      "Epoch 646/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7555e-05 - mae: 0.0033\n",
      "Epoch 646: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.8181e-05 - mae: 0.0034 - val_loss: 4.2299e-05 - val_mae: 0.0052\n",
      "Epoch 647/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.2218e-05 - mae: 0.0038\n",
      "Epoch 647: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.1527e-05 - mae: 0.0037 - val_loss: 4.2795e-05 - val_mae: 0.0053\n",
      "Epoch 648/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7671e-05 - mae: 0.0033\n",
      "Epoch 648: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7737e-05 - mae: 0.0033 - val_loss: 3.8769e-05 - val_mae: 0.0051\n",
      "Epoch 649/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6196e-05 - mae: 0.0032\n",
      "Epoch 649: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8991e-05 - mae: 0.0034 - val_loss: 4.0286e-05 - val_mae: 0.0053\n",
      "Epoch 650/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8100e-05 - mae: 0.0034\n",
      "Epoch 650: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8323e-05 - mae: 0.0034 - val_loss: 3.8835e-05 - val_mae: 0.0051\n",
      "Epoch 651/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9448e-05 - mae: 0.0035\n",
      "Epoch 651: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9530e-05 - mae: 0.0035 - val_loss: 3.9478e-05 - val_mae: 0.0052\n",
      "Epoch 652/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9572e-05 - mae: 0.0035\n",
      "Epoch 652: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8477e-05 - mae: 0.0034 - val_loss: 3.8796e-05 - val_mae: 0.0051\n",
      "Epoch 653/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6736e-05 - mae: 0.0033\n",
      "Epoch 653: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7625e-05 - mae: 0.0033 - val_loss: 3.8726e-05 - val_mae: 0.0051\n",
      "Epoch 654/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9842e-05 - mae: 0.0035\n",
      "Epoch 654: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8284e-05 - mae: 0.0033 - val_loss: 4.4830e-05 - val_mae: 0.0055\n",
      "Epoch 655/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9129e-05 - mae: 0.0035\n",
      "Epoch 655: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9184e-05 - mae: 0.0035 - val_loss: 4.3025e-05 - val_mae: 0.0053\n",
      "Epoch 656/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8320e-05 - mae: 0.0034\n",
      "Epoch 656: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8221e-05 - mae: 0.0034 - val_loss: 3.9685e-05 - val_mae: 0.0052\n",
      "Epoch 657/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6173e-05 - mae: 0.0031\n",
      "Epoch 657: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7606e-05 - mae: 0.0033 - val_loss: 3.8695e-05 - val_mae: 0.0051\n",
      "Epoch 658/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/800 [=================>............] - ETA: 0s - loss: 1.8245e-05 - mae: 0.0034\n",
      "Epoch 658: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8297e-05 - mae: 0.0034 - val_loss: 4.3983e-05 - val_mae: 0.0054\n",
      "Epoch 659/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8456e-05 - mae: 0.0034\n",
      "Epoch 659: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8565e-05 - mae: 0.0034 - val_loss: 3.8297e-05 - val_mae: 0.0051\n",
      "Epoch 660/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6577e-05 - mae: 0.0032\n",
      "Epoch 660: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8046e-05 - mae: 0.0034 - val_loss: 4.2958e-05 - val_mae: 0.0054\n",
      "Epoch 661/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7538e-05 - mae: 0.0033\n",
      "Epoch 661: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8316e-05 - mae: 0.0034 - val_loss: 4.2675e-05 - val_mae: 0.0054\n",
      "Epoch 662/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7776e-05 - mae: 0.0033\n",
      "Epoch 662: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8341e-05 - mae: 0.0034 - val_loss: 4.9091e-05 - val_mae: 0.0056\n",
      "Epoch 663/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0060e-05 - mae: 0.0036\n",
      "Epoch 663: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9620e-05 - mae: 0.0035 - val_loss: 3.8571e-05 - val_mae: 0.0051\n",
      "Epoch 664/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8177e-05 - mae: 0.0034\n",
      "Epoch 664: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8258e-05 - mae: 0.0034 - val_loss: 3.8557e-05 - val_mae: 0.0051\n",
      "Epoch 665/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7354e-05 - mae: 0.0033\n",
      "Epoch 665: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8698e-05 - mae: 0.0034 - val_loss: 3.8790e-05 - val_mae: 0.0051\n",
      "Epoch 666/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5242e-05 - mae: 0.0031\n",
      "Epoch 666: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7589e-05 - mae: 0.0033 - val_loss: 4.6385e-05 - val_mae: 0.0056\n",
      "Epoch 667/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8226e-05 - mae: 0.0034\n",
      "Epoch 667: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8715e-05 - mae: 0.0034 - val_loss: 3.9884e-05 - val_mae: 0.0051\n",
      "Epoch 668/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9542e-05 - mae: 0.0035\n",
      "Epoch 668: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8975e-05 - mae: 0.0035 - val_loss: 3.8533e-05 - val_mae: 0.0051\n",
      "Epoch 669/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8026e-05 - mae: 0.0034\n",
      "Epoch 669: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7988e-05 - mae: 0.0034 - val_loss: 3.8503e-05 - val_mae: 0.0051\n",
      "Epoch 670/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6933e-05 - mae: 0.0033\n",
      "Epoch 670: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.6981e-05 - mae: 0.0032 - val_loss: 4.2433e-05 - val_mae: 0.0054\n",
      "Epoch 671/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5167e-05 - mae: 0.0031\n",
      "Epoch 671: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7146e-05 - mae: 0.0033 - val_loss: 4.0926e-05 - val_mae: 0.0053\n",
      "Epoch 672/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8375e-05 - mae: 0.0034\n",
      "Epoch 672: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.8952e-05 - mae: 0.0034 - val_loss: 3.9275e-05 - val_mae: 0.0052\n",
      "Epoch 673/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.7227e-05 - mae: 0.0034\n",
      "Epoch 673: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.9706e-05 - mae: 0.0036 - val_loss: 5.0605e-05 - val_mae: 0.0058\n",
      "Epoch 674/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.9852e-05 - mae: 0.0035\n",
      "Epoch 674: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.1690e-05 - mae: 0.0037 - val_loss: 3.8507e-05 - val_mae: 0.0051\n",
      "Epoch 675/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7336e-05 - mae: 0.0033\n",
      "Epoch 675: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7696e-05 - mae: 0.0033 - val_loss: 3.8469e-05 - val_mae: 0.0051\n",
      "Epoch 676/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6479e-05 - mae: 0.0031\n",
      "Epoch 676: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7356e-05 - mae: 0.0033 - val_loss: 5.3142e-05 - val_mae: 0.0059\n",
      "Epoch 677/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1233e-05 - mae: 0.0037\n",
      "Epoch 677: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9691e-05 - mae: 0.0035 - val_loss: 3.8912e-05 - val_mae: 0.0051\n",
      "Epoch 678/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7830e-05 - mae: 0.0034\n",
      "Epoch 678: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7825e-05 - mae: 0.0033 - val_loss: 3.8735e-05 - val_mae: 0.0051\n",
      "Epoch 679/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6680e-05 - mae: 0.0032\n",
      "Epoch 679: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7387e-05 - mae: 0.0033 - val_loss: 3.8731e-05 - val_mae: 0.0051\n",
      "Epoch 680/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9367e-05 - mae: 0.0034\n",
      "Epoch 680: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.2823e-05 - mae: 0.0038 - val_loss: 4.7082e-05 - val_mae: 0.0056\n",
      "Epoch 681/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0186e-05 - mae: 0.0035\n",
      "Epoch 681: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0166e-05 - mae: 0.0035 - val_loss: 3.8812e-05 - val_mae: 0.0051\n",
      "Epoch 682/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7791e-05 - mae: 0.0034\n",
      "Epoch 682: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8594e-05 - mae: 0.0034 - val_loss: 4.1092e-05 - val_mae: 0.0053\n",
      "Epoch 683/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8677e-05 - mae: 0.0034\n",
      "Epoch 683: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8830e-05 - mae: 0.0034 - val_loss: 3.9243e-05 - val_mae: 0.0051\n",
      "Epoch 684/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9443e-05 - mae: 0.0035\n",
      "Epoch 684: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9391e-05 - mae: 0.0035 - val_loss: 3.8698e-05 - val_mae: 0.0051\n",
      "Epoch 685/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8639e-05 - mae: 0.0035\n",
      "Epoch 685: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8275e-05 - mae: 0.0034 - val_loss: 3.8883e-05 - val_mae: 0.0051\n",
      "Epoch 686/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7560e-05 - mae: 0.0033\n",
      "Epoch 686: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7886e-05 - mae: 0.0034 - val_loss: 3.8982e-05 - val_mae: 0.0051\n",
      "Epoch 687/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/800 [=================>............] - ETA: 0s - loss: 1.5513e-05 - mae: 0.0031\n",
      "Epoch 687: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7607e-05 - mae: 0.0033 - val_loss: 3.9289e-05 - val_mae: 0.0051\n",
      "Epoch 688/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.7994e-05 - mae: 0.0034\n",
      "Epoch 688: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7638e-05 - mae: 0.0033 - val_loss: 3.9311e-05 - val_mae: 0.0052\n",
      "Epoch 689/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6409e-05 - mae: 0.0032\n",
      "Epoch 689: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7464e-05 - mae: 0.0033 - val_loss: 3.8480e-05 - val_mae: 0.0051\n",
      "Epoch 690/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7317e-05 - mae: 0.0033\n",
      "Epoch 690: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7505e-05 - mae: 0.0033 - val_loss: 4.3106e-05 - val_mae: 0.0054\n",
      "Epoch 691/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6704e-05 - mae: 0.0033\n",
      "Epoch 691: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8712e-05 - mae: 0.0034 - val_loss: 3.8423e-05 - val_mae: 0.0051\n",
      "Epoch 692/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8752e-05 - mae: 0.0034\n",
      "Epoch 692: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9089e-05 - mae: 0.0035 - val_loss: 3.9144e-05 - val_mae: 0.0052\n",
      "Epoch 693/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0068e-05 - mae: 0.0035\n",
      "Epoch 693: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9253e-05 - mae: 0.0034 - val_loss: 3.9219e-05 - val_mae: 0.0052\n",
      "Epoch 694/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7619e-05 - mae: 0.0033\n",
      "Epoch 694: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7527e-05 - mae: 0.0033 - val_loss: 4.4883e-05 - val_mae: 0.0055\n",
      "Epoch 695/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7592e-05 - mae: 0.0033\n",
      "Epoch 695: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7332e-05 - mae: 0.0033 - val_loss: 3.9179e-05 - val_mae: 0.0052\n",
      "Epoch 696/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.8717e-05 - mae: 0.0034\n",
      "Epoch 696: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9372e-05 - mae: 0.0035 - val_loss: 4.0979e-05 - val_mae: 0.0053\n",
      "Epoch 697/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7486e-05 - mae: 0.0034\n",
      "Epoch 697: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8509e-05 - mae: 0.0034 - val_loss: 4.8952e-05 - val_mae: 0.0057\n",
      "Epoch 698/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7925e-05 - mae: 0.0034\n",
      "Epoch 698: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8930e-05 - mae: 0.0035 - val_loss: 3.8730e-05 - val_mae: 0.0051\n",
      "Epoch 699/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8312e-05 - mae: 0.0034\n",
      "Epoch 699: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8473e-05 - mae: 0.0034 - val_loss: 3.9109e-05 - val_mae: 0.0052\n",
      "Epoch 700/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8759e-05 - mae: 0.0035\n",
      "Epoch 700: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8298e-05 - mae: 0.0034 - val_loss: 3.8764e-05 - val_mae: 0.0051\n",
      "Epoch 701/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8148e-05 - mae: 0.0034\n",
      "Epoch 701: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.8005e-05 - mae: 0.0034 - val_loss: 3.8480e-05 - val_mae: 0.0051\n",
      "Epoch 702/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7641e-05 - mae: 0.0033\n",
      "Epoch 702: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8287e-05 - mae: 0.0034 - val_loss: 3.9972e-05 - val_mae: 0.0052\n",
      "Epoch 703/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8723e-05 - mae: 0.0034\n",
      "Epoch 703: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8758e-05 - mae: 0.0034 - val_loss: 3.8924e-05 - val_mae: 0.0051\n",
      "Epoch 704/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7530e-05 - mae: 0.0033\n",
      "Epoch 704: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7646e-05 - mae: 0.0033 - val_loss: 4.4698e-05 - val_mae: 0.0055\n",
      "Epoch 705/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9442e-05 - mae: 0.0035\n",
      "Epoch 705: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9699e-05 - mae: 0.0035 - val_loss: 3.9054e-05 - val_mae: 0.0051\n",
      "Epoch 706/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7510e-05 - mae: 0.0034\n",
      "Epoch 706: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9772e-05 - mae: 0.0035 - val_loss: 4.9052e-05 - val_mae: 0.0057\n",
      "Epoch 707/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8857e-05 - mae: 0.0035\n",
      "Epoch 707: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8800e-05 - mae: 0.0034 - val_loss: 4.0535e-05 - val_mae: 0.0053\n",
      "Epoch 708/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6070e-05 - mae: 0.0032\n",
      "Epoch 708: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.8605e-05 - mae: 0.0034 - val_loss: 3.8927e-05 - val_mae: 0.0051\n",
      "Epoch 709/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2166e-05 - mae: 0.0037\n",
      "Epoch 709: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1384e-05 - mae: 0.0037 - val_loss: 3.8836e-05 - val_mae: 0.0051\n",
      "Epoch 710/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8177e-05 - mae: 0.0034\n",
      "Epoch 710: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7788e-05 - mae: 0.0033 - val_loss: 3.9251e-05 - val_mae: 0.0052\n",
      "Epoch 711/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8787e-05 - mae: 0.0034\n",
      "Epoch 711: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.7969e-05 - mae: 0.0034 - val_loss: 3.9106e-05 - val_mae: 0.0052\n",
      "Epoch 712/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6073e-05 - mae: 0.0032\n",
      "Epoch 712: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7395e-05 - mae: 0.0033 - val_loss: 3.8555e-05 - val_mae: 0.0051\n",
      "Epoch 713/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7739e-05 - mae: 0.0033\n",
      "Epoch 713: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7544e-05 - mae: 0.0033 - val_loss: 4.0492e-05 - val_mae: 0.0053\n",
      "Epoch 714/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7727e-05 - mae: 0.0033\n",
      "Epoch 714: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7005e-05 - mae: 0.0033 - val_loss: 3.9142e-05 - val_mae: 0.0051\n",
      "Epoch 715/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7112e-05 - mae: 0.0032\n",
      "Epoch 715: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8687e-05 - mae: 0.0034 - val_loss: 4.1399e-05 - val_mae: 0.0053\n",
      "Epoch 716/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.7223e-05 - mae: 0.0033\n",
      "Epoch 716: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7739e-05 - mae: 0.0033 - val_loss: 3.9173e-05 - val_mae: 0.0051\n",
      "Epoch 717/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6818e-05 - mae: 0.0033\n",
      "Epoch 717: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0193e-05 - mae: 0.0035 - val_loss: 4.4486e-05 - val_mae: 0.0055\n",
      "Epoch 718/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7421e-05 - mae: 0.0034\n",
      "Epoch 718: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8126e-05 - mae: 0.0034 - val_loss: 4.0901e-05 - val_mae: 0.0053\n",
      "Epoch 719/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7037e-05 - mae: 0.0033\n",
      "Epoch 719: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7468e-05 - mae: 0.0033 - val_loss: 4.5661e-05 - val_mae: 0.0055\n",
      "Epoch 720/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0543e-05 - mae: 0.0036\n",
      "Epoch 720: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9364e-05 - mae: 0.0035 - val_loss: 3.8806e-05 - val_mae: 0.0051\n",
      "Epoch 721/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8555e-05 - mae: 0.0034\n",
      "Epoch 721: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7372e-05 - mae: 0.0033 - val_loss: 4.0542e-05 - val_mae: 0.0053\n",
      "Epoch 722/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7634e-05 - mae: 0.0033\n",
      "Epoch 722: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.7881e-05 - mae: 0.0033 - val_loss: 4.1339e-05 - val_mae: 0.0053\n",
      "Epoch 723/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7502e-05 - mae: 0.0034\n",
      "Epoch 723: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8769e-05 - mae: 0.0034 - val_loss: 4.5216e-05 - val_mae: 0.0054\n",
      "Epoch 724/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7979e-05 - mae: 0.0034\n",
      "Epoch 724: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7813e-05 - mae: 0.0033 - val_loss: 4.1645e-05 - val_mae: 0.0053\n",
      "Epoch 725/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8090e-05 - mae: 0.0033\n",
      "Epoch 725: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7904e-05 - mae: 0.0033 - val_loss: 3.9764e-05 - val_mae: 0.0052\n",
      "Epoch 726/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6086e-05 - mae: 0.0032\n",
      "Epoch 726: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7037e-05 - mae: 0.0033 - val_loss: 4.1468e-05 - val_mae: 0.0053\n",
      "Epoch 727/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7271e-05 - mae: 0.0033\n",
      "Epoch 727: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8068e-05 - mae: 0.0033 - val_loss: 3.8258e-05 - val_mae: 0.0051\n",
      "Epoch 728/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9598e-05 - mae: 0.0035\n",
      "Epoch 728: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8802e-05 - mae: 0.0034 - val_loss: 3.8516e-05 - val_mae: 0.0051\n",
      "Epoch 729/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7319e-05 - mae: 0.0033\n",
      "Epoch 729: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8555e-05 - mae: 0.0034 - val_loss: 3.9116e-05 - val_mae: 0.0051\n",
      "Epoch 730/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9094e-05 - mae: 0.0034\n",
      "Epoch 730: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.8135e-05 - mae: 0.0033 - val_loss: 3.9022e-05 - val_mae: 0.0051\n",
      "Epoch 731/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7913e-05 - mae: 0.0033\n",
      "Epoch 731: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.8852e-05 - mae: 0.0034 - val_loss: 4.1004e-05 - val_mae: 0.0053\n",
      "Epoch 732/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0435e-05 - mae: 0.0036\n",
      "Epoch 732: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9303e-05 - mae: 0.0035 - val_loss: 3.8037e-05 - val_mae: 0.0051\n",
      "Epoch 733/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.7249e-05 - mae: 0.0033\n",
      "Epoch 733: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.7295e-05 - mae: 0.0033 - val_loss: 3.9826e-05 - val_mae: 0.0052\n",
      "Epoch 734/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.6950e-05 - mae: 0.0032\n",
      "Epoch 734: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.8110e-05 - mae: 0.0033 - val_loss: 3.8798e-05 - val_mae: 0.0052\n",
      "Epoch 735/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.8576e-05 - mae: 0.0035\n",
      "Epoch 735: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.8814e-05 - mae: 0.0035 - val_loss: 3.8805e-05 - val_mae: 0.0051\n",
      "Epoch 736/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.8193e-05 - mae: 0.0034\n",
      "Epoch 736: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.9028e-05 - mae: 0.0034 - val_loss: 4.6721e-05 - val_mae: 0.0056\n",
      "Epoch 737/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.7583e-05 - mae: 0.0033\n",
      "Epoch 737: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.8385e-05 - mae: 0.0034 - val_loss: 3.8518e-05 - val_mae: 0.0051\n",
      "Epoch 738/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.8129e-05 - mae: 0.0033\n",
      "Epoch 738: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.7932e-05 - mae: 0.0033 - val_loss: 3.8046e-05 - val_mae: 0.0051\n",
      "Epoch 739/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7506e-05 - mae: 0.0033\n",
      "Epoch 739: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.7433e-05 - mae: 0.0033 - val_loss: 3.8723e-05 - val_mae: 0.0051\n",
      "Epoch 740/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.6660e-05 - mae: 0.0032\n",
      "Epoch 740: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7952e-05 - mae: 0.0033 - val_loss: 3.9198e-05 - val_mae: 0.0052\n",
      "Epoch 741/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.8851e-05 - mae: 0.0034\n",
      "Epoch 741: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.9232e-05 - mae: 0.0034 - val_loss: 4.0884e-05 - val_mae: 0.0052\n",
      "Epoch 742/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.8589e-05 - mae: 0.0033\n",
      "Epoch 742: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.8640e-05 - mae: 0.0034 - val_loss: 4.5405e-05 - val_mae: 0.0055\n",
      "Epoch 743/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.7307e-05 - mae: 0.0033\n",
      "Epoch 743: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.7726e-05 - mae: 0.0033 - val_loss: 3.8489e-05 - val_mae: 0.0051\n",
      "Epoch 744/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.8745e-05 - mae: 0.0034\n",
      "Epoch 744: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8727e-05 - mae: 0.0034 - val_loss: 4.4858e-05 - val_mae: 0.0055\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/800 [====================>.........] - ETA: 0s - loss: 1.9224e-05 - mae: 0.0035\n",
      "Epoch 745: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.9061e-05 - mae: 0.0035 - val_loss: 3.8497e-05 - val_mae: 0.0051\n",
      "Epoch 746/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.7967e-05 - mae: 0.0034\n",
      "Epoch 746: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8822e-05 - mae: 0.0034 - val_loss: 3.8995e-05 - val_mae: 0.0051\n",
      "Epoch 747/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.8415e-05 - mae: 0.0034\n",
      "Epoch 747: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.8289e-05 - mae: 0.0033 - val_loss: 3.8461e-05 - val_mae: 0.0051\n",
      "Epoch 748/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.8716e-05 - mae: 0.0034\n",
      "Epoch 748: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.8883e-05 - mae: 0.0034 - val_loss: 3.9236e-05 - val_mae: 0.0052\n",
      "Epoch 749/1000\n",
      "610/800 [=====================>........] - ETA: 0s - loss: 1.7974e-05 - mae: 0.0034\n",
      "Epoch 749: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.8660e-05 - mae: 0.0034 - val_loss: 4.2356e-05 - val_mae: 0.0053\n",
      "Epoch 750/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.7764e-05 - mae: 0.0034\n",
      "Epoch 750: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.7553e-05 - mae: 0.0033 - val_loss: 3.9355e-05 - val_mae: 0.0051\n",
      "Epoch 751/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.8304e-05 - mae: 0.0033\n",
      "Epoch 751: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.8159e-05 - mae: 0.0033 - val_loss: 3.8350e-05 - val_mae: 0.0051\n",
      "Epoch 752/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 2.0438e-05 - mae: 0.0035\n",
      "Epoch 752: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.9126e-05 - mae: 0.0034 - val_loss: 3.8758e-05 - val_mae: 0.0051\n",
      "Epoch 753/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.9112e-05 - mae: 0.0034\n",
      "Epoch 753: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.8263e-05 - mae: 0.0034 - val_loss: 3.8774e-05 - val_mae: 0.0051\n",
      "Epoch 754/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.8442e-05 - mae: 0.0034\n",
      "Epoch 754: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.9172e-05 - mae: 0.0035 - val_loss: 4.6618e-05 - val_mae: 0.0056\n",
      "Epoch 755/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7561e-05 - mae: 0.0033\n",
      "Epoch 755: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.9012e-05 - mae: 0.0034 - val_loss: 3.9542e-05 - val_mae: 0.0052\n",
      "Epoch 756/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9899e-05 - mae: 0.0035\n",
      "Epoch 756: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9582e-05 - mae: 0.0035 - val_loss: 4.0325e-05 - val_mae: 0.0052\n",
      "Epoch 757/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6430e-05 - mae: 0.0032\n",
      "Epoch 757: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.7038e-05 - mae: 0.0033 - val_loss: 3.8946e-05 - val_mae: 0.0051\n",
      "Epoch 758/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8895e-05 - mae: 0.0035\n",
      "Epoch 758: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.8941e-05 - mae: 0.0034 - val_loss: 3.9027e-05 - val_mae: 0.0052\n",
      "Epoch 759/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6439e-05 - mae: 0.0031\n",
      "Epoch 759: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7679e-05 - mae: 0.0033 - val_loss: 4.0219e-05 - val_mae: 0.0052\n",
      "Epoch 760/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8182e-05 - mae: 0.0033\n",
      "Epoch 760: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.8034e-05 - mae: 0.0034 - val_loss: 4.5555e-05 - val_mae: 0.0055\n",
      "Epoch 761/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7356e-05 - mae: 0.0033\n",
      "Epoch 761: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.7959e-05 - mae: 0.0034 - val_loss: 3.8774e-05 - val_mae: 0.0051\n",
      "Epoch 762/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8263e-05 - mae: 0.0034\n",
      "Epoch 762: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9621e-05 - mae: 0.0035 - val_loss: 3.8595e-05 - val_mae: 0.0051\n",
      "Epoch 763/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6208e-05 - mae: 0.0032\n",
      "Epoch 763: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7346e-05 - mae: 0.0033 - val_loss: 4.2746e-05 - val_mae: 0.0054\n",
      "Epoch 764/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7067e-05 - mae: 0.0033\n",
      "Epoch 764: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7414e-05 - mae: 0.0033 - val_loss: 3.8529e-05 - val_mae: 0.0051\n",
      "Epoch 765/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7049e-05 - mae: 0.0033\n",
      "Epoch 765: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7475e-05 - mae: 0.0033 - val_loss: 4.1221e-05 - val_mae: 0.0053\n",
      "Epoch 766/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9222e-05 - mae: 0.0035\n",
      "Epoch 766: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9642e-05 - mae: 0.0035 - val_loss: 3.8901e-05 - val_mae: 0.0051\n",
      "Epoch 767/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7719e-05 - mae: 0.0033\n",
      "Epoch 767: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7610e-05 - mae: 0.0033 - val_loss: 3.9084e-05 - val_mae: 0.0052\n",
      "Epoch 768/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.6015e-05 - mae: 0.0031\n",
      "Epoch 768: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7928e-05 - mae: 0.0033 - val_loss: 3.8473e-05 - val_mae: 0.0051\n",
      "Epoch 769/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9234e-05 - mae: 0.0035\n",
      "Epoch 769: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.1481e-05 - mae: 0.0037 - val_loss: 4.1563e-05 - val_mae: 0.0052\n",
      "Epoch 770/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9405e-05 - mae: 0.0035\n",
      "Epoch 770: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9133e-05 - mae: 0.0035 - val_loss: 4.0632e-05 - val_mae: 0.0053\n",
      "Epoch 771/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7993e-05 - mae: 0.0033\n",
      "Epoch 771: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7707e-05 - mae: 0.0033 - val_loss: 4.0031e-05 - val_mae: 0.0052\n",
      "Epoch 772/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7692e-05 - mae: 0.0033\n",
      "Epoch 772: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7393e-05 - mae: 0.0033 - val_loss: 3.9787e-05 - val_mae: 0.0052\n",
      "Epoch 773/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8078e-05 - mae: 0.0034\n",
      "Epoch 773: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8382e-05 - mae: 0.0034 - val_loss: 4.1085e-05 - val_mae: 0.0053\n",
      "Epoch 774/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7921e-05 - mae: 0.0034\n",
      "Epoch 774: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8674e-05 - mae: 0.0034 - val_loss: 4.3191e-05 - val_mae: 0.0054\n",
      "Epoch 775/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7402e-05 - mae: 0.0033\n",
      "Epoch 775: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7548e-05 - mae: 0.0033 - val_loss: 4.0933e-05 - val_mae: 0.0052\n",
      "Epoch 776/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8714e-05 - mae: 0.0034\n",
      "Epoch 776: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8869e-05 - mae: 0.0035 - val_loss: 3.9588e-05 - val_mae: 0.0052\n",
      "Epoch 777/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8099e-05 - mae: 0.0033\n",
      "Epoch 777: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7979e-05 - mae: 0.0033 - val_loss: 4.4705e-05 - val_mae: 0.0055\n",
      "Epoch 778/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7098e-05 - mae: 0.0032\n",
      "Epoch 778: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8501e-05 - mae: 0.0034 - val_loss: 4.2911e-05 - val_mae: 0.0054\n",
      "Epoch 779/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4546e-05 - mae: 0.0030\n",
      "Epoch 779: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.6882e-05 - mae: 0.0032 - val_loss: 3.8764e-05 - val_mae: 0.0051\n",
      "Epoch 780/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9128e-05 - mae: 0.0035\n",
      "Epoch 780: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.9479e-05 - mae: 0.0035 - val_loss: 4.9339e-05 - val_mae: 0.0057\n",
      "Epoch 781/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6936e-05 - mae: 0.0032\n",
      "Epoch 781: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7412e-05 - mae: 0.0033 - val_loss: 4.0710e-05 - val_mae: 0.0053\n",
      "Epoch 782/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6484e-05 - mae: 0.0032\n",
      "Epoch 782: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7533e-05 - mae: 0.0033 - val_loss: 3.8683e-05 - val_mae: 0.0051\n",
      "Epoch 783/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8618e-05 - mae: 0.0033\n",
      "Epoch 783: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8205e-05 - mae: 0.0033 - val_loss: 4.5899e-05 - val_mae: 0.0055\n",
      "Epoch 784/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9894e-05 - mae: 0.0036\n",
      "Epoch 784: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8394e-05 - mae: 0.0034 - val_loss: 3.8881e-05 - val_mae: 0.0051\n",
      "Epoch 785/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0467e-05 - mae: 0.0036\n",
      "Epoch 785: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8989e-05 - mae: 0.0035 - val_loss: 3.8809e-05 - val_mae: 0.0051\n",
      "Epoch 786/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8459e-05 - mae: 0.0034\n",
      "Epoch 786: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9040e-05 - mae: 0.0034 - val_loss: 4.0077e-05 - val_mae: 0.0052\n",
      "Epoch 787/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7802e-05 - mae: 0.0034\n",
      "Epoch 787: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7498e-05 - mae: 0.0033 - val_loss: 3.8724e-05 - val_mae: 0.0051\n",
      "Epoch 788/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7925e-05 - mae: 0.0034\n",
      "Epoch 788: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7457e-05 - mae: 0.0033 - val_loss: 4.3762e-05 - val_mae: 0.0054\n",
      "Epoch 789/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8909e-05 - mae: 0.0034\n",
      "Epoch 789: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9108e-05 - mae: 0.0034 - val_loss: 3.8866e-05 - val_mae: 0.0051\n",
      "Epoch 790/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1183e-05 - mae: 0.0037\n",
      "Epoch 790: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0710e-05 - mae: 0.0036 - val_loss: 5.4906e-05 - val_mae: 0.0060\n",
      "Epoch 791/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9989e-05 - mae: 0.0036\n",
      "Epoch 791: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9462e-05 - mae: 0.0035 - val_loss: 3.8414e-05 - val_mae: 0.0051\n",
      "Epoch 792/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6303e-05 - mae: 0.0032\n",
      "Epoch 792: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8611e-05 - mae: 0.0034 - val_loss: 3.8536e-05 - val_mae: 0.0051\n",
      "Epoch 793/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8701e-05 - mae: 0.0034\n",
      "Epoch 793: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9096e-05 - mae: 0.0035 - val_loss: 3.8879e-05 - val_mae: 0.0052\n",
      "Epoch 794/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6208e-05 - mae: 0.0032\n",
      "Epoch 794: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7652e-05 - mae: 0.0033 - val_loss: 4.0001e-05 - val_mae: 0.0052\n",
      "Epoch 795/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7224e-05 - mae: 0.0033\n",
      "Epoch 795: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8024e-05 - mae: 0.0033 - val_loss: 3.9829e-05 - val_mae: 0.0052\n",
      "Epoch 796/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7322e-05 - mae: 0.0034\n",
      "Epoch 796: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7163e-05 - mae: 0.0033 - val_loss: 3.8253e-05 - val_mae: 0.0051\n",
      "Epoch 797/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7404e-05 - mae: 0.0033\n",
      "Epoch 797: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7627e-05 - mae: 0.0033 - val_loss: 3.9083e-05 - val_mae: 0.0051\n",
      "Epoch 798/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6924e-05 - mae: 0.0033\n",
      "Epoch 798: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9567e-05 - mae: 0.0035 - val_loss: 4.2232e-05 - val_mae: 0.0052\n",
      "Epoch 799/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1444e-05 - mae: 0.0037\n",
      "Epoch 799: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8993e-05 - mae: 0.0035 - val_loss: 4.1582e-05 - val_mae: 0.0053\n",
      "Epoch 800/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7687e-05 - mae: 0.0033\n",
      "Epoch 800: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7373e-05 - mae: 0.0033 - val_loss: 3.8269e-05 - val_mae: 0.0051\n",
      "Epoch 801/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7772e-05 - mae: 0.0033\n",
      "Epoch 801: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.7504e-05 - mae: 0.0033 - val_loss: 3.8247e-05 - val_mae: 0.0051\n",
      "Epoch 802/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8448e-05 - mae: 0.0034\n",
      "Epoch 802: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.9750e-05 - mae: 0.0035 - val_loss: 4.2595e-05 - val_mae: 0.0054\n",
      "Epoch 803/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/800 [================>.............] - ETA: 0s - loss: 1.8290e-05 - mae: 0.0034\n",
      "Epoch 803: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8826e-05 - mae: 0.0034 - val_loss: 6.0585e-05 - val_mae: 0.0063\n",
      "Epoch 804/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9571e-05 - mae: 0.0036\n",
      "Epoch 804: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8394e-05 - mae: 0.0034 - val_loss: 3.8089e-05 - val_mae: 0.0051\n",
      "Epoch 805/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6912e-05 - mae: 0.0032\n",
      "Epoch 805: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7518e-05 - mae: 0.0033 - val_loss: 4.1624e-05 - val_mae: 0.0052\n",
      "Epoch 806/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7566e-05 - mae: 0.0033\n",
      "Epoch 806: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7633e-05 - mae: 0.0033 - val_loss: 3.9609e-05 - val_mae: 0.0051\n",
      "Epoch 807/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7193e-05 - mae: 0.0033\n",
      "Epoch 807: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7393e-05 - mae: 0.0033 - val_loss: 3.9494e-05 - val_mae: 0.0052\n",
      "Epoch 808/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7679e-05 - mae: 0.0033\n",
      "Epoch 808: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9296e-05 - mae: 0.0035 - val_loss: 5.0350e-05 - val_mae: 0.0058\n",
      "Epoch 809/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7723e-05 - mae: 0.0032\n",
      "Epoch 809: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8838e-05 - mae: 0.0034 - val_loss: 3.8231e-05 - val_mae: 0.0051\n",
      "Epoch 810/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8372e-05 - mae: 0.0034\n",
      "Epoch 810: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8201e-05 - mae: 0.0034 - val_loss: 3.8421e-05 - val_mae: 0.0051\n",
      "Epoch 811/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5605e-05 - mae: 0.0032\n",
      "Epoch 811: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7967e-05 - mae: 0.0034 - val_loss: 3.9498e-05 - val_mae: 0.0052\n",
      "Epoch 812/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7884e-05 - mae: 0.0033\n",
      "Epoch 812: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8481e-05 - mae: 0.0034 - val_loss: 4.3073e-05 - val_mae: 0.0054\n",
      "Epoch 813/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6107e-05 - mae: 0.0032\n",
      "Epoch 813: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7794e-05 - mae: 0.0034 - val_loss: 4.3405e-05 - val_mae: 0.0054\n",
      "Epoch 814/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7248e-05 - mae: 0.0033\n",
      "Epoch 814: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8151e-05 - mae: 0.0034 - val_loss: 3.8716e-05 - val_mae: 0.0051\n",
      "Epoch 815/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8531e-05 - mae: 0.0034\n",
      "Epoch 815: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8522e-05 - mae: 0.0034 - val_loss: 4.1095e-05 - val_mae: 0.0053\n",
      "Epoch 816/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8940e-05 - mae: 0.0035\n",
      "Epoch 816: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0365e-05 - mae: 0.0035 - val_loss: 4.0454e-05 - val_mae: 0.0052\n",
      "Epoch 817/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7947e-05 - mae: 0.0034\n",
      "Epoch 817: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8268e-05 - mae: 0.0034 - val_loss: 4.2133e-05 - val_mae: 0.0052\n",
      "Epoch 818/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8540e-05 - mae: 0.0034\n",
      "Epoch 818: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9476e-05 - mae: 0.0035 - val_loss: 4.1743e-05 - val_mae: 0.0052\n",
      "Epoch 819/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8666e-05 - mae: 0.0034\n",
      "Epoch 819: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8078e-05 - mae: 0.0034 - val_loss: 4.1191e-05 - val_mae: 0.0053\n",
      "Epoch 820/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7678e-05 - mae: 0.0033\n",
      "Epoch 820: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7490e-05 - mae: 0.0033 - val_loss: 3.9956e-05 - val_mae: 0.0051\n",
      "Epoch 821/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6362e-05 - mae: 0.0032\n",
      "Epoch 821: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7787e-05 - mae: 0.0033 - val_loss: 3.9964e-05 - val_mae: 0.0052\n",
      "Epoch 822/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7959e-05 - mae: 0.0034\n",
      "Epoch 822: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8062e-05 - mae: 0.0034 - val_loss: 4.8226e-05 - val_mae: 0.0057\n",
      "Epoch 823/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8750e-05 - mae: 0.0034\n",
      "Epoch 823: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8934e-05 - mae: 0.0034 - val_loss: 4.2141e-05 - val_mae: 0.0053\n",
      "Epoch 824/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7730e-05 - mae: 0.0033\n",
      "Epoch 824: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7730e-05 - mae: 0.0033 - val_loss: 4.0856e-05 - val_mae: 0.0053\n",
      "Epoch 825/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6758e-05 - mae: 0.0032\n",
      "Epoch 825: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7238e-05 - mae: 0.0033 - val_loss: 4.0050e-05 - val_mae: 0.0051\n",
      "Epoch 826/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1758e-05 - mae: 0.0038\n",
      "Epoch 826: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.2103e-05 - mae: 0.0038 - val_loss: 3.9095e-05 - val_mae: 0.0051\n",
      "Epoch 827/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6733e-05 - mae: 0.0033\n",
      "Epoch 827: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8018e-05 - mae: 0.0034 - val_loss: 3.8595e-05 - val_mae: 0.0051\n",
      "Epoch 828/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9590e-05 - mae: 0.0035\n",
      "Epoch 828: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8449e-05 - mae: 0.0034 - val_loss: 3.8542e-05 - val_mae: 0.0051\n",
      "Epoch 829/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9235e-05 - mae: 0.0035\n",
      "Epoch 829: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9820e-05 - mae: 0.0036 - val_loss: 3.8682e-05 - val_mae: 0.0051\n",
      "Epoch 830/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9458e-05 - mae: 0.0035\n",
      "Epoch 830: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8484e-05 - mae: 0.0034 - val_loss: 3.9010e-05 - val_mae: 0.0052\n",
      "Epoch 831/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7029e-05 - mae: 0.0032\n",
      "Epoch 831: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8166e-05 - mae: 0.0033 - val_loss: 3.8818e-05 - val_mae: 0.0051\n",
      "Epoch 832/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7833e-05 - mae: 0.0033\n",
      "Epoch 832: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7046e-05 - mae: 0.0033 - val_loss: 4.1406e-05 - val_mae: 0.0052\n",
      "Epoch 833/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8556e-05 - mae: 0.0034\n",
      "Epoch 833: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8672e-05 - mae: 0.0034 - val_loss: 3.9523e-05 - val_mae: 0.0052\n",
      "Epoch 834/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.6852e-05 - mae: 0.0032\n",
      "Epoch 834: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.6852e-05 - mae: 0.0032 - val_loss: 3.8836e-05 - val_mae: 0.0051\n",
      "Epoch 835/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6866e-05 - mae: 0.0032\n",
      "Epoch 835: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7802e-05 - mae: 0.0033 - val_loss: 4.2156e-05 - val_mae: 0.0053\n",
      "Epoch 836/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0082e-05 - mae: 0.0035\n",
      "Epoch 836: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8207e-05 - mae: 0.0034 - val_loss: 3.8703e-05 - val_mae: 0.0051\n",
      "Epoch 837/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8705e-05 - mae: 0.0034\n",
      "Epoch 837: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8705e-05 - mae: 0.0034 - val_loss: 3.9314e-05 - val_mae: 0.0052\n",
      "Epoch 838/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7920e-05 - mae: 0.0033\n",
      "Epoch 838: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7949e-05 - mae: 0.0033 - val_loss: 4.3230e-05 - val_mae: 0.0054\n",
      "Epoch 839/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6183e-05 - mae: 0.0032\n",
      "Epoch 839: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8191e-05 - mae: 0.0034 - val_loss: 3.9800e-05 - val_mae: 0.0052\n",
      "Epoch 840/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7431e-05 - mae: 0.0033\n",
      "Epoch 840: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7467e-05 - mae: 0.0033 - val_loss: 3.8793e-05 - val_mae: 0.0051\n",
      "Epoch 841/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7416e-05 - mae: 0.0033\n",
      "Epoch 841: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7756e-05 - mae: 0.0034 - val_loss: 3.9197e-05 - val_mae: 0.0052\n",
      "Epoch 842/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7842e-05 - mae: 0.0034\n",
      "Epoch 842: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7792e-05 - mae: 0.0034 - val_loss: 3.9884e-05 - val_mae: 0.0052\n",
      "Epoch 843/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6263e-05 - mae: 0.0032\n",
      "Epoch 843: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7225e-05 - mae: 0.0033 - val_loss: 3.9676e-05 - val_mae: 0.0051\n",
      "Epoch 844/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9097e-05 - mae: 0.0035\n",
      "Epoch 844: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8652e-05 - mae: 0.0034 - val_loss: 4.1918e-05 - val_mae: 0.0053\n",
      "Epoch 845/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.3799e-05 - mae: 0.0039\n",
      "Epoch 845: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.1522e-05 - mae: 0.0037 - val_loss: 3.9409e-05 - val_mae: 0.0052\n",
      "Epoch 846/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5708e-05 - mae: 0.0031\n",
      "Epoch 846: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7832e-05 - mae: 0.0033 - val_loss: 4.1246e-05 - val_mae: 0.0053\n",
      "Epoch 847/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7556e-05 - mae: 0.0033\n",
      "Epoch 847: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8244e-05 - mae: 0.0034 - val_loss: 3.9437e-05 - val_mae: 0.0052\n",
      "Epoch 848/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9479e-05 - mae: 0.0035\n",
      "Epoch 848: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8833e-05 - mae: 0.0034 - val_loss: 3.8733e-05 - val_mae: 0.0052\n",
      "Epoch 849/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5879e-05 - mae: 0.0032\n",
      "Epoch 849: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7806e-05 - mae: 0.0033 - val_loss: 4.0349e-05 - val_mae: 0.0053\n",
      "Epoch 850/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6024e-05 - mae: 0.0032\n",
      "Epoch 850: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7060e-05 - mae: 0.0032 - val_loss: 4.0976e-05 - val_mae: 0.0053\n",
      "Epoch 851/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7503e-05 - mae: 0.0033\n",
      "Epoch 851: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8009e-05 - mae: 0.0033 - val_loss: 4.0042e-05 - val_mae: 0.0052\n",
      "Epoch 852/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6978e-05 - mae: 0.0033\n",
      "Epoch 852: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8776e-05 - mae: 0.0035 - val_loss: 4.1710e-05 - val_mae: 0.0052\n",
      "Epoch 853/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9631e-05 - mae: 0.0035\n",
      "Epoch 853: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9050e-05 - mae: 0.0034 - val_loss: 3.9385e-05 - val_mae: 0.0052\n",
      "Epoch 854/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9982e-05 - mae: 0.0035\n",
      "Epoch 854: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8874e-05 - mae: 0.0034 - val_loss: 4.2334e-05 - val_mae: 0.0054\n",
      "Epoch 855/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6967e-05 - mae: 0.0032\n",
      "Epoch 855: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8021e-05 - mae: 0.0034 - val_loss: 3.9552e-05 - val_mae: 0.0052\n",
      "Epoch 856/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5791e-05 - mae: 0.0031\n",
      "Epoch 856: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7679e-05 - mae: 0.0033 - val_loss: 3.8930e-05 - val_mae: 0.0051\n",
      "Epoch 857/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7489e-05 - mae: 0.0034\n",
      "Epoch 857: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.8701e-05 - mae: 0.0035 - val_loss: 3.8743e-05 - val_mae: 0.0051\n",
      "Epoch 858/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.9237e-05 - mae: 0.0035\n",
      "Epoch 858: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.9093e-05 - mae: 0.0034 - val_loss: 3.8503e-05 - val_mae: 0.0051\n",
      "Epoch 859/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.9794e-05 - mae: 0.0035\n",
      "Epoch 859: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.9578e-05 - mae: 0.0035 - val_loss: 4.1788e-05 - val_mae: 0.0053\n",
      "Epoch 860/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.8626e-05 - mae: 0.0034\n",
      "Epoch 860: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.9284e-05 - mae: 0.0035 - val_loss: 4.0360e-05 - val_mae: 0.0052\n",
      "Epoch 861/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/800 [================>.............] - ETA: 0s - loss: 1.8394e-05 - mae: 0.0034\n",
      "Epoch 861: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9192e-05 - mae: 0.0035 - val_loss: 4.3400e-05 - val_mae: 0.0053\n",
      "Epoch 862/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9570e-05 - mae: 0.0035\n",
      "Epoch 862: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0270e-05 - mae: 0.0036 - val_loss: 3.9190e-05 - val_mae: 0.0052\n",
      "Epoch 863/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1074e-05 - mae: 0.0036\n",
      "Epoch 863: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0136e-05 - mae: 0.0036 - val_loss: 3.8491e-05 - val_mae: 0.0051\n",
      "Epoch 864/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6351e-05 - mae: 0.0032\n",
      "Epoch 864: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8503e-05 - mae: 0.0034 - val_loss: 4.0839e-05 - val_mae: 0.0052\n",
      "Epoch 865/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0392e-05 - mae: 0.0036\n",
      "Epoch 865: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0322e-05 - mae: 0.0036 - val_loss: 3.8803e-05 - val_mae: 0.0051\n",
      "Epoch 866/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8730e-05 - mae: 0.0034\n",
      "Epoch 866: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9104e-05 - mae: 0.0035 - val_loss: 4.1372e-05 - val_mae: 0.0052\n",
      "Epoch 867/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9437e-05 - mae: 0.0034\n",
      "Epoch 867: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9117e-05 - mae: 0.0034 - val_loss: 3.9132e-05 - val_mae: 0.0052\n",
      "Epoch 868/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8684e-05 - mae: 0.0033\n",
      "Epoch 868: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9025e-05 - mae: 0.0034 - val_loss: 3.9867e-05 - val_mae: 0.0052\n",
      "Epoch 869/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0985e-05 - mae: 0.0037\n",
      "Epoch 869: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9916e-05 - mae: 0.0036 - val_loss: 4.4556e-05 - val_mae: 0.0055\n",
      "Epoch 870/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7821e-05 - mae: 0.0034\n",
      "Epoch 870: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7878e-05 - mae: 0.0034 - val_loss: 5.9976e-05 - val_mae: 0.0063\n",
      "Epoch 871/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8546e-05 - mae: 0.0035\n",
      "Epoch 871: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9305e-05 - mae: 0.0035 - val_loss: 3.9359e-05 - val_mae: 0.0052\n",
      "Epoch 872/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7070e-05 - mae: 0.0033\n",
      "Epoch 872: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7498e-05 - mae: 0.0033 - val_loss: 4.3240e-05 - val_mae: 0.0054\n",
      "Epoch 873/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9345e-05 - mae: 0.0035\n",
      "Epoch 873: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0849e-05 - mae: 0.0036 - val_loss: 4.5141e-05 - val_mae: 0.0055\n",
      "Epoch 874/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8285e-05 - mae: 0.0034\n",
      "Epoch 874: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7410e-05 - mae: 0.0033 - val_loss: 3.9315e-05 - val_mae: 0.0052\n",
      "Epoch 875/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6655e-05 - mae: 0.0032\n",
      "Epoch 875: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7476e-05 - mae: 0.0033 - val_loss: 4.3106e-05 - val_mae: 0.0054\n",
      "Epoch 876/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8556e-05 - mae: 0.0034\n",
      "Epoch 876: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8073e-05 - mae: 0.0034 - val_loss: 4.4507e-05 - val_mae: 0.0054\n",
      "Epoch 877/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6243e-05 - mae: 0.0032\n",
      "Epoch 877: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7567e-05 - mae: 0.0033 - val_loss: 3.8471e-05 - val_mae: 0.0051\n",
      "Epoch 878/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5791e-05 - mae: 0.0032\n",
      "Epoch 878: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7413e-05 - mae: 0.0033 - val_loss: 4.0626e-05 - val_mae: 0.0053\n",
      "Epoch 879/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0482e-05 - mae: 0.0036\n",
      "Epoch 879: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8618e-05 - mae: 0.0034 - val_loss: 3.8721e-05 - val_mae: 0.0051\n",
      "Epoch 880/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8731e-05 - mae: 0.0034\n",
      "Epoch 880: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8834e-05 - mae: 0.0034 - val_loss: 3.9313e-05 - val_mae: 0.0052\n",
      "Epoch 881/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5795e-05 - mae: 0.0032\n",
      "Epoch 881: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8489e-05 - mae: 0.0034 - val_loss: 3.8792e-05 - val_mae: 0.0051\n",
      "Epoch 882/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6884e-05 - mae: 0.0032\n",
      "Epoch 882: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7713e-05 - mae: 0.0033 - val_loss: 3.8381e-05 - val_mae: 0.0051\n",
      "Epoch 883/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8005e-05 - mae: 0.0034\n",
      "Epoch 883: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8269e-05 - mae: 0.0034 - val_loss: 3.9434e-05 - val_mae: 0.0052\n",
      "Epoch 884/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7390e-05 - mae: 0.0033\n",
      "Epoch 884: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8386e-05 - mae: 0.0034 - val_loss: 3.9270e-05 - val_mae: 0.0052\n",
      "Epoch 885/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9071e-05 - mae: 0.0034\n",
      "Epoch 885: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8213e-05 - mae: 0.0034 - val_loss: 3.8350e-05 - val_mae: 0.0051\n",
      "Epoch 886/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6406e-05 - mae: 0.0032\n",
      "Epoch 886: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7486e-05 - mae: 0.0033 - val_loss: 4.0379e-05 - val_mae: 0.0051\n",
      "Epoch 887/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7609e-05 - mae: 0.0033\n",
      "Epoch 887: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7357e-05 - mae: 0.0033 - val_loss: 3.8445e-05 - val_mae: 0.0051\n",
      "Epoch 888/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7514e-05 - mae: 0.0033\n",
      "Epoch 888: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7789e-05 - mae: 0.0033 - val_loss: 4.5168e-05 - val_mae: 0.0055\n",
      "Epoch 889/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8653e-05 - mae: 0.0035\n",
      "Epoch 889: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8481e-05 - mae: 0.0034 - val_loss: 3.8497e-05 - val_mae: 0.0051\n",
      "Epoch 890/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.7622e-05 - mae: 0.0033\n",
      "Epoch 890: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7510e-05 - mae: 0.0033 - val_loss: 3.8885e-05 - val_mae: 0.0051\n",
      "Epoch 891/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7452e-05 - mae: 0.0033\n",
      "Epoch 891: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7394e-05 - mae: 0.0033 - val_loss: 3.9781e-05 - val_mae: 0.0052\n",
      "Epoch 892/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7019e-05 - mae: 0.0033\n",
      "Epoch 892: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7666e-05 - mae: 0.0033 - val_loss: 4.3293e-05 - val_mae: 0.0054\n",
      "Epoch 893/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7294e-05 - mae: 0.0032\n",
      "Epoch 893: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0335e-05 - mae: 0.0035 - val_loss: 4.4574e-05 - val_mae: 0.0054\n",
      "Epoch 894/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0513e-05 - mae: 0.0036\n",
      "Epoch 894: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9060e-05 - mae: 0.0035 - val_loss: 4.1296e-05 - val_mae: 0.0052\n",
      "Epoch 895/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9466e-05 - mae: 0.0035\n",
      "Epoch 895: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9466e-05 - mae: 0.0035 - val_loss: 3.8647e-05 - val_mae: 0.0051\n",
      "Epoch 896/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8860e-05 - mae: 0.0034\n",
      "Epoch 896: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8773e-05 - mae: 0.0034 - val_loss: 3.8536e-05 - val_mae: 0.0051\n",
      "Epoch 897/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6261e-05 - mae: 0.0032\n",
      "Epoch 897: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8814e-05 - mae: 0.0034 - val_loss: 3.9981e-05 - val_mae: 0.0052\n",
      "Epoch 898/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7757e-05 - mae: 0.0033\n",
      "Epoch 898: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7788e-05 - mae: 0.0033 - val_loss: 3.9641e-05 - val_mae: 0.0052\n",
      "Epoch 899/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7628e-05 - mae: 0.0033\n",
      "Epoch 899: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7625e-05 - mae: 0.0033 - val_loss: 3.9673e-05 - val_mae: 0.0051\n",
      "Epoch 900/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6335e-05 - mae: 0.0032\n",
      "Epoch 900: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7342e-05 - mae: 0.0033 - val_loss: 3.8499e-05 - val_mae: 0.0051\n",
      "Epoch 901/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9310e-05 - mae: 0.0035\n",
      "Epoch 901: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9196e-05 - mae: 0.0035 - val_loss: 3.8468e-05 - val_mae: 0.0051\n",
      "Epoch 902/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.8813e-05 - mae: 0.0034\n",
      "Epoch 902: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9663e-05 - mae: 0.0035 - val_loss: 4.5781e-05 - val_mae: 0.0055\n",
      "Epoch 903/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6554e-05 - mae: 0.0031\n",
      "Epoch 903: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7515e-05 - mae: 0.0032 - val_loss: 4.0124e-05 - val_mae: 0.0052\n",
      "Epoch 904/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.6421e-05 - mae: 0.0031\n",
      "Epoch 904: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8574e-05 - mae: 0.0034 - val_loss: 3.8520e-05 - val_mae: 0.0051\n",
      "Epoch 905/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7283e-05 - mae: 0.0032\n",
      "Epoch 905: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7298e-05 - mae: 0.0033 - val_loss: 3.8704e-05 - val_mae: 0.0051\n",
      "Epoch 906/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7178e-05 - mae: 0.0033\n",
      "Epoch 906: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8061e-05 - mae: 0.0033 - val_loss: 3.8913e-05 - val_mae: 0.0051\n",
      "Epoch 907/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6735e-05 - mae: 0.0032\n",
      "Epoch 907: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8305e-05 - mae: 0.0034 - val_loss: 3.8558e-05 - val_mae: 0.0051\n",
      "Epoch 908/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6529e-05 - mae: 0.0032\n",
      "Epoch 908: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.6968e-05 - mae: 0.0032 - val_loss: 3.9430e-05 - val_mae: 0.0052\n",
      "Epoch 909/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7872e-05 - mae: 0.0033\n",
      "Epoch 909: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8363e-05 - mae: 0.0034 - val_loss: 3.8561e-05 - val_mae: 0.0051\n",
      "Epoch 910/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9807e-05 - mae: 0.0035\n",
      "Epoch 910: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.9294e-05 - mae: 0.0035 - val_loss: 4.0540e-05 - val_mae: 0.0052\n",
      "Epoch 911/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8935e-05 - mae: 0.0035\n",
      "Epoch 911: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9196e-05 - mae: 0.0035 - val_loss: 4.7545e-05 - val_mae: 0.0056\n",
      "Epoch 912/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6328e-05 - mae: 0.0032\n",
      "Epoch 912: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8420e-05 - mae: 0.0034 - val_loss: 3.9112e-05 - val_mae: 0.0051\n",
      "Epoch 913/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9551e-05 - mae: 0.0035\n",
      "Epoch 913: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8671e-05 - mae: 0.0034 - val_loss: 4.1513e-05 - val_mae: 0.0052\n",
      "Epoch 914/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8865e-05 - mae: 0.0034\n",
      "Epoch 914: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8429e-05 - mae: 0.0034 - val_loss: 4.0732e-05 - val_mae: 0.0053\n",
      "Epoch 915/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6733e-05 - mae: 0.0032\n",
      "Epoch 915: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7833e-05 - mae: 0.0033 - val_loss: 4.1183e-05 - val_mae: 0.0052\n",
      "Epoch 916/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9251e-05 - mae: 0.0034\n",
      "Epoch 916: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7997e-05 - mae: 0.0033 - val_loss: 3.8728e-05 - val_mae: 0.0051\n",
      "Epoch 917/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9245e-05 - mae: 0.0035\n",
      "Epoch 917: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8370e-05 - mae: 0.0034 - val_loss: 4.2226e-05 - val_mae: 0.0053\n",
      "Epoch 918/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7436e-05 - mae: 0.0033\n",
      "Epoch 918: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8124e-05 - mae: 0.0034 - val_loss: 4.0357e-05 - val_mae: 0.0052\n",
      "Epoch 919/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8304e-05 - mae: 0.0033\n",
      "Epoch 919: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8500e-05 - mae: 0.0034 - val_loss: 3.9194e-05 - val_mae: 0.0052\n",
      "Epoch 920/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5950e-05 - mae: 0.0032\n",
      "Epoch 920: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7209e-05 - mae: 0.0033 - val_loss: 3.8350e-05 - val_mae: 0.0051\n",
      "Epoch 921/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6581e-05 - mae: 0.0032\n",
      "Epoch 921: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7511e-05 - mae: 0.0033 - val_loss: 3.8109e-05 - val_mae: 0.0051\n",
      "Epoch 922/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8008e-05 - mae: 0.0033\n",
      "Epoch 922: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9435e-05 - mae: 0.0035 - val_loss: 3.8732e-05 - val_mae: 0.0051\n",
      "Epoch 923/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.7121e-05 - mae: 0.0033\n",
      "Epoch 923: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7866e-05 - mae: 0.0033 - val_loss: 3.8162e-05 - val_mae: 0.0051\n",
      "Epoch 924/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8669e-05 - mae: 0.0034\n",
      "Epoch 924: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7994e-05 - mae: 0.0033 - val_loss: 4.8326e-05 - val_mae: 0.0057\n",
      "Epoch 925/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9160e-05 - mae: 0.0034\n",
      "Epoch 925: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7771e-05 - mae: 0.0033 - val_loss: 4.1495e-05 - val_mae: 0.0053\n",
      "Epoch 926/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6826e-05 - mae: 0.0032\n",
      "Epoch 926: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7241e-05 - mae: 0.0032 - val_loss: 4.1833e-05 - val_mae: 0.0053\n",
      "Epoch 927/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7518e-05 - mae: 0.0033\n",
      "Epoch 927: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8049e-05 - mae: 0.0033 - val_loss: 4.7621e-05 - val_mae: 0.0056\n",
      "Epoch 928/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7827e-05 - mae: 0.0033\n",
      "Epoch 928: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7957e-05 - mae: 0.0033 - val_loss: 3.8237e-05 - val_mae: 0.0051\n",
      "Epoch 929/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7425e-05 - mae: 0.0033\n",
      "Epoch 929: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8949e-05 - mae: 0.0035 - val_loss: 4.0112e-05 - val_mae: 0.0051\n",
      "Epoch 930/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0396e-05 - mae: 0.0036\n",
      "Epoch 930: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9572e-05 - mae: 0.0035 - val_loss: 3.8618e-05 - val_mae: 0.0051\n",
      "Epoch 931/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7188e-05 - mae: 0.0033\n",
      "Epoch 931: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8068e-05 - mae: 0.0034 - val_loss: 5.6063e-05 - val_mae: 0.0061\n",
      "Epoch 932/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.4213e-05 - mae: 0.0039\n",
      "Epoch 932: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.1354e-05 - mae: 0.0037 - val_loss: 4.6735e-05 - val_mae: 0.0056\n",
      "Epoch 933/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8119e-05 - mae: 0.0033\n",
      "Epoch 933: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8099e-05 - mae: 0.0033 - val_loss: 4.2484e-05 - val_mae: 0.0054\n",
      "Epoch 934/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6785e-05 - mae: 0.0032\n",
      "Epoch 934: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7364e-05 - mae: 0.0033 - val_loss: 3.9390e-05 - val_mae: 0.0052\n",
      "Epoch 935/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7797e-05 - mae: 0.0033\n",
      "Epoch 935: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7980e-05 - mae: 0.0034 - val_loss: 3.8555e-05 - val_mae: 0.0051\n",
      "Epoch 936/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6584e-05 - mae: 0.0032\n",
      "Epoch 936: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7450e-05 - mae: 0.0033 - val_loss: 3.8534e-05 - val_mae: 0.0051\n",
      "Epoch 937/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9217e-05 - mae: 0.0035\n",
      "Epoch 937: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8987e-05 - mae: 0.0034 - val_loss: 3.9522e-05 - val_mae: 0.0051\n",
      "Epoch 938/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8267e-05 - mae: 0.0033\n",
      "Epoch 938: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8826e-05 - mae: 0.0034 - val_loss: 3.9668e-05 - val_mae: 0.0051\n",
      "Epoch 939/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7574e-05 - mae: 0.0033\n",
      "Epoch 939: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8366e-05 - mae: 0.0034 - val_loss: 3.9976e-05 - val_mae: 0.0052\n",
      "Epoch 940/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7751e-05 - mae: 0.0033\n",
      "Epoch 940: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7768e-05 - mae: 0.0033 - val_loss: 5.0715e-05 - val_mae: 0.0058\n",
      "Epoch 941/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0766e-05 - mae: 0.0036\n",
      "Epoch 941: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9403e-05 - mae: 0.0035 - val_loss: 4.0111e-05 - val_mae: 0.0052\n",
      "Epoch 942/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6683e-05 - mae: 0.0032\n",
      "Epoch 942: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7922e-05 - mae: 0.0033 - val_loss: 3.8870e-05 - val_mae: 0.0051\n",
      "Epoch 943/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 2.1957e-05 - mae: 0.0037\n",
      "Epoch 943: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 2.0298e-05 - mae: 0.0035 - val_loss: 4.0368e-05 - val_mae: 0.0053\n",
      "Epoch 944/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7659e-05 - mae: 0.0033\n",
      "Epoch 944: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.7396e-05 - mae: 0.0033 - val_loss: 3.8948e-05 - val_mae: 0.0051\n",
      "Epoch 945/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7033e-05 - mae: 0.0033\n",
      "Epoch 945: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7680e-05 - mae: 0.0034 - val_loss: 4.7300e-05 - val_mae: 0.0056\n",
      "Epoch 946/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7110e-05 - mae: 0.0033\n",
      "Epoch 946: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7748e-05 - mae: 0.0033 - val_loss: 4.0387e-05 - val_mae: 0.0052\n",
      "Epoch 947/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7227e-05 - mae: 0.0033\n",
      "Epoch 947: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.9042e-05 - mae: 0.0034 - val_loss: 3.9810e-05 - val_mae: 0.0052\n",
      "Epoch 948/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/800 [================>.............] - ETA: 0s - loss: 1.8135e-05 - mae: 0.0033\n",
      "Epoch 948: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7760e-05 - mae: 0.0033 - val_loss: 3.8657e-05 - val_mae: 0.0051\n",
      "Epoch 949/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7192e-05 - mae: 0.0033\n",
      "Epoch 949: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7850e-05 - mae: 0.0033 - val_loss: 4.0982e-05 - val_mae: 0.0053\n",
      "Epoch 950/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8637e-05 - mae: 0.0034\n",
      "Epoch 950: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8351e-05 - mae: 0.0034 - val_loss: 3.8521e-05 - val_mae: 0.0051\n",
      "Epoch 951/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6930e-05 - mae: 0.0033\n",
      "Epoch 951: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8064e-05 - mae: 0.0034 - val_loss: 4.0285e-05 - val_mae: 0.0052\n",
      "Epoch 952/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9334e-05 - mae: 0.0034\n",
      "Epoch 952: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0130e-05 - mae: 0.0036 - val_loss: 3.8559e-05 - val_mae: 0.0051\n",
      "Epoch 953/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7265e-05 - mae: 0.0033\n",
      "Epoch 953: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.1286e-05 - mae: 0.0036 - val_loss: 4.0265e-05 - val_mae: 0.0052\n",
      "Epoch 954/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7796e-05 - mae: 0.0033\n",
      "Epoch 954: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7569e-05 - mae: 0.0033 - val_loss: 4.0037e-05 - val_mae: 0.0052\n",
      "Epoch 955/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7697e-05 - mae: 0.0033\n",
      "Epoch 955: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7977e-05 - mae: 0.0033 - val_loss: 5.7149e-05 - val_mae: 0.0061\n",
      "Epoch 956/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9695e-05 - mae: 0.0036\n",
      "Epoch 956: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9552e-05 - mae: 0.0035 - val_loss: 3.8526e-05 - val_mae: 0.0051\n",
      "Epoch 957/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8227e-05 - mae: 0.0033\n",
      "Epoch 957: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8193e-05 - mae: 0.0034 - val_loss: 3.9212e-05 - val_mae: 0.0052\n",
      "Epoch 958/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7815e-05 - mae: 0.0033\n",
      "Epoch 958: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7158e-05 - mae: 0.0033 - val_loss: 3.8703e-05 - val_mae: 0.0051\n",
      "Epoch 959/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6526e-05 - mae: 0.0033\n",
      "Epoch 959: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8838e-05 - mae: 0.0034 - val_loss: 3.9884e-05 - val_mae: 0.0052\n",
      "Epoch 960/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9058e-05 - mae: 0.0034\n",
      "Epoch 960: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8460e-05 - mae: 0.0034 - val_loss: 4.0274e-05 - val_mae: 0.0052\n",
      "Epoch 961/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8048e-05 - mae: 0.0033\n",
      "Epoch 961: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8111e-05 - mae: 0.0033 - val_loss: 4.3819e-05 - val_mae: 0.0054\n",
      "Epoch 962/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7805e-05 - mae: 0.0034\n",
      "Epoch 962: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7135e-05 - mae: 0.0033 - val_loss: 4.0807e-05 - val_mae: 0.0053\n",
      "Epoch 963/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7544e-05 - mae: 0.0033\n",
      "Epoch 963: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8968e-05 - mae: 0.0034 - val_loss: 4.1067e-05 - val_mae: 0.0053\n",
      "Epoch 964/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7382e-05 - mae: 0.0033\n",
      "Epoch 964: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7435e-05 - mae: 0.0033 - val_loss: 4.1166e-05 - val_mae: 0.0053\n",
      "Epoch 965/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8186e-05 - mae: 0.0034\n",
      "Epoch 965: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7831e-05 - mae: 0.0034 - val_loss: 3.9510e-05 - val_mae: 0.0051\n",
      "Epoch 966/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5769e-05 - mae: 0.0031\n",
      "Epoch 966: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7827e-05 - mae: 0.0033 - val_loss: 4.5892e-05 - val_mae: 0.0055\n",
      "Epoch 967/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9481e-05 - mae: 0.0034\n",
      "Epoch 967: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8622e-05 - mae: 0.0034 - val_loss: 4.3609e-05 - val_mae: 0.0054\n",
      "Epoch 968/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4792e-05 - mae: 0.0031\n",
      "Epoch 968: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7911e-05 - mae: 0.0033 - val_loss: 4.3898e-05 - val_mae: 0.0054\n",
      "Epoch 969/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7585e-05 - mae: 0.0033\n",
      "Epoch 969: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9049e-05 - mae: 0.0035 - val_loss: 3.9660e-05 - val_mae: 0.0052\n",
      "Epoch 970/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9029e-05 - mae: 0.0034\n",
      "Epoch 970: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9497e-05 - mae: 0.0034 - val_loss: 3.9951e-05 - val_mae: 0.0052\n",
      "Epoch 971/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7789e-05 - mae: 0.0033\n",
      "Epoch 971: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.7589e-05 - mae: 0.0033 - val_loss: 4.1512e-05 - val_mae: 0.0053\n",
      "Epoch 972/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7334e-05 - mae: 0.0033\n",
      "Epoch 972: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8066e-05 - mae: 0.0034 - val_loss: 4.3947e-05 - val_mae: 0.0054\n",
      "Epoch 973/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8277e-05 - mae: 0.0034\n",
      "Epoch 973: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8038e-05 - mae: 0.0033 - val_loss: 3.8762e-05 - val_mae: 0.0051\n",
      "Epoch 974/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.8746e-05 - mae: 0.0034\n",
      "Epoch 974: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 1.8833e-05 - mae: 0.0034 - val_loss: 4.2304e-05 - val_mae: 0.0054\n",
      "Epoch 975/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.9886e-05 - mae: 0.0035\n",
      "Epoch 975: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.9078e-05 - mae: 0.0035 - val_loss: 3.8675e-05 - val_mae: 0.0051\n",
      "Epoch 976/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7388e-05 - mae: 0.0033\n",
      "Epoch 976: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.7907e-05 - mae: 0.0033 - val_loss: 4.3102e-05 - val_mae: 0.0053\n",
      "Epoch 977/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9127e-05 - mae: 0.0034\n",
      "Epoch 977: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.9044e-05 - mae: 0.0034 - val_loss: 4.1589e-05 - val_mae: 0.0053\n",
      "Epoch 978/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8497e-05 - mae: 0.0034\n",
      "Epoch 978: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8593e-05 - mae: 0.0034 - val_loss: 3.8511e-05 - val_mae: 0.0051\n",
      "Epoch 979/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9758e-05 - mae: 0.0036\n",
      "Epoch 979: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9758e-05 - mae: 0.0036 - val_loss: 3.9394e-05 - val_mae: 0.0052\n",
      "Epoch 980/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7757e-05 - mae: 0.0033\n",
      "Epoch 980: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7592e-05 - mae: 0.0033 - val_loss: 3.9527e-05 - val_mae: 0.0051\n",
      "Epoch 981/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8681e-05 - mae: 0.0034\n",
      "Epoch 981: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8899e-05 - mae: 0.0035 - val_loss: 4.6342e-05 - val_mae: 0.0055\n",
      "Epoch 982/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7639e-05 - mae: 0.0033\n",
      "Epoch 982: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.7723e-05 - mae: 0.0033 - val_loss: 4.0476e-05 - val_mae: 0.0052\n",
      "Epoch 983/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7238e-05 - mae: 0.0033\n",
      "Epoch 983: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7265e-05 - mae: 0.0033 - val_loss: 4.2225e-05 - val_mae: 0.0053\n",
      "Epoch 984/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7386e-05 - mae: 0.0033\n",
      "Epoch 984: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.7407e-05 - mae: 0.0033 - val_loss: 4.2617e-05 - val_mae: 0.0054\n",
      "Epoch 985/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0259e-05 - mae: 0.0035\n",
      "Epoch 985: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8597e-05 - mae: 0.0034 - val_loss: 4.1829e-05 - val_mae: 0.0053\n",
      "Epoch 986/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8564e-05 - mae: 0.0033\n",
      "Epoch 986: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8795e-05 - mae: 0.0034 - val_loss: 3.8447e-05 - val_mae: 0.0051\n",
      "Epoch 987/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7198e-05 - mae: 0.0033\n",
      "Epoch 987: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.7526e-05 - mae: 0.0033 - val_loss: 3.8128e-05 - val_mae: 0.0051\n",
      "Epoch 988/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6789e-05 - mae: 0.0033\n",
      "Epoch 988: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7573e-05 - mae: 0.0033 - val_loss: 3.8632e-05 - val_mae: 0.0051\n",
      "Epoch 989/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7708e-05 - mae: 0.0033\n",
      "Epoch 989: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7764e-05 - mae: 0.0033 - val_loss: 3.9667e-05 - val_mae: 0.0052\n",
      "Epoch 990/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8229e-05 - mae: 0.0033\n",
      "Epoch 990: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7661e-05 - mae: 0.0033 - val_loss: 4.4688e-05 - val_mae: 0.0055\n",
      "Epoch 991/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8653e-05 - mae: 0.0034\n",
      "Epoch 991: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 210us/sample - loss: 1.8740e-05 - mae: 0.0034 - val_loss: 3.8550e-05 - val_mae: 0.0051\n",
      "Epoch 992/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7719e-05 - mae: 0.0033\n",
      "Epoch 992: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8598e-05 - mae: 0.0034 - val_loss: 3.9945e-05 - val_mae: 0.0052\n",
      "Epoch 993/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7782e-05 - mae: 0.0034\n",
      "Epoch 993: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8867e-05 - mae: 0.0035 - val_loss: 3.9804e-05 - val_mae: 0.0052\n",
      "Epoch 994/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8112e-05 - mae: 0.0034\n",
      "Epoch 994: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7368e-05 - mae: 0.0033 - val_loss: 3.8917e-05 - val_mae: 0.0051\n",
      "Epoch 995/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.6863e-05 - mae: 0.0033\n",
      "Epoch 995: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.7838e-05 - mae: 0.0033 - val_loss: 5.3633e-05 - val_mae: 0.0059\n",
      "Epoch 996/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9556e-05 - mae: 0.0036\n",
      "Epoch 996: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9395e-05 - mae: 0.0035 - val_loss: 3.9641e-05 - val_mae: 0.0052\n",
      "Epoch 997/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7021e-05 - mae: 0.0032\n",
      "Epoch 997: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7907e-05 - mae: 0.0033 - val_loss: 3.9342e-05 - val_mae: 0.0051\n",
      "Epoch 998/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6993e-05 - mae: 0.0032\n",
      "Epoch 998: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7276e-05 - mae: 0.0033 - val_loss: 4.5871e-05 - val_mae: 0.0055\n",
      "Epoch 999/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7722e-05 - mae: 0.0033\n",
      "Epoch 999: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8195e-05 - mae: 0.0034 - val_loss: 4.0088e-05 - val_mae: 0.0052\n",
      "Epoch 1000/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0432e-05 - mae: 0.0036\n",
      "Epoch 1000: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9977e-05 - mae: 0.0035 - val_loss: 3.9294e-05 - val_mae: 0.0051\n"
     ]
    }
   ],
   "source": [
    "Layers = [{'size': nx+1, 'activation': None    , 'use_bias': None},\n",
    "          {'size': 10 , 'activation': 'relu'  , 'use_bias': True},\n",
    "          {'size': 1  , 'activation': 'linear', 'use_bias': False}]\n",
    "Losses = [{'kind': 'mse', 'weight': 1.0}]\n",
    "\n",
    "K = TrainFullyConnectedNN(M_samples, H_samples, \n",
    "                    Layers, Losses,\n",
    "                    'adam', ['mae'], \n",
    "                    10, 1000, 0.2, \n",
    "                    'model', os.path.abspath(''))\n",
    "\n",
    "best_model = K.quickTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ccd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0795021663896939\r",
      "iteration 2 - y: 0.07935454251842705\r",
      "iteration 3 - y: 0.07920691864716022\r",
      "iteration 4 - y: 0.07905929477589337\r",
      "iteration 5 - y: 0.07891167090462652\r",
      "iteration 6 - y: 0.07876404703335968\r",
      "iteration 7 - y: 0.07861642316209283\r",
      "iteration 8 - y: 0.07846879929082598\r",
      "iteration 9 - y: 0.07832117541955913\r",
      "iteration 10 - y: 0.07817355154829228\r",
      "iteration 11 - y: 0.07802592767702546\r",
      "iteration 12 - y: 0.0778783038057586\r",
      "iteration 13 - y: 0.07773067993449174\r",
      "iteration 14 - y: 0.07758305606322491\r",
      "iteration 15 - y: 0.07743543219195806\r",
      "iteration 16 - y: 0.07728780832069121\r",
      "iteration 17 - y: 0.07714018444942436\r",
      "iteration 18 - y: 0.07699256057815751\r",
      "iteration 19 - y: 0.07684493670689067\r",
      "iteration 20 - y: 0.07669731283562384\r",
      "iteration 21 - y: 0.07654968896435699\r",
      "iteration 22 - y: 0.07640206509309014\r",
      "iteration 23 - y: 0.07625444122182329\r",
      "iteration 24 - y: 0.07610681735055644\r",
      "iteration 25 - y: 0.0759591934792896\r",
      "iteration 26 - y: 0.07581156960802275\r",
      "iteration 27 - y: 0.0756639457367559\r",
      "iteration 28 - y: 0.07551632186548907\r",
      "iteration 29 - y: 0.07536869799422222\r",
      "iteration 30 - y: 0.07522107412295537\r",
      "iteration 31 - y: 0.07507345025168852\r",
      "iteration 32 - y: 0.07492582638042167\r",
      "iteration 33 - y: 0.07477820250915483\r",
      "iteration 34 - y: 0.074630578637888\r",
      "iteration 35 - y: 0.07448295476662115\r",
      "iteration 36 - y: 0.0743353308953543\r",
      "iteration 37 - y: 0.07418770702408745\r",
      "iteration 38 - y: 0.0740400831528206\r",
      "iteration 39 - y: 0.07389245928155376\r",
      "iteration 40 - y: 0.07374483541028691\r",
      "iteration 41 - y: 0.07359721153902006\r",
      "iteration 42 - y: 0.07344958766775322\r",
      "iteration 43 - y: 0.07330196379648637\r",
      "iteration 44 - y: 0.07315433992521952\r",
      "iteration 45 - y: 0.07300671605395267\r",
      "iteration 46 - y: 0.07285909218268583\r",
      "iteration 47 - y: 0.07271146831141899\r",
      "iteration 48 - y: 0.07256384444015214\r",
      "iteration 49 - y: 0.07241622056888529\r",
      "iteration 50 - y: 0.07226859669761845\r",
      "iteration 51 - y: 0.0721209728263516\r",
      "iteration 52 - y: 0.07197334895508475\r",
      "iteration 53 - y: 0.0718257250838179\r",
      "iteration 54 - y: 0.07167810121255105\r",
      "iteration 55 - y: 0.07153047734128422\r",
      "iteration 56 - y: 0.07138285347001737\r",
      "iteration 57 - y: 0.07123522959875052\r",
      "iteration 58 - y: 0.07108760572748368\r",
      "iteration 59 - y: 0.07093998185621683\r",
      "iteration 60 - y: 0.07079235798494998\r",
      "iteration 61 - y: 0.07064473411368313\r",
      "iteration 62 - y: 0.07049711024241628\r",
      "iteration 63 - y: 0.07034948637114945\r",
      "iteration 64 - y: 0.0702018624998826\r",
      "iteration 65 - y: 0.07005423862861575\r",
      "iteration 66 - y: 0.06990661475734891\r",
      "iteration 67 - y: 0.06975899088608206\r",
      "iteration 68 - y: 0.06961136701481521\r",
      "iteration 69 - y: 0.06946374314354838\r",
      "iteration 70 - y: 0.06931611927228153\r",
      "iteration 71 - y: 0.06916849540101468\r",
      "iteration 72 - y: 0.06902087152974783\r",
      "iteration 73 - y: 0.06887324765848098\r",
      "iteration 74 - y: 0.06872562378721414\r",
      "iteration 75 - y: 0.06857799991594729\r",
      "iteration 76 - y: 0.06843037604468044\r",
      "iteration 77 - y: 0.0682827521734136\r",
      "iteration 78 - y: 0.06813512830214676\r",
      "iteration 79 - y: 0.0679875044308799\r",
      "iteration 80 - y: 0.06783988055961307\r",
      "iteration 81 - y: 0.06769225668834622\r",
      "iteration 82 - y: 0.06754463281707937\r",
      "iteration 83 - y: 0.06739700894581252\r",
      "iteration 84 - y: 0.06724938507454567\r",
      "iteration 85 - y: 0.06710176120327883\r",
      "iteration 86 - y: 0.06695413733201198\r",
      "iteration 87 - y: 0.06680651346074513\r",
      "iteration 88 - y: 0.0666588895894783\r",
      "iteration 89 - y: 0.06651126571821145\r",
      "iteration 90 - y: 0.0663636418469446\r",
      "iteration 91 - y: 0.06621601797567776\r",
      "iteration 92 - y: 0.06606839410441091\r",
      "iteration 93 - y: 0.06592077023314406\r",
      "iteration 94 - y: 0.06577314636187721\r",
      "iteration 95 - y: 0.06562552249061038\r",
      "iteration 96 - y: 0.06547789861934353\r",
      "iteration 97 - y: 0.06533027474807668\r",
      "iteration 98 - y: 0.06518265087680983\r",
      "iteration 99 - y: 0.06503502700554299\r",
      "iteration 100 - y: 0.06488740313427614\r",
      "iteration 101 - y: 0.06473977926300929\r",
      "iteration 102 - y: 0.06459215539174244\r",
      "iteration 103 - y: 0.0644445315204756\r",
      "iteration 104 - y: 0.06429690764920876\r",
      "iteration 105 - y: 0.0641492837779419\r",
      "iteration 106 - y: 0.06400165990667506\r",
      "iteration 107 - y: 0.06385403603540822\r",
      "iteration 108 - y: 0.06370641216414137\r",
      "iteration 109 - y: 0.06355878829287452\r",
      "iteration 110 - y: 0.06341116442160767\r",
      "iteration 111 - y: 0.06326354055034084\r",
      "iteration 112 - y: 0.06311591667907399\r",
      "iteration 113 - y: 0.06296829280780714\r",
      "iteration 114 - y: 0.06282066893654029\r",
      "iteration 115 - y: 0.06267304506527345\r",
      "iteration 116 - y: 0.0625254211940066\r",
      "iteration 117 - y: 0.06237779732273975\r",
      "iteration 118 - y: 0.0622301734514729\r",
      "iteration 119 - y: 0.06208254958020606\r",
      "iteration 120 - y: 0.061934925708939215\r",
      "iteration 121 - y: 0.06178730183767237\r",
      "iteration 122 - y: 0.06163967796640553\r",
      "iteration 123 - y: 0.06149205409513868\r",
      "iteration 124 - y: 0.06134443022387183\r",
      "iteration 125 - y: 0.06119680635260498\r",
      "iteration 126 - y: 0.06104918248133814\r",
      "iteration 127 - y: 0.060901558610071294\r",
      "iteration 128 - y: 0.060753934738804444\r",
      "iteration 129 - y: 0.06060631086753761\r",
      "iteration 130 - y: 0.06045868699627076\r",
      "iteration 131 - y: 0.06031106312500391\r",
      "iteration 132 - y: 0.060163439253737065\r",
      "iteration 133 - y: 0.06001581538247022\r",
      "iteration 134 - y: 0.05986819151120337\r",
      "iteration 135 - y: 0.05972056763993652\r",
      "iteration 136 - y: 0.05957294376866969\r",
      "iteration 137 - y: 0.05942531989740284\r",
      "iteration 138 - y: 0.05927769602613599\r",
      "iteration 139 - y: 0.05913007215486914\r",
      "iteration 140 - y: 0.0589824482836023\r",
      "iteration 141 - y: 0.05883482441233545\r",
      "iteration 142 - y: 0.0586872005410686\r",
      "iteration 143 - y: 0.058539576669801766\r",
      "iteration 144 - y: 0.058391952798534916\r",
      "iteration 145 - y: 0.058244328927268066\r",
      "iteration 146 - y: 0.058096705056001216\r",
      "iteration 147 - y: 0.057949081184734366\r",
      "iteration 148 - y: 0.05780145731346753\r",
      "iteration 149 - y: 0.05765383344220068\r",
      "iteration 150 - y: 0.05750620957093383\r",
      "iteration 151 - y: 0.057358585699666995\r",
      "iteration 152 - y: 0.057210961828400145\r",
      "iteration 153 - y: 0.057063337957133295\r",
      "iteration 154 - y: 0.056915714085866445\r",
      "iteration 155 - y: 0.0567680902145996\r",
      "iteration 156 - y: 0.05662046634333276\r",
      "iteration 157 - y: 0.05647284247206591\r",
      "iteration 158 - y: 0.05632521860079906\r",
      "iteration 159 - y: 0.056177594729532224\r",
      "iteration 160 - y: 0.056029970858265374\r",
      "iteration 161 - y: 0.055882346986998524\r",
      "iteration 162 - y: 0.05573472311573169\r",
      "iteration 163 - y: 0.05558709924446484\r",
      "iteration 164 - y: 0.055439475373197995\r",
      "iteration 165 - y: 0.055291851501931145\r",
      "iteration 166 - y: 0.0551442276306643\r",
      "iteration 167 - y: 0.05499660375939747\r",
      "iteration 168 - y: 0.05484897988813061\r",
      "iteration 169 - y: 0.05470135601686377\r",
      "iteration 170 - y: 0.05455373214559692\r",
      "iteration 171 - y: 0.05440610827433008\r",
      "iteration 172 - y: 0.05425848440306323\r",
      "iteration 173 - y: 0.054110860531796395\r",
      "iteration 174 - y: 0.053963236660529545\r",
      "iteration 175 - y: 0.0538156127892627\r",
      "iteration 176 - y: 0.05366798891799585\r",
      "iteration 177 - y: 0.05352036504672901\r",
      "iteration 178 - y: 0.05337274117546217\r",
      "iteration 179 - y: 0.05322511730419532\r",
      "iteration 180 - y: 0.053077493432928474\r",
      "iteration 181 - y: 0.052929869561661624\r",
      "iteration 182 - y: 0.05278224569039479\r",
      "iteration 183 - y: 0.05263462181912794\r",
      "iteration 184 - y: 0.052486997947861096\r",
      "iteration 185 - y: 0.05233937407659425\r",
      "iteration 186 - y: 0.0521917502053274\r",
      "iteration 187 - y: 0.05204412633406056\r",
      "iteration 188 - y: 0.05189650246279372\r",
      "iteration 189 - y: 0.05174887859152687\r",
      "iteration 190 - y: 0.051601254720260024\r",
      "iteration 191 - y: 0.05145363084899318\r",
      "iteration 192 - y: 0.05130600697772633\r",
      "iteration 193 - y: 0.051158383106459496\r",
      "iteration 194 - y: 0.051010759235192646\r",
      "iteration 195 - y: 0.050863135363925796\r",
      "iteration 196 - y: 0.05071551149265896\r",
      "iteration 197 - y: 0.05056788762139211\r",
      "iteration 198 - y: 0.05042026375012526\r",
      "iteration 199 - y: 0.050272639878858424\r",
      "iteration 200 - y: 0.050125016007591575\r",
      "iteration 201 - y: 0.04997739213632473\r",
      "iteration 202 - y: 0.04982976826505788\r",
      "iteration 203 - y: 0.04968214439379104\r",
      "iteration 204 - y: 0.04953452052252419\r",
      "iteration 205 - y: 0.049386896651257346\r",
      "iteration 206 - y: 0.0492392727799905\r",
      "iteration 207 - y: 0.049091648908723653\r",
      "iteration 208 - y: 0.04894402503745682\r",
      "iteration 209 - y: 0.04879640116618997\r",
      "iteration 210 - y: 0.048648777294923125\r",
      "iteration 211 - y: 0.04850115342365628\r",
      "iteration 212 - y: 0.04835352955238943\r",
      "iteration 213 - y: 0.04820590568112258\r",
      "iteration 214 - y: 0.048058281809855746\r",
      "iteration 215 - y: 0.047910657938588896\r",
      "iteration 216 - y: 0.047763034067322054\r",
      "iteration 217 - y: 0.04761541019605521\r",
      "iteration 218 - y: 0.04746778632478836\r",
      "iteration 219 - y: 0.047320162453521525\r",
      "iteration 220 - y: 0.047172538582254675\r",
      "iteration 221 - y: 0.047024914710987825\r",
      "iteration 222 - y: 0.04687729083972099\r",
      "iteration 223 - y: 0.04672966696845414\r",
      "iteration 224 - y: 0.04658204309718729\r",
      "iteration 225 - y: 0.046434419225920454\r",
      "iteration 226 - y: 0.046286795354653604\r",
      "iteration 227 - y: 0.04613917148338677\r",
      "iteration 228 - y: 0.04599154761211992\r",
      "iteration 229 - y: 0.04584392374085308\r",
      "iteration 230 - y: 0.04569629986958623\r",
      "iteration 231 - y: 0.045548675998319396\r",
      "iteration 232 - y: 0.045401052127052546\r",
      "iteration 233 - y: 0.04525342825578571\r",
      "iteration 234 - y: 0.045105804384518874\r",
      "iteration 235 - y: 0.044958180513252025\r",
      "iteration 236 - y: 0.04481055664198519\r",
      "iteration 237 - y: 0.044662932770718346\r",
      "iteration 238 - y: 0.0445153088994515\r",
      "iteration 239 - y: 0.04436768502818466\r",
      "iteration 240 - y: 0.04422006115691782\r",
      "iteration 241 - y: 0.04407243728565098\r",
      "iteration 242 - y: 0.04392481341438413\r",
      "iteration 243 - y: 0.043777189543117295\r",
      "iteration 244 - y: 0.043629565671850445\r",
      "iteration 245 - y: 0.04348194180058361\r",
      "iteration 246 - y: 0.04333431792931677\r",
      "iteration 247 - y: 0.043186694058049924\r",
      "iteration 248 - y: 0.04303907018678309\r",
      "iteration 249 - y: 0.04289144631551624\r",
      "iteration 250 - y: 0.0427438224442494\r",
      "iteration 251 - y: 0.04259619857298255\r",
      "iteration 252 - y: 0.042448574701715716\r",
      "iteration 253 - y: 0.04230095083044888\r",
      "iteration 254 - y: 0.04215332695918203\r",
      "iteration 255 - y: 0.042005703087915194\r",
      "iteration 256 - y: 0.041858079216648345\r",
      "iteration 257 - y: 0.04171045534538151\r",
      "iteration 258 - y: 0.04156283147411467\r",
      "iteration 259 - y: 0.04141520760284782\r",
      "iteration 260 - y: 0.04126758373158098\r",
      "iteration 261 - y: 0.04111995986031414\r",
      "iteration 262 - y: 0.040972335989047294\r",
      "iteration 263 - y: 0.04082471211778045\r",
      "iteration 264 - y: 0.040677088246513615\r",
      "iteration 265 - y: 0.04052946437524677\r",
      "iteration 266 - y: 0.04038184050397993\r",
      "iteration 267 - y: 0.040234216632713087\r",
      "iteration 268 - y: 0.040086592761446244\r",
      "iteration 269 - y: 0.03993896889017941\r",
      "iteration 270 - y: 0.03979134501891256\r",
      "iteration 271 - y: 0.03964372114764572\r",
      "iteration 272 - y: 0.03949609727637887\r",
      "iteration 273 - y: 0.039348473405112036\r",
      "iteration 274 - y: 0.039200849533845186\r",
      "iteration 275 - y: 0.03905322566257835\r",
      "iteration 276 - y: 0.038905601791311514\r",
      "iteration 277 - y: 0.038757977920044664\r",
      "iteration 278 - y: 0.03861035404877783\r",
      "iteration 279 - y: 0.038462730177510986\r",
      "iteration 280 - y: 0.03831510630624414\r",
      "iteration 281 - y: 0.0381674824349773\r",
      "iteration 282 - y: 0.03801985856371046\r",
      "iteration 283 - y: 0.037872234692443614\r",
      "iteration 284 - y: 0.03772461082117678\r",
      "iteration 285 - y: 0.037576986949909935\r",
      "iteration 286 - y: 0.037429363078643085\r",
      "iteration 287 - y: 0.03728173920737625\r",
      "iteration 288 - y: 0.037134115336109406\r",
      "iteration 289 - y: 0.036986491464842564\r",
      "iteration 290 - y: 0.03683886759357573\r",
      "iteration 291 - y: 0.03669124372230888\r",
      "iteration 292 - y: 0.03654361985104204\r",
      "iteration 293 - y: 0.03639599597977519\r",
      "iteration 294 - y: 0.036248372108508356\r",
      "iteration 295 - y: 0.03610074823724152\r",
      "iteration 296 - y: 0.03595312436597467\r",
      "iteration 297 - y: 0.03580550049470783\r",
      "iteration 298 - y: 0.035657876623440984\r",
      "iteration 299 - y: 0.03551025275217415\r",
      "iteration 300 - y: 0.035362628880907306\r",
      "iteration 301 - y: 0.03521500500964046\r",
      "iteration 302 - y: 0.03506738113837362\r",
      "iteration 303 - y: 0.03491975726710678\r",
      "iteration 304 - y: 0.03477213339583994\r",
      "iteration 305 - y: 0.0346245095245731\r",
      "iteration 306 - y: 0.034476885653306255\r",
      "iteration 307 - y: 0.03432926178203941\r",
      "iteration 308 - y: 0.03418163791077257\r",
      "iteration 309 - y: 0.03403401403950573\r",
      "iteration 310 - y: 0.033886390168238884\r",
      "iteration 311 - y: 0.03373876629697205\r",
      "iteration 312 - y: 0.0335911424257052\r",
      "iteration 313 - y: 0.03344351855443836\r",
      "iteration 314 - y: 0.03329589468317152\r",
      "iteration 315 - y: 0.033148270811904676\r",
      "iteration 316 - y: 0.03300064694063784\r",
      "iteration 317 - y: 0.032853023069371\r",
      "iteration 318 - y: 0.032705399198104154\r",
      "iteration 319 - y: 0.03255777532683732\r",
      "iteration 320 - y: 0.032410151455570475\r",
      "iteration 321 - y: 0.03226252758430363\r",
      "iteration 322 - y: 0.03211490371303679\r",
      "iteration 323 - y: 0.03196727984176995\r",
      "iteration 324 - y: 0.03181965597050311\r",
      "iteration 325 - y: 0.03167203209923627\r",
      "iteration 326 - y: 0.031524408227969425\r",
      "iteration 327 - y: 0.03137678435670259\r",
      "iteration 328 - y: 0.031229160485435743\r",
      "iteration 329 - y: 0.031081536614168903\r",
      "iteration 330 - y: 0.03093391274290206\r",
      "iteration 331 - y: 0.030786288871635224\r",
      "iteration 332 - y: 0.03063866500036838\r",
      "iteration 333 - y: 0.030491041129101542\r",
      "iteration 334 - y: 0.0303434172578347\r",
      "iteration 335 - y: 0.030195793386567856\r",
      "iteration 336 - y: 0.030048169515301017\r",
      "iteration 337 - y: 0.029900545644034174\r",
      "iteration 338 - y: 0.02975292177276733\r",
      "iteration 339 - y: 0.029605297901500495\r",
      "iteration 340 - y: 0.029457674030233652\r",
      "iteration 341 - y: 0.029310050158966813\r",
      "iteration 342 - y: 0.02916242628769997\r",
      "iteration 343 - y: 0.02901480241643313\r",
      "iteration 344 - y: 0.028867178545166287\r",
      "iteration 345 - y: 0.028719554673899444\r",
      "iteration 346 - y: 0.02857193080263261\r",
      "iteration 347 - y: 0.028424306931365766\r",
      "iteration 348 - y: 0.028276683060098923\r",
      "iteration 349 - y: 0.028129059188832083\r",
      "iteration 350 - y: 0.02798143531756524\r",
      "iteration 351 - y: 0.0278338114462984\r",
      "iteration 352 - y: 0.027686187575031558\r",
      "iteration 353 - y: 0.027538563703764722\r",
      "iteration 354 - y: 0.02739093983249788\r",
      "iteration 355 - y: 0.027243315961231036\r",
      "iteration 356 - y: 0.027095692089964197\r",
      "iteration 357 - y: 0.026948068218697354\r",
      "iteration 358 - y: 0.026800444347430515\r",
      "iteration 359 - y: 0.02665282047616367\r",
      "iteration 360 - y: 0.02650519660489683\r",
      "iteration 361 - y: 0.026357572733629993\r",
      "iteration 362 - y: 0.02620994886236315\r",
      "iteration 363 - y: 0.02606232499109631\r",
      "iteration 364 - y: 0.025914701119829464\r",
      "iteration 365 - y: 0.025767077248562628\r",
      "iteration 366 - y: 0.025619453377295785\r",
      "iteration 367 - y: 0.025471829506028946\r",
      "iteration 368 - y: 0.025324205634762106\r",
      "iteration 369 - y: 0.025176581763495263\r",
      "iteration 370 - y: 0.02502895789222842\r",
      "iteration 371 - y: 0.02488133402096158\r",
      "iteration 372 - y: 0.024733710149694742\r",
      "iteration 373 - y: 0.0245860862784279\r",
      "iteration 374 - y: 0.02443846240716106\r",
      "iteration 375 - y: 0.02429083853589422\r",
      "iteration 376 - y: 0.024143214664627377\r",
      "iteration 377 - y: 0.023995590793360534\r",
      "iteration 378 - y: 0.023847966922093698\r",
      "iteration 379 - y: 0.023700343050826855\r",
      "iteration 380 - y: 0.023552719179560012\r",
      "iteration 381 - y: 0.02340509530829317\r",
      "iteration 382 - y: 0.02325747143702633\r",
      "iteration 383 - y: 0.02310984756575949\r",
      "iteration 384 - y: 0.022962223694492648\r",
      "iteration 385 - y: 0.02281459982322581\r",
      "iteration 386 - y: 0.02266697595195897\r",
      "iteration 387 - y: 0.022519352080692126\r",
      "iteration 388 - y: 0.022371728209425287\r",
      "iteration 389 - y: 0.022224104338158444\r",
      "iteration 390 - y: 0.022076480466891604\r",
      "iteration 391 - y: 0.02192885659562476\r",
      "iteration 392 - y: 0.02178123272435792\r",
      "iteration 393 - y: 0.021633608853091083\r",
      "iteration 394 - y: 0.02148598498182424\r",
      "iteration 395 - y: 0.021338361110557397\r",
      "iteration 396 - y: 0.021190737239290554\r",
      "iteration 397 - y: 0.021043113368023714\r",
      "iteration 398 - y: 0.020895489496756875\r",
      "iteration 399 - y: 0.020747865625490032\r",
      "iteration 400 - y: 0.02060024175422319\r",
      "iteration 401 - y: 0.020452617882956346\r",
      "iteration 402 - y: 0.02030499401168951\r",
      "iteration 403 - y: 0.020157370140422667\r",
      "iteration 404 - y: 0.020009746269155824\r",
      "iteration 405 - y: 0.019862122397888985\r",
      "iteration 406 - y: 0.019714498526622142\r",
      "iteration 407 - y: 0.019566874655355303\r",
      "iteration 408 - y: 0.01941925078408846\r",
      "iteration 409 - y: 0.019271626912821617\r",
      "iteration 410 - y: 0.01912400304155478\r",
      "iteration 411 - y: 0.018976379170287935\r",
      "iteration 412 - y: 0.018828755299021095\r",
      "iteration 413 - y: 0.018681131427754252\r",
      "iteration 414 - y: 0.018533507556487413\r",
      "iteration 415 - y: 0.01838588368522057\r",
      "iteration 416 - y: 0.01823825981395373\r",
      "iteration 417 - y: 0.018090635942686888\r",
      "iteration 418 - y: 0.017943012071420048\r",
      "iteration 419 - y: 0.017795388200153205\r",
      "iteration 420 - y: 0.017647764328886366\r",
      "iteration 421 - y: 0.017500140457619523\r",
      "iteration 422 - y: 0.017352516586352684\r",
      "iteration 423 - y: 0.01720489271508584\r",
      "iteration 424 - y: 0.017057268843819\r",
      "iteration 425 - y: 0.01690964497255216\r",
      "iteration 426 - y: 0.016762021101285315\r",
      "iteration 427 - y: 0.01661439723001848\r",
      "iteration 428 - y: 0.016466773358751637\r",
      "iteration 429 - y: 0.016319149487484794\r",
      "iteration 430 - y: 0.01617152561621795\r",
      "iteration 431 - y: 0.01602390174495111\r",
      "iteration 432 - y: 0.01587627787368427\r",
      "iteration 433 - y: 0.01572865400241743\r",
      "iteration 434 - y: 0.015581030131150588\r",
      "iteration 435 - y: 0.015433406259883747\r",
      "iteration 436 - y: 0.015285782388616904\r",
      "iteration 437 - y: 0.015138158517350064\r",
      "iteration 438 - y: 0.014990534646083223\r",
      "iteration 439 - y: 0.014842910774816382\r",
      "iteration 440 - y: 0.01469528690354954\r",
      "iteration 441 - y: 0.0145476630322827\r",
      "iteration 442 - y: 0.014400039161015857\r",
      "iteration 443 - y: 0.014252415289749017\r",
      "iteration 444 - y: 0.014104791418482176\r",
      "iteration 445 - y: 0.013957167547215335\r",
      "iteration 446 - y: 0.013809543675948494\r",
      "iteration 447 - y: 0.013661919804681654\r",
      "iteration 448 - y: 0.013514295933414812\r",
      "iteration 449 - y: 0.013366672062147969\r",
      "iteration 450 - y: 0.013219048190881131\r",
      "iteration 451 - y: 0.013071424319614288\r",
      "iteration 452 - y: 0.012923800448347447\r",
      "iteration 453 - y: 0.012776176577080606\r",
      "iteration 454 - y: 0.012628552705813766\r",
      "iteration 455 - y: 0.012480928834546923\r",
      "iteration 456 - y: 0.012333304963280084\r",
      "iteration 457 - y: 0.012185681092013241\r",
      "iteration 458 - y: 0.0120380572207464\r",
      "iteration 459 - y: 0.011890433349479559\r",
      "iteration 460 - y: 0.011742809478212716\r",
      "iteration 461 - y: 0.011595185606945875\r",
      "iteration 462 - y: 0.011447561735679034\r",
      "iteration 463 - y: 0.01129993786441219\r",
      "iteration 464 - y: 0.01115231399314535\r",
      "iteration 465 - y: 0.011004690121878508\r",
      "iteration 466 - y: 0.010857066250611665\r",
      "iteration 467 - y: 0.010709442379344822\r",
      "iteration 468 - y: 0.010561818508077983\r",
      "iteration 469 - y: 0.01041419463681114\r",
      "iteration 470 - y: 0.010266570765544297\r",
      "iteration 471 - y: 0.010140202995731217\r",
      "iteration 472 - y: 0.010023988891734463\r",
      "iteration 473 - y: 0.00990777478773771\r",
      "iteration 474 - y: 0.009791560683740957\r",
      "iteration 475 - y: 0.009675346579744202\r",
      "iteration 476 - y: 0.00955913247574745\r",
      "iteration 477 - y: 0.009442918371750694\r",
      "iteration 478 - y: 0.009326704267753942\r",
      "iteration 479 - y: 0.009210490163757187\r",
      "iteration 480 - y: 0.009094276059760435\r",
      "iteration 481 - y: 0.00897806195576368\r",
      "iteration 482 - y: 0.008861847851766927\r",
      "iteration 483 - y: 0.008745633747770173\r",
      "iteration 484 - y: 0.00862941964377342\r",
      "iteration 485 - y: 0.008513205539776665\r",
      "iteration 486 - y: 0.008396991435779912\r",
      "iteration 487 - y: 0.008280777331783158\r",
      "iteration 488 - y: 0.008164563227786404\r",
      "iteration 489 - y: 0.00804834912378965\r",
      "iteration 490 - y: 0.007932135019792896\r",
      "iteration 491 - y: 0.007815920915796143\r",
      "iteration 492 - y: 0.007699706811799389\r",
      "iteration 493 - y: 0.007583492707802635\r",
      "iteration 494 - y: 0.007467278603805881\r",
      "iteration 495 - y: 0.007351064499809127\r",
      "iteration 496 - y: 0.0072348503958123735\r",
      "iteration 497 - y: 0.00711863629181562\r",
      "iteration 498 - y: 0.007002422187818866\r",
      "iteration 499 - y: 0.006886208083822112\r",
      "iteration 500 - y: 0.006769993979825359\r",
      "iteration 501 - y: 0.006653779875828606\r",
      "iteration 502 - y: 0.006537565771831852\r",
      "iteration 503 - y: 0.006421351667835099\r",
      "iteration 504 - y: 0.0063051375638383455\r",
      "iteration 505 - y: 0.006188923459841593\r",
      "iteration 506 - y: 0.006072709355844839\r",
      "iteration 507 - y: 0.005956495251848085\r",
      "iteration 508 - y: 0.005840281147851331\r",
      "iteration 509 - y: 0.005724067043854579\r",
      "iteration 510 - y: 0.005607852939857825\r",
      "iteration 511 - y: 0.005491638835861071\r",
      "iteration 512 - y: 0.005375424731864318\r",
      "iteration 513 - y: 0.005259210627867565\r",
      "iteration 514 - y: 0.005142996523870812\r",
      "iteration 515 - y: 0.005026782419874058\r",
      "iteration 516 - y: 0.004910568315877305\r",
      "iteration 517 - y: 0.004794354211880551\r",
      "iteration 518 - y: 0.004678140107883798\r",
      "iteration 519 - y: 0.004561926003887044\r",
      "iteration 520 - y: 0.0044457118998902905\r",
      "iteration 521 - y: 0.004329497795893538\r",
      "iteration 522 - y: 0.004213283691896784\r",
      "iteration 523 - y: 0.004097069587900031\r",
      "iteration 524 - y: 0.003980855483903277\r",
      "iteration 525 - y: 0.003864641379906523\r",
      "iteration 526 - y: 0.0037484272759097693\r",
      "iteration 527 - y: 0.0036322131719130155\r",
      "iteration 528 - y: 0.0035159990679162626\r",
      "iteration 529 - y: 0.0033997849639195088\r",
      "iteration 530 - y: 0.0032835708599227554\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 406 - y: 0.0009781167480891375\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-4.2270403e-07]], dtype=float32),\n",
       " array([[-2.1884101e-05]], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.001}\n",
    "X = XAIR(best_model, 'lrp.alpha_1_beta_0', 'letzgus', M_samples[:2], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f591db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0795021663896939\r",
      "iteration 2 - y: 0.07935454251842705\r",
      "iteration 3 - y: 0.07920691864716022\r",
      "iteration 4 - y: 0.07905929477589337\r",
      "iteration 5 - y: 0.07891167090462652\r",
      "iteration 6 - y: 0.07876404703335968\r",
      "iteration 7 - y: 0.07861642316209283\r",
      "iteration 8 - y: 0.07846879929082598\r",
      "iteration 9 - y: 0.07832117541955913\r",
      "iteration 10 - y: 0.07817355154829228\r",
      "iteration 11 - y: 0.07802592767702546\r",
      "iteration 12 - y: 0.0778783038057586\r",
      "iteration 13 - y: 0.07773067993449174\r",
      "iteration 14 - y: 0.07758305606322491\r",
      "iteration 15 - y: 0.07743543219195806\r",
      "iteration 16 - y: 0.07728780832069121\r",
      "iteration 17 - y: 0.07714018444942436\r",
      "iteration 18 - y: 0.07699256057815751\r",
      "iteration 19 - y: 0.07684493670689067\r",
      "iteration 20 - y: 0.07669731283562384\r",
      "iteration 21 - y: 0.07654968896435699\r",
      "iteration 22 - y: 0.07640206509309014\r",
      "iteration 23 - y: 0.07625444122182329\r",
      "iteration 24 - y: 0.07610681735055644\r",
      "iteration 25 - y: 0.0759591934792896\r",
      "iteration 26 - y: 0.07581156960802275\r",
      "iteration 27 - y: 0.0756639457367559\r",
      "iteration 28 - y: 0.07551632186548907\r",
      "iteration 29 - y: 0.07536869799422222\r",
      "iteration 30 - y: 0.07522107412295537\r",
      "iteration 31 - y: 0.07507345025168852\r",
      "iteration 32 - y: 0.07492582638042167\r",
      "iteration 33 - y: 0.07477820250915483\r",
      "iteration 34 - y: 0.074630578637888\r",
      "iteration 35 - y: 0.07448295476662115\r",
      "iteration 36 - y: 0.0743353308953543\r",
      "iteration 37 - y: 0.07418770702408745\r",
      "iteration 38 - y: 0.0740400831528206\r",
      "iteration 39 - y: 0.07389245928155376\r",
      "iteration 40 - y: 0.07374483541028691\r",
      "iteration 41 - y: 0.07359721153902006\r",
      "iteration 42 - y: 0.07344958766775322\r",
      "iteration 43 - y: 0.07330196379648637\r",
      "iteration 44 - y: 0.07315433992521952\r",
      "iteration 45 - y: 0.07300671605395267\r",
      "iteration 46 - y: 0.07285909218268583\r",
      "iteration 47 - y: 0.07271146831141899\r",
      "iteration 48 - y: 0.07256384444015214\r",
      "iteration 49 - y: 0.07241622056888529\r",
      "iteration 50 - y: 0.07226859669761845\r",
      "iteration 51 - y: 0.0721209728263516\r",
      "iteration 52 - y: 0.07197334895508475\r",
      "iteration 53 - y: 0.0718257250838179\r",
      "iteration 54 - y: 0.07167810121255105\r",
      "iteration 55 - y: 0.07153047734128422\r",
      "iteration 56 - y: 0.07138285347001737\r",
      "iteration 57 - y: 0.07123522959875052\r",
      "iteration 58 - y: 0.07108760572748368\r",
      "iteration 59 - y: 0.07093998185621683\r",
      "iteration 60 - y: 0.07079235798494998\r",
      "iteration 61 - y: 0.07064473411368313\r",
      "iteration 62 - y: 0.07049711024241628\r",
      "iteration 63 - y: 0.07034948637114945\r",
      "iteration 64 - y: 0.0702018624998826\r",
      "iteration 65 - y: 0.07005423862861575\r",
      "iteration 66 - y: 0.06990661475734891\r",
      "iteration 67 - y: 0.06975899088608206\r",
      "iteration 68 - y: 0.06961136701481521\r",
      "iteration 69 - y: 0.06946374314354838\r",
      "iteration 70 - y: 0.06931611927228153\r",
      "iteration 71 - y: 0.06916849540101468\r",
      "iteration 72 - y: 0.06902087152974783\r",
      "iteration 73 - y: 0.06887324765848098\r",
      "iteration 74 - y: 0.06872562378721414\r",
      "iteration 75 - y: 0.06857799991594729\r",
      "iteration 76 - y: 0.06843037604468044\r",
      "iteration 77 - y: 0.0682827521734136\r",
      "iteration 78 - y: 0.06813512830214676\r",
      "iteration 79 - y: 0.0679875044308799\r",
      "iteration 80 - y: 0.06783988055961307\r",
      "iteration 81 - y: 0.06769225668834622\r",
      "iteration 82 - y: 0.06754463281707937\r",
      "iteration 83 - y: 0.06739700894581252\r",
      "iteration 84 - y: 0.06724938507454567\r",
      "iteration 85 - y: 0.06710176120327883\r",
      "iteration 86 - y: 0.06695413733201198\r",
      "iteration 87 - y: 0.06680651346074513\r",
      "iteration 88 - y: 0.0666588895894783\r",
      "iteration 89 - y: 0.06651126571821145\r",
      "iteration 90 - y: 0.0663636418469446\r",
      "iteration 91 - y: 0.06621601797567776\r",
      "iteration 92 - y: 0.06606839410441091\r",
      "iteration 93 - y: 0.06592077023314406\r",
      "iteration 94 - y: 0.06577314636187721\r",
      "iteration 95 - y: 0.06562552249061038\r",
      "iteration 96 - y: 0.06547789861934353\r",
      "iteration 97 - y: 0.06533027474807668\r",
      "iteration 98 - y: 0.06518265087680983\r",
      "iteration 99 - y: 0.06503502700554299\r",
      "iteration 100 - y: 0.06488740313427614\r",
      "iteration 101 - y: 0.06473977926300929\r",
      "iteration 102 - y: 0.06459215539174244\r",
      "iteration 103 - y: 0.0644445315204756\r",
      "iteration 104 - y: 0.06429690764920876\r",
      "iteration 105 - y: 0.0641492837779419\r",
      "iteration 106 - y: 0.06400165990667506\r",
      "iteration 107 - y: 0.06385403603540822\r",
      "iteration 108 - y: 0.06370641216414137\r",
      "iteration 109 - y: 0.06355878829287452\r",
      "iteration 110 - y: 0.06341116442160767\r",
      "iteration 111 - y: 0.06326354055034084\r",
      "iteration 112 - y: 0.06311591667907399\r",
      "iteration 113 - y: 0.06296829280780714\r",
      "iteration 114 - y: 0.06282066893654029\r",
      "iteration 115 - y: 0.06267304506527345\r",
      "iteration 116 - y: 0.0625254211940066\r",
      "iteration 117 - y: 0.06237779732273975\r",
      "iteration 118 - y: 0.0622301734514729\r",
      "iteration 119 - y: 0.06208254958020606\r",
      "iteration 120 - y: 0.061934925708939215\r",
      "iteration 121 - y: 0.06178730183767237\r",
      "iteration 122 - y: 0.06163967796640553\r",
      "iteration 123 - y: 0.06149205409513868\r",
      "iteration 124 - y: 0.06134443022387183\r",
      "iteration 125 - y: 0.06119680635260498\r",
      "iteration 126 - y: 0.06104918248133814\r",
      "iteration 127 - y: 0.060901558610071294\r",
      "iteration 128 - y: 0.060753934738804444\r",
      "iteration 129 - y: 0.06060631086753761\r",
      "iteration 130 - y: 0.06045868699627076\r",
      "iteration 131 - y: 0.06031106312500391\r",
      "iteration 132 - y: 0.060163439253737065\r",
      "iteration 133 - y: 0.06001581538247022\r",
      "iteration 134 - y: 0.05986819151120337\r",
      "iteration 135 - y: 0.05972056763993652\r",
      "iteration 136 - y: 0.05957294376866969\r",
      "iteration 137 - y: 0.05942531989740284\r",
      "iteration 138 - y: 0.05927769602613599\r",
      "iteration 139 - y: 0.05913007215486914\r",
      "iteration 140 - y: 0.0589824482836023\r",
      "iteration 141 - y: 0.05883482441233545\r",
      "iteration 142 - y: 0.0586872005410686\r",
      "iteration 143 - y: 0.058539576669801766\r",
      "iteration 144 - y: 0.058391952798534916\r",
      "iteration 145 - y: 0.058244328927268066\r",
      "iteration 146 - y: 0.058096705056001216\r",
      "iteration 147 - y: 0.057949081184734366\r",
      "iteration 148 - y: 0.05780145731346753\r",
      "iteration 149 - y: 0.05765383344220068\r",
      "iteration 150 - y: 0.05750620957093383\r",
      "iteration 151 - y: 0.057358585699666995\r",
      "iteration 152 - y: 0.057210961828400145\r",
      "iteration 153 - y: 0.057063337957133295\r",
      "iteration 154 - y: 0.056915714085866445\r",
      "iteration 155 - y: 0.0567680902145996\r",
      "iteration 156 - y: 0.05662046634333276\r",
      "iteration 157 - y: 0.05647284247206591\r",
      "iteration 158 - y: 0.05632521860079906\r",
      "iteration 159 - y: 0.056177594729532224\r",
      "iteration 160 - y: 0.056029970858265374\r",
      "iteration 161 - y: 0.055882346986998524\r",
      "iteration 162 - y: 0.05573472311573169\r",
      "iteration 163 - y: 0.05558709924446484\r",
      "iteration 164 - y: 0.055439475373197995\r",
      "iteration 165 - y: 0.055291851501931145\r",
      "iteration 166 - y: 0.0551442276306643\r",
      "iteration 167 - y: 0.05499660375939747\r",
      "iteration 168 - y: 0.05484897988813061\r",
      "iteration 169 - y: 0.05470135601686377\r",
      "iteration 170 - y: 0.05455373214559692\r",
      "iteration 171 - y: 0.05440610827433008\r",
      "iteration 172 - y: 0.05425848440306323\r",
      "iteration 173 - y: 0.054110860531796395\r",
      "iteration 174 - y: 0.053963236660529545\r",
      "iteration 175 - y: 0.0538156127892627\r",
      "iteration 176 - y: 0.05366798891799585\r",
      "iteration 177 - y: 0.05352036504672901\r",
      "iteration 178 - y: 0.05337274117546217\r",
      "iteration 179 - y: 0.05322511730419532\r",
      "iteration 180 - y: 0.053077493432928474\r",
      "iteration 181 - y: 0.052929869561661624\r",
      "iteration 182 - y: 0.05278224569039479\r",
      "iteration 183 - y: 0.05263462181912794\r",
      "iteration 184 - y: 0.052486997947861096\r",
      "iteration 185 - y: 0.05233937407659425\r",
      "iteration 186 - y: 0.0521917502053274\r",
      "iteration 187 - y: 0.05204412633406056\r",
      "iteration 188 - y: 0.05189650246279372\r",
      "iteration 189 - y: 0.05174887859152687\r",
      "iteration 190 - y: 0.051601254720260024\r",
      "iteration 191 - y: 0.05145363084899318\r",
      "iteration 192 - y: 0.05130600697772633\r",
      "iteration 193 - y: 0.051158383106459496\r",
      "iteration 194 - y: 0.051010759235192646\r",
      "iteration 195 - y: 0.050863135363925796\r",
      "iteration 196 - y: 0.05071551149265896\r",
      "iteration 197 - y: 0.05056788762139211\r",
      "iteration 198 - y: 0.05042026375012526\r",
      "iteration 199 - y: 0.050272639878858424\r",
      "iteration 200 - y: 0.050125016007591575\r",
      "iteration 201 - y: 0.04997739213632473\r",
      "iteration 202 - y: 0.04982976826505788\r",
      "iteration 203 - y: 0.04968214439379104\r",
      "iteration 204 - y: 0.04953452052252419\r",
      "iteration 205 - y: 0.049386896651257346\r",
      "iteration 206 - y: 0.0492392727799905\r",
      "iteration 207 - y: 0.049091648908723653\r",
      "iteration 208 - y: 0.04894402503745682\r",
      "iteration 209 - y: 0.04879640116618997\r",
      "iteration 210 - y: 0.048648777294923125\r",
      "iteration 211 - y: 0.04850115342365628\r",
      "iteration 212 - y: 0.04835352955238943\r",
      "iteration 213 - y: 0.04820590568112258\r",
      "iteration 214 - y: 0.048058281809855746\r",
      "iteration 215 - y: 0.047910657938588896\r",
      "iteration 216 - y: 0.047763034067322054\r",
      "iteration 217 - y: 0.04761541019605521\r",
      "iteration 218 - y: 0.04746778632478836\r",
      "iteration 219 - y: 0.047320162453521525\r",
      "iteration 220 - y: 0.047172538582254675\r",
      "iteration 221 - y: 0.047024914710987825\r",
      "iteration 222 - y: 0.04687729083972099\r",
      "iteration 223 - y: 0.04672966696845414\r",
      "iteration 224 - y: 0.04658204309718729\r",
      "iteration 225 - y: 0.046434419225920454\r",
      "iteration 226 - y: 0.046286795354653604\r",
      "iteration 227 - y: 0.04613917148338677\r",
      "iteration 228 - y: 0.04599154761211992\r",
      "iteration 229 - y: 0.04584392374085308\r",
      "iteration 230 - y: 0.04569629986958623\r",
      "iteration 231 - y: 0.045548675998319396\r",
      "iteration 232 - y: 0.045401052127052546\r",
      "iteration 233 - y: 0.04525342825578571\r",
      "iteration 234 - y: 0.045105804384518874\r",
      "iteration 235 - y: 0.044958180513252025\r",
      "iteration 236 - y: 0.04481055664198519\r",
      "iteration 237 - y: 0.044662932770718346\r",
      "iteration 238 - y: 0.0445153088994515\r",
      "iteration 239 - y: 0.04436768502818466\r",
      "iteration 240 - y: 0.04422006115691782\r",
      "iteration 241 - y: 0.04407243728565098\r",
      "iteration 242 - y: 0.04392481341438413\r",
      "iteration 243 - y: 0.043777189543117295\r",
      "iteration 244 - y: 0.043629565671850445\r",
      "iteration 245 - y: 0.04348194180058361\r",
      "iteration 246 - y: 0.04333431792931677\r",
      "iteration 247 - y: 0.043186694058049924\r",
      "iteration 248 - y: 0.04303907018678309\r",
      "iteration 249 - y: 0.04289144631551624\r",
      "iteration 250 - y: 0.0427438224442494\r",
      "iteration 251 - y: 0.04259619857298255\r",
      "iteration 252 - y: 0.042448574701715716\r",
      "iteration 253 - y: 0.04230095083044888\r",
      "iteration 254 - y: 0.04215332695918203\r",
      "iteration 255 - y: 0.042005703087915194\r",
      "iteration 256 - y: 0.041858079216648345\r",
      "iteration 257 - y: 0.04171045534538151\r",
      "iteration 258 - y: 0.04156283147411467\r",
      "iteration 259 - y: 0.04141520760284782\r",
      "iteration 260 - y: 0.04126758373158098\r",
      "iteration 261 - y: 0.04111995986031414\r",
      "iteration 262 - y: 0.040972335989047294\r",
      "iteration 263 - y: 0.04082471211778045\r",
      "iteration 264 - y: 0.040677088246513615\r",
      "iteration 265 - y: 0.04052946437524677\r",
      "iteration 266 - y: 0.04038184050397993\r",
      "iteration 267 - y: 0.040234216632713087\r",
      "iteration 268 - y: 0.040086592761446244\r",
      "iteration 269 - y: 0.03993896889017941\r",
      "iteration 270 - y: 0.03979134501891256\r",
      "iteration 271 - y: 0.03964372114764572\r",
      "iteration 272 - y: 0.03949609727637887\r",
      "iteration 273 - y: 0.039348473405112036\r",
      "iteration 274 - y: 0.039200849533845186\r",
      "iteration 275 - y: 0.03905322566257835\r",
      "iteration 276 - y: 0.038905601791311514\r",
      "iteration 277 - y: 0.038757977920044664\r",
      "iteration 278 - y: 0.03861035404877783\r",
      "iteration 279 - y: 0.038462730177510986\r",
      "iteration 280 - y: 0.03831510630624414\r",
      "iteration 281 - y: 0.0381674824349773\r",
      "iteration 282 - y: 0.03801985856371046\r",
      "iteration 283 - y: 0.037872234692443614\r",
      "iteration 284 - y: 0.03772461082117678\r",
      "iteration 285 - y: 0.037576986949909935\r",
      "iteration 286 - y: 0.037429363078643085\r",
      "iteration 287 - y: 0.03728173920737625\r",
      "iteration 288 - y: 0.037134115336109406\r",
      "iteration 289 - y: 0.036986491464842564\r",
      "iteration 290 - y: 0.03683886759357573\r",
      "iteration 291 - y: 0.03669124372230888\r",
      "iteration 292 - y: 0.03654361985104204\r",
      "iteration 293 - y: 0.03639599597977519\r",
      "iteration 294 - y: 0.036248372108508356\r",
      "iteration 295 - y: 0.03610074823724152\r",
      "iteration 296 - y: 0.03595312436597467\r",
      "iteration 297 - y: 0.03580550049470783\r",
      "iteration 298 - y: 0.035657876623440984\r",
      "iteration 299 - y: 0.03551025275217415\r",
      "iteration 300 - y: 0.035362628880907306\r",
      "iteration 301 - y: 0.03521500500964046\r",
      "iteration 302 - y: 0.03506738113837362\r",
      "iteration 303 - y: 0.03491975726710678\r",
      "iteration 304 - y: 0.03477213339583994\r",
      "iteration 305 - y: 0.0346245095245731\r",
      "iteration 306 - y: 0.034476885653306255\r",
      "iteration 307 - y: 0.03432926178203941\r",
      "iteration 308 - y: 0.03418163791077257\r",
      "iteration 309 - y: 0.03403401403950573\r",
      "iteration 310 - y: 0.033886390168238884\r",
      "iteration 311 - y: 0.03373876629697205\r",
      "iteration 312 - y: 0.0335911424257052\r",
      "iteration 313 - y: 0.03344351855443836\r",
      "iteration 314 - y: 0.03329589468317152\r",
      "iteration 315 - y: 0.033148270811904676\r",
      "iteration 316 - y: 0.03300064694063784\r",
      "iteration 317 - y: 0.032853023069371\r",
      "iteration 318 - y: 0.032705399198104154\r",
      "iteration 319 - y: 0.03255777532683732\r",
      "iteration 320 - y: 0.032410151455570475\r",
      "iteration 321 - y: 0.03226252758430363\r",
      "iteration 322 - y: 0.03211490371303679\r",
      "iteration 323 - y: 0.03196727984176995\r",
      "iteration 324 - y: 0.03181965597050311\r",
      "iteration 325 - y: 0.03167203209923627\r",
      "iteration 326 - y: 0.031524408227969425\r",
      "iteration 327 - y: 0.03137678435670259\r",
      "iteration 328 - y: 0.031229160485435743\r",
      "iteration 329 - y: 0.031081536614168903\r",
      "iteration 330 - y: 0.03093391274290206\r",
      "iteration 331 - y: 0.030786288871635224\r",
      "iteration 332 - y: 0.03063866500036838\r",
      "iteration 333 - y: 0.030491041129101542\r",
      "iteration 334 - y: 0.0303434172578347\r",
      "iteration 335 - y: 0.030195793386567856\r",
      "iteration 336 - y: 0.030048169515301017\r",
      "iteration 337 - y: 0.029900545644034174\r",
      "iteration 338 - y: 0.02975292177276733\r",
      "iteration 339 - y: 0.029605297901500495\r",
      "iteration 340 - y: 0.029457674030233652\r",
      "iteration 341 - y: 0.029310050158966813\r",
      "iteration 342 - y: 0.02916242628769997\r",
      "iteration 343 - y: 0.02901480241643313\r",
      "iteration 344 - y: 0.028867178545166287\r",
      "iteration 345 - y: 0.028719554673899444\r",
      "iteration 346 - y: 0.02857193080263261\r",
      "iteration 347 - y: 0.028424306931365766\r",
      "iteration 348 - y: 0.028276683060098923\r",
      "iteration 349 - y: 0.028129059188832083\r",
      "iteration 350 - y: 0.02798143531756524\r",
      "iteration 351 - y: 0.0278338114462984\r",
      "iteration 352 - y: 0.027686187575031558\r",
      "iteration 353 - y: 0.027538563703764722\r",
      "iteration 354 - y: 0.02739093983249788\r",
      "iteration 355 - y: 0.027243315961231036\r",
      "iteration 356 - y: 0.027095692089964197\r",
      "iteration 357 - y: 0.026948068218697354\r",
      "iteration 358 - y: 0.026800444347430515\r",
      "iteration 359 - y: 0.02665282047616367\r",
      "iteration 360 - y: 0.02650519660489683\r",
      "iteration 361 - y: 0.026357572733629993\r",
      "iteration 362 - y: 0.02620994886236315\r",
      "iteration 363 - y: 0.02606232499109631\r",
      "iteration 364 - y: 0.025914701119829464\r",
      "iteration 365 - y: 0.025767077248562628\r",
      "iteration 366 - y: 0.025619453377295785\r",
      "iteration 367 - y: 0.025471829506028946\r",
      "iteration 368 - y: 0.025324205634762106\r",
      "iteration 369 - y: 0.025176581763495263\r",
      "iteration 370 - y: 0.02502895789222842\r",
      "iteration 371 - y: 0.02488133402096158\r",
      "iteration 372 - y: 0.024733710149694742\r",
      "iteration 373 - y: 0.0245860862784279\r",
      "iteration 374 - y: 0.02443846240716106\r",
      "iteration 375 - y: 0.02429083853589422\r",
      "iteration 376 - y: 0.024143214664627377\r",
      "iteration 377 - y: 0.023995590793360534\r",
      "iteration 378 - y: 0.023847966922093698\r",
      "iteration 379 - y: 0.023700343050826855\r",
      "iteration 380 - y: 0.023552719179560012\r",
      "iteration 381 - y: 0.02340509530829317\r",
      "iteration 382 - y: 0.02325747143702633\r",
      "iteration 383 - y: 0.02310984756575949\r",
      "iteration 384 - y: 0.022962223694492648\r",
      "iteration 385 - y: 0.02281459982322581\r",
      "iteration 386 - y: 0.02266697595195897\r",
      "iteration 387 - y: 0.022519352080692126\r",
      "iteration 388 - y: 0.022371728209425287\r",
      "iteration 389 - y: 0.022224104338158444\r",
      "iteration 390 - y: 0.022076480466891604\r",
      "iteration 391 - y: 0.02192885659562476\r",
      "iteration 392 - y: 0.02178123272435792\r",
      "iteration 393 - y: 0.021633608853091083\r",
      "iteration 394 - y: 0.02148598498182424\r",
      "iteration 395 - y: 0.021338361110557397\r",
      "iteration 396 - y: 0.021190737239290554\r",
      "iteration 397 - y: 0.021043113368023714\r",
      "iteration 398 - y: 0.020895489496756875\r",
      "iteration 399 - y: 0.020747865625490032\r",
      "iteration 400 - y: 0.02060024175422319\r",
      "iteration 401 - y: 0.020452617882956346\r",
      "iteration 402 - y: 0.02030499401168951\r",
      "iteration 403 - y: 0.020157370140422667\r",
      "iteration 404 - y: 0.020009746269155824\r",
      "iteration 405 - y: 0.019862122397888985\r",
      "iteration 406 - y: 0.019714498526622142\r",
      "iteration 407 - y: 0.019566874655355303\r",
      "iteration 408 - y: 0.01941925078408846\r",
      "iteration 409 - y: 0.019271626912821617\r",
      "iteration 410 - y: 0.01912400304155478\r",
      "iteration 411 - y: 0.018976379170287935\r",
      "iteration 412 - y: 0.018828755299021095\r",
      "iteration 413 - y: 0.018681131427754252\r",
      "iteration 414 - y: 0.018533507556487413\r",
      "iteration 415 - y: 0.01838588368522057\r",
      "iteration 416 - y: 0.01823825981395373\r",
      "iteration 417 - y: 0.018090635942686888\r",
      "iteration 418 - y: 0.017943012071420048\r",
      "iteration 419 - y: 0.017795388200153205\r",
      "iteration 420 - y: 0.017647764328886366\r",
      "iteration 421 - y: 0.017500140457619523\r",
      "iteration 422 - y: 0.017352516586352684\r",
      "iteration 423 - y: 0.01720489271508584\r",
      "iteration 424 - y: 0.017057268843819\r",
      "iteration 425 - y: 0.01690964497255216\r",
      "iteration 426 - y: 0.016762021101285315\r",
      "iteration 427 - y: 0.01661439723001848\r",
      "iteration 428 - y: 0.016466773358751637\r",
      "iteration 429 - y: 0.016319149487484794\r",
      "iteration 430 - y: 0.01617152561621795\r",
      "iteration 431 - y: 0.01602390174495111\r",
      "iteration 432 - y: 0.01587627787368427\r",
      "iteration 433 - y: 0.01572865400241743\r",
      "iteration 434 - y: 0.015581030131150588\r",
      "iteration 435 - y: 0.015433406259883747\r",
      "iteration 436 - y: 0.015285782388616904\r",
      "iteration 437 - y: 0.015138158517350064\r",
      "iteration 438 - y: 0.014990534646083223\r",
      "iteration 439 - y: 0.014842910774816382\r",
      "iteration 440 - y: 0.01469528690354954\r",
      "iteration 441 - y: 0.0145476630322827\r",
      "iteration 442 - y: 0.014400039161015857\r",
      "iteration 443 - y: 0.014252415289749017\r",
      "iteration 444 - y: 0.014104791418482176\r",
      "iteration 445 - y: 0.013957167547215335\r",
      "iteration 446 - y: 0.013809543675948494\r",
      "iteration 447 - y: 0.013661919804681654\r",
      "iteration 448 - y: 0.013514295933414812\r",
      "iteration 449 - y: 0.013366672062147969\r",
      "iteration 450 - y: 0.013219048190881131\r",
      "iteration 451 - y: 0.013071424319614288\r",
      "iteration 452 - y: 0.012923800448347447\r",
      "iteration 453 - y: 0.012776176577080606\r",
      "iteration 454 - y: 0.012628552705813766\r",
      "iteration 455 - y: 0.012480928834546923\r",
      "iteration 456 - y: 0.012333304963280084\r",
      "iteration 457 - y: 0.012185681092013241\r",
      "iteration 458 - y: 0.0120380572207464\r",
      "iteration 459 - y: 0.011890433349479559\r",
      "iteration 460 - y: 0.011742809478212716\r",
      "iteration 461 - y: 0.011595185606945875\r",
      "iteration 462 - y: 0.011447561735679034\r",
      "iteration 463 - y: 0.01129993786441219\r",
      "iteration 464 - y: 0.01115231399314535\r",
      "iteration 465 - y: 0.011004690121878508\r",
      "iteration 466 - y: 0.010857066250611665\r",
      "iteration 467 - y: 0.010709442379344822\r",
      "iteration 468 - y: 0.010561818508077983\r",
      "iteration 469 - y: 0.01041419463681114\r",
      "iteration 470 - y: 0.010266570765544297\r",
      "iteration 471 - y: 0.010140202995731217\r",
      "iteration 472 - y: 0.010023988891734463\r",
      "iteration 473 - y: 0.00990777478773771\r",
      "iteration 474 - y: 0.009791560683740957\r",
      "iteration 475 - y: 0.009675346579744202\r",
      "iteration 476 - y: 0.00955913247574745\r",
      "iteration 477 - y: 0.009442918371750694\r",
      "iteration 478 - y: 0.009326704267753942\r",
      "iteration 479 - y: 0.009210490163757187\r",
      "iteration 480 - y: 0.009094276059760435\r",
      "iteration 481 - y: 0.00897806195576368\r",
      "iteration 482 - y: 0.008861847851766927\r",
      "iteration 483 - y: 0.008745633747770173\r",
      "iteration 484 - y: 0.00862941964377342\r",
      "iteration 485 - y: 0.008513205539776665\r",
      "iteration 486 - y: 0.008396991435779912\r",
      "iteration 487 - y: 0.008280777331783158\r",
      "iteration 488 - y: 0.008164563227786404\r",
      "iteration 489 - y: 0.00804834912378965\r",
      "iteration 490 - y: 0.007932135019792896\r",
      "iteration 491 - y: 0.007815920915796143\r",
      "iteration 492 - y: 0.007699706811799389\r",
      "iteration 493 - y: 0.007583492707802635\r",
      "iteration 494 - y: 0.007467278603805881\r",
      "iteration 495 - y: 0.007351064499809127\r",
      "iteration 496 - y: 0.0072348503958123735\r",
      "iteration 497 - y: 0.00711863629181562\r",
      "iteration 498 - y: 0.007002422187818866\r",
      "iteration 499 - y: 0.006886208083822112\r",
      "iteration 500 - y: 0.006769993979825359\r",
      "iteration 501 - y: 0.006653779875828606\r",
      "iteration 502 - y: 0.006537565771831852\r",
      "iteration 503 - y: 0.006421351667835099\r",
      "iteration 504 - y: 0.0063051375638383455\r",
      "iteration 505 - y: 0.006188923459841593\r",
      "iteration 506 - y: 0.006072709355844839\r",
      "iteration 507 - y: 0.005956495251848085\r",
      "iteration 508 - y: 0.005840281147851331\r",
      "iteration 509 - y: 0.005724067043854579\r",
      "iteration 510 - y: 0.005607852939857825\r",
      "iteration 511 - y: 0.005491638835861071\r",
      "iteration 512 - y: 0.005375424731864318\r",
      "iteration 513 - y: 0.005259210627867565\r",
      "iteration 514 - y: 0.005142996523870812\r",
      "iteration 515 - y: 0.005026782419874058\r",
      "iteration 516 - y: 0.004910568315877305\r",
      "iteration 517 - y: 0.004794354211880551\r",
      "iteration 518 - y: 0.004678140107883798\r",
      "iteration 519 - y: 0.004561926003887044\r",
      "iteration 520 - y: 0.0044457118998902905\r",
      "iteration 521 - y: 0.004329497795893538\r",
      "iteration 522 - y: 0.004213283691896784\r",
      "iteration 523 - y: 0.004097069587900031\r",
      "iteration 524 - y: 0.003980855483903277\r",
      "iteration 525 - y: 0.003864641379906523\r",
      "iteration 526 - y: 0.0037484272759097693\r",
      "iteration 527 - y: 0.0036322131719130155\r",
      "iteration 528 - y: 0.0035159990679162626\r",
      "iteration 529 - y: 0.0033997849639195088\r",
      "iteration 530 - y: 0.0032835708599227554\r",
      "iteration 531 - y: 0.003167356755926002\r",
      "iteration 532 - y: 0.003051142651929248\r",
      "iteration 533 - y: 0.0029349285479324944\r",
      "iteration 534 - y: 0.0028187144439357405\r",
      "iteration 535 - y: 0.0027025003399389876\r",
      "iteration 536 - y: 0.0025862862359422338\r",
      "iteration 537 - y: 0.0024725627588877137\r",
      "iteration 538 - y: 0.002394271368051475\r",
      "iteration 539 - y: 0.002319655487845367\r",
      "iteration 540 - y: 0.0022644105652074274\r",
      "iteration 541 - y: 0.002213422386544314\r",
      "iteration 542 - y: 0.002187595875757304\r",
      "iteration 543 - y: 0.002161769364970294\r",
      "iteration 544 - y: 0.002135942854183284\r",
      "iteration 545 - y: 0.0021101163433962736\r",
      "iteration 546 - y: 0.0020842898326092637\r",
      "iteration 547 - y: 0.0020584633218222537\r",
      "iteration 548 - y: 0.0020326368110352437\r",
      "iteration 549 - y: 0.0020068103002482333\r",
      "iteration 550 - y: 0.0019809837894612234\r",
      "iteration 551 - y: 0.001955157278674213\r",
      "iteration 552 - y: 0.0019293307678872028\r",
      "iteration 553 - y: 0.0019035042571001924\r",
      "iteration 554 - y: 0.0018776777463131822\r",
      "iteration 555 - y: 0.001851851235526172\r",
      "iteration 556 - y: 0.0018260247247391616\r",
      "iteration 557 - y: 0.0018001982139521514\r",
      "iteration 558 - y: 0.0017743717031651412\r",
      "iteration 559 - y: 0.0017485451923781308\r",
      "iteration 560 - y: 0.0017227186815911206\r",
      "iteration 561 - y: 0.0016968921708041104\r",
      "iteration 562 - y: 0.0016710656600171\r",
      "iteration 563 - y: 0.0016452391492300898\r",
      "iteration 564 - y: 0.0016194126384430796\r",
      "iteration 565 - y: 0.0015935861276560692\r",
      "iteration 566 - y: 0.001567759616869059\r",
      "iteration 567 - y: 0.0015419331060820488\r",
      "iteration 568 - y: 0.0015161065952950384\r",
      "iteration 569 - y: 0.0014902800845080283\r",
      "iteration 570 - y: 0.001464453573721018\r",
      "iteration 571 - y: 0.0014386270629340077\r",
      "iteration 572 - y: 0.0014128005521469975\r",
      "iteration 573 - y: 0.0013869740413599873\r",
      "iteration 574 - y: 0.0013611475305729769\r",
      "iteration 575 - y: 0.0013353210197859667\r",
      "iteration 576 - y: 0.0013094945089989565\r",
      "iteration 577 - y: 0.001283667998211946\r",
      "iteration 578 - y: 0.001257841487424936\r",
      "iteration 579 - y: 0.0012320149766379257\r",
      "iteration 580 - y: 0.0012061884658509153\r",
      "iteration 581 - y: 0.0011803619550639051\r",
      "iteration 582 - y: 0.001154535444276895\r",
      "iteration 583 - y: 0.0011287089334898845\r",
      "iteration 584 - y: 0.0011028824227028743\r",
      "iteration 585 - y: 0.0010770559119158642\r",
      "iteration 586 - y: 0.0010512294011288538\r",
      "iteration 587 - y: 0.0010254028903418436\r",
      "iteration 588 - y: 0.0009995763795548334\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.05200480548359063\r",
      "iteration 2 - y: 0.05185718161232379\r",
      "iteration 3 - y: 0.05170955774105694\r",
      "iteration 4 - y: 0.05156193386979009\r",
      "iteration 5 - y: 0.05141430999852325\r",
      "iteration 6 - y: 0.051266686127256406\r",
      "iteration 7 - y: 0.05111906225598956\r",
      "iteration 8 - y: 0.05097143838472271\r",
      "iteration 9 - y: 0.05082381451345587\r",
      "iteration 10 - y: 0.05067619064218903\r",
      "iteration 11 - y: 0.050528566770922184\r",
      "iteration 12 - y: 0.050380942899655334\r",
      "iteration 13 - y: 0.050233319028388485\r",
      "iteration 14 - y: 0.05008569515712165\r",
      "iteration 15 - y: 0.0499380712858548\r",
      "iteration 16 - y: 0.04979044741458795\r",
      "iteration 17 - y: 0.04964282354332111\r",
      "iteration 18 - y: 0.04949519967205427\r",
      "iteration 19 - y: 0.04934757580078742\r",
      "iteration 20 - y: 0.04919995192952058\r",
      "iteration 21 - y: 0.04905232805825373\r",
      "iteration 22 - y: 0.04890470418698689\r",
      "iteration 23 - y: 0.04875708031572004\r",
      "iteration 24 - y: 0.0486094564444532\r",
      "iteration 25 - y: 0.048461832573186356\r",
      "iteration 26 - y: 0.04831420870191951\r",
      "iteration 27 - y: 0.04816658483065267\r",
      "iteration 28 - y: 0.04801896095938582\r",
      "iteration 29 - y: 0.047871337088118984\r",
      "iteration 30 - y: 0.047723713216852134\r",
      "iteration 31 - y: 0.04757608934558529\r",
      "iteration 32 - y: 0.04742846547431845\r",
      "iteration 33 - y: 0.047280841603051606\r",
      "iteration 34 - y: 0.04713321773178476\r",
      "iteration 35 - y: 0.04698559386051791\r",
      "iteration 36 - y: 0.04683796998925107\r",
      "iteration 37 - y: 0.04669034611798423\r",
      "iteration 38 - y: 0.04654272224671738\r",
      "iteration 39 - y: 0.04639509837545054\r",
      "iteration 40 - y: 0.04624747450418369\r",
      "iteration 41 - y: 0.046099850632916856\r",
      "iteration 42 - y: 0.045952226761650006\r",
      "iteration 43 - y: 0.04580460289038316\r",
      "iteration 44 - y: 0.04565697901911632\r",
      "iteration 45 - y: 0.04550935514784948\r",
      "iteration 46 - y: 0.045361731276582634\r",
      "iteration 47 - y: 0.04521410740531579\r",
      "iteration 48 - y: 0.04506648353404895\r",
      "iteration 49 - y: 0.0449188596627821\r",
      "iteration 50 - y: 0.04477123579151526\r",
      "iteration 51 - y: 0.04462361192024841\r",
      "iteration 52 - y: 0.04447598804898157\r",
      "iteration 53 - y: 0.04432836417771473\r",
      "iteration 54 - y: 0.04418074030644788\r",
      "iteration 55 - y: 0.044033116435181034\r",
      "iteration 56 - y: 0.04388549256391419\r",
      "iteration 57 - y: 0.04373786869264734\r",
      "iteration 58 - y: 0.043590244821380505\r",
      "iteration 59 - y: 0.04344262095011366\r",
      "iteration 60 - y: 0.04329499707884681\r",
      "iteration 61 - y: 0.04314737320757998\r",
      "iteration 62 - y: 0.042999749336313134\r",
      "iteration 63 - y: 0.04285212546504629\r",
      "iteration 64 - y: 0.04270450159377945\r",
      "iteration 65 - y: 0.04255687772251261\r",
      "iteration 66 - y: 0.04240925385124576\r",
      "iteration 67 - y: 0.04226162997997892\r",
      "iteration 68 - y: 0.04211400610871208\r",
      "iteration 69 - y: 0.04196638223744524\r",
      "iteration 70 - y: 0.041818758366178405\r",
      "iteration 71 - y: 0.041671134494911555\r",
      "iteration 72 - y: 0.04152351062364471\r",
      "iteration 73 - y: 0.04137588675237787\r",
      "iteration 74 - y: 0.04122826288111103\r",
      "iteration 75 - y: 0.04108063900984419\r",
      "iteration 76 - y: 0.04093301513857735\r",
      "iteration 77 - y: 0.040785391267310504\r",
      "iteration 78 - y: 0.04063776739604366\r",
      "iteration 79 - y: 0.04049014352477682\r",
      "iteration 80 - y: 0.04034251965350998\r",
      "iteration 81 - y: 0.04019489578224313\r",
      "iteration 82 - y: 0.0400472719109763\r",
      "iteration 83 - y: 0.039899648039709454\r",
      "iteration 84 - y: 0.03975202416844261\r",
      "iteration 85 - y: 0.03960440029717577\r",
      "iteration 86 - y: 0.03945677642590893\r",
      "iteration 87 - y: 0.03930915255464208\r",
      "iteration 88 - y: 0.039161528683375246\r",
      "iteration 89 - y: 0.0390139048121084\r",
      "iteration 90 - y: 0.03886628094084156\r",
      "iteration 91 - y: 0.03871865706957472\r",
      "iteration 92 - y: 0.038571033198307875\r",
      "iteration 93 - y: 0.03842340932704103\r",
      "iteration 94 - y: 0.03827578545577419\r",
      "iteration 95 - y: 0.03812816158450735\r",
      "iteration 96 - y: 0.03798053771324051\r",
      "iteration 97 - y: 0.03783291384197366\r",
      "iteration 98 - y: 0.037685289970706824\r",
      "iteration 99 - y: 0.03753766609943998\r",
      "iteration 100 - y: 0.037390042228173145\r",
      "iteration 101 - y: 0.0372424183569063\r",
      "iteration 102 - y: 0.03709479448563946\r",
      "iteration 103 - y: 0.03694717061437262\r",
      "iteration 104 - y: 0.036799546743105774\r",
      "iteration 105 - y: 0.03665192287183893\r",
      "iteration 106 - y: 0.03650429900057209\r",
      "iteration 107 - y: 0.03635667512930525\r",
      "iteration 108 - y: 0.0362090512580384\r",
      "iteration 109 - y: 0.03606142738677156\r",
      "iteration 110 - y: 0.03591380351550472\r",
      "iteration 111 - y: 0.03576617964423788\r",
      "iteration 112 - y: 0.03561855577297104\r",
      "iteration 113 - y: 0.035470931901704195\r",
      "iteration 114 - y: 0.03532330803043735\r",
      "iteration 115 - y: 0.035175684159170516\r",
      "iteration 116 - y: 0.03502806028790367\r",
      "iteration 117 - y: 0.03488043641663683\r",
      "iteration 118 - y: 0.03473281254536999\r",
      "iteration 119 - y: 0.034585188674103144\r",
      "iteration 120 - y: 0.0344375648028363\r",
      "iteration 121 - y: 0.034289940931569465\r",
      "iteration 122 - y: 0.03414231706030262\r",
      "iteration 123 - y: 0.03399469318903578\r",
      "iteration 124 - y: 0.03384706931776894\r",
      "iteration 125 - y: 0.033699445446502094\r",
      "iteration 126 - y: 0.03355182157523525\r",
      "iteration 127 - y: 0.03340419770396841\r",
      "iteration 128 - y: 0.03325657383270157\r",
      "iteration 129 - y: 0.03310894996143472\r",
      "iteration 130 - y: 0.03296132609016788\r",
      "iteration 131 - y: 0.03281370221890104\r",
      "iteration 132 - y: 0.0326660783476342\r",
      "iteration 133 - y: 0.032518454476367364\r",
      "iteration 134 - y: 0.03237083060510052\r",
      "iteration 135 - y: 0.03222320673383368\r",
      "iteration 136 - y: 0.032075582862566836\r",
      "iteration 137 - y: 0.03192795899129999\r",
      "iteration 138 - y: 0.03178033512003316\r",
      "iteration 139 - y: 0.031632711248766314\r",
      "iteration 140 - y: 0.03148508737749947\r",
      "iteration 141 - y: 0.031337463506232635\r",
      "iteration 142 - y: 0.031189839634965792\r",
      "iteration 143 - y: 0.031042215763698953\r",
      "iteration 144 - y: 0.03089459189243211\r",
      "iteration 145 - y: 0.03074696802116527\r",
      "iteration 146 - y: 0.030599344149898428\r",
      "iteration 147 - y: 0.030451720278631585\r",
      "iteration 148 - y: 0.030304096407364745\r",
      "iteration 149 - y: 0.030156472536097906\r",
      "iteration 150 - y: 0.030008848664831063\r",
      "iteration 151 - y: 0.02986122479356422\r",
      "iteration 152 - y: 0.02971360092229738\r",
      "iteration 153 - y: 0.02956597705103054\r",
      "iteration 154 - y: 0.029418353179763698\r",
      "iteration 155 - y: 0.029270729308496862\r",
      "iteration 156 - y: 0.029123105437230016\r",
      "iteration 157 - y: 0.028975481565963176\r",
      "iteration 158 - y: 0.028827857694696337\r",
      "iteration 159 - y: 0.028680233823429494\r",
      "iteration 160 - y: 0.028532609952162655\r",
      "iteration 161 - y: 0.028384986080895815\r",
      "iteration 162 - y: 0.02823736220962897\r",
      "iteration 163 - y: 0.028089738338362133\r",
      "iteration 164 - y: 0.02794211446709529\r",
      "iteration 165 - y: 0.027794490595828447\r",
      "iteration 166 - y: 0.027646866724561608\r",
      "iteration 167 - y: 0.027499242853294765\r",
      "iteration 168 - y: 0.027351618982027925\r",
      "iteration 169 - y: 0.027203995110761083\r",
      "iteration 170 - y: 0.027056371239494247\r",
      "iteration 171 - y: 0.026908747368227404\r",
      "iteration 172 - y: 0.02676112349696056\r",
      "iteration 173 - y: 0.02661349962569372\r",
      "iteration 174 - y: 0.026465875754426882\r",
      "iteration 175 - y: 0.02631825188316004\r",
      "iteration 176 - y: 0.026170628011893196\r",
      "iteration 177 - y: 0.026023004140626357\r",
      "iteration 178 - y: 0.025875380269359517\r",
      "iteration 179 - y: 0.025727756398092674\r",
      "iteration 180 - y: 0.025580132526825835\r",
      "iteration 181 - y: 0.025432508655558992\r",
      "iteration 182 - y: 0.025284884784292153\r",
      "iteration 183 - y: 0.02513726091302531\r",
      "iteration 184 - y: 0.024989637041758467\r",
      "iteration 185 - y: 0.02484201317049163\r",
      "iteration 186 - y: 0.024694389299224788\r",
      "iteration 187 - y: 0.024546765427957945\r",
      "iteration 188 - y: 0.024399141556691106\r",
      "iteration 189 - y: 0.024251517685424263\r",
      "iteration 190 - y: 0.024103893814157423\r",
      "iteration 191 - y: 0.02395626994289058\r",
      "iteration 192 - y: 0.02380864607162374\r",
      "iteration 193 - y: 0.0236610222003569\r",
      "iteration 194 - y: 0.023513398329090055\r",
      "iteration 195 - y: 0.023365774457823216\r",
      "iteration 196 - y: 0.023218150586556373\r",
      "iteration 197 - y: 0.023070526715289533\r",
      "iteration 198 - y: 0.02292290284402269\r",
      "iteration 199 - y: 0.02277527897275585\r",
      "iteration 200 - y: 0.022627655101489008\r",
      "iteration 201 - y: 0.02248003123022217\r",
      "iteration 202 - y: 0.022332407358955326\r",
      "iteration 203 - y: 0.022184783487688483\r",
      "iteration 204 - y: 0.022037159616421644\r",
      "iteration 205 - y: 0.021889535745154804\r",
      "iteration 206 - y: 0.02174191187388796\r",
      "iteration 207 - y: 0.021594288002621122\r",
      "iteration 208 - y: 0.02144666413135428\r",
      "iteration 209 - y: 0.021299040260087436\r",
      "iteration 210 - y: 0.0211514163888206\r",
      "iteration 211 - y: 0.021003792517553754\r",
      "iteration 212 - y: 0.020856168646286914\r",
      "iteration 213 - y: 0.02070854477502007\r",
      "iteration 214 - y: 0.020560920903753232\r",
      "iteration 215 - y: 0.02041329703248639\r",
      "iteration 216 - y: 0.02026567316121955\r",
      "iteration 217 - y: 0.020118049289952707\r",
      "iteration 218 - y: 0.019970425418685864\r",
      "iteration 219 - y: 0.019822801547419024\r",
      "iteration 220 - y: 0.019675177676152185\r",
      "iteration 221 - y: 0.019527553804885342\r",
      "iteration 222 - y: 0.0193799299336185\r",
      "iteration 223 - y: 0.019232306062351656\r",
      "iteration 224 - y: 0.01908468219108482\r",
      "iteration 225 - y: 0.018937058319817977\r",
      "iteration 226 - y: 0.018789434448551138\r",
      "iteration 227 - y: 0.0186418105772843\r",
      "iteration 228 - y: 0.018494186706017456\r",
      "iteration 229 - y: 0.018346562834750613\r",
      "iteration 230 - y: 0.01819893896348377\r",
      "iteration 231 - y: 0.01805131509221693\r",
      "iteration 232 - y: 0.01790369122095009\r",
      "iteration 233 - y: 0.017756067349683248\r",
      "iteration 234 - y: 0.01760844347841641\r",
      "iteration 235 - y: 0.017460819607149566\r",
      "iteration 236 - y: 0.017313195735882726\r",
      "iteration 237 - y: 0.017165571864615883\r",
      "iteration 238 - y: 0.01701794799334904\r",
      "iteration 239 - y: 0.016870324122082204\r",
      "iteration 240 - y: 0.01672270025081536\r",
      "iteration 241 - y: 0.01657507637954852\r",
      "iteration 242 - y: 0.01642745250828168\r",
      "iteration 243 - y: 0.01627982863701484\r",
      "iteration 244 - y: 0.016132204765747997\r",
      "iteration 245 - y: 0.015984580894481154\r",
      "iteration 246 - y: 0.01583695702321431\r",
      "iteration 247 - y: 0.01568933315194747\r",
      "iteration 248 - y: 0.01554170928068063\r",
      "iteration 249 - y: 0.01539408540941379\r",
      "iteration 250 - y: 0.01524646153814695\r",
      "iteration 251 - y: 0.015098837666880107\r",
      "iteration 252 - y: 0.014951213795613266\r",
      "iteration 253 - y: 0.014803589924346425\r",
      "iteration 254 - y: 0.014655966053079585\r",
      "iteration 255 - y: 0.014508342181812742\r",
      "iteration 256 - y: 0.0143607183105459\r",
      "iteration 257 - y: 0.01421309443927906\r",
      "iteration 258 - y: 0.014065470568012217\r",
      "iteration 259 - y: 0.013917846696745378\r",
      "iteration 260 - y: 0.013770222825478535\r",
      "iteration 261 - y: 0.01364504262332542\r",
      "iteration 262 - y: 0.013528828519328667\r",
      "iteration 263 - y: 0.013412614415331913\r",
      "iteration 264 - y: 0.01329640031133516\r",
      "iteration 265 - y: 0.013180186207338407\r",
      "iteration 266 - y: 0.013063972103341653\r",
      "iteration 267 - y: 0.012947757999344901\r",
      "iteration 268 - y: 0.012831543895348147\r",
      "iteration 269 - y: 0.012715329791351393\r",
      "iteration 270 - y: 0.01259911568735464\r",
      "iteration 271 - y: 0.012482901583357887\r",
      "iteration 272 - y: 0.012366687479361134\r",
      "iteration 273 - y: 0.01225047337536438\r",
      "iteration 274 - y: 0.012134259271367628\r",
      "iteration 275 - y: 0.012018045167370876\r",
      "iteration 276 - y: 0.01190183106337412\r",
      "iteration 277 - y: 0.011785616959377368\r",
      "iteration 278 - y: 0.011669402855380616\r",
      "iteration 279 - y: 0.01155318875138386\r",
      "iteration 280 - y: 0.011436974647387108\r",
      "iteration 281 - y: 0.011320760543390354\r",
      "iteration 282 - y: 0.011204546439393602\r",
      "iteration 283 - y: 0.011088332335396849\r",
      "iteration 284 - y: 0.010972118231400095\r",
      "iteration 285 - y: 0.010855904127403343\r",
      "iteration 286 - y: 0.010739690023406589\r",
      "iteration 287 - y: 0.010623475919409835\r",
      "iteration 288 - y: 0.010507261815413083\r",
      "iteration 289 - y: 0.010391047711416329\r",
      "iteration 290 - y: 0.010274833607419575\r",
      "iteration 291 - y: 0.010158619503422821\r",
      "iteration 292 - y: 0.01004240539942607\r",
      "iteration 293 - y: 0.009926191295429316\r",
      "iteration 294 - y: 0.009809977191432562\r",
      "iteration 295 - y: 0.00969376308743581\r",
      "iteration 296 - y: 0.009577548983439054\r",
      "iteration 297 - y: 0.009461334879442302\r",
      "iteration 298 - y: 0.009345120775445546\r",
      "iteration 299 - y: 0.009228906671448794\r",
      "iteration 300 - y: 0.00911269256745204\r",
      "iteration 301 - y: 0.008996478463455287\r",
      "iteration 302 - y: 0.008880264359458533\r",
      "iteration 303 - y: 0.008764050255461779\r",
      "iteration 304 - y: 0.008647836151465027\r",
      "iteration 305 - y: 0.008531622047468271\r",
      "iteration 306 - y: 0.00841540794347152\r",
      "iteration 307 - y: 0.008299193839474764\r",
      "iteration 308 - y: 0.008182979735478012\r",
      "iteration 309 - y: 0.008066765631481258\r",
      "iteration 310 - y: 0.007950551527484504\r",
      "iteration 311 - y: 0.00783433742348775\r",
      "iteration 312 - y: 0.0077181233194909965\r",
      "iteration 313 - y: 0.007601909215494243\r",
      "iteration 314 - y: 0.007485695111497489\r",
      "iteration 315 - y: 0.007369481007500736\r",
      "iteration 316 - y: 0.007253266903503982\r",
      "iteration 317 - y: 0.007137052799507228\r",
      "iteration 318 - y: 0.007020838695510475\r",
      "iteration 319 - y: 0.006904624591513721\r",
      "iteration 320 - y: 0.006788410487516968\r",
      "iteration 321 - y: 0.006672196383520214\r",
      "iteration 322 - y: 0.00655598227952346\r",
      "iteration 323 - y: 0.006439768175526706\r",
      "iteration 324 - y: 0.006323554071529952\r",
      "iteration 325 - y: 0.006207339967533199\r",
      "iteration 326 - y: 0.006091125863536445\r",
      "iteration 327 - y: 0.005974911759539691\r",
      "iteration 328 - y: 0.005858697655542937\r",
      "iteration 329 - y: 0.005742483551546183\r",
      "iteration 330 - y: 0.0056262694475494295\r",
      "iteration 331 - y: 0.005510055343552677\r",
      "iteration 332 - y: 0.005393841239555923\r",
      "iteration 333 - y: 0.005277627135559169\r",
      "iteration 334 - y: 0.005184153649847515\r",
      "iteration 335 - y: 0.005097357957701691\r",
      "iteration 336 - y: 0.005010562265555867\r",
      "iteration 337 - y: 0.004923766573410043\r",
      "iteration 338 - y: 0.004836970881264219\r",
      "iteration 339 - y: 0.004750175189118393\r",
      "iteration 340 - y: 0.0046633794969725695\r",
      "iteration 341 - y: 0.0045765838048267456\r",
      "iteration 342 - y: 0.004489788112680922\r",
      "iteration 343 - y: 0.004402992420535098\r",
      "iteration 344 - y: 0.004316196728389274\r",
      "iteration 345 - y: 0.00422940103624345\r",
      "iteration 346 - y: 0.004142605344097625\r",
      "iteration 347 - y: 0.0040558096519518\r",
      "iteration 348 - y: 0.003969013959805976\r",
      "iteration 349 - y: 0.0038822182676601525\r",
      "iteration 350 - y: 0.0037954225755143286\r",
      "iteration 351 - y: 0.0037086268833685047\r",
      "iteration 352 - y: 0.0036218311912226803\r",
      "iteration 353 - y: 0.003535035499076856\r",
      "iteration 354 - y: 0.003448239806931031\r",
      "iteration 355 - y: 0.0033614441147852073\r",
      "iteration 356 - y: 0.003274648422639383\r",
      "iteration 357 - y: 0.0031878527304935586\r",
      "iteration 358 - y: 0.0031010570383477347\r",
      "iteration 359 - y: 0.00301426134620191\r",
      "iteration 360 - y: 0.0029274656540560855\r",
      "iteration 361 - y: 0.002840669961910261\r",
      "iteration 362 - y: 0.0027538742697644373\r",
      "iteration 363 - y: 0.002667078577618613\r",
      "iteration 364 - y: 0.0025802828854727886\r",
      "iteration 365 - y: 0.0024934871933269646\r",
      "iteration 366 - y: 0.00240669150118114\r",
      "iteration 367 - y: 0.002319895809035315\r",
      "iteration 368 - y: 0.0022331001168894907\r",
      "iteration 369 - y: 0.0021463044247436664\r",
      "iteration 370 - y: 0.0020595087325978416\r",
      "iteration 371 - y: 0.0019727130404520172\r",
      "iteration 372 - y: 0.0018883959443370642\r",
      "iteration 373 - y: 0.001830391604060476\r",
      "iteration 374 - y: 0.0018045650932734657\r",
      "iteration 375 - y: 0.0017787385824864555\r",
      "iteration 376 - y: 0.0017529120716994451\r",
      "iteration 377 - y: 0.001727085560912435\r",
      "iteration 378 - y: 0.0017012590501254247\r",
      "iteration 379 - y: 0.0016754325393384143\r",
      "iteration 380 - y: 0.0016496060285514042\r",
      "iteration 381 - y: 0.001623779517764394\r",
      "iteration 382 - y: 0.0015979530069773836\r",
      "iteration 383 - y: 0.0015721264961903734\r",
      "iteration 384 - y: 0.0015462999854033632\r",
      "iteration 385 - y: 0.0015204734746163528\r",
      "iteration 386 - y: 0.0014946469638293426\r",
      "iteration 387 - y: 0.0014688204530423324\r",
      "iteration 388 - y: 0.001442993942255322\r",
      "iteration 389 - y: 0.0014171674314683118\r",
      "iteration 390 - y: 0.0013913409206813016\r",
      "iteration 391 - y: 0.0013655144098942912\r",
      "iteration 392 - y: 0.001339687899107281\r",
      "iteration 393 - y: 0.0013138613883202708\r",
      "iteration 394 - y: 0.0012880348775332604\r",
      "iteration 395 - y: 0.0012622083667462502\r",
      "iteration 396 - y: 0.00123638185595924\r",
      "iteration 397 - y: 0.0012105553451722297\r",
      "iteration 398 - y: 0.0011847288343852195\r",
      "iteration 399 - y: 0.0011589023235982093\r",
      "iteration 400 - y: 0.0011330758128111989\r",
      "iteration 401 - y: 0.0011072493020241887\r",
      "iteration 402 - y: 0.0010814227912371785\r",
      "iteration 403 - y: 0.001055596280450168\r",
      "iteration 404 - y: 0.001029769769663158\r",
      "iteration 405 - y: 0.0010039432588761477\r",
      "iteration 406 - y: 0.0009781167480891375\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6630f56e00>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658003da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0795021663896939\r",
      "iteration 2 - y: 0.07935454251842705\r",
      "iteration 3 - y: 0.07920691864716022\r",
      "iteration 4 - y: 0.07905929477589337\r",
      "iteration 5 - y: 0.07891167090462652\r",
      "iteration 6 - y: 0.07876404703335968\r",
      "iteration 7 - y: 0.07861642316209283\r",
      "iteration 8 - y: 0.07846879929082598\r",
      "iteration 9 - y: 0.07832117541955913\r",
      "iteration 10 - y: 0.07817355154829228\r",
      "iteration 11 - y: 0.07802592767702546\r",
      "iteration 12 - y: 0.0778783038057586\r",
      "iteration 13 - y: 0.07773067993449174\r",
      "iteration 14 - y: 0.07758305606322491\r",
      "iteration 15 - y: 0.07743543219195806\r",
      "iteration 16 - y: 0.07728780832069121\r",
      "iteration 17 - y: 0.07714018444942436\r",
      "iteration 18 - y: 0.07699256057815751\r",
      "iteration 19 - y: 0.07684493670689067\r",
      "iteration 20 - y: 0.07669731283562384\r",
      "iteration 21 - y: 0.07654968896435699\r",
      "iteration 22 - y: 0.07640206509309014\r",
      "iteration 23 - y: 0.07625444122182329\r",
      "iteration 24 - y: 0.07610681735055644\r",
      "iteration 25 - y: 0.0759591934792896\r",
      "iteration 26 - y: 0.07581156960802275\r",
      "iteration 27 - y: 0.0756639457367559\r",
      "iteration 28 - y: 0.07551632186548907\r",
      "iteration 29 - y: 0.07536869799422222\r",
      "iteration 30 - y: 0.07522107412295537\r",
      "iteration 31 - y: 0.07507345025168852\r",
      "iteration 32 - y: 0.07492582638042167\r",
      "iteration 33 - y: 0.07477820250915483\r",
      "iteration 34 - y: 0.074630578637888\r",
      "iteration 35 - y: 0.07448295476662115\r",
      "iteration 36 - y: 0.0743353308953543\r",
      "iteration 37 - y: 0.07418770702408745\r",
      "iteration 38 - y: 0.0740400831528206\r",
      "iteration 39 - y: 0.07389245928155376\r",
      "iteration 40 - y: 0.07374483541028691\r",
      "iteration 41 - y: 0.07359721153902006\r",
      "iteration 42 - y: 0.07344958766775322\r",
      "iteration 43 - y: 0.07330196379648637\r",
      "iteration 44 - y: 0.07315433992521952\r",
      "iteration 45 - y: 0.07300671605395267\r",
      "iteration 46 - y: 0.07285909218268583\r",
      "iteration 47 - y: 0.07271146831141899\r",
      "iteration 48 - y: 0.07256384444015214\r",
      "iteration 49 - y: 0.07241622056888529\r",
      "iteration 50 - y: 0.07226859669761845\r",
      "iteration 51 - y: 0.0721209728263516\r",
      "iteration 52 - y: 0.07197334895508475\r",
      "iteration 53 - y: 0.0718257250838179\r",
      "iteration 54 - y: 0.07167810121255105\r",
      "iteration 55 - y: 0.07153047734128422\r",
      "iteration 56 - y: 0.07138285347001737\r",
      "iteration 57 - y: 0.07123522959875052\r",
      "iteration 58 - y: 0.07108760572748368\r",
      "iteration 59 - y: 0.07093998185621683\r",
      "iteration 60 - y: 0.07079235798494998\r",
      "iteration 61 - y: 0.07064473411368313\r",
      "iteration 62 - y: 0.07049711024241628\r",
      "iteration 63 - y: 0.07034948637114945\r",
      "iteration 64 - y: 0.0702018624998826\r",
      "iteration 65 - y: 0.07005423862861575\r",
      "iteration 66 - y: 0.06990661475734891\r",
      "iteration 67 - y: 0.06975899088608206\r",
      "iteration 68 - y: 0.06961136701481521\r",
      "iteration 69 - y: 0.06946374314354838\r",
      "iteration 70 - y: 0.06931611927228153\r",
      "iteration 71 - y: 0.06916849540101468\r",
      "iteration 72 - y: 0.06902087152974783\r",
      "iteration 73 - y: 0.06887324765848098\r",
      "iteration 74 - y: 0.06872562378721414\r",
      "iteration 75 - y: 0.06857799991594729\r",
      "iteration 76 - y: 0.06843037604468044\r",
      "iteration 77 - y: 0.0682827521734136\r",
      "iteration 78 - y: 0.06813512830214676\r",
      "iteration 79 - y: 0.0679875044308799\r",
      "iteration 80 - y: 0.06783988055961307\r",
      "iteration 81 - y: 0.06769225668834622\r",
      "iteration 82 - y: 0.06754463281707937\r",
      "iteration 83 - y: 0.06739700894581252\r",
      "iteration 84 - y: 0.06724938507454567\r",
      "iteration 85 - y: 0.06710176120327883\r",
      "iteration 86 - y: 0.06695413733201198\r",
      "iteration 87 - y: 0.06680651346074513\r",
      "iteration 88 - y: 0.0666588895894783\r",
      "iteration 89 - y: 0.06651126571821145\r",
      "iteration 90 - y: 0.0663636418469446\r",
      "iteration 91 - y: 0.06621601797567776\r",
      "iteration 92 - y: 0.06606839410441091\r",
      "iteration 93 - y: 0.06592077023314406\r",
      "iteration 94 - y: 0.06577314636187721\r",
      "iteration 95 - y: 0.06562552249061038\r",
      "iteration 96 - y: 0.06547789861934353\r",
      "iteration 97 - y: 0.06533027474807668\r",
      "iteration 98 - y: 0.06518265087680983\r",
      "iteration 99 - y: 0.06503502700554299\r",
      "iteration 100 - y: 0.06488740313427614\r",
      "iteration 101 - y: 0.06473977926300929\r",
      "iteration 102 - y: 0.06459215539174244\r",
      "iteration 103 - y: 0.0644445315204756\r",
      "iteration 104 - y: 0.06429690764920876\r",
      "iteration 105 - y: 0.0641492837779419\r",
      "iteration 106 - y: 0.06400165990667506\r",
      "iteration 107 - y: 0.06385403603540822\r",
      "iteration 108 - y: 0.06370641216414137\r",
      "iteration 109 - y: 0.06355878829287452\r",
      "iteration 110 - y: 0.06341116442160767\r",
      "iteration 111 - y: 0.06326354055034084\r",
      "iteration 112 - y: 0.06311591667907399\r",
      "iteration 113 - y: 0.06296829280780714\r",
      "iteration 114 - y: 0.06282066893654029\r",
      "iteration 115 - y: 0.06267304506527345\r",
      "iteration 116 - y: 0.0625254211940066\r",
      "iteration 117 - y: 0.06237779732273975\r",
      "iteration 118 - y: 0.0622301734514729\r",
      "iteration 119 - y: 0.06208254958020606\r",
      "iteration 120 - y: 0.061934925708939215\r",
      "iteration 121 - y: 0.06178730183767237\r",
      "iteration 122 - y: 0.06163967796640553\r",
      "iteration 123 - y: 0.06149205409513868\r",
      "iteration 124 - y: 0.06134443022387183\r",
      "iteration 125 - y: 0.06119680635260498\r",
      "iteration 126 - y: 0.06104918248133814\r",
      "iteration 127 - y: 0.060901558610071294\r",
      "iteration 128 - y: 0.060753934738804444\r",
      "iteration 129 - y: 0.06060631086753761\r",
      "iteration 130 - y: 0.06045868699627076\r",
      "iteration 131 - y: 0.06031106312500391\r",
      "iteration 132 - y: 0.060163439253737065\r",
      "iteration 133 - y: 0.06001581538247022\r",
      "iteration 134 - y: 0.05986819151120337\r",
      "iteration 135 - y: 0.05972056763993652\r",
      "iteration 136 - y: 0.05957294376866969\r",
      "iteration 137 - y: 0.05942531989740284\r",
      "iteration 138 - y: 0.05927769602613599\r",
      "iteration 139 - y: 0.05913007215486914\r",
      "iteration 140 - y: 0.0589824482836023\r",
      "iteration 141 - y: 0.05883482441233545\r",
      "iteration 142 - y: 0.0586872005410686\r",
      "iteration 143 - y: 0.058539576669801766\r",
      "iteration 144 - y: 0.058391952798534916\r",
      "iteration 145 - y: 0.058244328927268066\r",
      "iteration 146 - y: 0.058096705056001216\r",
      "iteration 147 - y: 0.057949081184734366\r",
      "iteration 148 - y: 0.05780145731346753\r",
      "iteration 149 - y: 0.05765383344220068\r",
      "iteration 150 - y: 0.05750620957093383\r",
      "iteration 151 - y: 0.057358585699666995\r",
      "iteration 152 - y: 0.057210961828400145\r",
      "iteration 153 - y: 0.057063337957133295\r",
      "iteration 154 - y: 0.056915714085866445\r",
      "iteration 155 - y: 0.0567680902145996\r",
      "iteration 156 - y: 0.05662046634333276\r",
      "iteration 157 - y: 0.05647284247206591\r",
      "iteration 158 - y: 0.05632521860079906\r",
      "iteration 159 - y: 0.056177594729532224\r",
      "iteration 160 - y: 0.056029970858265374\r",
      "iteration 161 - y: 0.055882346986998524\r",
      "iteration 162 - y: 0.05573472311573169\r",
      "iteration 163 - y: 0.05558709924446484\r",
      "iteration 164 - y: 0.055439475373197995\r",
      "iteration 165 - y: 0.055291851501931145\r",
      "iteration 166 - y: 0.0551442276306643\r",
      "iteration 167 - y: 0.05499660375939747\r",
      "iteration 168 - y: 0.05484897988813061\r",
      "iteration 169 - y: 0.05470135601686377\r",
      "iteration 170 - y: 0.05455373214559692\r",
      "iteration 171 - y: 0.05440610827433008\r",
      "iteration 172 - y: 0.05425848440306323\r",
      "iteration 173 - y: 0.054110860531796395\r",
      "iteration 174 - y: 0.053963236660529545\r",
      "iteration 175 - y: 0.0538156127892627\r",
      "iteration 176 - y: 0.05366798891799585\r",
      "iteration 177 - y: 0.05352036504672901\r",
      "iteration 178 - y: 0.05337274117546217\r",
      "iteration 179 - y: 0.05322511730419532\r",
      "iteration 180 - y: 0.053077493432928474\r",
      "iteration 181 - y: 0.052929869561661624\r",
      "iteration 182 - y: 0.05278224569039479\r",
      "iteration 183 - y: 0.05263462181912794\r",
      "iteration 184 - y: 0.052486997947861096\r",
      "iteration 185 - y: 0.05233937407659425\r",
      "iteration 186 - y: 0.0521917502053274\r",
      "iteration 187 - y: 0.05204412633406056\r",
      "iteration 188 - y: 0.05189650246279372\r",
      "iteration 189 - y: 0.05174887859152687\r",
      "iteration 190 - y: 0.051601254720260024\r",
      "iteration 191 - y: 0.05145363084899318\r",
      "iteration 192 - y: 0.05130600697772633\r",
      "iteration 193 - y: 0.051158383106459496\r",
      "iteration 194 - y: 0.051010759235192646\r",
      "iteration 195 - y: 0.050863135363925796\r",
      "iteration 196 - y: 0.05071551149265896\r",
      "iteration 197 - y: 0.05056788762139211\r",
      "iteration 198 - y: 0.05042026375012526\r",
      "iteration 199 - y: 0.050272639878858424\r",
      "iteration 200 - y: 0.050125016007591575\r",
      "iteration 201 - y: 0.04997739213632473\r",
      "iteration 202 - y: 0.04982976826505788\r",
      "iteration 203 - y: 0.04968214439379104\r",
      "iteration 204 - y: 0.04953452052252419\r",
      "iteration 205 - y: 0.049386896651257346\r",
      "iteration 206 - y: 0.0492392727799905\r",
      "iteration 207 - y: 0.049091648908723653\r",
      "iteration 208 - y: 0.04894402503745682\r",
      "iteration 209 - y: 0.04879640116618997\r",
      "iteration 210 - y: 0.048648777294923125\r",
      "iteration 211 - y: 0.04850115342365628\r",
      "iteration 212 - y: 0.04835352955238943\r",
      "iteration 213 - y: 0.04820590568112258\r",
      "iteration 214 - y: 0.048058281809855746\r",
      "iteration 215 - y: 0.047910657938588896\r",
      "iteration 216 - y: 0.047763034067322054\r",
      "iteration 217 - y: 0.04761541019605521\r",
      "iteration 218 - y: 0.04746778632478836\r",
      "iteration 219 - y: 0.047320162453521525\r",
      "iteration 220 - y: 0.047172538582254675\r",
      "iteration 221 - y: 0.047024914710987825\r",
      "iteration 222 - y: 0.04687729083972099\r",
      "iteration 223 - y: 0.04672966696845414\r",
      "iteration 224 - y: 0.04658204309718729\r",
      "iteration 225 - y: 0.046434419225920454\r",
      "iteration 226 - y: 0.046286795354653604\r",
      "iteration 227 - y: 0.04613917148338677\r",
      "iteration 228 - y: 0.04599154761211992\r",
      "iteration 229 - y: 0.04584392374085308\r",
      "iteration 230 - y: 0.04569629986958623\r",
      "iteration 231 - y: 0.045548675998319396\r",
      "iteration 232 - y: 0.045401052127052546\r",
      "iteration 233 - y: 0.04525342825578571\r",
      "iteration 234 - y: 0.045105804384518874\r",
      "iteration 235 - y: 0.044958180513252025\r",
      "iteration 236 - y: 0.04481055664198519\r",
      "iteration 237 - y: 0.044662932770718346\r",
      "iteration 238 - y: 0.0445153088994515\r",
      "iteration 239 - y: 0.04436768502818466\r",
      "iteration 240 - y: 0.04422006115691782\r",
      "iteration 241 - y: 0.04407243728565098\r",
      "iteration 242 - y: 0.04392481341438413\r",
      "iteration 243 - y: 0.043777189543117295\r",
      "iteration 244 - y: 0.043629565671850445\r",
      "iteration 245 - y: 0.04348194180058361\r",
      "iteration 246 - y: 0.04333431792931677\r",
      "iteration 247 - y: 0.043186694058049924\r",
      "iteration 248 - y: 0.04303907018678309\r",
      "iteration 249 - y: 0.04289144631551624\r",
      "iteration 250 - y: 0.0427438224442494\r",
      "iteration 251 - y: 0.04259619857298255\r",
      "iteration 252 - y: 0.042448574701715716\r",
      "iteration 253 - y: 0.04230095083044888\r",
      "iteration 254 - y: 0.04215332695918203\r",
      "iteration 255 - y: 0.042005703087915194\r",
      "iteration 256 - y: 0.041858079216648345\r",
      "iteration 257 - y: 0.04171045534538151\r",
      "iteration 258 - y: 0.04156283147411467\r",
      "iteration 259 - y: 0.04141520760284782\r",
      "iteration 260 - y: 0.04126758373158098\r",
      "iteration 261 - y: 0.04111995986031414\r",
      "iteration 262 - y: 0.040972335989047294\r",
      "iteration 263 - y: 0.04082471211778045\r",
      "iteration 264 - y: 0.040677088246513615\r",
      "iteration 265 - y: 0.04052946437524677\r",
      "iteration 266 - y: 0.04038184050397993\r",
      "iteration 267 - y: 0.040234216632713087\r",
      "iteration 268 - y: 0.040086592761446244\r",
      "iteration 269 - y: 0.03993896889017941\r",
      "iteration 270 - y: 0.03979134501891256\r",
      "iteration 271 - y: 0.03964372114764572\r",
      "iteration 272 - y: 0.03949609727637887\r",
      "iteration 273 - y: 0.039348473405112036\r",
      "iteration 274 - y: 0.039200849533845186\r",
      "iteration 275 - y: 0.03905322566257835\r",
      "iteration 276 - y: 0.038905601791311514\r",
      "iteration 277 - y: 0.038757977920044664\r",
      "iteration 278 - y: 0.03861035404877783\r",
      "iteration 279 - y: 0.038462730177510986\r",
      "iteration 280 - y: 0.03831510630624414\r",
      "iteration 281 - y: 0.0381674824349773\r",
      "iteration 282 - y: 0.03801985856371046\r",
      "iteration 283 - y: 0.037872234692443614\r",
      "iteration 284 - y: 0.03772461082117678\r",
      "iteration 285 - y: 0.037576986949909935\r",
      "iteration 286 - y: 0.037429363078643085\r",
      "iteration 287 - y: 0.03728173920737625\r",
      "iteration 288 - y: 0.037134115336109406\r",
      "iteration 289 - y: 0.036986491464842564\r",
      "iteration 290 - y: 0.03683886759357573\r",
      "iteration 291 - y: 0.03669124372230888\r",
      "iteration 292 - y: 0.03654361985104204\r",
      "iteration 293 - y: 0.03639599597977519\r",
      "iteration 294 - y: 0.036248372108508356\r",
      "iteration 295 - y: 0.03610074823724152\r",
      "iteration 296 - y: 0.03595312436597467\r",
      "iteration 297 - y: 0.03580550049470783\r",
      "iteration 298 - y: 0.035657876623440984\r",
      "iteration 299 - y: 0.03551025275217415\r",
      "iteration 300 - y: 0.035362628880907306\r",
      "iteration 301 - y: 0.03521500500964046\r",
      "iteration 302 - y: 0.03506738113837362\r",
      "iteration 303 - y: 0.03491975726710678\r",
      "iteration 304 - y: 0.03477213339583994\r",
      "iteration 305 - y: 0.0346245095245731\r",
      "iteration 306 - y: 0.034476885653306255\r",
      "iteration 307 - y: 0.03432926178203941\r",
      "iteration 308 - y: 0.03418163791077257\r",
      "iteration 309 - y: 0.03403401403950573\r",
      "iteration 310 - y: 0.033886390168238884\r",
      "iteration 311 - y: 0.03373876629697205\r",
      "iteration 312 - y: 0.0335911424257052\r",
      "iteration 313 - y: 0.03344351855443836\r",
      "iteration 314 - y: 0.03329589468317152\r",
      "iteration 315 - y: 0.033148270811904676\r",
      "iteration 316 - y: 0.03300064694063784\r",
      "iteration 317 - y: 0.032853023069371\r",
      "iteration 318 - y: 0.032705399198104154\r",
      "iteration 319 - y: 0.03255777532683732\r",
      "iteration 320 - y: 0.032410151455570475\r",
      "iteration 321 - y: 0.03226252758430363\r",
      "iteration 322 - y: 0.03211490371303679\r",
      "iteration 323 - y: 0.03196727984176995\r",
      "iteration 324 - y: 0.03181965597050311\r",
      "iteration 325 - y: 0.03167203209923627\r",
      "iteration 326 - y: 0.031524408227969425\r",
      "iteration 327 - y: 0.03137678435670259\r",
      "iteration 328 - y: 0.031229160485435743\r",
      "iteration 329 - y: 0.031081536614168903\r",
      "iteration 330 - y: 0.03093391274290206\r",
      "iteration 331 - y: 0.030786288871635224\r",
      "iteration 332 - y: 0.03063866500036838\r",
      "iteration 333 - y: 0.030491041129101542\r",
      "iteration 334 - y: 0.0303434172578347\r",
      "iteration 335 - y: 0.030195793386567856\r",
      "iteration 336 - y: 0.030048169515301017\r",
      "iteration 337 - y: 0.029900545644034174\r",
      "iteration 338 - y: 0.02975292177276733\r",
      "iteration 339 - y: 0.029605297901500495\r",
      "iteration 340 - y: 0.029457674030233652\r",
      "iteration 341 - y: 0.029310050158966813\r",
      "iteration 342 - y: 0.02916242628769997\r",
      "iteration 343 - y: 0.02901480241643313\r",
      "iteration 344 - y: 0.028867178545166287\r",
      "iteration 345 - y: 0.028719554673899444\r",
      "iteration 346 - y: 0.02857193080263261\r",
      "iteration 347 - y: 0.028424306931365766\r",
      "iteration 348 - y: 0.028276683060098923\r",
      "iteration 349 - y: 0.028129059188832083\r",
      "iteration 350 - y: 0.02798143531756524\r",
      "iteration 351 - y: 0.0278338114462984\r",
      "iteration 352 - y: 0.027686187575031558\r",
      "iteration 353 - y: 0.027538563703764722\r",
      "iteration 354 - y: 0.02739093983249788\r",
      "iteration 355 - y: 0.027243315961231036\r",
      "iteration 356 - y: 0.027095692089964197\r",
      "iteration 357 - y: 0.026948068218697354\r",
      "iteration 358 - y: 0.026800444347430515\r",
      "iteration 359 - y: 0.02665282047616367\r",
      "iteration 360 - y: 0.02650519660489683\r",
      "iteration 361 - y: 0.026357572733629993\r",
      "iteration 362 - y: 0.02620994886236315\r",
      "iteration 363 - y: 0.02606232499109631\r",
      "iteration 364 - y: 0.025914701119829464\r",
      "iteration 365 - y: 0.025767077248562628\r",
      "iteration 366 - y: 0.025619453377295785\r",
      "iteration 367 - y: 0.025471829506028946\r",
      "iteration 368 - y: 0.025324205634762106\r",
      "iteration 369 - y: 0.025176581763495263\r",
      "iteration 370 - y: 0.02502895789222842\r",
      "iteration 371 - y: 0.02488133402096158\r",
      "iteration 372 - y: 0.024733710149694742\r",
      "iteration 373 - y: 0.0245860862784279\r",
      "iteration 374 - y: 0.02443846240716106\r",
      "iteration 375 - y: 0.02429083853589422\r",
      "iteration 376 - y: 0.024143214664627377\r",
      "iteration 377 - y: 0.023995590793360534\r",
      "iteration 378 - y: 0.023847966922093698\r",
      "iteration 379 - y: 0.023700343050826855\r",
      "iteration 380 - y: 0.023552719179560012\r",
      "iteration 381 - y: 0.02340509530829317\r",
      "iteration 382 - y: 0.02325747143702633\r",
      "iteration 383 - y: 0.02310984756575949\r",
      "iteration 384 - y: 0.022962223694492648\r",
      "iteration 385 - y: 0.02281459982322581\r",
      "iteration 386 - y: 0.02266697595195897\r",
      "iteration 387 - y: 0.022519352080692126\r",
      "iteration 388 - y: 0.022371728209425287\r",
      "iteration 389 - y: 0.022224104338158444\r",
      "iteration 390 - y: 0.022076480466891604\r",
      "iteration 391 - y: 0.02192885659562476\r",
      "iteration 392 - y: 0.02178123272435792\r",
      "iteration 393 - y: 0.021633608853091083\r",
      "iteration 394 - y: 0.02148598498182424\r",
      "iteration 395 - y: 0.021338361110557397\r",
      "iteration 396 - y: 0.021190737239290554\r",
      "iteration 397 - y: 0.021043113368023714\r",
      "iteration 398 - y: 0.020895489496756875\r",
      "iteration 399 - y: 0.020747865625490032\r",
      "iteration 400 - y: 0.02060024175422319\r",
      "iteration 401 - y: 0.020452617882956346\r",
      "iteration 402 - y: 0.02030499401168951\r",
      "iteration 403 - y: 0.020157370140422667\r",
      "iteration 404 - y: 0.020009746269155824\r",
      "iteration 405 - y: 0.019862122397888985\r",
      "iteration 406 - y: 0.019714498526622142\r",
      "iteration 407 - y: 0.019566874655355303\r",
      "iteration 408 - y: 0.01941925078408846\r",
      "iteration 409 - y: 0.019271626912821617\r",
      "iteration 410 - y: 0.01912400304155478\r",
      "iteration 411 - y: 0.018976379170287935\r",
      "iteration 412 - y: 0.018828755299021095\r",
      "iteration 413 - y: 0.018681131427754252\r",
      "iteration 414 - y: 0.018533507556487413\r",
      "iteration 415 - y: 0.01838588368522057\r",
      "iteration 416 - y: 0.01823825981395373\r",
      "iteration 417 - y: 0.018090635942686888\r",
      "iteration 418 - y: 0.017943012071420048\r",
      "iteration 419 - y: 0.017795388200153205\r",
      "iteration 420 - y: 0.017647764328886366\r",
      "iteration 421 - y: 0.017500140457619523\r",
      "iteration 422 - y: 0.017352516586352684\r",
      "iteration 423 - y: 0.01720489271508584\r",
      "iteration 424 - y: 0.017057268843819\r",
      "iteration 425 - y: 0.01690964497255216\r",
      "iteration 426 - y: 0.016762021101285315\r",
      "iteration 427 - y: 0.01661439723001848\r",
      "iteration 428 - y: 0.016466773358751637\r",
      "iteration 429 - y: 0.016319149487484794\r",
      "iteration 430 - y: 0.01617152561621795\r",
      "iteration 431 - y: 0.01602390174495111\r",
      "iteration 432 - y: 0.01587627787368427\r",
      "iteration 433 - y: 0.01572865400241743\r",
      "iteration 434 - y: 0.015581030131150588\r",
      "iteration 435 - y: 0.015433406259883747\r",
      "iteration 436 - y: 0.015285782388616904\r",
      "iteration 437 - y: 0.015138158517350064\r",
      "iteration 438 - y: 0.014990534646083223\r",
      "iteration 439 - y: 0.014842910774816382\r",
      "iteration 440 - y: 0.01469528690354954\r",
      "iteration 441 - y: 0.0145476630322827\r",
      "iteration 442 - y: 0.014400039161015857\r",
      "iteration 443 - y: 0.014252415289749017\r",
      "iteration 444 - y: 0.014104791418482176\r",
      "iteration 445 - y: 0.013957167547215335\r",
      "iteration 446 - y: 0.013809543675948494\r",
      "iteration 447 - y: 0.013661919804681654\r",
      "iteration 448 - y: 0.013514295933414812\r",
      "iteration 449 - y: 0.013366672062147969\r",
      "iteration 450 - y: 0.013219048190881131\r",
      "iteration 451 - y: 0.013071424319614288\r",
      "iteration 452 - y: 0.012923800448347447\r",
      "iteration 453 - y: 0.012776176577080606\r",
      "iteration 454 - y: 0.012628552705813766\r",
      "iteration 455 - y: 0.012480928834546923\r",
      "iteration 456 - y: 0.012333304963280084\r",
      "iteration 457 - y: 0.012185681092013241\r",
      "iteration 458 - y: 0.0120380572207464\r",
      "iteration 459 - y: 0.011890433349479559\r",
      "iteration 460 - y: 0.011742809478212716\r",
      "iteration 461 - y: 0.011595185606945875\r",
      "iteration 462 - y: 0.011447561735679034\r",
      "iteration 463 - y: 0.01129993786441219\r",
      "iteration 464 - y: 0.01115231399314535\r",
      "iteration 465 - y: 0.011004690121878508\r",
      "iteration 466 - y: 0.010857066250611665\r",
      "iteration 467 - y: 0.010709442379344822\r",
      "iteration 468 - y: 0.010561818508077983\r",
      "iteration 469 - y: 0.01041419463681114\r",
      "iteration 470 - y: 0.010266570765544297\r",
      "iteration 471 - y: 0.010140202995731217\r",
      "iteration 472 - y: 0.010023988891734463\r",
      "iteration 473 - y: 0.00990777478773771\r",
      "iteration 474 - y: 0.009791560683740957\r",
      "iteration 475 - y: 0.009675346579744202\r",
      "iteration 476 - y: 0.00955913247574745\r",
      "iteration 477 - y: 0.009442918371750694\r",
      "iteration 478 - y: 0.009326704267753942\r",
      "iteration 479 - y: 0.009210490163757187\r",
      "iteration 480 - y: 0.009094276059760435\r",
      "iteration 481 - y: 0.00897806195576368\r",
      "iteration 482 - y: 0.008861847851766927\r",
      "iteration 483 - y: 0.008745633747770173\r",
      "iteration 484 - y: 0.00862941964377342\r",
      "iteration 485 - y: 0.008513205539776665\r",
      "iteration 486 - y: 0.008396991435779912\r",
      "iteration 487 - y: 0.008280777331783158\r",
      "iteration 488 - y: 0.008164563227786404\r",
      "iteration 489 - y: 0.00804834912378965\r",
      "iteration 490 - y: 0.007932135019792896\r",
      "iteration 491 - y: 0.007815920915796143\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 554 - y: 0.0009774164927449363\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-4.2270403e-07]], dtype=float32),\n",
       " array([[-2.258073e-05]], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ref = 0.001\n",
    "best_model = K.loadBestModel()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': y_ref}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a05202",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b66315bed10>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de3cb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6631647790>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = TrainLR(M_samples, H_samples, y_ref = y_ref, fit_intercept = False)\n",
    "regr = L.quickTrain()\n",
    "\n",
    "XL = XLR(regr, M_samples)\n",
    "a_LR, stats_LR = XL.quick_analyze()\n",
    "\n",
    "plt.plot(a_LR[0])\n",
    "plt.plot(a_LR[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb211cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0795021663896939\r",
      "iteration 2 - y: 0.07935454251842705\r",
      "iteration 3 - y: 0.07920691864716022\r",
      "iteration 4 - y: 0.07905929477589337\r",
      "iteration 5 - y: 0.07891167090462652\r",
      "iteration 6 - y: 0.07876404703335968\r",
      "iteration 7 - y: 0.07861642316209283\r",
      "iteration 8 - y: 0.07846879929082598\r",
      "iteration 9 - y: 0.07832117541955913\r",
      "iteration 10 - y: 0.07817355154829228\r",
      "iteration 11 - y: 0.07802592767702546\r",
      "iteration 12 - y: 0.0778783038057586\r",
      "iteration 13 - y: 0.07773067993449174\r",
      "iteration 14 - y: 0.07758305606322491\r",
      "iteration 15 - y: 0.07743543219195806\r",
      "iteration 16 - y: 0.07728780832069121\r",
      "iteration 17 - y: 0.07714018444942436\r",
      "iteration 18 - y: 0.07699256057815751\r",
      "iteration 19 - y: 0.07684493670689067\r",
      "iteration 20 - y: 0.07669731283562384\r",
      "iteration 21 - y: 0.07654968896435699\r",
      "iteration 22 - y: 0.07640206509309014\r",
      "iteration 23 - y: 0.07625444122182329\r",
      "iteration 24 - y: 0.07610681735055644\r",
      "iteration 25 - y: 0.0759591934792896\r",
      "iteration 26 - y: 0.07581156960802275\r",
      "iteration 27 - y: 0.0756639457367559\r",
      "iteration 28 - y: 0.07551632186548907\r",
      "iteration 29 - y: 0.07536869799422222\r",
      "iteration 30 - y: 0.07522107412295537\r",
      "iteration 31 - y: 0.07507345025168852\r",
      "iteration 32 - y: 0.07492582638042167\r",
      "iteration 33 - y: 0.07477820250915483\r",
      "iteration 34 - y: 0.074630578637888\r",
      "iteration 35 - y: 0.07448295476662115\r",
      "iteration 36 - y: 0.0743353308953543\r",
      "iteration 37 - y: 0.07418770702408745\r",
      "iteration 38 - y: 0.0740400831528206\r",
      "iteration 39 - y: 0.07389245928155376\r",
      "iteration 40 - y: 0.07374483541028691\r",
      "iteration 41 - y: 0.07359721153902006\r",
      "iteration 42 - y: 0.07344958766775322\r",
      "iteration 43 - y: 0.07330196379648637\r",
      "iteration 44 - y: 0.07315433992521952\r",
      "iteration 45 - y: 0.07300671605395267\r",
      "iteration 46 - y: 0.07285909218268583\r",
      "iteration 47 - y: 0.07271146831141899\r",
      "iteration 48 - y: 0.07256384444015214\r",
      "iteration 49 - y: 0.07241622056888529\r",
      "iteration 50 - y: 0.07226859669761845\r",
      "iteration 51 - y: 0.0721209728263516\r",
      "iteration 52 - y: 0.07197334895508475\r",
      "iteration 53 - y: 0.0718257250838179\r",
      "iteration 54 - y: 0.07167810121255105\r",
      "iteration 55 - y: 0.07153047734128422\r",
      "iteration 56 - y: 0.07138285347001737\r",
      "iteration 57 - y: 0.07123522959875052\r",
      "iteration 58 - y: 0.07108760572748368\r",
      "iteration 59 - y: 0.07093998185621683\r",
      "iteration 60 - y: 0.07079235798494998\r",
      "iteration 61 - y: 0.07064473411368313\r",
      "iteration 62 - y: 0.07049711024241628\r",
      "iteration 63 - y: 0.07034948637114945\r",
      "iteration 64 - y: 0.0702018624998826\r",
      "iteration 65 - y: 0.07005423862861575\r",
      "iteration 66 - y: 0.06990661475734891\r",
      "iteration 67 - y: 0.06975899088608206\r",
      "iteration 68 - y: 0.06961136701481521\r",
      "iteration 69 - y: 0.06946374314354838\r",
      "iteration 70 - y: 0.06931611927228153\r",
      "iteration 71 - y: 0.06916849540101468\r",
      "iteration 72 - y: 0.06902087152974783\r",
      "iteration 73 - y: 0.06887324765848098\r",
      "iteration 74 - y: 0.06872562378721414\r",
      "iteration 75 - y: 0.06857799991594729\r",
      "iteration 76 - y: 0.06843037604468044\r",
      "iteration 77 - y: 0.0682827521734136\r",
      "iteration 78 - y: 0.06813512830214676\r",
      "iteration 79 - y: 0.0679875044308799\r",
      "iteration 80 - y: 0.06783988055961307\r",
      "iteration 81 - y: 0.06769225668834622\r",
      "iteration 82 - y: 0.06754463281707937\r",
      "iteration 83 - y: 0.06739700894581252\r",
      "iteration 84 - y: 0.06724938507454567\r",
      "iteration 85 - y: 0.06710176120327883\r",
      "iteration 86 - y: 0.06695413733201198\r",
      "iteration 87 - y: 0.06680651346074513\r",
      "iteration 88 - y: 0.0666588895894783\r",
      "iteration 89 - y: 0.06651126571821145\r",
      "iteration 90 - y: 0.0663636418469446\r",
      "iteration 91 - y: 0.06621601797567776\r",
      "iteration 92 - y: 0.06606839410441091\r",
      "iteration 93 - y: 0.06592077023314406\r",
      "iteration 94 - y: 0.06577314636187721\r",
      "iteration 95 - y: 0.06562552249061038\r",
      "iteration 96 - y: 0.06547789861934353\r",
      "iteration 97 - y: 0.06533027474807668\r",
      "iteration 98 - y: 0.06518265087680983\r",
      "iteration 99 - y: 0.06503502700554299\r",
      "iteration 100 - y: 0.06488740313427614\r",
      "iteration 101 - y: 0.06473977926300929\r",
      "iteration 102 - y: 0.06459215539174244\r",
      "iteration 103 - y: 0.0644445315204756\r",
      "iteration 104 - y: 0.06429690764920876\r",
      "iteration 105 - y: 0.0641492837779419\r",
      "iteration 106 - y: 0.06400165990667506\r",
      "iteration 107 - y: 0.06385403603540822\r",
      "iteration 108 - y: 0.06370641216414137\r",
      "iteration 109 - y: 0.06355878829287452\r",
      "iteration 110 - y: 0.06341116442160767\r",
      "iteration 111 - y: 0.06326354055034084\r",
      "iteration 112 - y: 0.06311591667907399\r",
      "iteration 113 - y: 0.06296829280780714\r",
      "iteration 114 - y: 0.06282066893654029\r",
      "iteration 115 - y: 0.06267304506527345\r",
      "iteration 116 - y: 0.0625254211940066\r",
      "iteration 117 - y: 0.06237779732273975\r",
      "iteration 118 - y: 0.0622301734514729\r",
      "iteration 119 - y: 0.06208254958020606\r",
      "iteration 120 - y: 0.061934925708939215\r",
      "iteration 121 - y: 0.06178730183767237\r",
      "iteration 122 - y: 0.06163967796640553\r",
      "iteration 123 - y: 0.06149205409513868\r",
      "iteration 124 - y: 0.06134443022387183\r",
      "iteration 125 - y: 0.06119680635260498\r",
      "iteration 126 - y: 0.06104918248133814\r",
      "iteration 127 - y: 0.060901558610071294\r",
      "iteration 128 - y: 0.060753934738804444\r",
      "iteration 129 - y: 0.06060631086753761\r",
      "iteration 130 - y: 0.06045868699627076\r",
      "iteration 131 - y: 0.06031106312500391\r",
      "iteration 132 - y: 0.060163439253737065\r",
      "iteration 133 - y: 0.06001581538247022\r",
      "iteration 134 - y: 0.05986819151120337\r",
      "iteration 135 - y: 0.05972056763993652\r",
      "iteration 136 - y: 0.05957294376866969\r",
      "iteration 137 - y: 0.05942531989740284\r",
      "iteration 138 - y: 0.05927769602613599\r",
      "iteration 139 - y: 0.05913007215486914\r",
      "iteration 140 - y: 0.0589824482836023\r",
      "iteration 141 - y: 0.05883482441233545\r",
      "iteration 142 - y: 0.0586872005410686\r",
      "iteration 143 - y: 0.058539576669801766\r",
      "iteration 144 - y: 0.058391952798534916\r",
      "iteration 145 - y: 0.058244328927268066\r",
      "iteration 146 - y: 0.058096705056001216\r",
      "iteration 147 - y: 0.057949081184734366\r",
      "iteration 148 - y: 0.05780145731346753\r",
      "iteration 149 - y: 0.05765383344220068\r",
      "iteration 150 - y: 0.05750620957093383\r",
      "iteration 151 - y: 0.057358585699666995\r",
      "iteration 152 - y: 0.057210961828400145\r",
      "iteration 153 - y: 0.057063337957133295\r",
      "iteration 154 - y: 0.056915714085866445\r",
      "iteration 155 - y: 0.0567680902145996\r",
      "iteration 156 - y: 0.05662046634333276\r",
      "iteration 157 - y: 0.05647284247206591\r",
      "iteration 158 - y: 0.05632521860079906\r",
      "iteration 159 - y: 0.056177594729532224\r",
      "iteration 160 - y: 0.056029970858265374\r",
      "iteration 161 - y: 0.055882346986998524\r",
      "iteration 162 - y: 0.05573472311573169\r",
      "iteration 163 - y: 0.05558709924446484\r",
      "iteration 164 - y: 0.055439475373197995\r",
      "iteration 165 - y: 0.055291851501931145\r",
      "iteration 166 - y: 0.0551442276306643\r",
      "iteration 167 - y: 0.05499660375939747\r",
      "iteration 168 - y: 0.05484897988813061\r",
      "iteration 169 - y: 0.05470135601686377\r",
      "iteration 170 - y: 0.05455373214559692\r",
      "iteration 171 - y: 0.05440610827433008\r",
      "iteration 172 - y: 0.05425848440306323\r",
      "iteration 173 - y: 0.054110860531796395\r",
      "iteration 174 - y: 0.053963236660529545\r",
      "iteration 175 - y: 0.0538156127892627\r",
      "iteration 176 - y: 0.05366798891799585\r",
      "iteration 177 - y: 0.05352036504672901\r",
      "iteration 178 - y: 0.05337274117546217\r",
      "iteration 179 - y: 0.05322511730419532\r",
      "iteration 180 - y: 0.053077493432928474\r",
      "iteration 181 - y: 0.052929869561661624\r",
      "iteration 182 - y: 0.05278224569039479\r",
      "iteration 183 - y: 0.05263462181912794\r",
      "iteration 184 - y: 0.052486997947861096\r",
      "iteration 185 - y: 0.05233937407659425\r",
      "iteration 186 - y: 0.0521917502053274\r",
      "iteration 187 - y: 0.05204412633406056\r",
      "iteration 188 - y: 0.05189650246279372\r",
      "iteration 189 - y: 0.05174887859152687\r",
      "iteration 190 - y: 0.051601254720260024\r",
      "iteration 191 - y: 0.05145363084899318\r",
      "iteration 192 - y: 0.05130600697772633\r",
      "iteration 193 - y: 0.051158383106459496\r",
      "iteration 194 - y: 0.051010759235192646\r",
      "iteration 195 - y: 0.050863135363925796\r",
      "iteration 196 - y: 0.05071551149265896\r",
      "iteration 197 - y: 0.05056788762139211\r",
      "iteration 198 - y: 0.05042026375012526\r",
      "iteration 199 - y: 0.050272639878858424\r",
      "iteration 200 - y: 0.050125016007591575\r",
      "iteration 201 - y: 0.04997739213632473\r",
      "iteration 202 - y: 0.04982976826505788\r",
      "iteration 203 - y: 0.04968214439379104\r",
      "iteration 204 - y: 0.04953452052252419\r",
      "iteration 205 - y: 0.049386896651257346\r",
      "iteration 206 - y: 0.0492392727799905\r",
      "iteration 207 - y: 0.049091648908723653\r",
      "iteration 208 - y: 0.04894402503745682\r",
      "iteration 209 - y: 0.04879640116618997\r",
      "iteration 210 - y: 0.048648777294923125\r",
      "iteration 211 - y: 0.04850115342365628\r",
      "iteration 212 - y: 0.04835352955238943\r",
      "iteration 213 - y: 0.04820590568112258\r",
      "iteration 214 - y: 0.048058281809855746\r",
      "iteration 215 - y: 0.047910657938588896\r",
      "iteration 216 - y: 0.047763034067322054\r",
      "iteration 217 - y: 0.04761541019605521\r",
      "iteration 218 - y: 0.04746778632478836\r",
      "iteration 219 - y: 0.047320162453521525\r",
      "iteration 220 - y: 0.047172538582254675\r",
      "iteration 221 - y: 0.047024914710987825\r",
      "iteration 222 - y: 0.04687729083972099\r",
      "iteration 223 - y: 0.04672966696845414\r",
      "iteration 224 - y: 0.04658204309718729\r",
      "iteration 225 - y: 0.046434419225920454\r",
      "iteration 226 - y: 0.046286795354653604\r",
      "iteration 227 - y: 0.04613917148338677\r",
      "iteration 228 - y: 0.04599154761211992\r",
      "iteration 229 - y: 0.04584392374085308\r",
      "iteration 230 - y: 0.04569629986958623\r",
      "iteration 231 - y: 0.045548675998319396\r",
      "iteration 232 - y: 0.045401052127052546\r",
      "iteration 233 - y: 0.04525342825578571\r",
      "iteration 234 - y: 0.045105804384518874\r",
      "iteration 235 - y: 0.044958180513252025\r",
      "iteration 236 - y: 0.04481055664198519\r",
      "iteration 237 - y: 0.044662932770718346\r",
      "iteration 238 - y: 0.0445153088994515\r",
      "iteration 239 - y: 0.04436768502818466\r",
      "iteration 240 - y: 0.04422006115691782\r",
      "iteration 241 - y: 0.04407243728565098\r",
      "iteration 242 - y: 0.04392481341438413\r",
      "iteration 243 - y: 0.043777189543117295\r",
      "iteration 244 - y: 0.043629565671850445\r",
      "iteration 245 - y: 0.04348194180058361\r",
      "iteration 246 - y: 0.04333431792931677\r",
      "iteration 247 - y: 0.043186694058049924\r",
      "iteration 248 - y: 0.04303907018678309\r",
      "iteration 249 - y: 0.04289144631551624\r",
      "iteration 250 - y: 0.0427438224442494\r",
      "iteration 251 - y: 0.04259619857298255\r",
      "iteration 252 - y: 0.042448574701715716\r",
      "iteration 253 - y: 0.04230095083044888\r",
      "iteration 254 - y: 0.04215332695918203\r",
      "iteration 255 - y: 0.042005703087915194\r",
      "iteration 256 - y: 0.041858079216648345\r",
      "iteration 257 - y: 0.04171045534538151\r",
      "iteration 258 - y: 0.04156283147411467\r",
      "iteration 259 - y: 0.04141520760284782\r",
      "iteration 260 - y: 0.04126758373158098\r",
      "iteration 261 - y: 0.04111995986031414\r",
      "iteration 262 - y: 0.040972335989047294\r",
      "iteration 263 - y: 0.04082471211778045\r",
      "iteration 264 - y: 0.040677088246513615\r",
      "iteration 265 - y: 0.04052946437524677\r",
      "iteration 266 - y: 0.04038184050397993\r",
      "iteration 267 - y: 0.040234216632713087\r",
      "iteration 268 - y: 0.040086592761446244\r",
      "iteration 269 - y: 0.03993896889017941\r",
      "iteration 270 - y: 0.03979134501891256\r",
      "iteration 271 - y: 0.03964372114764572\r",
      "iteration 272 - y: 0.03949609727637887\r",
      "iteration 273 - y: 0.039348473405112036\r",
      "iteration 274 - y: 0.039200849533845186\r",
      "iteration 275 - y: 0.03905322566257835\r",
      "iteration 276 - y: 0.038905601791311514\r",
      "iteration 277 - y: 0.038757977920044664\r",
      "iteration 278 - y: 0.03861035404877783\r",
      "iteration 279 - y: 0.038462730177510986\r",
      "iteration 280 - y: 0.03831510630624414\r",
      "iteration 281 - y: 0.0381674824349773\r",
      "iteration 282 - y: 0.03801985856371046\r",
      "iteration 283 - y: 0.037872234692443614\r",
      "iteration 284 - y: 0.03772461082117678\r",
      "iteration 285 - y: 0.037576986949909935\r",
      "iteration 286 - y: 0.037429363078643085\r",
      "iteration 287 - y: 0.03728173920737625\r",
      "iteration 288 - y: 0.037134115336109406\r",
      "iteration 289 - y: 0.036986491464842564\r",
      "iteration 290 - y: 0.03683886759357573\r",
      "iteration 291 - y: 0.03669124372230888\r",
      "iteration 292 - y: 0.03654361985104204\r",
      "iteration 293 - y: 0.03639599597977519\r",
      "iteration 294 - y: 0.036248372108508356\r",
      "iteration 295 - y: 0.03610074823724152\r",
      "iteration 296 - y: 0.03595312436597467\r",
      "iteration 297 - y: 0.03580550049470783\r",
      "iteration 298 - y: 0.035657876623440984\r",
      "iteration 299 - y: 0.03551025275217415\r",
      "iteration 300 - y: 0.035362628880907306\r",
      "iteration 301 - y: 0.03521500500964046\r",
      "iteration 302 - y: 0.03506738113837362\r",
      "iteration 303 - y: 0.03491975726710678\r",
      "iteration 304 - y: 0.03477213339583994\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 305 - y: 0.0346245095245731\r",
      "iteration 306 - y: 0.034476885653306255\r",
      "iteration 307 - y: 0.03432926178203941\r",
      "iteration 308 - y: 0.03418163791077257\r",
      "iteration 309 - y: 0.03403401403950573\r",
      "iteration 310 - y: 0.033886390168238884\r",
      "iteration 311 - y: 0.03373876629697205\r",
      "iteration 312 - y: 0.0335911424257052\r",
      "iteration 313 - y: 0.03344351855443836\r",
      "iteration 314 - y: 0.03329589468317152\r",
      "iteration 315 - y: 0.033148270811904676\r",
      "iteration 316 - y: 0.03300064694063784\r",
      "iteration 317 - y: 0.032853023069371\r",
      "iteration 318 - y: 0.032705399198104154\r",
      "iteration 319 - y: 0.03255777532683732\r",
      "iteration 320 - y: 0.032410151455570475\r",
      "iteration 321 - y: 0.03226252758430363\r",
      "iteration 322 - y: 0.03211490371303679\r",
      "iteration 323 - y: 0.03196727984176995\r",
      "iteration 324 - y: 0.03181965597050311\r",
      "iteration 325 - y: 0.03167203209923627\r",
      "iteration 326 - y: 0.031524408227969425\r",
      "iteration 327 - y: 0.03137678435670259\r",
      "iteration 328 - y: 0.031229160485435743\r",
      "iteration 329 - y: 0.031081536614168903\r",
      "iteration 330 - y: 0.03093391274290206\r",
      "iteration 331 - y: 0.030786288871635224\r",
      "iteration 332 - y: 0.03063866500036838\r",
      "iteration 333 - y: 0.030491041129101542\r",
      "iteration 334 - y: 0.0303434172578347\r",
      "iteration 335 - y: 0.030195793386567856\r",
      "iteration 336 - y: 0.030048169515301017\r",
      "iteration 337 - y: 0.029900545644034174\r",
      "iteration 338 - y: 0.02975292177276733\r",
      "iteration 339 - y: 0.029605297901500495\r",
      "iteration 340 - y: 0.029457674030233652\r",
      "iteration 341 - y: 0.029310050158966813\r",
      "iteration 342 - y: 0.02916242628769997\r",
      "iteration 343 - y: 0.02901480241643313\r",
      "iteration 344 - y: 0.028867178545166287\r",
      "iteration 345 - y: 0.028719554673899444\r",
      "iteration 346 - y: 0.02857193080263261\r",
      "iteration 347 - y: 0.028424306931365766\r",
      "iteration 348 - y: 0.028276683060098923\r",
      "iteration 349 - y: 0.028129059188832083\r",
      "iteration 350 - y: 0.02798143531756524\r",
      "iteration 351 - y: 0.0278338114462984\r",
      "iteration 352 - y: 0.027686187575031558\r",
      "iteration 353 - y: 0.027538563703764722\r",
      "iteration 354 - y: 0.02739093983249788\r",
      "iteration 355 - y: 0.027243315961231036\r",
      "iteration 356 - y: 0.027095692089964197\r",
      "iteration 357 - y: 0.026948068218697354\r",
      "iteration 358 - y: 0.026800444347430515\r",
      "iteration 359 - y: 0.02665282047616367\r",
      "iteration 360 - y: 0.02650519660489683\r",
      "iteration 361 - y: 0.026357572733629993\r",
      "iteration 362 - y: 0.02620994886236315\r",
      "iteration 363 - y: 0.02606232499109631\r",
      "iteration 364 - y: 0.025914701119829464\r",
      "iteration 365 - y: 0.025767077248562628\r",
      "iteration 366 - y: 0.025619453377295785\r",
      "iteration 367 - y: 0.025471829506028946\r",
      "iteration 368 - y: 0.025324205634762106\r",
      "iteration 369 - y: 0.025176581763495263\r",
      "iteration 370 - y: 0.02502895789222842\r",
      "iteration 371 - y: 0.02488133402096158\r",
      "iteration 372 - y: 0.024733710149694742\r",
      "iteration 373 - y: 0.0245860862784279\r",
      "iteration 374 - y: 0.02443846240716106\r",
      "iteration 375 - y: 0.02429083853589422\r",
      "iteration 376 - y: 0.024143214664627377\r",
      "iteration 377 - y: 0.023995590793360534\r",
      "iteration 378 - y: 0.023847966922093698\r",
      "iteration 379 - y: 0.023700343050826855\r",
      "iteration 380 - y: 0.023552719179560012\r",
      "iteration 381 - y: 0.02340509530829317\r",
      "iteration 382 - y: 0.02325747143702633\r",
      "iteration 383 - y: 0.02310984756575949\r",
      "iteration 384 - y: 0.022962223694492648\r",
      "iteration 385 - y: 0.02281459982322581\r",
      "iteration 386 - y: 0.02266697595195897\r",
      "iteration 387 - y: 0.022519352080692126\r",
      "iteration 388 - y: 0.022371728209425287\r",
      "iteration 389 - y: 0.022224104338158444\r",
      "iteration 390 - y: 0.022076480466891604\r",
      "iteration 391 - y: 0.02192885659562476\r",
      "iteration 392 - y: 0.02178123272435792\r",
      "iteration 393 - y: 0.021633608853091083\r",
      "iteration 394 - y: 0.02148598498182424\r",
      "iteration 395 - y: 0.021338361110557397\r",
      "iteration 396 - y: 0.021190737239290554\r",
      "iteration 397 - y: 0.021043113368023714\r",
      "iteration 398 - y: 0.020895489496756875\r",
      "iteration 399 - y: 0.020747865625490032\r",
      "iteration 400 - y: 0.02060024175422319\r",
      "iteration 401 - y: 0.020452617882956346\r",
      "iteration 402 - y: 0.02030499401168951\r",
      "iteration 403 - y: 0.020157370140422667\r",
      "iteration 404 - y: 0.020009746269155824\r",
      "iteration 405 - y: 0.019862122397888985\r",
      "iteration 406 - y: 0.019714498526622142\r",
      "iteration 407 - y: 0.019566874655355303\r",
      "iteration 408 - y: 0.01941925078408846\r",
      "iteration 409 - y: 0.019271626912821617\r",
      "iteration 410 - y: 0.01912400304155478\r",
      "iteration 411 - y: 0.018976379170287935\r",
      "iteration 412 - y: 0.018828755299021095\r",
      "iteration 413 - y: 0.018681131427754252\r",
      "iteration 414 - y: 0.018533507556487413\r",
      "iteration 415 - y: 0.01838588368522057\r",
      "iteration 416 - y: 0.01823825981395373\r",
      "iteration 417 - y: 0.018090635942686888\r",
      "iteration 418 - y: 0.017943012071420048\r",
      "iteration 419 - y: 0.017795388200153205\r",
      "iteration 420 - y: 0.017647764328886366\r",
      "iteration 421 - y: 0.017500140457619523\r",
      "iteration 422 - y: 0.017352516586352684\r",
      "iteration 423 - y: 0.01720489271508584\r",
      "iteration 424 - y: 0.017057268843819\r",
      "iteration 425 - y: 0.01690964497255216\r",
      "iteration 426 - y: 0.016762021101285315\r",
      "iteration 427 - y: 0.01661439723001848\r",
      "iteration 428 - y: 0.016466773358751637\r",
      "iteration 429 - y: 0.016319149487484794\r",
      "iteration 430 - y: 0.01617152561621795\r",
      "iteration 431 - y: 0.01602390174495111\r",
      "iteration 432 - y: 0.01587627787368427\r",
      "iteration 433 - y: 0.01572865400241743\r",
      "iteration 434 - y: 0.015581030131150588\r",
      "iteration 435 - y: 0.015433406259883747\r",
      "iteration 436 - y: 0.015285782388616904\r",
      "iteration 437 - y: 0.015138158517350064\r",
      "iteration 438 - y: 0.014990534646083223\r",
      "iteration 439 - y: 0.014842910774816382\r",
      "iteration 440 - y: 0.01469528690354954\r",
      "iteration 441 - y: 0.0145476630322827\r",
      "iteration 442 - y: 0.014400039161015857\r",
      "iteration 443 - y: 0.014252415289749017\r",
      "iteration 444 - y: 0.014104791418482176\r",
      "iteration 445 - y: 0.013957167547215335\r",
      "iteration 446 - y: 0.013809543675948494\r",
      "iteration 447 - y: 0.013661919804681654\r",
      "iteration 448 - y: 0.013514295933414812\r",
      "iteration 449 - y: 0.013366672062147969\r",
      "iteration 450 - y: 0.013219048190881131\r",
      "iteration 451 - y: 0.013071424319614288\r",
      "iteration 452 - y: 0.012923800448347447\r",
      "iteration 453 - y: 0.012776176577080606\r",
      "iteration 454 - y: 0.012628552705813766\r",
      "iteration 455 - y: 0.012480928834546923\r",
      "iteration 456 - y: 0.012333304963280084\r",
      "iteration 457 - y: 0.012185681092013241\r",
      "iteration 458 - y: 0.0120380572207464\r",
      "iteration 459 - y: 0.011890433349479559\r",
      "iteration 460 - y: 0.011742809478212716\r",
      "iteration 461 - y: 0.011595185606945875\r",
      "iteration 462 - y: 0.011447561735679034\r",
      "iteration 463 - y: 0.01129993786441219\r",
      "iteration 464 - y: 0.01115231399314535\r",
      "iteration 465 - y: 0.011004690121878508\r",
      "iteration 466 - y: 0.010857066250611665\r",
      "iteration 467 - y: 0.010709442379344822\r",
      "iteration 468 - y: 0.010561818508077983\r",
      "iteration 469 - y: 0.01041419463681114\r",
      "iteration 470 - y: 0.010266570765544297\r",
      "iteration 471 - y: 0.010140202995731217\r",
      "iteration 472 - y: 0.010023988891734463\r",
      "iteration 473 - y: 0.00990777478773771\r",
      "iteration 474 - y: 0.009791560683740957\r",
      "iteration 475 - y: 0.009675346579744202\r",
      "iteration 476 - y: 0.00955913247574745\r",
      "iteration 477 - y: 0.009442918371750694\r",
      "iteration 478 - y: 0.009326704267753942\r",
      "iteration 479 - y: 0.009210490163757187\r",
      "iteration 480 - y: 0.009094276059760435\r",
      "iteration 481 - y: 0.00897806195576368\r",
      "iteration 482 - y: 0.008861847851766927\r",
      "iteration 483 - y: 0.008745633747770173\r",
      "iteration 484 - y: 0.00862941964377342\r",
      "iteration 485 - y: 0.008513205539776665\r",
      "iteration 486 - y: 0.008396991435779912\r",
      "iteration 487 - y: 0.008280777331783158\r",
      "iteration 488 - y: 0.008164563227786404\r",
      "iteration 489 - y: 0.00804834912378965\r",
      "iteration 490 - y: 0.007932135019792896\r",
      "iteration 491 - y: 0.007815920915796143\r",
      "iteration 492 - y: 0.007699706811799389\r",
      "iteration 493 - y: 0.007583492707802635\r",
      "iteration 494 - y: 0.007467278603805881\r",
      "iteration 495 - y: 0.007351064499809127\r",
      "iteration 496 - y: 0.0072348503958123735\r",
      "iteration 497 - y: 0.00711863629181562\r",
      "iteration 498 - y: 0.007002422187818866\r",
      "iteration 499 - y: 0.006886208083822112\r",
      "iteration 500 - y: 0.006769993979825359\r",
      "iteration 501 - y: 0.006653779875828606\r",
      "iteration 502 - y: 0.006537565771831852\r",
      "iteration 503 - y: 0.006421351667835099\r",
      "iteration 504 - y: 0.0063051375638383455\r",
      "iteration 505 - y: 0.006188923459841593\r",
      "iteration 506 - y: 0.006072709355844839\r",
      "iteration 507 - y: 0.005956495251848085\r",
      "iteration 508 - y: 0.005840281147851331\r",
      "iteration 509 - y: 0.005724067043854579\r",
      "iteration 510 - y: 0.005607852939857825\r",
      "iteration 511 - y: 0.005491638835861071\r",
      "iteration 512 - y: 0.005375424731864318\r",
      "iteration 513 - y: 0.005259210627867565\r",
      "iteration 514 - y: 0.005142996523870812\r",
      "iteration 515 - y: 0.005026782419874058\r",
      "iteration 516 - y: 0.004910568315877305\r",
      "iteration 517 - y: 0.004794354211880551\r",
      "iteration 518 - y: 0.004678140107883798\r",
      "iteration 519 - y: 0.004561926003887044\r",
      "iteration 520 - y: 0.0044457118998902905\r",
      "iteration 521 - y: 0.004329497795893538\r",
      "iteration 522 - y: 0.004213283691896784\r",
      "iteration 523 - y: 0.004097069587900031\r",
      "iteration 524 - y: 0.003980855483903277\r",
      "iteration 525 - y: 0.003864641379906523\r",
      "iteration 526 - y: 0.0037484272759097693\r",
      "iteration 527 - y: 0.0036322131719130155\r",
      "iteration 528 - y: 0.0035159990679162626\r",
      "iteration 529 - y: 0.0033997849639195088\r",
      "iteration 530 - y: 0.0032835708599227554\r",
      "iteration 531 - y: 0.003167356755926002\r",
      "iteration 532 - y: 0.003051142651929248\r",
      "iteration 533 - y: 0.0029349285479324944\r",
      "iteration 534 - y: 0.0028187144439357405\r",
      "iteration 535 - y: 0.0027025003399389876\r",
      "iteration 536 - y: 0.0025862862359422338\r",
      "iteration 537 - y: 0.0024725627588877137\r",
      "iteration 538 - y: 0.002394271368051475\r",
      "iteration 539 - y: 0.002319655487845367\r",
      "iteration 540 - y: 0.0022644105652074274\r",
      "iteration 541 - y: 0.002213422386544314\r",
      "iteration 542 - y: 0.002187595875757304\r",
      "iteration 543 - y: 0.002161769364970294\r",
      "iteration 544 - y: 0.002135942854183284\r",
      "iteration 545 - y: 0.0021101163433962736\r",
      "iteration 546 - y: 0.0020842898326092637\r",
      "iteration 547 - y: 0.0020584633218222537\r",
      "iteration 548 - y: 0.0020326368110352437\r",
      "iteration 549 - y: 0.0020068103002482333\r",
      "iteration 550 - y: 0.0019809837894612234\r",
      "iteration 551 - y: 0.001955157278674213\r",
      "iteration 552 - y: 0.0019293307678872028\r",
      "iteration 553 - y: 0.0019035042571001924\r",
      "iteration 554 - y: 0.0018776777463131822\r",
      "iteration 555 - y: 0.001851851235526172\r",
      "iteration 556 - y: 0.0018260247247391616\r",
      "iteration 557 - y: 0.0018001982139521514\r",
      "iteration 558 - y: 0.0017743717031651412\r",
      "iteration 559 - y: 0.0017485451923781308\r",
      "iteration 560 - y: 0.0017227186815911206\r",
      "iteration 561 - y: 0.0016968921708041104\r",
      "iteration 562 - y: 0.0016710656600171\r",
      "iteration 563 - y: 0.0016452391492300898\r",
      "iteration 564 - y: 0.0016194126384430796\r",
      "iteration 565 - y: 0.0015935861276560692\r",
      "iteration 566 - y: 0.001567759616869059\r",
      "iteration 567 - y: 0.0015419331060820488\r",
      "iteration 568 - y: 0.0015161065952950384\r",
      "iteration 569 - y: 0.0014902800845080283\r",
      "iteration 570 - y: 0.001464453573721018\r",
      "iteration 571 - y: 0.0014386270629340077\r",
      "iteration 572 - y: 0.0014128005521469975\r",
      "iteration 573 - y: 0.0013869740413599873\r",
      "iteration 574 - y: 0.0013611475305729769\r",
      "iteration 575 - y: 0.0013353210197859667\r",
      "iteration 576 - y: 0.0013094945089989565\r",
      "iteration 577 - y: 0.001283667998211946\r",
      "iteration 578 - y: 0.001257841487424936\r",
      "iteration 579 - y: 0.0012320149766379257\r",
      "iteration 580 - y: 0.0012061884658509153\r",
      "iteration 581 - y: 0.0011803619550639051\r",
      "iteration 582 - y: 0.001154535444276895\r",
      "iteration 583 - y: 0.0011287089334898845\r",
      "iteration 584 - y: 0.0011028824227028743\r",
      "iteration 585 - y: 0.0010770559119158642\r",
      "iteration 586 - y: 0.0010512294011288538\r",
      "iteration 587 - y: 0.0010254028903418436\r",
      "iteration 588 - y: 0.0009995763795548334\r",
      "iteration 589 - y: 0.0009737498687678233\r",
      "iteration 590 - y: 0.0009479233579808131\r",
      "iteration 591 - y: 0.0009220968471938029\r",
      "iteration 592 - y: 0.0008962703364067928\r",
      "iteration 593 - y: 0.0008704438256197827\r",
      "iteration 594 - y: 0.0008446173148327726\r",
      "iteration 595 - y: 0.0008187908040457624\r",
      "iteration 596 - y: 0.0007929642932587522\r",
      "iteration 597 - y: 0.0007671377824717421\r",
      "iteration 598 - y: 0.0007413112716847319\r",
      "iteration 599 - y: 0.0007154847608977217\r",
      "iteration 600 - y: 0.0006896582501107117\r",
      "iteration 601 - y: 0.0006638317393237015\r",
      "iteration 602 - y: 0.0006380052285366914\r",
      "iteration 603 - y: 0.0006121787177496812\r",
      "iteration 604 - y: 0.000586352206962671\r",
      "iteration 605 - y: 0.0005605256961756609\r",
      "iteration 606 - y: 0.0005346991853886507\r",
      "iteration 607 - y: 0.0005088726746016406\r",
      "iteration 608 - y: 0.00048304616381463037\r",
      "iteration 609 - y: 0.0004572196530276202\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 573 - y: 0.00038063091461326114\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-2.0258798e-05]], dtype=float32),\n",
       " array([[-1.9364728e-05]], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ref = 0.0004\n",
    "best_model = K.loadBestModel()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': y_ref}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1753dc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6631437e80>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b3550ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b663118aa70>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = TrainLR(M_samples, H_samples, y_ref = y_ref, fit_intercept = False)\n",
    "regr = L.quickTrain()\n",
    "\n",
    "XL = XLR(regr, M_samples)\n",
    "a_LR, stats_LR = XL.quick_analyze()\n",
    "\n",
    "plt.plot(a_LR[0])\n",
    "plt.plot(a_LR[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa0ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 7.9643e-04 - mae: 0.0229\n",
      "Epoch 1: val_loss improved from inf to 0.00047, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 6.8431e-04 - mae: 0.0215 - val_loss: 4.6773e-04 - val_mae: 0.0184\n",
      "Epoch 2/1000\n",
      "370/800 [============>.................] - ETA: 0s - loss: 4.8740e-04 - mae: 0.0188\n",
      "Epoch 2: val_loss did not improve from 0.00047\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 4.9489e-04 - mae: 0.0189 - val_loss: 4.7989e-04 - val_mae: 0.0183\n",
      "Epoch 3/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 4.8779e-04 - mae: 0.0191\n",
      "Epoch 3: val_loss improved from 0.00047 to 0.00045, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 4.9202e-04 - mae: 0.0190 - val_loss: 4.5271e-04 - val_mae: 0.0181\n",
      "Epoch 4/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 4.6946e-04 - mae: 0.0184\n",
      "Epoch 4: val_loss improved from 0.00045 to 0.00045, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 4.7265e-04 - mae: 0.0185 - val_loss: 4.4529e-04 - val_mae: 0.0181\n",
      "Epoch 5/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 4.5620e-04 - mae: 0.0183\n",
      "Epoch 5: val_loss did not improve from 0.00045\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 4.6410e-04 - mae: 0.0183 - val_loss: 4.4597e-04 - val_mae: 0.0178\n",
      "Epoch 6/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 4.1807e-04 - mae: 0.0175\n",
      "Epoch 6: val_loss improved from 0.00045 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 4.5532e-04 - mae: 0.0181 - val_loss: 4.3908e-04 - val_mae: 0.0179\n",
      "Epoch 7/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 4.6781e-04 - mae: 0.0185\n",
      "Epoch 7: val_loss improved from 0.00044 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 4.3632e-04 - mae: 0.0178 - val_loss: 4.2199e-04 - val_mae: 0.0175\n",
      "Epoch 8/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 4.3551e-04 - mae: 0.0177\n",
      "Epoch 8: val_loss did not improve from 0.00042\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 4.3551e-04 - mae: 0.0177 - val_loss: 4.4699e-04 - val_mae: 0.0175\n",
      "Epoch 9/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 4.2048e-04 - mae: 0.0175\n",
      "Epoch 9: val_loss improved from 0.00042 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 4.1697e-04 - mae: 0.0174 - val_loss: 4.0694e-04 - val_mae: 0.0172\n",
      "Epoch 10/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 4.0484e-04 - mae: 0.0170\n",
      "Epoch 10: val_loss improved from 0.00041 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 4.0293e-04 - mae: 0.0170 - val_loss: 4.0010e-04 - val_mae: 0.0171\n",
      "Epoch 11/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 4.0253e-04 - mae: 0.0172\n",
      "Epoch 11: val_loss improved from 0.00040 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 3.9540e-04 - mae: 0.0170 - val_loss: 3.9615e-04 - val_mae: 0.0167\n",
      "Epoch 12/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 3.8194e-04 - mae: 0.0166\n",
      "Epoch 12: val_loss improved from 0.00040 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 3.7735e-04 - mae: 0.0165 - val_loss: 3.7637e-04 - val_mae: 0.0164\n",
      "Epoch 13/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 3.6400e-04 - mae: 0.0162\n",
      "Epoch 13: val_loss improved from 0.00038 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 3.6465e-04 - mae: 0.0162 - val_loss: 3.6609e-04 - val_mae: 0.0162\n",
      "Epoch 14/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.4555e-04 - mae: 0.0158\n",
      "Epoch 14: val_loss improved from 0.00037 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 3.4338e-04 - mae: 0.0158 - val_loss: 3.5473e-04 - val_mae: 0.0159\n",
      "Epoch 15/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 3.3148e-04 - mae: 0.0153\n",
      "Epoch 15: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 3.3374e-04 - mae: 0.0154 - val_loss: 3.4842e-04 - val_mae: 0.0158\n",
      "Epoch 16/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 3.2571e-04 - mae: 0.0152\n",
      "Epoch 16: val_loss improved from 0.00035 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 3.3567e-04 - mae: 0.0155 - val_loss: 3.3841e-04 - val_mae: 0.0153\n",
      "Epoch 17/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 3.0137e-04 - mae: 0.0147\n",
      "Epoch 17: val_loss did not improve from 0.00034\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 3.0045e-04 - mae: 0.0146 - val_loss: 3.3880e-04 - val_mae: 0.0156\n",
      "Epoch 18/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.8703e-04 - mae: 0.0142\n",
      "Epoch 18: val_loss improved from 0.00034 to 0.00031, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 207us/sample - loss: 2.8695e-04 - mae: 0.0142 - val_loss: 3.1070e-04 - val_mae: 0.0148\n",
      "Epoch 19/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.7093e-04 - mae: 0.0139\n",
      "Epoch 19: val_loss improved from 0.00031 to 0.00030, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 2.6695e-04 - mae: 0.0138 - val_loss: 3.0358e-04 - val_mae: 0.0143\n",
      "Epoch 20/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.4612e-04 - mae: 0.0132\n",
      "Epoch 20: val_loss improved from 0.00030 to 0.00029, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 2.4782e-04 - mae: 0.0132 - val_loss: 2.8768e-04 - val_mae: 0.0140\n",
      "Epoch 21/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.4597e-04 - mae: 0.0129\n",
      "Epoch 21: val_loss improved from 0.00029 to 0.00027, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 213us/sample - loss: 2.3635e-04 - mae: 0.0127 - val_loss: 2.7392e-04 - val_mae: 0.0137\n",
      "Epoch 22/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0737e-04 - mae: 0.0120\n",
      "Epoch 22: val_loss improved from 0.00027 to 0.00027, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 2.1329e-04 - mae: 0.0122 - val_loss: 2.7212e-04 - val_mae: 0.0139\n",
      "Epoch 23/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.0845e-04 - mae: 0.0119\n",
      "Epoch 23: val_loss improved from 0.00027 to 0.00026, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 2.0530e-04 - mae: 0.0118 - val_loss: 2.5912e-04 - val_mae: 0.0135\n",
      "Epoch 24/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 2.0132e-04 - mae: 0.0117\n",
      "Epoch 24: val_loss improved from 0.00026 to 0.00024, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 220us/sample - loss: 1.9319e-04 - mae: 0.0114 - val_loss: 2.4168e-04 - val_mae: 0.0126\n",
      "Epoch 25/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7835e-04 - mae: 0.0111\n",
      "Epoch 25: val_loss improved from 0.00024 to 0.00023, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 1.7741e-04 - mae: 0.0110 - val_loss: 2.3430e-04 - val_mae: 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.6183e-04 - mae: 0.0105\n",
      "Epoch 26: val_loss improved from 0.00023 to 0.00022, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 210us/sample - loss: 1.6438e-04 - mae: 0.0106 - val_loss: 2.1825e-04 - val_mae: 0.0121\n",
      "Epoch 27/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4690e-04 - mae: 0.0099\n",
      "Epoch 27: val_loss improved from 0.00022 to 0.00021, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.5170e-04 - mae: 0.0101 - val_loss: 2.1107e-04 - val_mae: 0.0119\n",
      "Epoch 28/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.3639e-04 - mae: 0.0095\n",
      "Epoch 28: val_loss improved from 0.00021 to 0.00021, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 200us/sample - loss: 1.3869e-04 - mae: 0.0096 - val_loss: 2.0806e-04 - val_mae: 0.0115\n",
      "Epoch 29/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.3109e-04 - mae: 0.0093\n",
      "Epoch 29: val_loss improved from 0.00021 to 0.00019, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 1.3082e-04 - mae: 0.0093 - val_loss: 1.9403e-04 - val_mae: 0.0114\n",
      "Epoch 30/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.2735e-04 - mae: 0.0092\n",
      "Epoch 30: val_loss improved from 0.00019 to 0.00019, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 1.2634e-04 - mae: 0.0091 - val_loss: 1.8967e-04 - val_mae: 0.0113\n",
      "Epoch 31/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.0944e-04 - mae: 0.0085\n",
      "Epoch 31: val_loss improved from 0.00019 to 0.00018, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 196us/sample - loss: 1.1005e-04 - mae: 0.0085 - val_loss: 1.8483e-04 - val_mae: 0.0111\n",
      "Epoch 32/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.0163e-04 - mae: 0.0081\n",
      "Epoch 32: val_loss did not improve from 0.00018\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.0463e-04 - mae: 0.0082 - val_loss: 1.9690e-04 - val_mae: 0.0115\n",
      "Epoch 33/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.0001e-04 - mae: 0.0081\n",
      "Epoch 33: val_loss improved from 0.00018 to 0.00018, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 9.9156e-05 - mae: 0.0081 - val_loss: 1.7645e-04 - val_mae: 0.0105\n",
      "Epoch 34/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 9.7260e-05 - mae: 0.0080\n",
      "Epoch 34: val_loss improved from 0.00018 to 0.00015, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 9.6436e-05 - mae: 0.0079 - val_loss: 1.5347e-04 - val_mae: 0.0099\n",
      "Epoch 35/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 8.8338e-05 - mae: 0.0075\n",
      "Epoch 35: val_loss improved from 0.00015 to 0.00014, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 8.8840e-05 - mae: 0.0076 - val_loss: 1.4477e-04 - val_mae: 0.0097\n",
      "Epoch 36/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 8.4666e-05 - mae: 0.0074\n",
      "Epoch 36: val_loss improved from 0.00014 to 0.00014, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 230us/sample - loss: 8.4703e-05 - mae: 0.0074 - val_loss: 1.4289e-04 - val_mae: 0.0095\n",
      "Epoch 37/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 7.8050e-05 - mae: 0.0072\n",
      "Epoch 37: val_loss improved from 0.00014 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 215us/sample - loss: 7.6280e-05 - mae: 0.0071 - val_loss: 1.3374e-04 - val_mae: 0.0093\n",
      "Epoch 38/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 7.2423e-05 - mae: 0.0069\n",
      "Epoch 38: val_loss improved from 0.00013 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 232us/sample - loss: 7.1739e-05 - mae: 0.0068 - val_loss: 1.2979e-04 - val_mae: 0.0092\n",
      "Epoch 39/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 7.2681e-05 - mae: 0.0068\n",
      "Epoch 39: val_loss improved from 0.00013 to 0.00012, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 7.1367e-05 - mae: 0.0067 - val_loss: 1.2266e-04 - val_mae: 0.0088\n",
      "Epoch 40/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 6.4197e-05 - mae: 0.0064\n",
      "Epoch 40: val_loss improved from 0.00012 to 0.00012, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 206us/sample - loss: 6.4399e-05 - mae: 0.0064 - val_loss: 1.1700e-04 - val_mae: 0.0087\n",
      "Epoch 41/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 5.8692e-05 - mae: 0.0062\n",
      "Epoch 41: val_loss improved from 0.00012 to 0.00012, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 5.9152e-05 - mae: 0.0062 - val_loss: 1.1662e-04 - val_mae: 0.0086\n",
      "Epoch 42/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 5.7886e-05 - mae: 0.0062\n",
      "Epoch 42: val_loss improved from 0.00012 to 0.00011, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 5.7865e-05 - mae: 0.0061 - val_loss: 1.0750e-04 - val_mae: 0.0083\n",
      "Epoch 43/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 5.2736e-05 - mae: 0.0058\n",
      "Epoch 43: val_loss did not improve from 0.00011\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 5.4164e-05 - mae: 0.0059 - val_loss: 1.1001e-04 - val_mae: 0.0084\n",
      "Epoch 44/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 5.4837e-05 - mae: 0.0059\n",
      "Epoch 44: val_loss improved from 0.00011 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 5.5155e-05 - mae: 0.0059 - val_loss: 1.0315e-04 - val_mae: 0.0082\n",
      "Epoch 45/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 4.7561e-05 - mae: 0.0055\n",
      "Epoch 45: val_loss improved from 0.00010 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 4.8021e-05 - mae: 0.0056 - val_loss: 9.5853e-05 - val_mae: 0.0078\n",
      "Epoch 46/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 4.4116e-05 - mae: 0.0053\n",
      "Epoch 46: val_loss improved from 0.00010 to 0.00009, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 4.5512e-05 - mae: 0.0054 - val_loss: 9.4664e-05 - val_mae: 0.0078\n",
      "Epoch 47/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 4.5738e-05 - mae: 0.0054\n",
      "Epoch 47: val_loss improved from 0.00009 to 0.00009, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 4.4948e-05 - mae: 0.0054 - val_loss: 8.9189e-05 - val_mae: 0.0076\n",
      "Epoch 48/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 4.0433e-05 - mae: 0.0051\n",
      "Epoch 48: val_loss did not improve from 0.00009\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 4.1440e-05 - mae: 0.0052 - val_loss: 9.1045e-05 - val_mae: 0.0077\n",
      "Epoch 49/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 4.3011e-05 - mae: 0.0053\n",
      "Epoch 49: val_loss improved from 0.00009 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 4.2761e-05 - mae: 0.0053 - val_loss: 8.3081e-05 - val_mae: 0.0073\n",
      "Epoch 50/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 3.7690e-05 - mae: 0.0048\n",
      "Epoch 50: val_loss improved from 0.00008 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 201us/sample - loss: 3.8069e-05 - mae: 0.0049 - val_loss: 8.1811e-05 - val_mae: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 3.5825e-05 - mae: 0.0048\n",
      "Epoch 51: val_loss did not improve from 0.00008\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 3.6786e-05 - mae: 0.0049 - val_loss: 8.6632e-05 - val_mae: 0.0076\n",
      "Epoch 52/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 3.4710e-05 - mae: 0.0048\n",
      "Epoch 52: val_loss did not improve from 0.00008\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 3.6777e-05 - mae: 0.0049 - val_loss: 8.5273e-05 - val_mae: 0.0075\n",
      "Epoch 53/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 4.2013e-05 - mae: 0.0052\n",
      "Epoch 53: val_loss improved from 0.00008 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 4.1430e-05 - mae: 0.0052 - val_loss: 7.6695e-05 - val_mae: 0.0071\n",
      "Epoch 54/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 3.2160e-05 - mae: 0.0045\n",
      "Epoch 54: val_loss improved from 0.00008 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 3.1616e-05 - mae: 0.0045 - val_loss: 7.1222e-05 - val_mae: 0.0068\n",
      "Epoch 55/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 3.5396e-05 - mae: 0.0047\n",
      "Epoch 55: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 3.4445e-05 - mae: 0.0047 - val_loss: 6.9736e-05 - val_mae: 0.0068\n",
      "Epoch 56/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 3.1649e-05 - mae: 0.0045\n",
      "Epoch 56: val_loss did not improve from 0.00007\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 3.2165e-05 - mae: 0.0045 - val_loss: 7.5143e-05 - val_mae: 0.0071\n",
      "Epoch 57/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 3.5336e-05 - mae: 0.0048\n",
      "Epoch 57: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 200us/sample - loss: 3.5464e-05 - mae: 0.0048 - val_loss: 6.8044e-05 - val_mae: 0.0068\n",
      "Epoch 58/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.9063e-05 - mae: 0.0043\n",
      "Epoch 58: val_loss improved from 0.00007 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 196us/sample - loss: 2.9445e-05 - mae: 0.0043 - val_loss: 6.3713e-05 - val_mae: 0.0065\n",
      "Epoch 59/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.7224e-05 - mae: 0.0041\n",
      "Epoch 59: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.7892e-05 - mae: 0.0042 - val_loss: 7.3611e-05 - val_mae: 0.0071\n",
      "Epoch 60/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.9429e-05 - mae: 0.0043\n",
      "Epoch 60: val_loss improved from 0.00006 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 3.0559e-05 - mae: 0.0044 - val_loss: 5.9907e-05 - val_mae: 0.0063\n",
      "Epoch 61/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.7487e-05 - mae: 0.0042\n",
      "Epoch 61: val_loss improved from 0.00006 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 2.7624e-05 - mae: 0.0042 - val_loss: 5.9901e-05 - val_mae: 0.0063\n",
      "Epoch 62/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.5603e-05 - mae: 0.0040\n",
      "Epoch 62: val_loss improved from 0.00006 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 2.5513e-05 - mae: 0.0040 - val_loss: 5.7300e-05 - val_mae: 0.0062\n",
      "Epoch 63/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.6082e-05 - mae: 0.0040\n",
      "Epoch 63: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.6057e-05 - mae: 0.0040 - val_loss: 5.8115e-05 - val_mae: 0.0062\n",
      "Epoch 64/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.7231e-05 - mae: 0.0041\n",
      "Epoch 64: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.6976e-05 - mae: 0.0041 - val_loss: 6.3235e-05 - val_mae: 0.0065\n",
      "Epoch 65/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.5917e-05 - mae: 0.0040\n",
      "Epoch 65: val_loss improved from 0.00006 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 2.6773e-05 - mae: 0.0041 - val_loss: 5.5209e-05 - val_mae: 0.0061\n",
      "Epoch 66/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.4988e-05 - mae: 0.0040\n",
      "Epoch 66: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.4953e-05 - mae: 0.0040 - val_loss: 7.0624e-05 - val_mae: 0.0069\n",
      "Epoch 67/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2283e-05 - mae: 0.0037\n",
      "Epoch 67: val_loss improved from 0.00006 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 2.4105e-05 - mae: 0.0039 - val_loss: 5.1885e-05 - val_mae: 0.0058\n",
      "Epoch 68/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.3047e-05 - mae: 0.0038\n",
      "Epoch 68: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.3625e-05 - mae: 0.0039 - val_loss: 5.3289e-05 - val_mae: 0.0060\n",
      "Epoch 69/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9172e-05 - mae: 0.0034\n",
      "Epoch 69: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.2498e-05 - mae: 0.0037 - val_loss: 6.0207e-05 - val_mae: 0.0064\n",
      "Epoch 70/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2299e-05 - mae: 0.0038\n",
      "Epoch 70: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.3331e-05 - mae: 0.0039 - val_loss: 4.9052e-05 - val_mae: 0.0057\n",
      "Epoch 71/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.2401e-05 - mae: 0.0037\n",
      "Epoch 71: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 2.1984e-05 - mae: 0.0037 - val_loss: 4.8222e-05 - val_mae: 0.0056\n",
      "Epoch 72/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.2258e-05 - mae: 0.0038\n",
      "Epoch 72: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.1852e-05 - mae: 0.0037 - val_loss: 5.8130e-05 - val_mae: 0.0063\n",
      "Epoch 73/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3089e-05 - mae: 0.0038\n",
      "Epoch 73: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.3125e-05 - mae: 0.0038 - val_loss: 4.8926e-05 - val_mae: 0.0056\n",
      "Epoch 74/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0850e-05 - mae: 0.0036\n",
      "Epoch 74: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0915e-05 - mae: 0.0036 - val_loss: 4.8683e-05 - val_mae: 0.0057\n",
      "Epoch 75/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9043e-05 - mae: 0.0035\n",
      "Epoch 75: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.1262e-05 - mae: 0.0037 - val_loss: 4.5610e-05 - val_mae: 0.0055\n",
      "Epoch 76/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.0788e-05 - mae: 0.0036\n",
      "Epoch 76: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0762e-05 - mae: 0.0036 - val_loss: 5.1952e-05 - val_mae: 0.0059\n",
      "Epoch 77/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0918e-05 - mae: 0.0036\n",
      "Epoch 77: val_loss improved from 0.00005 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 200us/sample - loss: 2.1485e-05 - mae: 0.0037 - val_loss: 4.4695e-05 - val_mae: 0.0054\n",
      "Epoch 78/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0197e-05 - mae: 0.0036\n",
      "Epoch 78: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 2.0282e-05 - mae: 0.0036 - val_loss: 4.4572e-05 - val_mae: 0.0054\n",
      "Epoch 79/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.2713e-05 - mae: 0.0038\n",
      "Epoch 79: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 2.2611e-05 - mae: 0.0037 - val_loss: 4.3703e-05 - val_mae: 0.0054\n",
      "Epoch 80/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0773e-05 - mae: 0.0037\n",
      "Epoch 80: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 2.0752e-05 - mae: 0.0037 - val_loss: 4.3669e-05 - val_mae: 0.0054\n",
      "Epoch 81/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.1973e-05 - mae: 0.0037\n",
      "Epoch 81: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 2.1994e-05 - mae: 0.0038 - val_loss: 4.2859e-05 - val_mae: 0.0053\n",
      "Epoch 82/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9675e-05 - mae: 0.0035\n",
      "Epoch 82: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 198us/sample - loss: 2.0174e-05 - mae: 0.0035 - val_loss: 4.1954e-05 - val_mae: 0.0053\n",
      "Epoch 83/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.0781e-05 - mae: 0.0036\n",
      "Epoch 83: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.0818e-05 - mae: 0.0037 - val_loss: 4.3542e-05 - val_mae: 0.0054\n",
      "Epoch 84/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.1496e-05 - mae: 0.0036\n",
      "Epoch 84: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.1415e-05 - mae: 0.0036 - val_loss: 4.2737e-05 - val_mae: 0.0053\n",
      "Epoch 85/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9026e-05 - mae: 0.0034\n",
      "Epoch 85: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9169e-05 - mae: 0.0035 - val_loss: 4.4031e-05 - val_mae: 0.0054\n",
      "Epoch 86/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0026e-05 - mae: 0.0036\n",
      "Epoch 86: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9571e-05 - mae: 0.0035 - val_loss: 4.6356e-05 - val_mae: 0.0056\n",
      "Epoch 87/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1606e-05 - mae: 0.0037\n",
      "Epoch 87: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 2.1274e-05 - mae: 0.0037 - val_loss: 4.0889e-05 - val_mae: 0.0052\n",
      "Epoch 88/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0699e-05 - mae: 0.0036\n",
      "Epoch 88: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 2.1606e-05 - mae: 0.0037 - val_loss: 4.0447e-05 - val_mae: 0.0052\n",
      "Epoch 89/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0719e-05 - mae: 0.0036\n",
      "Epoch 89: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 2.0401e-05 - mae: 0.0036 - val_loss: 4.0309e-05 - val_mae: 0.0052\n",
      "Epoch 90/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9013e-05 - mae: 0.0035\n",
      "Epoch 90: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9394e-05 - mae: 0.0035 - val_loss: 4.4415e-05 - val_mae: 0.0055\n",
      "Epoch 91/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9493e-05 - mae: 0.0035\n",
      "Epoch 91: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0201e-05 - mae: 0.0035 - val_loss: 4.2676e-05 - val_mae: 0.0054\n",
      "Epoch 92/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6844e-05 - mae: 0.0032\n",
      "Epoch 92: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8518e-05 - mae: 0.0034 - val_loss: 4.6032e-05 - val_mae: 0.0056\n",
      "Epoch 93/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8724e-05 - mae: 0.0035\n",
      "Epoch 93: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0018e-05 - mae: 0.0036 - val_loss: 4.5047e-05 - val_mae: 0.0055\n",
      "Epoch 94/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1356e-05 - mae: 0.0036\n",
      "Epoch 94: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0025e-05 - mae: 0.0035 - val_loss: 4.3146e-05 - val_mae: 0.0054\n",
      "Epoch 95/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9277e-05 - mae: 0.0035\n",
      "Epoch 95: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 1.9414e-05 - mae: 0.0035 - val_loss: 3.9173e-05 - val_mae: 0.0051\n",
      "Epoch 96/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.2300e-05 - mae: 0.0037\n",
      "Epoch 96: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.1984e-05 - mae: 0.0037 - val_loss: 3.9520e-05 - val_mae: 0.0052\n",
      "Epoch 97/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7337e-05 - mae: 0.0033\n",
      "Epoch 97: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0175e-05 - mae: 0.0036 - val_loss: 4.1997e-05 - val_mae: 0.0053\n",
      "Epoch 98/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1519e-05 - mae: 0.0036\n",
      "Epoch 98: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0016e-05 - mae: 0.0035 - val_loss: 4.2845e-05 - val_mae: 0.0054\n",
      "Epoch 99/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1257e-05 - mae: 0.0037\n",
      "Epoch 99: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 2.2176e-05 - mae: 0.0037 - val_loss: 3.8422e-05 - val_mae: 0.0051\n",
      "Epoch 100/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9049e-05 - mae: 0.0035\n",
      "Epoch 100: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 1.8638e-05 - mae: 0.0034 - val_loss: 3.8101e-05 - val_mae: 0.0051\n",
      "Epoch 101/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.1535e-05 - mae: 0.0037\n",
      "Epoch 101: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.2019e-05 - mae: 0.0037 - val_loss: 3.9879e-05 - val_mae: 0.0052\n",
      "Epoch 102/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7359e-05 - mae: 0.0033\n",
      "Epoch 102: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9035e-05 - mae: 0.0035 - val_loss: 3.8399e-05 - val_mae: 0.0051\n",
      "Epoch 103/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9540e-05 - mae: 0.0035\n",
      "Epoch 103: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9282e-05 - mae: 0.0035 - val_loss: 3.8000e-05 - val_mae: 0.0051\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9194e-05 - mae: 0.0035\n",
      "Epoch 104: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.9245e-05 - mae: 0.0035 - val_loss: 4.7233e-05 - val_mae: 0.0055\n",
      "Epoch 105/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.1788e-05 - mae: 0.0037\n",
      "Epoch 105: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 2.1326e-05 - mae: 0.0037 - val_loss: 3.7686e-05 - val_mae: 0.0050\n",
      "Epoch 106/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9321e-05 - mae: 0.0035\n",
      "Epoch 106: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9312e-05 - mae: 0.0035 - val_loss: 4.0002e-05 - val_mae: 0.0052\n",
      "Epoch 107/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9464e-05 - mae: 0.0034\n",
      "Epoch 107: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9464e-05 - mae: 0.0034 - val_loss: 3.8103e-05 - val_mae: 0.0051\n",
      "Epoch 108/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.0944e-05 - mae: 0.0036\n",
      "Epoch 108: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.1657e-05 - mae: 0.0037 - val_loss: 4.6997e-05 - val_mae: 0.0055\n",
      "Epoch 109/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1315e-05 - mae: 0.0036\n",
      "Epoch 109: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.1315e-05 - mae: 0.0036 - val_loss: 4.5689e-05 - val_mae: 0.0055\n",
      "Epoch 110/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9623e-05 - mae: 0.0035\n",
      "Epoch 110: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9696e-05 - mae: 0.0035 - val_loss: 3.9059e-05 - val_mae: 0.0051\n",
      "Epoch 111/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.1003e-05 - mae: 0.0036\n",
      "Epoch 111: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.1357e-05 - mae: 0.0037 - val_loss: 4.0599e-05 - val_mae: 0.0052\n",
      "Epoch 112/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8584e-05 - mae: 0.0035\n",
      "Epoch 112: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.2069e-05 - mae: 0.0037 - val_loss: 4.7497e-05 - val_mae: 0.0056\n",
      "Epoch 113/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9581e-05 - mae: 0.0035\n",
      "Epoch 113: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9780e-05 - mae: 0.0035 - val_loss: 3.8181e-05 - val_mae: 0.0051\n",
      "Epoch 114/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9157e-05 - mae: 0.0035\n",
      "Epoch 114: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8978e-05 - mae: 0.0035 - val_loss: 4.1085e-05 - val_mae: 0.0053\n",
      "Epoch 115/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9879e-05 - mae: 0.0036\n",
      "Epoch 115: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 2.0544e-05 - mae: 0.0036 - val_loss: 3.8712e-05 - val_mae: 0.0051\n",
      "Epoch 116/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8714e-05 - mae: 0.0034\n",
      "Epoch 116: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8572e-05 - mae: 0.0034 - val_loss: 4.1805e-05 - val_mae: 0.0053\n",
      "Epoch 117/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1026e-05 - mae: 0.0037\n",
      "Epoch 117: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9762e-05 - mae: 0.0036 - val_loss: 3.8297e-05 - val_mae: 0.0051\n",
      "Epoch 118/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7409e-05 - mae: 0.0033\n",
      "Epoch 118: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7840e-05 - mae: 0.0034 - val_loss: 4.0812e-05 - val_mae: 0.0052\n",
      "Epoch 119/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.2438e-05 - mae: 0.0038\n",
      "Epoch 119: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.2321e-05 - mae: 0.0038 - val_loss: 3.9857e-05 - val_mae: 0.0052\n",
      "Epoch 120/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7971e-05 - mae: 0.0034\n",
      "Epoch 120: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7971e-05 - mae: 0.0034 - val_loss: 3.8249e-05 - val_mae: 0.0051\n",
      "Epoch 121/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9862e-05 - mae: 0.0035\n",
      "Epoch 121: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0788e-05 - mae: 0.0037 - val_loss: 3.9277e-05 - val_mae: 0.0051\n",
      "Epoch 122/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0363e-05 - mae: 0.0036\n",
      "Epoch 122: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0558e-05 - mae: 0.0036 - val_loss: 4.0569e-05 - val_mae: 0.0052\n",
      "Epoch 123/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7470e-05 - mae: 0.0033\n",
      "Epoch 123: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8724e-05 - mae: 0.0034 - val_loss: 4.0391e-05 - val_mae: 0.0052\n",
      "Epoch 124/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0069e-05 - mae: 0.0035\n",
      "Epoch 124: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9946e-05 - mae: 0.0035 - val_loss: 3.7976e-05 - val_mae: 0.0051\n",
      "Epoch 125/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8293e-05 - mae: 0.0034\n",
      "Epoch 125: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9804e-05 - mae: 0.0036 - val_loss: 3.8522e-05 - val_mae: 0.0051\n",
      "Epoch 126/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8098e-05 - mae: 0.0034\n",
      "Epoch 126: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8822e-05 - mae: 0.0035 - val_loss: 3.9684e-05 - val_mae: 0.0051\n",
      "Epoch 127/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7156e-05 - mae: 0.0033\n",
      "Epoch 127: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8286e-05 - mae: 0.0034 - val_loss: 4.1129e-05 - val_mae: 0.0053\n",
      "Epoch 128/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9353e-05 - mae: 0.0035\n",
      "Epoch 128: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0645e-05 - mae: 0.0036 - val_loss: 3.9598e-05 - val_mae: 0.0052\n",
      "Epoch 129/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9398e-05 - mae: 0.0035\n",
      "Epoch 129: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8335e-05 - mae: 0.0034 - val_loss: 3.8198e-05 - val_mae: 0.0051\n",
      "Epoch 130/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2474e-05 - mae: 0.0037\n",
      "Epoch 130: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0848e-05 - mae: 0.0036 - val_loss: 3.8568e-05 - val_mae: 0.0051\n",
      "Epoch 131/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.5773e-05 - mae: 0.0041\n",
      "Epoch 131: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 2.3356e-05 - mae: 0.0039 - val_loss: 3.9989e-05 - val_mae: 0.0051\n",
      "Epoch 132/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.9273e-05 - mae: 0.0035\n",
      "Epoch 132: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.9451e-05 - mae: 0.0035 - val_loss: 3.7800e-05 - val_mae: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.9078e-05 - mae: 0.0035\n",
      "Epoch 133: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.9552e-05 - mae: 0.0035 - val_loss: 3.9218e-05 - val_mae: 0.0052\n",
      "Epoch 134/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8796e-05 - mae: 0.0035\n",
      "Epoch 134: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 2.0558e-05 - mae: 0.0036 - val_loss: 5.3311e-05 - val_mae: 0.0059\n",
      "Epoch 135/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 2.0594e-05 - mae: 0.0036\n",
      "Epoch 135: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 2.0078e-05 - mae: 0.0036 - val_loss: 3.8389e-05 - val_mae: 0.0051\n",
      "Epoch 136/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.8525e-05 - mae: 0.0034\n",
      "Epoch 136: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.8705e-05 - mae: 0.0034 - val_loss: 4.7139e-05 - val_mae: 0.0056\n",
      "Epoch 137/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8443e-05 - mae: 0.0034\n",
      "Epoch 137: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8335e-05 - mae: 0.0034 - val_loss: 3.7737e-05 - val_mae: 0.0050\n",
      "Epoch 138/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9057e-05 - mae: 0.0035\n",
      "Epoch 138: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9196e-05 - mae: 0.0035 - val_loss: 3.8047e-05 - val_mae: 0.0051\n",
      "Epoch 139/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.0108e-05 - mae: 0.0035\n",
      "Epoch 139: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.1096e-05 - mae: 0.0036 - val_loss: 4.5825e-05 - val_mae: 0.0055\n",
      "Epoch 140/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.4177e-05 - mae: 0.0039\n",
      "Epoch 140: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.3651e-05 - mae: 0.0039 - val_loss: 3.8165e-05 - val_mae: 0.0051\n",
      "Epoch 141/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0430e-05 - mae: 0.0036\n",
      "Epoch 141: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0499e-05 - mae: 0.0036 - val_loss: 3.9191e-05 - val_mae: 0.0051\n",
      "Epoch 142/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7888e-05 - mae: 0.0034\n",
      "Epoch 142: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8375e-05 - mae: 0.0034 - val_loss: 3.8313e-05 - val_mae: 0.0051\n",
      "Epoch 143/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8244e-05 - mae: 0.0034\n",
      "Epoch 143: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8594e-05 - mae: 0.0034 - val_loss: 3.9083e-05 - val_mae: 0.0051\n",
      "Epoch 144/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9313e-05 - mae: 0.0035\n",
      "Epoch 144: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9331e-05 - mae: 0.0035 - val_loss: 3.8066e-05 - val_mae: 0.0051\n",
      "Epoch 145/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8540e-05 - mae: 0.0034\n",
      "Epoch 145: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.0313e-05 - mae: 0.0035 - val_loss: 3.7626e-05 - val_mae: 0.0050\n",
      "Epoch 146/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9011e-05 - mae: 0.0035\n",
      "Epoch 146: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 525us/sample - loss: 1.8920e-05 - mae: 0.0035 - val_loss: 3.7420e-05 - val_mae: 0.0050\n",
      "Epoch 147/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.8447e-05 - mae: 0.0034\n",
      "Epoch 147: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.9803e-05 - mae: 0.0035 - val_loss: 4.2730e-05 - val_mae: 0.0053\n",
      "Epoch 148/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7946e-05 - mae: 0.0034\n",
      "Epoch 148: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8495e-05 - mae: 0.0034 - val_loss: 4.2952e-05 - val_mae: 0.0053\n",
      "Epoch 149/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8543e-05 - mae: 0.0034\n",
      "Epoch 149: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8516e-05 - mae: 0.0034 - val_loss: 3.8677e-05 - val_mae: 0.0051\n",
      "Epoch 150/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8404e-05 - mae: 0.0034\n",
      "Epoch 150: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9535e-05 - mae: 0.0035 - val_loss: 3.7433e-05 - val_mae: 0.0050\n",
      "Epoch 151/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9114e-05 - mae: 0.0034\n",
      "Epoch 151: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9060e-05 - mae: 0.0034 - val_loss: 3.7456e-05 - val_mae: 0.0050\n",
      "Epoch 152/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1077e-05 - mae: 0.0036\n",
      "Epoch 152: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.1086e-05 - mae: 0.0036 - val_loss: 3.7685e-05 - val_mae: 0.0050\n",
      "Epoch 153/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7825e-05 - mae: 0.0033\n",
      "Epoch 153: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7994e-05 - mae: 0.0034 - val_loss: 3.8465e-05 - val_mae: 0.0051\n",
      "Epoch 154/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2418e-05 - mae: 0.0037\n",
      "Epoch 154: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.1497e-05 - mae: 0.0036 - val_loss: 4.0477e-05 - val_mae: 0.0052\n",
      "Epoch 155/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8545e-05 - mae: 0.0034\n",
      "Epoch 155: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8450e-05 - mae: 0.0034 - val_loss: 3.8117e-05 - val_mae: 0.0051\n",
      "Epoch 156/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7869e-05 - mae: 0.0034\n",
      "Epoch 156: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8998e-05 - mae: 0.0035 - val_loss: 4.1707e-05 - val_mae: 0.0052\n",
      "Epoch 157/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.9929e-05 - mae: 0.0044\n",
      "Epoch 157: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.4035e-05 - mae: 0.0039 - val_loss: 3.9450e-05 - val_mae: 0.0052\n",
      "Epoch 158/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7572e-05 - mae: 0.0033\n",
      "Epoch 158: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8085e-05 - mae: 0.0034 - val_loss: 3.9456e-05 - val_mae: 0.0052\n",
      "Epoch 159/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8741e-05 - mae: 0.0034\n",
      "Epoch 159: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8741e-05 - mae: 0.0034 - val_loss: 3.8613e-05 - val_mae: 0.0051\n",
      "Epoch 160/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8100e-05 - mae: 0.0034\n",
      "Epoch 160: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8215e-05 - mae: 0.0034 - val_loss: 3.7788e-05 - val_mae: 0.0050\n",
      "Epoch 161/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8465e-05 - mae: 0.0034\n",
      "Epoch 161: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8465e-05 - mae: 0.0034 - val_loss: 3.7993e-05 - val_mae: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.2105e-05 - mae: 0.0038\n",
      "Epoch 162: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.1800e-05 - mae: 0.0037 - val_loss: 3.7609e-05 - val_mae: 0.0050\n",
      "Epoch 163/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8532e-05 - mae: 0.0034\n",
      "Epoch 163: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8582e-05 - mae: 0.0034 - val_loss: 3.7736e-05 - val_mae: 0.0050\n",
      "Epoch 164/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9170e-05 - mae: 0.0035\n",
      "Epoch 164: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9393e-05 - mae: 0.0035 - val_loss: 5.3838e-05 - val_mae: 0.0058\n",
      "Epoch 165/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9218e-05 - mae: 0.0034\n",
      "Epoch 165: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1364e-05 - mae: 0.0036 - val_loss: 4.1814e-05 - val_mae: 0.0053\n",
      "Epoch 166/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9357e-05 - mae: 0.0035\n",
      "Epoch 166: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9929e-05 - mae: 0.0036 - val_loss: 3.9929e-05 - val_mae: 0.0051\n",
      "Epoch 167/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8902e-05 - mae: 0.0034\n",
      "Epoch 167: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 2.1081e-05 - mae: 0.0036 - val_loss: 3.7663e-05 - val_mae: 0.0051\n",
      "Epoch 168/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.2845e-05 - mae: 0.0039\n",
      "Epoch 168: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.2877e-05 - mae: 0.0039 - val_loss: 4.3778e-05 - val_mae: 0.0053\n",
      "Epoch 169/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8943e-05 - mae: 0.0035\n",
      "Epoch 169: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8799e-05 - mae: 0.0034 - val_loss: 4.5190e-05 - val_mae: 0.0055\n",
      "Epoch 170/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8506e-05 - mae: 0.0034\n",
      "Epoch 170: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8509e-05 - mae: 0.0034 - val_loss: 3.8712e-05 - val_mae: 0.0051\n",
      "Epoch 171/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7941e-05 - mae: 0.0034\n",
      "Epoch 171: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.8136e-05 - mae: 0.0034 - val_loss: 3.7652e-05 - val_mae: 0.0050\n",
      "Epoch 172/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8495e-05 - mae: 0.0034\n",
      "Epoch 172: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8838e-05 - mae: 0.0035 - val_loss: 5.9168e-05 - val_mae: 0.0062\n",
      "Epoch 173/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0997e-05 - mae: 0.0036\n",
      "Epoch 173: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.0995e-05 - mae: 0.0037 - val_loss: 3.7546e-05 - val_mae: 0.0050\n",
      "Epoch 174/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.2110e-05 - mae: 0.0038\n",
      "Epoch 174: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.2317e-05 - mae: 0.0038 - val_loss: 3.8011e-05 - val_mae: 0.0051\n",
      "Epoch 175/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7742e-05 - mae: 0.0034\n",
      "Epoch 175: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8707e-05 - mae: 0.0034 - val_loss: 4.1532e-05 - val_mae: 0.0053\n",
      "Epoch 176/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9761e-05 - mae: 0.0035\n",
      "Epoch 176: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0271e-05 - mae: 0.0035 - val_loss: 3.8144e-05 - val_mae: 0.0051\n",
      "Epoch 177/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9081e-05 - mae: 0.0035\n",
      "Epoch 177: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9465e-05 - mae: 0.0035 - val_loss: 4.7030e-05 - val_mae: 0.0056\n",
      "Epoch 178/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7940e-05 - mae: 0.0034\n",
      "Epoch 178: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8299e-05 - mae: 0.0034 - val_loss: 4.1347e-05 - val_mae: 0.0053\n",
      "Epoch 179/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8381e-05 - mae: 0.0033\n",
      "Epoch 179: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9037e-05 - mae: 0.0034 - val_loss: 4.8476e-05 - val_mae: 0.0056\n",
      "Epoch 180/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.1455e-05 - mae: 0.0036\n",
      "Epoch 180: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.1142e-05 - mae: 0.0036 - val_loss: 3.9772e-05 - val_mae: 0.0052\n",
      "Epoch 181/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8697e-05 - mae: 0.0034\n",
      "Epoch 181: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8912e-05 - mae: 0.0035 - val_loss: 3.8918e-05 - val_mae: 0.0052\n",
      "Epoch 182/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9638e-05 - mae: 0.0035\n",
      "Epoch 182: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9262e-05 - mae: 0.0035 - val_loss: 3.8130e-05 - val_mae: 0.0051\n",
      "Epoch 183/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9104e-05 - mae: 0.0035\n",
      "Epoch 183: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9103e-05 - mae: 0.0035 - val_loss: 3.9362e-05 - val_mae: 0.0052\n",
      "Epoch 184/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9232e-05 - mae: 0.0035\n",
      "Epoch 184: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9163e-05 - mae: 0.0035 - val_loss: 3.8281e-05 - val_mae: 0.0051\n",
      "Epoch 185/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6956e-05 - mae: 0.0032\n",
      "Epoch 185: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8053e-05 - mae: 0.0034 - val_loss: 3.9023e-05 - val_mae: 0.0051\n",
      "Epoch 186/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.1110e-05 - mae: 0.0036\n",
      "Epoch 186: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.1031e-05 - mae: 0.0036 - val_loss: 4.1281e-05 - val_mae: 0.0053\n",
      "Epoch 187/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0906e-05 - mae: 0.0037\n",
      "Epoch 187: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.0735e-05 - mae: 0.0036 - val_loss: 3.9674e-05 - val_mae: 0.0052\n",
      "Epoch 188/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9990e-05 - mae: 0.0036\n",
      "Epoch 188: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9382e-05 - mae: 0.0035 - val_loss: 3.9971e-05 - val_mae: 0.0052\n",
      "Epoch 189/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9354e-05 - mae: 0.0035\n",
      "Epoch 189: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8637e-05 - mae: 0.0034 - val_loss: 3.7860e-05 - val_mae: 0.0051\n",
      "Epoch 190/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8090e-05 - mae: 0.0034\n",
      "Epoch 190: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.1066e-05 - mae: 0.0037 - val_loss: 3.8167e-05 - val_mae: 0.0051\n",
      "Epoch 191/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 0s - loss: 2.1765e-05 - mae: 0.0037\n",
      "Epoch 191: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.1765e-05 - mae: 0.0037 - val_loss: 4.1020e-05 - val_mae: 0.0053\n",
      "Epoch 192/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7772e-05 - mae: 0.0034\n",
      "Epoch 192: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8596e-05 - mae: 0.0034 - val_loss: 4.4107e-05 - val_mae: 0.0053\n",
      "Epoch 193/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0905e-05 - mae: 0.0037\n",
      "Epoch 193: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0498e-05 - mae: 0.0036 - val_loss: 4.8103e-05 - val_mae: 0.0055\n",
      "Epoch 194/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.1594e-05 - mae: 0.0037\n",
      "Epoch 194: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9543e-05 - mae: 0.0035 - val_loss: 4.1082e-05 - val_mae: 0.0053\n",
      "Epoch 195/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0414e-05 - mae: 0.0036\n",
      "Epoch 195: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0952e-05 - mae: 0.0037 - val_loss: 4.0484e-05 - val_mae: 0.0052\n",
      "Epoch 196/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0450e-05 - mae: 0.0036\n",
      "Epoch 196: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.0263e-05 - mae: 0.0036 - val_loss: 3.8341e-05 - val_mae: 0.0051\n",
      "Epoch 197/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8634e-05 - mae: 0.0034\n",
      "Epoch 197: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8634e-05 - mae: 0.0034 - val_loss: 3.8473e-05 - val_mae: 0.0051\n",
      "Epoch 198/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.2715e-05 - mae: 0.0038\n",
      "Epoch 198: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1005e-05 - mae: 0.0036 - val_loss: 4.0107e-05 - val_mae: 0.0051\n",
      "Epoch 199/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9938e-05 - mae: 0.0036\n",
      "Epoch 199: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0248e-05 - mae: 0.0036 - val_loss: 3.8909e-05 - val_mae: 0.0051\n",
      "Epoch 200/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0262e-05 - mae: 0.0036\n",
      "Epoch 200: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9856e-05 - mae: 0.0036 - val_loss: 3.9015e-05 - val_mae: 0.0051\n",
      "Epoch 201/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8226e-05 - mae: 0.0034\n",
      "Epoch 201: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8687e-05 - mae: 0.0034 - val_loss: 4.2249e-05 - val_mae: 0.0053\n",
      "Epoch 202/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8014e-05 - mae: 0.0033\n",
      "Epoch 202: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8232e-05 - mae: 0.0034 - val_loss: 3.9249e-05 - val_mae: 0.0051\n",
      "Epoch 203/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 2.2200e-05 - mae: 0.0038\n",
      "Epoch 203: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 2.1952e-05 - mae: 0.0037 - val_loss: 3.8198e-05 - val_mae: 0.0051\n",
      "Epoch 204/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9264e-05 - mae: 0.0035\n",
      "Epoch 204: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.9268e-05 - mae: 0.0035 - val_loss: 4.6509e-05 - val_mae: 0.0056\n",
      "Epoch 205/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9652e-05 - mae: 0.0035\n",
      "Epoch 205: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.9582e-05 - mae: 0.0035 - val_loss: 3.9618e-05 - val_mae: 0.0052\n",
      "Epoch 206/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.1993e-05 - mae: 0.0037\n",
      "Epoch 206: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.1713e-05 - mae: 0.0037 - val_loss: 4.1640e-05 - val_mae: 0.0053\n",
      "Epoch 207/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3518e-05 - mae: 0.0039\n",
      "Epoch 207: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.1737e-05 - mae: 0.0037 - val_loss: 4.9411e-05 - val_mae: 0.0057\n",
      "Epoch 208/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8866e-05 - mae: 0.0035\n",
      "Epoch 208: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 2.0097e-05 - mae: 0.0036 - val_loss: 3.9000e-05 - val_mae: 0.0051\n",
      "Epoch 209/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1134e-05 - mae: 0.0036\n",
      "Epoch 209: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.1134e-05 - mae: 0.0036 - val_loss: 4.6606e-05 - val_mae: 0.0055\n",
      "Epoch 210/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0339e-05 - mae: 0.0036\n",
      "Epoch 210: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0339e-05 - mae: 0.0036 - val_loss: 3.7871e-05 - val_mae: 0.0051\n",
      "Epoch 211/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0546e-05 - mae: 0.0037\n",
      "Epoch 211: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.1204e-05 - mae: 0.0037 - val_loss: 3.8026e-05 - val_mae: 0.0051\n",
      "Epoch 212/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7683e-05 - mae: 0.0033\n",
      "Epoch 212: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8522e-05 - mae: 0.0034 - val_loss: 5.0399e-05 - val_mae: 0.0057\n",
      "Epoch 213/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8460e-05 - mae: 0.0035\n",
      "Epoch 213: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8248e-05 - mae: 0.0034 - val_loss: 4.3120e-05 - val_mae: 0.0054\n",
      "Epoch 214/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9032e-05 - mae: 0.0034\n",
      "Epoch 214: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.2114e-05 - mae: 0.0037 - val_loss: 4.5879e-05 - val_mae: 0.0055\n",
      "Epoch 215/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0366e-05 - mae: 0.0036\n",
      "Epoch 215: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9685e-05 - mae: 0.0035 - val_loss: 3.8553e-05 - val_mae: 0.0051\n",
      "Epoch 216/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0746e-05 - mae: 0.0036\n",
      "Epoch 216: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1881e-05 - mae: 0.0037 - val_loss: 3.9003e-05 - val_mae: 0.0051\n",
      "Epoch 217/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0036e-05 - mae: 0.0036\n",
      "Epoch 217: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9868e-05 - mae: 0.0036 - val_loss: 3.8202e-05 - val_mae: 0.0051\n",
      "Epoch 218/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8826e-05 - mae: 0.0034\n",
      "Epoch 218: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8015e-05 - mae: 0.0034 - val_loss: 4.4183e-05 - val_mae: 0.0054\n",
      "Epoch 219/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.4299e-05 - mae: 0.0039\n",
      "Epoch 219: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.3145e-05 - mae: 0.0038 - val_loss: 4.8659e-05 - val_mae: 0.0057\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8628e-05 - mae: 0.0035\n",
      "Epoch 220: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8298e-05 - mae: 0.0034 - val_loss: 3.8182e-05 - val_mae: 0.0051\n",
      "Epoch 221/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8572e-05 - mae: 0.0034\n",
      "Epoch 221: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0581e-05 - mae: 0.0036 - val_loss: 3.8580e-05 - val_mae: 0.0051\n",
      "Epoch 222/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7389e-05 - mae: 0.0033\n",
      "Epoch 222: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8675e-05 - mae: 0.0034 - val_loss: 4.1066e-05 - val_mae: 0.0053\n",
      "Epoch 223/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9401e-05 - mae: 0.0035\n",
      "Epoch 223: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0648e-05 - mae: 0.0036 - val_loss: 3.8307e-05 - val_mae: 0.0051\n",
      "Epoch 224/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.1790e-05 - mae: 0.0037\n",
      "Epoch 224: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.2452e-05 - mae: 0.0038 - val_loss: 4.6471e-05 - val_mae: 0.0055\n",
      "Epoch 225/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.3204e-05 - mae: 0.0038\n",
      "Epoch 225: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.1404e-05 - mae: 0.0037 - val_loss: 4.3790e-05 - val_mae: 0.0053\n",
      "Epoch 226/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1577e-05 - mae: 0.0038\n",
      "Epoch 226: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9894e-05 - mae: 0.0036 - val_loss: 3.9316e-05 - val_mae: 0.0052\n",
      "Epoch 227/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7698e-05 - mae: 0.0033\n",
      "Epoch 227: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1251e-05 - mae: 0.0037 - val_loss: 5.1665e-05 - val_mae: 0.0058\n",
      "Epoch 228/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9160e-05 - mae: 0.0035\n",
      "Epoch 228: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.0296e-05 - mae: 0.0036 - val_loss: 4.8718e-05 - val_mae: 0.0057\n",
      "Epoch 229/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6922e-05 - mae: 0.0033\n",
      "Epoch 229: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8635e-05 - mae: 0.0034 - val_loss: 3.8364e-05 - val_mae: 0.0051\n",
      "Epoch 230/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1013e-05 - mae: 0.0038\n",
      "Epoch 230: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9291e-05 - mae: 0.0035 - val_loss: 4.1856e-05 - val_mae: 0.0053\n",
      "Epoch 231/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.3948e-05 - mae: 0.0040\n",
      "Epoch 231: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.1015e-05 - mae: 0.0037 - val_loss: 3.9882e-05 - val_mae: 0.0052\n",
      "Epoch 232/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0067e-05 - mae: 0.0036\n",
      "Epoch 232: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0436e-05 - mae: 0.0036 - val_loss: 5.9879e-05 - val_mae: 0.0063\n",
      "Epoch 233/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0501e-05 - mae: 0.0036\n",
      "Epoch 233: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.3084e-05 - mae: 0.0038 - val_loss: 3.8677e-05 - val_mae: 0.0051\n",
      "Epoch 234/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8813e-05 - mae: 0.0035\n",
      "Epoch 234: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8702e-05 - mae: 0.0035 - val_loss: 4.0674e-05 - val_mae: 0.0052\n",
      "Epoch 235/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9130e-05 - mae: 0.0035\n",
      "Epoch 235: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9130e-05 - mae: 0.0035 - val_loss: 3.8949e-05 - val_mae: 0.0051\n",
      "Epoch 236/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9346e-05 - mae: 0.0035\n",
      "Epoch 236: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9295e-05 - mae: 0.0035 - val_loss: 3.8340e-05 - val_mae: 0.0051\n",
      "Epoch 237/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8206e-05 - mae: 0.0034\n",
      "Epoch 237: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8369e-05 - mae: 0.0034 - val_loss: 4.1252e-05 - val_mae: 0.0052\n",
      "Epoch 238/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8667e-05 - mae: 0.0034\n",
      "Epoch 238: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9023e-05 - mae: 0.0034 - val_loss: 4.1061e-05 - val_mae: 0.0053\n",
      "Epoch 239/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0564e-05 - mae: 0.0036\n",
      "Epoch 239: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0426e-05 - mae: 0.0036 - val_loss: 4.1150e-05 - val_mae: 0.0052\n",
      "Epoch 240/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9960e-05 - mae: 0.0035\n",
      "Epoch 240: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9648e-05 - mae: 0.0035 - val_loss: 4.0869e-05 - val_mae: 0.0052\n",
      "Epoch 241/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.1299e-05 - mae: 0.0037\n",
      "Epoch 241: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.1640e-05 - mae: 0.0037 - val_loss: 3.9931e-05 - val_mae: 0.0052\n",
      "Epoch 242/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0149e-05 - mae: 0.0036\n",
      "Epoch 242: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0149e-05 - mae: 0.0036 - val_loss: 3.8699e-05 - val_mae: 0.0051\n",
      "Epoch 243/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9697e-05 - mae: 0.0035\n",
      "Epoch 243: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9504e-05 - mae: 0.0035 - val_loss: 3.8597e-05 - val_mae: 0.0051\n",
      "Epoch 244/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9975e-05 - mae: 0.0036\n",
      "Epoch 244: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9350e-05 - mae: 0.0035 - val_loss: 4.6650e-05 - val_mae: 0.0056\n",
      "Epoch 245/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8482e-05 - mae: 0.0034\n",
      "Epoch 245: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8593e-05 - mae: 0.0034 - val_loss: 3.8611e-05 - val_mae: 0.0051\n",
      "Epoch 246/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7839e-05 - mae: 0.0033\n",
      "Epoch 246: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8021e-05 - mae: 0.0034 - val_loss: 3.9971e-05 - val_mae: 0.0052\n",
      "Epoch 247/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.2373e-05 - mae: 0.0037\n",
      "Epoch 247: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.2100e-05 - mae: 0.0037 - val_loss: 4.0339e-05 - val_mae: 0.0052\n",
      "Epoch 248/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9744e-05 - mae: 0.0035\n",
      "Epoch 248: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8956e-05 - mae: 0.0034 - val_loss: 3.8744e-05 - val_mae: 0.0051\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9763e-05 - mae: 0.0035\n",
      "Epoch 249: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9453e-05 - mae: 0.0035 - val_loss: 3.9666e-05 - val_mae: 0.0052\n",
      "Epoch 250/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9078e-05 - mae: 0.0035\n",
      "Epoch 250: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8975e-05 - mae: 0.0035 - val_loss: 3.8469e-05 - val_mae: 0.0051\n",
      "Epoch 251/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8294e-05 - mae: 0.0034\n",
      "Epoch 251: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8426e-05 - mae: 0.0034 - val_loss: 4.0900e-05 - val_mae: 0.0052\n",
      "Epoch 252/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.7848e-05 - mae: 0.0034\n",
      "Epoch 252: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8899e-05 - mae: 0.0035 - val_loss: 4.1505e-05 - val_mae: 0.0053\n",
      "Epoch 253/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0435e-05 - mae: 0.0037\n",
      "Epoch 253: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.1062e-05 - mae: 0.0037 - val_loss: 6.6392e-05 - val_mae: 0.0066\n",
      "Epoch 254/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2583e-05 - mae: 0.0038\n",
      "Epoch 254: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0963e-05 - mae: 0.0036 - val_loss: 4.1655e-05 - val_mae: 0.0053\n",
      "Epoch 255/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7988e-05 - mae: 0.0034\n",
      "Epoch 255: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9628e-05 - mae: 0.0035 - val_loss: 3.9371e-05 - val_mae: 0.0051\n",
      "Epoch 256/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8990e-05 - mae: 0.0035\n",
      "Epoch 256: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8990e-05 - mae: 0.0035 - val_loss: 5.0848e-05 - val_mae: 0.0058\n",
      "Epoch 257/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8032e-05 - mae: 0.0034\n",
      "Epoch 257: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8209e-05 - mae: 0.0034 - val_loss: 4.2319e-05 - val_mae: 0.0053\n",
      "Epoch 258/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6900e-05 - mae: 0.0033\n",
      "Epoch 258: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7908e-05 - mae: 0.0033 - val_loss: 3.8427e-05 - val_mae: 0.0051\n",
      "Epoch 259/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2045e-05 - mae: 0.0037\n",
      "Epoch 259: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1245e-05 - mae: 0.0036 - val_loss: 4.1959e-05 - val_mae: 0.0053\n",
      "Epoch 260/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9541e-05 - mae: 0.0035\n",
      "Epoch 260: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9301e-05 - mae: 0.0035 - val_loss: 4.5141e-05 - val_mae: 0.0055\n",
      "Epoch 261/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.5927e-05 - mae: 0.0041\n",
      "Epoch 261: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.5614e-05 - mae: 0.0040 - val_loss: 3.8715e-05 - val_mae: 0.0051\n",
      "Epoch 262/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1172e-05 - mae: 0.0036\n",
      "Epoch 262: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.0821e-05 - mae: 0.0036 - val_loss: 3.9945e-05 - val_mae: 0.0052\n",
      "Epoch 263/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8141e-05 - mae: 0.0034\n",
      "Epoch 263: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8283e-05 - mae: 0.0034 - val_loss: 4.0005e-05 - val_mae: 0.0052\n",
      "Epoch 264/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0215e-05 - mae: 0.0036\n",
      "Epoch 264: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.0359e-05 - mae: 0.0036 - val_loss: 4.4822e-05 - val_mae: 0.0054\n",
      "Epoch 265/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9041e-05 - mae: 0.0034\n",
      "Epoch 265: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.8730e-05 - mae: 0.0034 - val_loss: 4.0089e-05 - val_mae: 0.0052\n",
      "Epoch 266/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0372e-05 - mae: 0.0036\n",
      "Epoch 266: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0372e-05 - mae: 0.0036 - val_loss: 4.2118e-05 - val_mae: 0.0053\n",
      "Epoch 267/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0815e-05 - mae: 0.0036\n",
      "Epoch 267: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.0478e-05 - mae: 0.0036 - val_loss: 4.2251e-05 - val_mae: 0.0053\n",
      "Epoch 268/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9157e-05 - mae: 0.0035\n",
      "Epoch 268: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9016e-05 - mae: 0.0034 - val_loss: 3.7913e-05 - val_mae: 0.0051\n",
      "Epoch 269/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8691e-05 - mae: 0.0035\n",
      "Epoch 269: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8731e-05 - mae: 0.0034 - val_loss: 3.8760e-05 - val_mae: 0.0051\n",
      "Epoch 270/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1199e-05 - mae: 0.0036\n",
      "Epoch 270: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9650e-05 - mae: 0.0035 - val_loss: 4.6041e-05 - val_mae: 0.0055\n",
      "Epoch 271/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8352e-05 - mae: 0.0034\n",
      "Epoch 271: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9998e-05 - mae: 0.0035 - val_loss: 4.0159e-05 - val_mae: 0.0052\n",
      "Epoch 272/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8793e-05 - mae: 0.0034\n",
      "Epoch 272: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8793e-05 - mae: 0.0034 - val_loss: 3.9161e-05 - val_mae: 0.0051\n",
      "Epoch 273/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9112e-05 - mae: 0.0035\n",
      "Epoch 273: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9100e-05 - mae: 0.0035 - val_loss: 4.0399e-05 - val_mae: 0.0052\n",
      "Epoch 274/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 2.5347e-05 - mae: 0.0041\n",
      "Epoch 274: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.3690e-05 - mae: 0.0039 - val_loss: 3.9712e-05 - val_mae: 0.0052\n",
      "Epoch 275/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6836e-05 - mae: 0.0033\n",
      "Epoch 275: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8597e-05 - mae: 0.0034 - val_loss: 3.8678e-05 - val_mae: 0.0051\n",
      "Epoch 276/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7442e-05 - mae: 0.0032\n",
      "Epoch 276: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8591e-05 - mae: 0.0034 - val_loss: 3.9160e-05 - val_mae: 0.0051\n",
      "Epoch 277/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1602e-05 - mae: 0.0037\n",
      "Epoch 277: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0315e-05 - mae: 0.0036 - val_loss: 3.9339e-05 - val_mae: 0.0051\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/800 [==============>...............] - ETA: 0s - loss: 2.4837e-05 - mae: 0.0040\n",
      "Epoch 278: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.4216e-05 - mae: 0.0040 - val_loss: 5.5337e-05 - val_mae: 0.0060\n",
      "Epoch 279/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7430e-05 - mae: 0.0033\n",
      "Epoch 279: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.1343e-05 - mae: 0.0037 - val_loss: 3.7906e-05 - val_mae: 0.0051\n",
      "Epoch 280/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0648e-05 - mae: 0.0036\n",
      "Epoch 280: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9558e-05 - mae: 0.0035 - val_loss: 3.8248e-05 - val_mae: 0.0051\n",
      "Epoch 281/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.1834e-05 - mae: 0.0037\n",
      "Epoch 281: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.1858e-05 - mae: 0.0037 - val_loss: 3.9823e-05 - val_mae: 0.0052\n",
      "Epoch 282/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9002e-05 - mae: 0.0034\n",
      "Epoch 282: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.9051e-05 - mae: 0.0034 - val_loss: 3.7949e-05 - val_mae: 0.0051\n",
      "Epoch 283/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8404e-05 - mae: 0.0035\n",
      "Epoch 283: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8727e-05 - mae: 0.0035 - val_loss: 3.8472e-05 - val_mae: 0.0051\n",
      "Epoch 284/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0029e-05 - mae: 0.0036\n",
      "Epoch 284: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9998e-05 - mae: 0.0036 - val_loss: 3.8068e-05 - val_mae: 0.0051\n",
      "Epoch 285/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9041e-05 - mae: 0.0035\n",
      "Epoch 285: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.8935e-05 - mae: 0.0035 - val_loss: 3.8019e-05 - val_mae: 0.0051\n",
      "Epoch 286/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8754e-05 - mae: 0.0035\n",
      "Epoch 286: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8589e-05 - mae: 0.0034 - val_loss: 3.8189e-05 - val_mae: 0.0051\n",
      "Epoch 287/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6425e-05 - mae: 0.0032\n",
      "Epoch 287: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8611e-05 - mae: 0.0034 - val_loss: 3.8788e-05 - val_mae: 0.0051\n",
      "Epoch 288/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1919e-05 - mae: 0.0037\n",
      "Epoch 288: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9847e-05 - mae: 0.0035 - val_loss: 3.8199e-05 - val_mae: 0.0051\n",
      "Epoch 289/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9186e-05 - mae: 0.0035\n",
      "Epoch 289: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9101e-05 - mae: 0.0035 - val_loss: 3.8098e-05 - val_mae: 0.0051\n",
      "Epoch 290/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8741e-05 - mae: 0.0034\n",
      "Epoch 290: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8925e-05 - mae: 0.0034 - val_loss: 4.1790e-05 - val_mae: 0.0053\n",
      "Epoch 291/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7883e-05 - mae: 0.0034\n",
      "Epoch 291: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8379e-05 - mae: 0.0034 - val_loss: 4.2274e-05 - val_mae: 0.0053\n",
      "Epoch 292/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9176e-05 - mae: 0.0035\n",
      "Epoch 292: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8417e-05 - mae: 0.0034 - val_loss: 3.9905e-05 - val_mae: 0.0052\n",
      "Epoch 293/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9313e-05 - mae: 0.0035\n",
      "Epoch 293: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8252e-05 - mae: 0.0034 - val_loss: 4.1083e-05 - val_mae: 0.0052\n",
      "Epoch 294/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7711e-05 - mae: 0.0033\n",
      "Epoch 294: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.1195e-05 - mae: 0.0037 - val_loss: 4.9207e-05 - val_mae: 0.0057\n",
      "Epoch 295/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1588e-05 - mae: 0.0036\n",
      "Epoch 295: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0992e-05 - mae: 0.0036 - val_loss: 4.3772e-05 - val_mae: 0.0054\n",
      "Epoch 296/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6260e-05 - mae: 0.0032\n",
      "Epoch 296: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9428e-05 - mae: 0.0035 - val_loss: 3.7787e-05 - val_mae: 0.0050\n",
      "Epoch 297/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.2445e-05 - mae: 0.0038\n",
      "Epoch 297: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.2952e-05 - mae: 0.0038 - val_loss: 4.2294e-05 - val_mae: 0.0052\n",
      "Epoch 298/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0253e-05 - mae: 0.0036\n",
      "Epoch 298: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9973e-05 - mae: 0.0036 - val_loss: 3.8399e-05 - val_mae: 0.0051\n",
      "Epoch 299/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8673e-05 - mae: 0.0035\n",
      "Epoch 299: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8892e-05 - mae: 0.0035 - val_loss: 3.8079e-05 - val_mae: 0.0050\n",
      "Epoch 300/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.0595e-05 - mae: 0.0036\n",
      "Epoch 300: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.0859e-05 - mae: 0.0036 - val_loss: 4.3312e-05 - val_mae: 0.0054\n",
      "Epoch 301/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9394e-05 - mae: 0.0035\n",
      "Epoch 301: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9401e-05 - mae: 0.0035 - val_loss: 4.0420e-05 - val_mae: 0.0052\n",
      "Epoch 302/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.2966e-05 - mae: 0.0038\n",
      "Epoch 302: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0835e-05 - mae: 0.0036 - val_loss: 3.9210e-05 - val_mae: 0.0051\n",
      "Epoch 303/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8788e-05 - mae: 0.0034\n",
      "Epoch 303: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8788e-05 - mae: 0.0034 - val_loss: 3.9203e-05 - val_mae: 0.0051\n",
      "Epoch 304/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9234e-05 - mae: 0.0035\n",
      "Epoch 304: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9685e-05 - mae: 0.0035 - val_loss: 3.8981e-05 - val_mae: 0.0051\n",
      "Epoch 305/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9557e-05 - mae: 0.0035\n",
      "Epoch 305: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9557e-05 - mae: 0.0035 - val_loss: 3.9360e-05 - val_mae: 0.0052\n",
      "Epoch 306/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9479e-05 - mae: 0.0034\n",
      "Epoch 306: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9222e-05 - mae: 0.0034 - val_loss: 3.8885e-05 - val_mae: 0.0051\n",
      "Epoch 307/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7313e-05 - mae: 0.0032\n",
      "Epoch 307: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8482e-05 - mae: 0.0034 - val_loss: 3.9592e-05 - val_mae: 0.0052\n",
      "Epoch 308/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1050e-05 - mae: 0.0036\n",
      "Epoch 308: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.0842e-05 - mae: 0.0036 - val_loss: 3.8878e-05 - val_mae: 0.0051\n",
      "Epoch 309/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8718e-05 - mae: 0.0034\n",
      "Epoch 309: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0259e-05 - mae: 0.0035 - val_loss: 4.4602e-05 - val_mae: 0.0054\n",
      "Epoch 310/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8627e-05 - mae: 0.0035\n",
      "Epoch 310: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8949e-05 - mae: 0.0035 - val_loss: 3.8587e-05 - val_mae: 0.0051\n",
      "Epoch 311/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8775e-05 - mae: 0.0034\n",
      "Epoch 311: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8705e-05 - mae: 0.0034 - val_loss: 3.8630e-05 - val_mae: 0.0051\n",
      "Epoch 312/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0037e-05 - mae: 0.0036\n",
      "Epoch 312: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8967e-05 - mae: 0.0035 - val_loss: 3.8955e-05 - val_mae: 0.0051\n",
      "Epoch 313/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6659e-05 - mae: 0.0032\n",
      "Epoch 313: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8776e-05 - mae: 0.0034 - val_loss: 3.9289e-05 - val_mae: 0.0051\n",
      "Epoch 314/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8459e-05 - mae: 0.0034\n",
      "Epoch 314: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.9004e-05 - mae: 0.0035 - val_loss: 4.5954e-05 - val_mae: 0.0055\n",
      "Epoch 315/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7681e-05 - mae: 0.0034\n",
      "Epoch 315: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.8502e-05 - mae: 0.0034 - val_loss: 4.5118e-05 - val_mae: 0.0055\n",
      "Epoch 316/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9934e-05 - mae: 0.0035\n",
      "Epoch 316: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.0195e-05 - mae: 0.0035 - val_loss: 4.3522e-05 - val_mae: 0.0053\n",
      "Epoch 317/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.1686e-05 - mae: 0.0037\n",
      "Epoch 317: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.0743e-05 - mae: 0.0036 - val_loss: 3.9580e-05 - val_mae: 0.0052\n",
      "Epoch 318/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0756e-05 - mae: 0.0036\n",
      "Epoch 318: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0170e-05 - mae: 0.0035 - val_loss: 4.0651e-05 - val_mae: 0.0052\n",
      "Epoch 319/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0026e-05 - mae: 0.0036\n",
      "Epoch 319: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.0053e-05 - mae: 0.0036 - val_loss: 3.9860e-05 - val_mae: 0.0051\n",
      "Epoch 320/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0688e-05 - mae: 0.0036\n",
      "Epoch 320: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8552e-05 - mae: 0.0034 - val_loss: 3.9058e-05 - val_mae: 0.0051\n",
      "Epoch 321/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8226e-05 - mae: 0.0034\n",
      "Epoch 321: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8226e-05 - mae: 0.0034 - val_loss: 3.9863e-05 - val_mae: 0.0052\n",
      "Epoch 322/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8853e-05 - mae: 0.0034\n",
      "Epoch 322: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9132e-05 - mae: 0.0034 - val_loss: 4.5098e-05 - val_mae: 0.0055\n",
      "Epoch 323/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 2.0090e-05 - mae: 0.0036\n",
      "Epoch 323: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9480e-05 - mae: 0.0035 - val_loss: 4.2325e-05 - val_mae: 0.0053\n",
      "Epoch 324/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3132e-05 - mae: 0.0038\n",
      "Epoch 324: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.1684e-05 - mae: 0.0037 - val_loss: 3.9890e-05 - val_mae: 0.0051\n",
      "Epoch 325/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9387e-05 - mae: 0.0035\n",
      "Epoch 325: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0345e-05 - mae: 0.0036 - val_loss: 4.1275e-05 - val_mae: 0.0053\n",
      "Epoch 326/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9595e-05 - mae: 0.0035\n",
      "Epoch 326: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8510e-05 - mae: 0.0034 - val_loss: 5.0948e-05 - val_mae: 0.0058\n",
      "Epoch 327/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9879e-05 - mae: 0.0036\n",
      "Epoch 327: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9509e-05 - mae: 0.0035 - val_loss: 4.1940e-05 - val_mae: 0.0053\n",
      "Epoch 328/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8132e-05 - mae: 0.0034\n",
      "Epoch 328: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9457e-05 - mae: 0.0034 - val_loss: 3.9759e-05 - val_mae: 0.0052\n",
      "Epoch 329/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.2102e-05 - mae: 0.0037\n",
      "Epoch 329: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.2246e-05 - mae: 0.0037 - val_loss: 3.7741e-05 - val_mae: 0.0050\n",
      "Epoch 330/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0643e-05 - mae: 0.0036\n",
      "Epoch 330: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 2.0732e-05 - mae: 0.0036 - val_loss: 5.7989e-05 - val_mae: 0.0062\n",
      "Epoch 331/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7945e-05 - mae: 0.0034\n",
      "Epoch 331: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8122e-05 - mae: 0.0034 - val_loss: 3.8326e-05 - val_mae: 0.0051\n",
      "Epoch 332/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7431e-05 - mae: 0.0033\n",
      "Epoch 332: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8277e-05 - mae: 0.0034 - val_loss: 3.8040e-05 - val_mae: 0.0051\n",
      "Epoch 333/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0733e-05 - mae: 0.0036\n",
      "Epoch 333: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0985e-05 - mae: 0.0036 - val_loss: 4.4764e-05 - val_mae: 0.0054\n",
      "Epoch 334/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9644e-05 - mae: 0.0035\n",
      "Epoch 334: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9981e-05 - mae: 0.0036 - val_loss: 3.8216e-05 - val_mae: 0.0051\n",
      "Epoch 335/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9441e-05 - mae: 0.0035\n",
      "Epoch 335: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9375e-05 - mae: 0.0035 - val_loss: 3.8199e-05 - val_mae: 0.0051\n",
      "Epoch 336/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0767e-05 - mae: 0.0036\n",
      "Epoch 336: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.0852e-05 - mae: 0.0036 - val_loss: 3.8410e-05 - val_mae: 0.0051\n",
      "Epoch 337/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8159e-05 - mae: 0.0034\n",
      "Epoch 337: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9194e-05 - mae: 0.0035 - val_loss: 4.7180e-05 - val_mae: 0.0056\n",
      "Epoch 338/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7155e-05 - mae: 0.0033\n",
      "Epoch 338: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9242e-05 - mae: 0.0035 - val_loss: 3.8173e-05 - val_mae: 0.0051\n",
      "Epoch 339/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7506e-05 - mae: 0.0034\n",
      "Epoch 339: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.1573e-05 - mae: 0.0036 - val_loss: 4.3073e-05 - val_mae: 0.0054\n",
      "Epoch 340/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0131e-05 - mae: 0.0036\n",
      "Epoch 340: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0170e-05 - mae: 0.0036 - val_loss: 4.0613e-05 - val_mae: 0.0052\n",
      "Epoch 341/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9490e-05 - mae: 0.0035\n",
      "Epoch 341: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9476e-05 - mae: 0.0035 - val_loss: 4.0465e-05 - val_mae: 0.0052\n",
      "Epoch 342/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0230e-05 - mae: 0.0036\n",
      "Epoch 342: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9715e-05 - mae: 0.0035 - val_loss: 4.0331e-05 - val_mae: 0.0052\n",
      "Epoch 343/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8482e-05 - mae: 0.0035\n",
      "Epoch 343: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8290e-05 - mae: 0.0034 - val_loss: 3.8058e-05 - val_mae: 0.0050\n",
      "Epoch 344/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7635e-05 - mae: 0.0033\n",
      "Epoch 344: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8425e-05 - mae: 0.0034 - val_loss: 3.8697e-05 - val_mae: 0.0051\n",
      "Epoch 345/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1236e-05 - mae: 0.0036\n",
      "Epoch 345: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9046e-05 - mae: 0.0034 - val_loss: 4.7853e-05 - val_mae: 0.0056\n",
      "Epoch 346/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6232e-05 - mae: 0.0032\n",
      "Epoch 346: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1276e-05 - mae: 0.0037 - val_loss: 5.5239e-05 - val_mae: 0.0060\n",
      "Epoch 347/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9571e-05 - mae: 0.0035\n",
      "Epoch 347: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0231e-05 - mae: 0.0036 - val_loss: 3.8593e-05 - val_mae: 0.0051\n",
      "Epoch 348/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1099e-05 - mae: 0.0037\n",
      "Epoch 348: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1339e-05 - mae: 0.0037 - val_loss: 3.8323e-05 - val_mae: 0.0051\n",
      "Epoch 349/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8098e-05 - mae: 0.0034\n",
      "Epoch 349: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8377e-05 - mae: 0.0034 - val_loss: 3.8417e-05 - val_mae: 0.0051\n",
      "Epoch 350/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7229e-05 - mae: 0.0033\n",
      "Epoch 350: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8608e-05 - mae: 0.0035 - val_loss: 3.9026e-05 - val_mae: 0.0051\n",
      "Epoch 351/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0173e-05 - mae: 0.0036\n",
      "Epoch 351: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9583e-05 - mae: 0.0035 - val_loss: 4.0371e-05 - val_mae: 0.0052\n",
      "Epoch 352/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0735e-05 - mae: 0.0036\n",
      "Epoch 352: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0988e-05 - mae: 0.0037 - val_loss: 3.9526e-05 - val_mae: 0.0051\n",
      "Epoch 353/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0117e-05 - mae: 0.0036\n",
      "Epoch 353: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9504e-05 - mae: 0.0035 - val_loss: 3.8599e-05 - val_mae: 0.0051\n",
      "Epoch 354/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8972e-05 - mae: 0.0034\n",
      "Epoch 354: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8972e-05 - mae: 0.0034 - val_loss: 4.3162e-05 - val_mae: 0.0053\n",
      "Epoch 355/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0083e-05 - mae: 0.0036\n",
      "Epoch 355: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9925e-05 - mae: 0.0036 - val_loss: 4.3094e-05 - val_mae: 0.0053\n",
      "Epoch 356/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1181e-05 - mae: 0.0037\n",
      "Epoch 356: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1529e-05 - mae: 0.0037 - val_loss: 3.8713e-05 - val_mae: 0.0051\n",
      "Epoch 357/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8401e-05 - mae: 0.0034\n",
      "Epoch 357: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7759e-05 - mae: 0.0033 - val_loss: 4.1154e-05 - val_mae: 0.0053\n",
      "Epoch 358/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7715e-05 - mae: 0.0033\n",
      "Epoch 358: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9898e-05 - mae: 0.0036 - val_loss: 3.9338e-05 - val_mae: 0.0051\n",
      "Epoch 359/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6153e-05 - mae: 0.0032\n",
      "Epoch 359: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7872e-05 - mae: 0.0033 - val_loss: 3.8970e-05 - val_mae: 0.0051\n",
      "Epoch 360/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7855e-05 - mae: 0.0034\n",
      "Epoch 360: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8543e-05 - mae: 0.0034 - val_loss: 3.9742e-05 - val_mae: 0.0051\n",
      "Epoch 361/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7971e-05 - mae: 0.0033\n",
      "Epoch 361: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8065e-05 - mae: 0.0033 - val_loss: 3.9273e-05 - val_mae: 0.0051\n",
      "Epoch 362/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6559e-05 - mae: 0.0032\n",
      "Epoch 362: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8403e-05 - mae: 0.0034 - val_loss: 4.4520e-05 - val_mae: 0.0054\n",
      "Epoch 363/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9937e-05 - mae: 0.0035\n",
      "Epoch 363: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.0073e-05 - mae: 0.0035 - val_loss: 3.8350e-05 - val_mae: 0.0051\n",
      "Epoch 364/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7349e-05 - mae: 0.0032\n",
      "Epoch 364: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8339e-05 - mae: 0.0034 - val_loss: 3.8115e-05 - val_mae: 0.0051\n",
      "Epoch 365/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8848e-05 - mae: 0.0035\n",
      "Epoch 365: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8039e-05 - mae: 0.0034 - val_loss: 5.0543e-05 - val_mae: 0.0057\n",
      "Epoch 366/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8933e-05 - mae: 0.0035\n",
      "Epoch 366: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8750e-05 - mae: 0.0035 - val_loss: 3.8131e-05 - val_mae: 0.0051\n",
      "Epoch 367/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8477e-05 - mae: 0.0034\n",
      "Epoch 367: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8929e-05 - mae: 0.0034 - val_loss: 3.7906e-05 - val_mae: 0.0050\n",
      "Epoch 368/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9115e-05 - mae: 0.0035\n",
      "Epoch 368: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8722e-05 - mae: 0.0035 - val_loss: 3.9834e-05 - val_mae: 0.0051\n",
      "Epoch 369/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0005e-05 - mae: 0.0036\n",
      "Epoch 369: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.1833e-05 - mae: 0.0037 - val_loss: 4.7510e-05 - val_mae: 0.0056\n",
      "Epoch 370/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1072e-05 - mae: 0.0037\n",
      "Epoch 370: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0336e-05 - mae: 0.0036 - val_loss: 3.8205e-05 - val_mae: 0.0051\n",
      "Epoch 371/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.3062e-05 - mae: 0.0039\n",
      "Epoch 371: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.3124e-05 - mae: 0.0039 - val_loss: 4.1908e-05 - val_mae: 0.0053\n",
      "Epoch 372/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9305e-05 - mae: 0.0035\n",
      "Epoch 372: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9926e-05 - mae: 0.0036 - val_loss: 3.7743e-05 - val_mae: 0.0050\n",
      "Epoch 373/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7086e-05 - mae: 0.0033\n",
      "Epoch 373: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7896e-05 - mae: 0.0034 - val_loss: 3.7987e-05 - val_mae: 0.0051\n",
      "Epoch 374/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0253e-05 - mae: 0.0035\n",
      "Epoch 374: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.2134e-05 - mae: 0.0037 - val_loss: 4.0471e-05 - val_mae: 0.0052\n",
      "Epoch 375/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.5663e-05 - mae: 0.0041\n",
      "Epoch 375: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.2436e-05 - mae: 0.0038 - val_loss: 3.9034e-05 - val_mae: 0.0051\n",
      "Epoch 376/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8064e-05 - mae: 0.0034\n",
      "Epoch 376: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8672e-05 - mae: 0.0034 - val_loss: 4.8370e-05 - val_mae: 0.0056\n",
      "Epoch 377/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9256e-05 - mae: 0.0034\n",
      "Epoch 377: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8854e-05 - mae: 0.0034 - val_loss: 3.8112e-05 - val_mae: 0.0051\n",
      "Epoch 378/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0930e-05 - mae: 0.0036\n",
      "Epoch 378: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9305e-05 - mae: 0.0035 - val_loss: 3.7755e-05 - val_mae: 0.0050\n",
      "Epoch 379/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8006e-05 - mae: 0.0034\n",
      "Epoch 379: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8006e-05 - mae: 0.0034 - val_loss: 3.7468e-05 - val_mae: 0.0050\n",
      "Epoch 380/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8627e-05 - mae: 0.0034\n",
      "Epoch 380: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8342e-05 - mae: 0.0034 - val_loss: 3.7791e-05 - val_mae: 0.0050\n",
      "Epoch 381/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7834e-05 - mae: 0.0034\n",
      "Epoch 381: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7579e-05 - mae: 0.0034 - val_loss: 3.7839e-05 - val_mae: 0.0050\n",
      "Epoch 382/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1366e-05 - mae: 0.0036\n",
      "Epoch 382: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9398e-05 - mae: 0.0034 - val_loss: 4.2670e-05 - val_mae: 0.0053\n",
      "Epoch 383/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8811e-05 - mae: 0.0035\n",
      "Epoch 383: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0356e-05 - mae: 0.0036 - val_loss: 4.6849e-05 - val_mae: 0.0055\n",
      "Epoch 384/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9282e-05 - mae: 0.0035\n",
      "Epoch 384: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0581e-05 - mae: 0.0036 - val_loss: 4.0601e-05 - val_mae: 0.0052\n",
      "Epoch 385/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9222e-05 - mae: 0.0035\n",
      "Epoch 385: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9399e-05 - mae: 0.0035 - val_loss: 4.2462e-05 - val_mae: 0.0053\n",
      "Epoch 386/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1106e-05 - mae: 0.0036\n",
      "Epoch 386: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0600e-05 - mae: 0.0036 - val_loss: 3.8121e-05 - val_mae: 0.0051\n",
      "Epoch 387/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8399e-05 - mae: 0.0034\n",
      "Epoch 387: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9364e-05 - mae: 0.0035 - val_loss: 4.2314e-05 - val_mae: 0.0053\n",
      "Epoch 388/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9199e-05 - mae: 0.0035\n",
      "Epoch 388: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8850e-05 - mae: 0.0034 - val_loss: 3.8572e-05 - val_mae: 0.0051\n",
      "Epoch 389/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7612e-05 - mae: 0.0033\n",
      "Epoch 389: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9225e-05 - mae: 0.0035 - val_loss: 5.2144e-05 - val_mae: 0.0058\n",
      "Epoch 390/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8909e-05 - mae: 0.0034\n",
      "Epoch 390: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9063e-05 - mae: 0.0035 - val_loss: 3.8162e-05 - val_mae: 0.0050\n",
      "Epoch 391/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6171e-05 - mae: 0.0032\n",
      "Epoch 391: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8151e-05 - mae: 0.0034 - val_loss: 3.9049e-05 - val_mae: 0.0051\n",
      "Epoch 392/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7113e-05 - mae: 0.0032\n",
      "Epoch 392: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8643e-05 - mae: 0.0034 - val_loss: 3.9172e-05 - val_mae: 0.0051\n",
      "Epoch 393/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8425e-05 - mae: 0.0034\n",
      "Epoch 393: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7996e-05 - mae: 0.0034 - val_loss: 3.7898e-05 - val_mae: 0.0050\n",
      "Epoch 394/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9200e-05 - mae: 0.0035\n",
      "Epoch 394: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.9504e-05 - mae: 0.0035 - val_loss: 4.7049e-05 - val_mae: 0.0056\n",
      "Epoch 395/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.2189e-05 - mae: 0.0037\n",
      "Epoch 395: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.1758e-05 - mae: 0.0037 - val_loss: 3.8430e-05 - val_mae: 0.0051\n",
      "Epoch 396/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.6362e-05 - mae: 0.0032\n",
      "Epoch 396: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.7604e-05 - mae: 0.0033 - val_loss: 3.8621e-05 - val_mae: 0.0051\n",
      "Epoch 397/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9320e-05 - mae: 0.0035\n",
      "Epoch 397: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9371e-05 - mae: 0.0035 - val_loss: 3.8137e-05 - val_mae: 0.0051\n",
      "Epoch 398/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8767e-05 - mae: 0.0034\n",
      "Epoch 398: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8623e-05 - mae: 0.0034 - val_loss: 4.0081e-05 - val_mae: 0.0052\n",
      "Epoch 399/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9186e-05 - mae: 0.0035\n",
      "Epoch 399: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.9620e-05 - mae: 0.0036 - val_loss: 4.6992e-05 - val_mae: 0.0056\n",
      "Epoch 400/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.3325e-05 - mae: 0.0038\n",
      "Epoch 400: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.3222e-05 - mae: 0.0038 - val_loss: 3.7681e-05 - val_mae: 0.0050\n",
      "Epoch 401/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.0040e-05 - mae: 0.0035\n",
      "Epoch 401: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9695e-05 - mae: 0.0035 - val_loss: 4.1108e-05 - val_mae: 0.0052\n",
      "Epoch 402/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9859e-05 - mae: 0.0035\n",
      "Epoch 402: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9859e-05 - mae: 0.0035 - val_loss: 3.8779e-05 - val_mae: 0.0051\n",
      "Epoch 403/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7376e-05 - mae: 0.0033\n",
      "Epoch 403: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8391e-05 - mae: 0.0034 - val_loss: 3.8249e-05 - val_mae: 0.0051\n",
      "Epoch 404/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7450e-05 - mae: 0.0033\n",
      "Epoch 404: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9040e-05 - mae: 0.0034 - val_loss: 3.8180e-05 - val_mae: 0.0051\n",
      "Epoch 405/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9331e-05 - mae: 0.0035\n",
      "Epoch 405: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9801e-05 - mae: 0.0036 - val_loss: 3.7681e-05 - val_mae: 0.0050\n",
      "Epoch 406/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9937e-05 - mae: 0.0035\n",
      "Epoch 406: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9820e-05 - mae: 0.0035 - val_loss: 4.2172e-05 - val_mae: 0.0053\n",
      "Epoch 407/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8655e-05 - mae: 0.0034\n",
      "Epoch 407: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8232e-05 - mae: 0.0034 - val_loss: 4.3792e-05 - val_mae: 0.0054\n",
      "Epoch 408/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8709e-05 - mae: 0.0034\n",
      "Epoch 408: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8709e-05 - mae: 0.0034 - val_loss: 3.8562e-05 - val_mae: 0.0051\n",
      "Epoch 409/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8076e-05 - mae: 0.0034\n",
      "Epoch 409: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8016e-05 - mae: 0.0033 - val_loss: 3.9003e-05 - val_mae: 0.0051\n",
      "Epoch 410/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 2.1210e-05 - mae: 0.0037\n",
      "Epoch 410: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0198e-05 - mae: 0.0036 - val_loss: 3.8559e-05 - val_mae: 0.0051\n",
      "Epoch 411/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7029e-05 - mae: 0.0033\n",
      "Epoch 411: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8763e-05 - mae: 0.0034 - val_loss: 4.5688e-05 - val_mae: 0.0054\n",
      "Epoch 412/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0587e-05 - mae: 0.0036\n",
      "Epoch 412: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.9938e-05 - mae: 0.0035 - val_loss: 3.8330e-05 - val_mae: 0.0051\n",
      "Epoch 413/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7884e-05 - mae: 0.0033\n",
      "Epoch 413: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7819e-05 - mae: 0.0033 - val_loss: 3.9075e-05 - val_mae: 0.0051\n",
      "Epoch 414/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6928e-05 - mae: 0.0033\n",
      "Epoch 414: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7967e-05 - mae: 0.0034 - val_loss: 3.9821e-05 - val_mae: 0.0052\n",
      "Epoch 415/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7632e-05 - mae: 0.0033\n",
      "Epoch 415: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8247e-05 - mae: 0.0034 - val_loss: 3.9844e-05 - val_mae: 0.0051\n",
      "Epoch 416/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8642e-05 - mae: 0.0034\n",
      "Epoch 416: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8527e-05 - mae: 0.0034 - val_loss: 4.2171e-05 - val_mae: 0.0053\n",
      "Epoch 417/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7984e-05 - mae: 0.0033\n",
      "Epoch 417: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7963e-05 - mae: 0.0034 - val_loss: 4.1865e-05 - val_mae: 0.0053\n",
      "Epoch 418/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0787e-05 - mae: 0.0036\n",
      "Epoch 418: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9236e-05 - mae: 0.0035 - val_loss: 4.2692e-05 - val_mae: 0.0053\n",
      "Epoch 419/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8563e-05 - mae: 0.0034\n",
      "Epoch 419: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9937e-05 - mae: 0.0036 - val_loss: 3.9233e-05 - val_mae: 0.0051\n",
      "Epoch 420/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0052e-05 - mae: 0.0036\n",
      "Epoch 420: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.1097e-05 - mae: 0.0037 - val_loss: 4.8420e-05 - val_mae: 0.0056\n",
      "Epoch 421/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0677e-05 - mae: 0.0036\n",
      "Epoch 421: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.1462e-05 - mae: 0.0037 - val_loss: 4.3691e-05 - val_mae: 0.0053\n",
      "Epoch 422/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2112e-05 - mae: 0.0037\n",
      "Epoch 422: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0969e-05 - mae: 0.0036 - val_loss: 4.7692e-05 - val_mae: 0.0056\n",
      "Epoch 423/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8780e-05 - mae: 0.0035\n",
      "Epoch 423: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8838e-05 - mae: 0.0035 - val_loss: 4.4561e-05 - val_mae: 0.0054\n",
      "Epoch 424/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7862e-05 - mae: 0.0034\n",
      "Epoch 424: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8832e-05 - mae: 0.0035 - val_loss: 3.8194e-05 - val_mae: 0.0051\n",
      "Epoch 425/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8898e-05 - mae: 0.0034\n",
      "Epoch 425: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8901e-05 - mae: 0.0034 - val_loss: 3.8922e-05 - val_mae: 0.0051\n",
      "Epoch 426/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8522e-05 - mae: 0.0034\n",
      "Epoch 426: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.8567e-05 - mae: 0.0034 - val_loss: 3.8589e-05 - val_mae: 0.0051\n",
      "Epoch 427/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.8987e-05 - mae: 0.0035\n",
      "Epoch 427: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.9184e-05 - mae: 0.0035 - val_loss: 4.1674e-05 - val_mae: 0.0053\n",
      "Epoch 428/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8242e-05 - mae: 0.0034\n",
      "Epoch 428: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.8734e-05 - mae: 0.0035 - val_loss: 3.9877e-05 - val_mae: 0.0051\n",
      "Epoch 429/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8219e-05 - mae: 0.0034\n",
      "Epoch 429: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.8798e-05 - mae: 0.0034 - val_loss: 3.7641e-05 - val_mae: 0.0050\n",
      "Epoch 430/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9377e-05 - mae: 0.0035\n",
      "Epoch 430: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.9157e-05 - mae: 0.0035 - val_loss: 4.0226e-05 - val_mae: 0.0052\n",
      "Epoch 431/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9830e-05 - mae: 0.0035\n",
      "Epoch 431: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.9235e-05 - mae: 0.0034 - val_loss: 4.0181e-05 - val_mae: 0.0052\n",
      "Epoch 432/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7799e-05 - mae: 0.0034\n",
      "Epoch 432: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.9601e-05 - mae: 0.0035 - val_loss: 3.9442e-05 - val_mae: 0.0052\n",
      "Epoch 433/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8755e-05 - mae: 0.0034\n",
      "Epoch 433: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.9367e-05 - mae: 0.0035 - val_loss: 4.1562e-05 - val_mae: 0.0053\n",
      "Epoch 434/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9285e-05 - mae: 0.0035\n",
      "Epoch 434: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.9173e-05 - mae: 0.0035 - val_loss: 3.8059e-05 - val_mae: 0.0050\n",
      "Epoch 435/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9786e-05 - mae: 0.0036\n",
      "Epoch 435: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.9248e-05 - mae: 0.0035 - val_loss: 3.9533e-05 - val_mae: 0.0052\n",
      "Epoch 436/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7447e-05 - mae: 0.0033\n",
      "Epoch 436: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.9831e-05 - mae: 0.0035 - val_loss: 3.8129e-05 - val_mae: 0.0051\n",
      "Epoch 437/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.7934e-05 - mae: 0.0034\n",
      "Epoch 437: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.9482e-05 - mae: 0.0035 - val_loss: 3.7887e-05 - val_mae: 0.0050\n",
      "Epoch 438/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8329e-05 - mae: 0.0034\n",
      "Epoch 438: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.9606e-05 - mae: 0.0035 - val_loss: 3.7838e-05 - val_mae: 0.0050\n",
      "Epoch 439/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3573e-05 - mae: 0.0039\n",
      "Epoch 439: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 2.2194e-05 - mae: 0.0038 - val_loss: 3.9591e-05 - val_mae: 0.0051\n",
      "Epoch 440/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.8661e-05 - mae: 0.0034\n",
      "Epoch 440: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.8689e-05 - mae: 0.0034 - val_loss: 3.8533e-05 - val_mae: 0.0051\n",
      "Epoch 441/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9581e-05 - mae: 0.0035\n",
      "Epoch 441: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.8600e-05 - mae: 0.0034 - val_loss: 3.8603e-05 - val_mae: 0.0051\n",
      "Epoch 442/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9788e-05 - mae: 0.0035\n",
      "Epoch 442: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8765e-05 - mae: 0.0034 - val_loss: 3.8016e-05 - val_mae: 0.0050\n",
      "Epoch 443/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6911e-05 - mae: 0.0033\n",
      "Epoch 443: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.7768e-05 - mae: 0.0034 - val_loss: 3.8715e-05 - val_mae: 0.0051\n",
      "Epoch 444/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8097e-05 - mae: 0.0033\n",
      "Epoch 444: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8625e-05 - mae: 0.0034 - val_loss: 4.2174e-05 - val_mae: 0.0053\n",
      "Epoch 445/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.6011e-05 - mae: 0.0031\n",
      "Epoch 445: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.7935e-05 - mae: 0.0033 - val_loss: 4.8930e-05 - val_mae: 0.0056\n",
      "Epoch 446/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.0900e-05 - mae: 0.0037\n",
      "Epoch 446: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.9664e-05 - mae: 0.0035 - val_loss: 3.8415e-05 - val_mae: 0.0051\n",
      "Epoch 447/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7757e-05 - mae: 0.0033\n",
      "Epoch 447: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 2.0340e-05 - mae: 0.0036 - val_loss: 4.8516e-05 - val_mae: 0.0056\n",
      "Epoch 448/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8156e-05 - mae: 0.0034\n",
      "Epoch 448: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9442e-05 - mae: 0.0035 - val_loss: 3.9603e-05 - val_mae: 0.0052\n",
      "Epoch 449/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8311e-05 - mae: 0.0034\n",
      "Epoch 449: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.1174e-05 - mae: 0.0036 - val_loss: 4.1438e-05 - val_mae: 0.0052\n",
      "Epoch 450/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0702e-05 - mae: 0.0036\n",
      "Epoch 450: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.0250e-05 - mae: 0.0036 - val_loss: 4.0701e-05 - val_mae: 0.0052\n",
      "Epoch 451/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7993e-05 - mae: 0.0034\n",
      "Epoch 451: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8102e-05 - mae: 0.0034 - val_loss: 3.8465e-05 - val_mae: 0.0051\n",
      "Epoch 452/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8064e-05 - mae: 0.0034\n",
      "Epoch 452: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9108e-05 - mae: 0.0035 - val_loss: 3.8230e-05 - val_mae: 0.0051\n",
      "Epoch 453/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7336e-05 - mae: 0.0033\n",
      "Epoch 453: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9339e-05 - mae: 0.0035 - val_loss: 4.1010e-05 - val_mae: 0.0053\n",
      "Epoch 454/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8610e-05 - mae: 0.0034\n",
      "Epoch 454: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8435e-05 - mae: 0.0034 - val_loss: 3.8430e-05 - val_mae: 0.0051\n",
      "Epoch 455/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7754e-05 - mae: 0.0033\n",
      "Epoch 455: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8509e-05 - mae: 0.0034 - val_loss: 3.8538e-05 - val_mae: 0.0051\n",
      "Epoch 456/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.2646e-05 - mae: 0.0038\n",
      "Epoch 456: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.1284e-05 - mae: 0.0036 - val_loss: 3.8070e-05 - val_mae: 0.0050\n",
      "Epoch 457/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8837e-05 - mae: 0.0034\n",
      "Epoch 457: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8837e-05 - mae: 0.0034 - val_loss: 3.8030e-05 - val_mae: 0.0050\n",
      "Epoch 458/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7064e-05 - mae: 0.0033\n",
      "Epoch 458: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8408e-05 - mae: 0.0034 - val_loss: 4.6673e-05 - val_mae: 0.0055\n",
      "Epoch 459/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8968e-05 - mae: 0.0034\n",
      "Epoch 459: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9810e-05 - mae: 0.0035 - val_loss: 3.8710e-05 - val_mae: 0.0051\n",
      "Epoch 460/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 2.1890e-05 - mae: 0.0038\n",
      "Epoch 460: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.0414e-05 - mae: 0.0036 - val_loss: 3.8505e-05 - val_mae: 0.0051\n",
      "Epoch 461/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8198e-05 - mae: 0.0034\n",
      "Epoch 461: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9090e-05 - mae: 0.0034 - val_loss: 4.4375e-05 - val_mae: 0.0054\n",
      "Epoch 462/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1317e-05 - mae: 0.0037\n",
      "Epoch 462: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0093e-05 - mae: 0.0035 - val_loss: 3.9007e-05 - val_mae: 0.0051\n",
      "Epoch 463/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.2794e-05 - mae: 0.0038\n",
      "Epoch 463: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 2.1565e-05 - mae: 0.0037 - val_loss: 4.3965e-05 - val_mae: 0.0054\n",
      "Epoch 464/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8555e-05 - mae: 0.0034\n",
      "Epoch 464: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9093e-05 - mae: 0.0035 - val_loss: 3.8681e-05 - val_mae: 0.0051\n",
      "Epoch 465/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0209e-05 - mae: 0.0036\n",
      "Epoch 465: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.1124e-05 - mae: 0.0037 - val_loss: 3.9226e-05 - val_mae: 0.0052\n",
      "Epoch 466/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8905e-05 - mae: 0.0034\n",
      "Epoch 466: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0971e-05 - mae: 0.0036 - val_loss: 4.1362e-05 - val_mae: 0.0052\n",
      "Epoch 467/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7493e-05 - mae: 0.0034\n",
      "Epoch 467: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8907e-05 - mae: 0.0035 - val_loss: 4.6975e-05 - val_mae: 0.0055\n",
      "Epoch 468/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8835e-05 - mae: 0.0034\n",
      "Epoch 468: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0101e-05 - mae: 0.0036 - val_loss: 3.8895e-05 - val_mae: 0.0051\n",
      "Epoch 469/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7601e-05 - mae: 0.0033\n",
      "Epoch 469: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8418e-05 - mae: 0.0034 - val_loss: 4.0474e-05 - val_mae: 0.0052\n",
      "Epoch 470/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0048e-05 - mae: 0.0035\n",
      "Epoch 470: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8708e-05 - mae: 0.0034 - val_loss: 3.8800e-05 - val_mae: 0.0051\n",
      "Epoch 471/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8113e-05 - mae: 0.0034\n",
      "Epoch 471: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8311e-05 - mae: 0.0034 - val_loss: 4.0122e-05 - val_mae: 0.0052\n",
      "Epoch 472/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8008e-05 - mae: 0.0033\n",
      "Epoch 472: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8047e-05 - mae: 0.0033 - val_loss: 4.2265e-05 - val_mae: 0.0053\n",
      "Epoch 473/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0077e-05 - mae: 0.0036\n",
      "Epoch 473: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9980e-05 - mae: 0.0036 - val_loss: 4.3153e-05 - val_mae: 0.0053\n",
      "Epoch 474/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7331e-05 - mae: 0.0034\n",
      "Epoch 474: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8208e-05 - mae: 0.0034 - val_loss: 4.6100e-05 - val_mae: 0.0055\n",
      "Epoch 475/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8705e-05 - mae: 0.0035\n",
      "Epoch 475: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8932e-05 - mae: 0.0035 - val_loss: 3.9887e-05 - val_mae: 0.0052\n",
      "Epoch 476/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8455e-05 - mae: 0.0034\n",
      "Epoch 476: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8231e-05 - mae: 0.0034 - val_loss: 4.3155e-05 - val_mae: 0.0054\n",
      "Epoch 477/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6241e-05 - mae: 0.0031\n",
      "Epoch 477: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7959e-05 - mae: 0.0033 - val_loss: 3.8436e-05 - val_mae: 0.0051\n",
      "Epoch 478/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8634e-05 - mae: 0.0034\n",
      "Epoch 478: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8647e-05 - mae: 0.0034 - val_loss: 3.8931e-05 - val_mae: 0.0051\n",
      "Epoch 479/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8806e-05 - mae: 0.0035\n",
      "Epoch 479: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9301e-05 - mae: 0.0035 - val_loss: 3.8564e-05 - val_mae: 0.0051\n",
      "Epoch 480/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8379e-05 - mae: 0.0034\n",
      "Epoch 480: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8979e-05 - mae: 0.0035 - val_loss: 3.8453e-05 - val_mae: 0.0051\n",
      "Epoch 481/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.9904e-05 - mae: 0.0035\n",
      "Epoch 481: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0445e-05 - mae: 0.0036 - val_loss: 3.9503e-05 - val_mae: 0.0052\n",
      "Epoch 482/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8616e-05 - mae: 0.0033\n",
      "Epoch 482: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.0017e-05 - mae: 0.0035 - val_loss: 5.3954e-05 - val_mae: 0.0059\n",
      "Epoch 483/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1137e-05 - mae: 0.0036\n",
      "Epoch 483: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9965e-05 - mae: 0.0035 - val_loss: 3.8584e-05 - val_mae: 0.0051\n",
      "Epoch 484/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.8050e-05 - mae: 0.0034\n",
      "Epoch 484: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8893e-05 - mae: 0.0034 - val_loss: 4.0949e-05 - val_mae: 0.0053\n",
      "Epoch 485/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0053e-05 - mae: 0.0036\n",
      "Epoch 485: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0849e-05 - mae: 0.0036 - val_loss: 4.8499e-05 - val_mae: 0.0056\n",
      "Epoch 486/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8003e-05 - mae: 0.0034\n",
      "Epoch 486: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9985e-05 - mae: 0.0036 - val_loss: 3.8576e-05 - val_mae: 0.0051\n",
      "Epoch 487/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9150e-05 - mae: 0.0035\n",
      "Epoch 487: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9741e-05 - mae: 0.0035 - val_loss: 4.1467e-05 - val_mae: 0.0052\n",
      "Epoch 488/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7894e-05 - mae: 0.0033\n",
      "Epoch 488: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8456e-05 - mae: 0.0034 - val_loss: 3.8274e-05 - val_mae: 0.0051\n",
      "Epoch 489/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9612e-05 - mae: 0.0035\n",
      "Epoch 489: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0678e-05 - mae: 0.0036 - val_loss: 3.8318e-05 - val_mae: 0.0051\n",
      "Epoch 490/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.3049e-05 - mae: 0.0038\n",
      "Epoch 490: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.1627e-05 - mae: 0.0037 - val_loss: 4.3316e-05 - val_mae: 0.0054\n",
      "Epoch 491/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0193e-05 - mae: 0.0036\n",
      "Epoch 491: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.0359e-05 - mae: 0.0036 - val_loss: 4.7967e-05 - val_mae: 0.0056\n",
      "Epoch 492/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1121e-05 - mae: 0.0037\n",
      "Epoch 492: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1121e-05 - mae: 0.0037 - val_loss: 4.0622e-05 - val_mae: 0.0052\n",
      "Epoch 493/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0550e-05 - mae: 0.0036\n",
      "Epoch 493: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.1063e-05 - mae: 0.0036 - val_loss: 4.3374e-05 - val_mae: 0.0053\n",
      "Epoch 494/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9730e-05 - mae: 0.0035\n",
      "Epoch 494: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8414e-05 - mae: 0.0034 - val_loss: 3.9518e-05 - val_mae: 0.0052\n",
      "Epoch 495/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6464e-05 - mae: 0.0032\n",
      "Epoch 495: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8574e-05 - mae: 0.0034 - val_loss: 4.4714e-05 - val_mae: 0.0055\n",
      "Epoch 496/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1451e-05 - mae: 0.0037\n",
      "Epoch 496: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0165e-05 - mae: 0.0036 - val_loss: 3.8382e-05 - val_mae: 0.0051\n",
      "Epoch 497/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8873e-05 - mae: 0.0035\n",
      "Epoch 497: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8269e-05 - mae: 0.0034 - val_loss: 3.8225e-05 - val_mae: 0.0051\n",
      "Epoch 498/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0050e-05 - mae: 0.0035\n",
      "Epoch 498: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 2.0640e-05 - mae: 0.0036 - val_loss: 4.2851e-05 - val_mae: 0.0054\n",
      "Epoch 499/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.8694e-05 - mae: 0.0034\n",
      "Epoch 499: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.8507e-05 - mae: 0.0034 - val_loss: 3.8811e-05 - val_mae: 0.0051\n",
      "Epoch 500/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.6427e-05 - mae: 0.0033\n",
      "Epoch 500: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.9026e-05 - mae: 0.0035 - val_loss: 3.8328e-05 - val_mae: 0.0051\n",
      "Epoch 501/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.9014e-05 - mae: 0.0035\n",
      "Epoch 501: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 2.0455e-05 - mae: 0.0036 - val_loss: 4.3657e-05 - val_mae: 0.0053\n",
      "Epoch 502/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 2.3877e-05 - mae: 0.0040\n",
      "Epoch 502: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 2.2706e-05 - mae: 0.0038 - val_loss: 3.9333e-05 - val_mae: 0.0052\n",
      "Epoch 503/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.7671e-05 - mae: 0.0033\n",
      "Epoch 503: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.8739e-05 - mae: 0.0034 - val_loss: 4.5030e-05 - val_mae: 0.0055\n",
      "Epoch 504/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3136e-05 - mae: 0.0038\n",
      "Epoch 504: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0660e-05 - mae: 0.0036 - val_loss: 4.1451e-05 - val_mae: 0.0053\n",
      "Epoch 505/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.8915e-05 - mae: 0.0034\n",
      "Epoch 505: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.9050e-05 - mae: 0.0034 - val_loss: 4.1701e-05 - val_mae: 0.0053\n",
      "Epoch 506/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.7931e-05 - mae: 0.0034\n",
      "Epoch 506: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.8526e-05 - mae: 0.0034 - val_loss: 4.8494e-05 - val_mae: 0.0056\n",
      "Epoch 507/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.1846e-05 - mae: 0.0038\n",
      "Epoch 507: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 2.0929e-05 - mae: 0.0036 - val_loss: 3.9737e-05 - val_mae: 0.0052\n",
      "Epoch 508/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8548e-05 - mae: 0.0034\n",
      "Epoch 508: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.9026e-05 - mae: 0.0035 - val_loss: 5.3901e-05 - val_mae: 0.0059\n",
      "Epoch 509/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.1380e-05 - mae: 0.0037\n",
      "Epoch 509: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 2.0947e-05 - mae: 0.0037 - val_loss: 3.9303e-05 - val_mae: 0.0051\n",
      "Epoch 510/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/800 [==================>...........] - ETA: 0s - loss: 2.1006e-05 - mae: 0.0036\n",
      "Epoch 510: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 2.0198e-05 - mae: 0.0036 - val_loss: 3.8651e-05 - val_mae: 0.0051\n",
      "Epoch 511/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.8020e-05 - mae: 0.0034\n",
      "Epoch 511: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.8598e-05 - mae: 0.0035 - val_loss: 4.3755e-05 - val_mae: 0.0054\n",
      "Epoch 512/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8023e-05 - mae: 0.0034\n",
      "Epoch 512: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8874e-05 - mae: 0.0034 - val_loss: 4.2227e-05 - val_mae: 0.0053\n",
      "Epoch 513/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8369e-05 - mae: 0.0034\n",
      "Epoch 513: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8813e-05 - mae: 0.0034 - val_loss: 4.0375e-05 - val_mae: 0.0052\n",
      "Epoch 514/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9203e-05 - mae: 0.0035\n",
      "Epoch 514: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9183e-05 - mae: 0.0035 - val_loss: 3.9515e-05 - val_mae: 0.0051\n",
      "Epoch 515/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6822e-05 - mae: 0.0033\n",
      "Epoch 515: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0331e-05 - mae: 0.0036 - val_loss: 3.7966e-05 - val_mae: 0.0050\n",
      "Epoch 516/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8093e-05 - mae: 0.0034\n",
      "Epoch 516: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9370e-05 - mae: 0.0035 - val_loss: 3.8896e-05 - val_mae: 0.0051\n",
      "Epoch 517/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6544e-05 - mae: 0.0032\n",
      "Epoch 517: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0111e-05 - mae: 0.0035 - val_loss: 3.8672e-05 - val_mae: 0.0051\n",
      "Epoch 518/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8396e-05 - mae: 0.0034\n",
      "Epoch 518: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8215e-05 - mae: 0.0033 - val_loss: 3.8680e-05 - val_mae: 0.0051\n",
      "Epoch 519/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7388e-05 - mae: 0.0033\n",
      "Epoch 519: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9656e-05 - mae: 0.0035 - val_loss: 3.8241e-05 - val_mae: 0.0051\n",
      "Epoch 520/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7348e-05 - mae: 0.0033\n",
      "Epoch 520: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7764e-05 - mae: 0.0033 - val_loss: 3.8405e-05 - val_mae: 0.0051\n",
      "Epoch 521/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8111e-05 - mae: 0.0034\n",
      "Epoch 521: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8111e-05 - mae: 0.0034 - val_loss: 3.8249e-05 - val_mae: 0.0051\n",
      "Epoch 522/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7563e-05 - mae: 0.0033\n",
      "Epoch 522: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8357e-05 - mae: 0.0034 - val_loss: 3.9847e-05 - val_mae: 0.0052\n",
      "Epoch 523/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8503e-05 - mae: 0.0034\n",
      "Epoch 523: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8445e-05 - mae: 0.0034 - val_loss: 4.0495e-05 - val_mae: 0.0052\n",
      "Epoch 524/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7424e-05 - mae: 0.0033\n",
      "Epoch 524: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8164e-05 - mae: 0.0033 - val_loss: 4.3597e-05 - val_mae: 0.0054\n",
      "Epoch 525/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9904e-05 - mae: 0.0035\n",
      "Epoch 525: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9142e-05 - mae: 0.0035 - val_loss: 3.8276e-05 - val_mae: 0.0051\n",
      "Epoch 526/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9967e-05 - mae: 0.0035\n",
      "Epoch 526: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9222e-05 - mae: 0.0035 - val_loss: 4.0132e-05 - val_mae: 0.0052\n",
      "Epoch 527/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0799e-05 - mae: 0.0036\n",
      "Epoch 527: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.0826e-05 - mae: 0.0036 - val_loss: 3.8334e-05 - val_mae: 0.0051\n",
      "Epoch 528/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8887e-05 - mae: 0.0035\n",
      "Epoch 528: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9207e-05 - mae: 0.0035 - val_loss: 3.9098e-05 - val_mae: 0.0051\n",
      "Epoch 529/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1769e-05 - mae: 0.0037\n",
      "Epoch 529: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0952e-05 - mae: 0.0037 - val_loss: 3.8251e-05 - val_mae: 0.0051\n",
      "Epoch 530/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8558e-05 - mae: 0.0034\n",
      "Epoch 530: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8063e-05 - mae: 0.0034 - val_loss: 3.9806e-05 - val_mae: 0.0052\n",
      "Epoch 531/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7078e-05 - mae: 0.0033\n",
      "Epoch 531: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7929e-05 - mae: 0.0034 - val_loss: 3.9853e-05 - val_mae: 0.0052\n",
      "Epoch 532/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9343e-05 - mae: 0.0035\n",
      "Epoch 532: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8761e-05 - mae: 0.0035 - val_loss: 5.3930e-05 - val_mae: 0.0059\n",
      "Epoch 533/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.9609e-05 - mae: 0.0044\n",
      "Epoch 533: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.4274e-05 - mae: 0.0039 - val_loss: 4.2471e-05 - val_mae: 0.0053\n",
      "Epoch 534/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8991e-05 - mae: 0.0035\n",
      "Epoch 534: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8887e-05 - mae: 0.0035 - val_loss: 4.0686e-05 - val_mae: 0.0052\n",
      "Epoch 535/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9187e-05 - mae: 0.0034\n",
      "Epoch 535: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1527e-05 - mae: 0.0037 - val_loss: 4.6364e-05 - val_mae: 0.0055\n",
      "Epoch 536/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8212e-05 - mae: 0.0034\n",
      "Epoch 536: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8630e-05 - mae: 0.0034 - val_loss: 3.8807e-05 - val_mae: 0.0051\n",
      "Epoch 537/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.2638e-05 - mae: 0.0038\n",
      "Epoch 537: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0133e-05 - mae: 0.0036 - val_loss: 4.2190e-05 - val_mae: 0.0053\n",
      "Epoch 538/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8591e-05 - mae: 0.0034\n",
      "Epoch 538: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8441e-05 - mae: 0.0034 - val_loss: 4.0287e-05 - val_mae: 0.0052\n",
      "Epoch 539/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/800 [=============>................] - ETA: 0s - loss: 1.6611e-05 - mae: 0.0033\n",
      "Epoch 539: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8592e-05 - mae: 0.0034 - val_loss: 4.5722e-05 - val_mae: 0.0054\n",
      "Epoch 540/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9981e-05 - mae: 0.0036\n",
      "Epoch 540: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.0128e-05 - mae: 0.0036 - val_loss: 4.0629e-05 - val_mae: 0.0052\n",
      "Epoch 541/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0521e-05 - mae: 0.0037\n",
      "Epoch 541: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.1457e-05 - mae: 0.0037 - val_loss: 3.8629e-05 - val_mae: 0.0051\n",
      "Epoch 542/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9938e-05 - mae: 0.0035\n",
      "Epoch 542: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8962e-05 - mae: 0.0035 - val_loss: 4.9430e-05 - val_mae: 0.0057\n",
      "Epoch 543/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.1634e-05 - mae: 0.0037\n",
      "Epoch 543: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.1605e-05 - mae: 0.0037 - val_loss: 4.3928e-05 - val_mae: 0.0054\n",
      "Epoch 544/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8074e-05 - mae: 0.0034\n",
      "Epoch 544: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8031e-05 - mae: 0.0034 - val_loss: 3.8505e-05 - val_mae: 0.0051\n",
      "Epoch 545/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0612e-05 - mae: 0.0036\n",
      "Epoch 545: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0612e-05 - mae: 0.0036 - val_loss: 3.8898e-05 - val_mae: 0.0051\n",
      "Epoch 546/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7664e-05 - mae: 0.0033\n",
      "Epoch 546: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7998e-05 - mae: 0.0034 - val_loss: 3.8452e-05 - val_mae: 0.0051\n",
      "Epoch 547/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6637e-05 - mae: 0.0032\n",
      "Epoch 547: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7849e-05 - mae: 0.0033 - val_loss: 3.8601e-05 - val_mae: 0.0051\n",
      "Epoch 548/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9186e-05 - mae: 0.0034\n",
      "Epoch 548: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9201e-05 - mae: 0.0034 - val_loss: 3.9975e-05 - val_mae: 0.0052\n",
      "Epoch 549/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9835e-05 - mae: 0.0035\n",
      "Epoch 549: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9698e-05 - mae: 0.0035 - val_loss: 4.3568e-05 - val_mae: 0.0054\n",
      "Epoch 550/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9648e-05 - mae: 0.0035\n",
      "Epoch 550: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9393e-05 - mae: 0.0035 - val_loss: 3.8434e-05 - val_mae: 0.0051\n",
      "Epoch 551/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8962e-05 - mae: 0.0035\n",
      "Epoch 551: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9809e-05 - mae: 0.0035 - val_loss: 3.8919e-05 - val_mae: 0.0051\n",
      "Epoch 552/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8572e-05 - mae: 0.0034\n",
      "Epoch 552: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8263e-05 - mae: 0.0034 - val_loss: 4.5919e-05 - val_mae: 0.0054\n",
      "Epoch 553/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.8388e-05 - mae: 0.0043\n",
      "Epoch 553: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.7188e-05 - mae: 0.0042 - val_loss: 4.2202e-05 - val_mae: 0.0053\n",
      "Epoch 554/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9043e-05 - mae: 0.0035\n",
      "Epoch 554: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0247e-05 - mae: 0.0036 - val_loss: 4.0577e-05 - val_mae: 0.0052\n",
      "Epoch 555/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9003e-05 - mae: 0.0035\n",
      "Epoch 555: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8130e-05 - mae: 0.0034 - val_loss: 4.1373e-05 - val_mae: 0.0053\n",
      "Epoch 556/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7865e-05 - mae: 0.0034\n",
      "Epoch 556: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8205e-05 - mae: 0.0034 - val_loss: 4.0090e-05 - val_mae: 0.0052\n",
      "Epoch 557/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9557e-05 - mae: 0.0035\n",
      "Epoch 557: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9417e-05 - mae: 0.0035 - val_loss: 4.4365e-05 - val_mae: 0.0054\n",
      "Epoch 558/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9102e-05 - mae: 0.0035\n",
      "Epoch 558: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9252e-05 - mae: 0.0035 - val_loss: 3.8360e-05 - val_mae: 0.0051\n",
      "Epoch 559/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7387e-05 - mae: 0.0033\n",
      "Epoch 559: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9008e-05 - mae: 0.0035 - val_loss: 4.0254e-05 - val_mae: 0.0052\n",
      "Epoch 560/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5463e-05 - mae: 0.0032\n",
      "Epoch 560: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8601e-05 - mae: 0.0034 - val_loss: 3.8224e-05 - val_mae: 0.0051\n",
      "Epoch 561/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8843e-05 - mae: 0.0035\n",
      "Epoch 561: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9310e-05 - mae: 0.0035 - val_loss: 3.8393e-05 - val_mae: 0.0051\n",
      "Epoch 562/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8413e-05 - mae: 0.0034\n",
      "Epoch 562: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9612e-05 - mae: 0.0035 - val_loss: 4.0302e-05 - val_mae: 0.0052\n",
      "Epoch 563/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1319e-05 - mae: 0.0036\n",
      "Epoch 563: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 2.1446e-05 - mae: 0.0037 - val_loss: 4.3042e-05 - val_mae: 0.0054\n",
      "Epoch 564/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.9676e-05 - mae: 0.0035\n",
      "Epoch 564: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 1.8990e-05 - mae: 0.0035 - val_loss: 3.9141e-05 - val_mae: 0.0051\n",
      "Epoch 565/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8414e-05 - mae: 0.0034\n",
      "Epoch 565: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7637e-05 - mae: 0.0033 - val_loss: 3.9682e-05 - val_mae: 0.0051\n",
      "Epoch 566/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8410e-05 - mae: 0.0034\n",
      "Epoch 566: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.8776e-05 - mae: 0.0035 - val_loss: 4.1935e-05 - val_mae: 0.0053\n",
      "Epoch 567/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6656e-05 - mae: 0.0033\n",
      "Epoch 567: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.7320e-05 - mae: 0.0033 - val_loss: 3.8081e-05 - val_mae: 0.0051\n",
      "Epoch 568/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/800 [=================>............] - ETA: 0s - loss: 1.9059e-05 - mae: 0.0034\n",
      "Epoch 568: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.9580e-05 - mae: 0.0035 - val_loss: 4.0059e-05 - val_mae: 0.0051\n",
      "Epoch 569/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8982e-05 - mae: 0.0035\n",
      "Epoch 569: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.9195e-05 - mae: 0.0035 - val_loss: 3.8408e-05 - val_mae: 0.0051\n",
      "Epoch 570/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7979e-05 - mae: 0.0034\n",
      "Epoch 570: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8338e-05 - mae: 0.0034 - val_loss: 3.9110e-05 - val_mae: 0.0051\n",
      "Epoch 571/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8901e-05 - mae: 0.0035\n",
      "Epoch 571: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 2.0261e-05 - mae: 0.0036 - val_loss: 4.1423e-05 - val_mae: 0.0053\n",
      "Epoch 572/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.9250e-05 - mae: 0.0035\n",
      "Epoch 572: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.9582e-05 - mae: 0.0035 - val_loss: 3.9846e-05 - val_mae: 0.0052\n",
      "Epoch 573/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.9261e-05 - mae: 0.0035\n",
      "Epoch 573: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 1.9633e-05 - mae: 0.0035 - val_loss: 3.8730e-05 - val_mae: 0.0051\n",
      "Epoch 574/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.7748e-05 - mae: 0.0033\n",
      "Epoch 574: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.7793e-05 - mae: 0.0033 - val_loss: 3.8251e-05 - val_mae: 0.0051\n",
      "Epoch 575/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9677e-05 - mae: 0.0035\n",
      "Epoch 575: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 2.0000e-05 - mae: 0.0036 - val_loss: 4.1873e-05 - val_mae: 0.0053\n",
      "Epoch 576/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8714e-05 - mae: 0.0035\n",
      "Epoch 576: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8679e-05 - mae: 0.0035 - val_loss: 3.9677e-05 - val_mae: 0.0052\n",
      "Epoch 577/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 1.6585e-05 - mae: 0.0032\n",
      "Epoch 577: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 1.7936e-05 - mae: 0.0033 - val_loss: 4.1307e-05 - val_mae: 0.0053\n",
      "Epoch 578/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8212e-05 - mae: 0.0033\n",
      "Epoch 578: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8381e-05 - mae: 0.0034 - val_loss: 4.0660e-05 - val_mae: 0.0052\n",
      "Epoch 579/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9331e-05 - mae: 0.0035\n",
      "Epoch 579: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9431e-05 - mae: 0.0035 - val_loss: 4.4797e-05 - val_mae: 0.0055\n",
      "Epoch 580/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7427e-05 - mae: 0.0033\n",
      "Epoch 580: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8345e-05 - mae: 0.0034 - val_loss: 3.8400e-05 - val_mae: 0.0051\n",
      "Epoch 581/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8377e-05 - mae: 0.0035\n",
      "Epoch 581: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8147e-05 - mae: 0.0034 - val_loss: 3.9217e-05 - val_mae: 0.0051\n",
      "Epoch 582/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8271e-05 - mae: 0.0034\n",
      "Epoch 582: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0337e-05 - mae: 0.0036 - val_loss: 4.2651e-05 - val_mae: 0.0053\n",
      "Epoch 583/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7354e-05 - mae: 0.0032\n",
      "Epoch 583: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7935e-05 - mae: 0.0033 - val_loss: 3.8360e-05 - val_mae: 0.0051\n",
      "Epoch 584/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7854e-05 - mae: 0.0034\n",
      "Epoch 584: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9861e-05 - mae: 0.0036 - val_loss: 3.8697e-05 - val_mae: 0.0051\n",
      "Epoch 585/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7901e-05 - mae: 0.0034\n",
      "Epoch 585: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9236e-05 - mae: 0.0035 - val_loss: 4.2235e-05 - val_mae: 0.0053\n",
      "Epoch 586/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0147e-05 - mae: 0.0036\n",
      "Epoch 586: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9961e-05 - mae: 0.0035 - val_loss: 4.1533e-05 - val_mae: 0.0053\n",
      "Epoch 587/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8278e-05 - mae: 0.0034\n",
      "Epoch 587: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8439e-05 - mae: 0.0034 - val_loss: 4.8258e-05 - val_mae: 0.0056\n",
      "Epoch 588/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8929e-05 - mae: 0.0035\n",
      "Epoch 588: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8649e-05 - mae: 0.0034 - val_loss: 4.0163e-05 - val_mae: 0.0052\n",
      "Epoch 589/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8865e-05 - mae: 0.0034\n",
      "Epoch 589: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8643e-05 - mae: 0.0034 - val_loss: 4.1334e-05 - val_mae: 0.0053\n",
      "Epoch 590/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7360e-05 - mae: 0.0033\n",
      "Epoch 590: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9243e-05 - mae: 0.0035 - val_loss: 3.9620e-05 - val_mae: 0.0051\n",
      "Epoch 591/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5770e-05 - mae: 0.0031\n",
      "Epoch 591: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7156e-05 - mae: 0.0033 - val_loss: 3.8234e-05 - val_mae: 0.0051\n",
      "Epoch 592/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8026e-05 - mae: 0.0033\n",
      "Epoch 592: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0529e-05 - mae: 0.0036 - val_loss: 3.9788e-05 - val_mae: 0.0052\n",
      "Epoch 593/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9617e-05 - mae: 0.0035\n",
      "Epoch 593: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9663e-05 - mae: 0.0035 - val_loss: 4.3745e-05 - val_mae: 0.0053\n",
      "Epoch 594/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8990e-05 - mae: 0.0034\n",
      "Epoch 594: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0275e-05 - mae: 0.0036 - val_loss: 4.0757e-05 - val_mae: 0.0052\n",
      "Epoch 595/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9454e-05 - mae: 0.0035\n",
      "Epoch 595: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9610e-05 - mae: 0.0035 - val_loss: 3.8313e-05 - val_mae: 0.0051\n",
      "Epoch 596/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8047e-05 - mae: 0.0034\n",
      "Epoch 596: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9494e-05 - mae: 0.0035 - val_loss: 3.8481e-05 - val_mae: 0.0051\n",
      "Epoch 597/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.7006e-05 - mae: 0.0033\n",
      "Epoch 597: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7551e-05 - mae: 0.0034 - val_loss: 3.9101e-05 - val_mae: 0.0051\n",
      "Epoch 598/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8774e-05 - mae: 0.0035\n",
      "Epoch 598: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8348e-05 - mae: 0.0034 - val_loss: 3.8272e-05 - val_mae: 0.0051\n",
      "Epoch 599/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9445e-05 - mae: 0.0035\n",
      "Epoch 599: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8459e-05 - mae: 0.0034 - val_loss: 4.0753e-05 - val_mae: 0.0052\n",
      "Epoch 600/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6633e-05 - mae: 0.0033\n",
      "Epoch 600: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9346e-05 - mae: 0.0035 - val_loss: 3.7788e-05 - val_mae: 0.0050\n",
      "Epoch 601/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9438e-05 - mae: 0.0035\n",
      "Epoch 601: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9770e-05 - mae: 0.0035 - val_loss: 3.8202e-05 - val_mae: 0.0051\n",
      "Epoch 602/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9501e-05 - mae: 0.0035\n",
      "Epoch 602: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8689e-05 - mae: 0.0034 - val_loss: 3.9606e-05 - val_mae: 0.0052\n",
      "Epoch 603/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8166e-05 - mae: 0.0034\n",
      "Epoch 603: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8003e-05 - mae: 0.0034 - val_loss: 3.7876e-05 - val_mae: 0.0050\n",
      "Epoch 604/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6261e-05 - mae: 0.0032\n",
      "Epoch 604: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8689e-05 - mae: 0.0034 - val_loss: 3.9581e-05 - val_mae: 0.0051\n",
      "Epoch 605/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9153e-05 - mae: 0.0034\n",
      "Epoch 605: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9344e-05 - mae: 0.0035 - val_loss: 3.9370e-05 - val_mae: 0.0052\n",
      "Epoch 606/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8408e-05 - mae: 0.0034\n",
      "Epoch 606: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0122e-05 - mae: 0.0036 - val_loss: 4.3000e-05 - val_mae: 0.0054\n",
      "Epoch 607/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7436e-05 - mae: 0.0033\n",
      "Epoch 607: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7420e-05 - mae: 0.0034 - val_loss: 4.2880e-05 - val_mae: 0.0053\n",
      "Epoch 608/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7851e-05 - mae: 0.0034\n",
      "Epoch 608: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8360e-05 - mae: 0.0034 - val_loss: 3.8297e-05 - val_mae: 0.0051\n",
      "Epoch 609/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6212e-05 - mae: 0.0032\n",
      "Epoch 609: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9550e-05 - mae: 0.0035 - val_loss: 4.2040e-05 - val_mae: 0.0052\n",
      "Epoch 610/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0742e-05 - mae: 0.0036\n",
      "Epoch 610: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0189e-05 - mae: 0.0036 - val_loss: 3.7905e-05 - val_mae: 0.0050\n",
      "Epoch 611/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1290e-05 - mae: 0.0036\n",
      "Epoch 611: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.4452e-05 - mae: 0.0039 - val_loss: 5.2703e-05 - val_mae: 0.0059\n",
      "Epoch 612/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.3290e-05 - mae: 0.0039\n",
      "Epoch 612: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 2.2914e-05 - mae: 0.0038 - val_loss: 3.8114e-05 - val_mae: 0.0050\n",
      "Epoch 613/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7257e-05 - mae: 0.0033\n",
      "Epoch 613: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7856e-05 - mae: 0.0034 - val_loss: 5.4799e-05 - val_mae: 0.0060\n",
      "Epoch 614/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9705e-05 - mae: 0.0035\n",
      "Epoch 614: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0975e-05 - mae: 0.0036 - val_loss: 3.7647e-05 - val_mae: 0.0050\n",
      "Epoch 615/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9584e-05 - mae: 0.0035\n",
      "Epoch 615: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9358e-05 - mae: 0.0035 - val_loss: 3.8194e-05 - val_mae: 0.0050\n",
      "Epoch 616/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0395e-05 - mae: 0.0036\n",
      "Epoch 616: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0180e-05 - mae: 0.0036 - val_loss: 3.9297e-05 - val_mae: 0.0051\n",
      "Epoch 617/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8907e-05 - mae: 0.0035\n",
      "Epoch 617: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8389e-05 - mae: 0.0034 - val_loss: 3.8874e-05 - val_mae: 0.0051\n",
      "Epoch 618/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8346e-05 - mae: 0.0035\n",
      "Epoch 618: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8925e-05 - mae: 0.0035 - val_loss: 3.8747e-05 - val_mae: 0.0051\n",
      "Epoch 619/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8395e-05 - mae: 0.0034\n",
      "Epoch 619: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8504e-05 - mae: 0.0034 - val_loss: 5.1571e-05 - val_mae: 0.0058\n",
      "Epoch 620/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0189e-05 - mae: 0.0036\n",
      "Epoch 620: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0311e-05 - mae: 0.0036 - val_loss: 3.8643e-05 - val_mae: 0.0051\n",
      "Epoch 621/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8685e-05 - mae: 0.0035\n",
      "Epoch 621: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9286e-05 - mae: 0.0035 - val_loss: 3.8539e-05 - val_mae: 0.0051\n",
      "Epoch 622/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9882e-05 - mae: 0.0036\n",
      "Epoch 622: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8189e-05 - mae: 0.0034 - val_loss: 4.1314e-05 - val_mae: 0.0053\n",
      "Epoch 623/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7111e-05 - mae: 0.0032\n",
      "Epoch 623: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7648e-05 - mae: 0.0033 - val_loss: 3.8352e-05 - val_mae: 0.0051\n",
      "Epoch 624/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6303e-05 - mae: 0.0032\n",
      "Epoch 624: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7522e-05 - mae: 0.0034 - val_loss: 3.8511e-05 - val_mae: 0.0051\n",
      "Epoch 625/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8146e-05 - mae: 0.0033\n",
      "Epoch 625: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8341e-05 - mae: 0.0034 - val_loss: 3.9548e-05 - val_mae: 0.0051\n",
      "Epoch 626/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7539e-05 - mae: 0.0032\n",
      "Epoch 626: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0714e-05 - mae: 0.0036 - val_loss: 4.3865e-05 - val_mae: 0.0053\n",
      "Epoch 627/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.1116e-05 - mae: 0.0037\n",
      "Epoch 627: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.0343e-05 - mae: 0.0036 - val_loss: 4.2249e-05 - val_mae: 0.0053\n",
      "Epoch 628/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5781e-05 - mae: 0.0031\n",
      "Epoch 628: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9732e-05 - mae: 0.0035 - val_loss: 4.2531e-05 - val_mae: 0.0053\n",
      "Epoch 629/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7545e-05 - mae: 0.0034\n",
      "Epoch 629: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7872e-05 - mae: 0.0034 - val_loss: 4.2176e-05 - val_mae: 0.0053\n",
      "Epoch 630/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3306e-05 - mae: 0.0038\n",
      "Epoch 630: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.1705e-05 - mae: 0.0038 - val_loss: 4.0018e-05 - val_mae: 0.0051\n",
      "Epoch 631/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7702e-05 - mae: 0.0034\n",
      "Epoch 631: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8941e-05 - mae: 0.0035 - val_loss: 3.8457e-05 - val_mae: 0.0051\n",
      "Epoch 632/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8333e-05 - mae: 0.0034\n",
      "Epoch 632: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8369e-05 - mae: 0.0034 - val_loss: 4.0777e-05 - val_mae: 0.0052\n",
      "Epoch 633/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9476e-05 - mae: 0.0036\n",
      "Epoch 633: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8542e-05 - mae: 0.0035 - val_loss: 3.8247e-05 - val_mae: 0.0051\n",
      "Epoch 634/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8012e-05 - mae: 0.0034\n",
      "Epoch 634: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0710e-05 - mae: 0.0036 - val_loss: 4.6007e-05 - val_mae: 0.0055\n",
      "Epoch 635/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.4008e-05 - mae: 0.0039\n",
      "Epoch 635: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.2346e-05 - mae: 0.0038 - val_loss: 4.1635e-05 - val_mae: 0.0052\n",
      "Epoch 636/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8052e-05 - mae: 0.0033\n",
      "Epoch 636: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8193e-05 - mae: 0.0033 - val_loss: 4.0801e-05 - val_mae: 0.0052\n",
      "Epoch 637/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9409e-05 - mae: 0.0035\n",
      "Epoch 637: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9409e-05 - mae: 0.0035 - val_loss: 4.4434e-05 - val_mae: 0.0054\n",
      "Epoch 638/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0499e-05 - mae: 0.0036\n",
      "Epoch 638: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 2.1210e-05 - mae: 0.0037 - val_loss: 3.8144e-05 - val_mae: 0.0051\n",
      "Epoch 639/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6841e-05 - mae: 0.0034\n",
      "Epoch 639: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7686e-05 - mae: 0.0034 - val_loss: 4.1237e-05 - val_mae: 0.0053\n",
      "Epoch 640/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7903e-05 - mae: 0.0033\n",
      "Epoch 640: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7692e-05 - mae: 0.0033 - val_loss: 4.2383e-05 - val_mae: 0.0053\n",
      "Epoch 641/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9858e-05 - mae: 0.0035\n",
      "Epoch 641: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9187e-05 - mae: 0.0034 - val_loss: 3.8048e-05 - val_mae: 0.0051\n",
      "Epoch 642/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8663e-05 - mae: 0.0034\n",
      "Epoch 642: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8155e-05 - mae: 0.0034 - val_loss: 3.8026e-05 - val_mae: 0.0051\n",
      "Epoch 643/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0446e-05 - mae: 0.0036\n",
      "Epoch 643: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0517e-05 - mae: 0.0036 - val_loss: 3.9990e-05 - val_mae: 0.0052\n",
      "Epoch 644/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8865e-05 - mae: 0.0034\n",
      "Epoch 644: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8675e-05 - mae: 0.0034 - val_loss: 3.9914e-05 - val_mae: 0.0052\n",
      "Epoch 645/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8716e-05 - mae: 0.0034\n",
      "Epoch 645: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8630e-05 - mae: 0.0034 - val_loss: 3.9610e-05 - val_mae: 0.0052\n",
      "Epoch 646/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8670e-05 - mae: 0.0035\n",
      "Epoch 646: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8838e-05 - mae: 0.0035 - val_loss: 3.8405e-05 - val_mae: 0.0051\n",
      "Epoch 647/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9575e-05 - mae: 0.0035\n",
      "Epoch 647: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8656e-05 - mae: 0.0034 - val_loss: 3.9941e-05 - val_mae: 0.0052\n",
      "Epoch 648/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.1328e-05 - mae: 0.0037\n",
      "Epoch 648: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.1274e-05 - mae: 0.0037 - val_loss: 4.1633e-05 - val_mae: 0.0052\n",
      "Epoch 649/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8205e-05 - mae: 0.0034\n",
      "Epoch 649: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8703e-05 - mae: 0.0035 - val_loss: 3.8263e-05 - val_mae: 0.0051\n",
      "Epoch 650/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.6285e-05 - mae: 0.0033\n",
      "Epoch 650: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7615e-05 - mae: 0.0034 - val_loss: 3.7944e-05 - val_mae: 0.0051\n",
      "Epoch 651/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9449e-05 - mae: 0.0035\n",
      "Epoch 651: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9374e-05 - mae: 0.0035 - val_loss: 3.9710e-05 - val_mae: 0.0052\n",
      "Epoch 652/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9965e-05 - mae: 0.0036\n",
      "Epoch 652: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9128e-05 - mae: 0.0035 - val_loss: 4.1407e-05 - val_mae: 0.0053\n",
      "Epoch 653/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0542e-05 - mae: 0.0036\n",
      "Epoch 653: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.1015e-05 - mae: 0.0036 - val_loss: 4.3618e-05 - val_mae: 0.0054\n",
      "Epoch 654/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.2932e-05 - mae: 0.0038\n",
      "Epoch 654: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.2844e-05 - mae: 0.0038 - val_loss: 3.8263e-05 - val_mae: 0.0051\n",
      "Epoch 655/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9070e-05 - mae: 0.0035\n",
      "Epoch 655: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9099e-05 - mae: 0.0035 - val_loss: 4.1967e-05 - val_mae: 0.0053\n",
      "Epoch 656/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7812e-05 - mae: 0.0033\n",
      "Epoch 656: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8902e-05 - mae: 0.0034 - val_loss: 3.8339e-05 - val_mae: 0.0051\n",
      "Epoch 657/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7339e-05 - mae: 0.0033\n",
      "Epoch 657: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9234e-05 - mae: 0.0035 - val_loss: 4.0423e-05 - val_mae: 0.0052\n",
      "Epoch 658/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8929e-05 - mae: 0.0035\n",
      "Epoch 658: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9105e-05 - mae: 0.0035 - val_loss: 3.8095e-05 - val_mae: 0.0051\n",
      "Epoch 659/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6556e-05 - mae: 0.0032\n",
      "Epoch 659: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9609e-05 - mae: 0.0035 - val_loss: 3.9948e-05 - val_mae: 0.0052\n",
      "Epoch 660/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1797e-05 - mae: 0.0037\n",
      "Epoch 660: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.1155e-05 - mae: 0.0037 - val_loss: 4.5037e-05 - val_mae: 0.0055\n",
      "Epoch 661/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6817e-05 - mae: 0.0033\n",
      "Epoch 661: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8889e-05 - mae: 0.0034 - val_loss: 4.1296e-05 - val_mae: 0.0053\n",
      "Epoch 662/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9022e-05 - mae: 0.0035\n",
      "Epoch 662: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8884e-05 - mae: 0.0035 - val_loss: 3.9734e-05 - val_mae: 0.0052\n",
      "Epoch 663/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8304e-05 - mae: 0.0034\n",
      "Epoch 663: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8304e-05 - mae: 0.0034 - val_loss: 4.2972e-05 - val_mae: 0.0053\n",
      "Epoch 664/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.6047e-05 - mae: 0.0041\n",
      "Epoch 664: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.4053e-05 - mae: 0.0039 - val_loss: 3.9290e-05 - val_mae: 0.0052\n",
      "Epoch 665/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8841e-05 - mae: 0.0035\n",
      "Epoch 665: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9053e-05 - mae: 0.0035 - val_loss: 4.5920e-05 - val_mae: 0.0055\n",
      "Epoch 666/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9363e-05 - mae: 0.0035\n",
      "Epoch 666: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9963e-05 - mae: 0.0036 - val_loss: 3.8646e-05 - val_mae: 0.0051\n",
      "Epoch 667/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2713e-05 - mae: 0.0038\n",
      "Epoch 667: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.3795e-05 - mae: 0.0039 - val_loss: 4.0491e-05 - val_mae: 0.0053\n",
      "Epoch 668/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9941e-05 - mae: 0.0036\n",
      "Epoch 668: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8621e-05 - mae: 0.0034 - val_loss: 4.1791e-05 - val_mae: 0.0053\n",
      "Epoch 669/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9135e-05 - mae: 0.0035\n",
      "Epoch 669: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9006e-05 - mae: 0.0035 - val_loss: 4.0084e-05 - val_mae: 0.0052\n",
      "Epoch 670/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9616e-05 - mae: 0.0036\n",
      "Epoch 670: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9221e-05 - mae: 0.0035 - val_loss: 3.8568e-05 - val_mae: 0.0051\n",
      "Epoch 671/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7741e-05 - mae: 0.0033\n",
      "Epoch 671: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8108e-05 - mae: 0.0034 - val_loss: 3.8536e-05 - val_mae: 0.0051\n",
      "Epoch 672/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9967e-05 - mae: 0.0036\n",
      "Epoch 672: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9925e-05 - mae: 0.0036 - val_loss: 5.0957e-05 - val_mae: 0.0058\n",
      "Epoch 673/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9570e-05 - mae: 0.0035\n",
      "Epoch 673: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9473e-05 - mae: 0.0035 - val_loss: 3.8343e-05 - val_mae: 0.0051\n",
      "Epoch 674/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7911e-05 - mae: 0.0034\n",
      "Epoch 674: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7873e-05 - mae: 0.0034 - val_loss: 3.9575e-05 - val_mae: 0.0051\n",
      "Epoch 675/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7258e-05 - mae: 0.0033\n",
      "Epoch 675: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9335e-05 - mae: 0.0035 - val_loss: 4.0836e-05 - val_mae: 0.0053\n",
      "Epoch 676/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9565e-05 - mae: 0.0035\n",
      "Epoch 676: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9309e-05 - mae: 0.0035 - val_loss: 3.8600e-05 - val_mae: 0.0051\n",
      "Epoch 677/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7356e-05 - mae: 0.0033\n",
      "Epoch 677: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7841e-05 - mae: 0.0033 - val_loss: 3.8439e-05 - val_mae: 0.0051\n",
      "Epoch 678/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9116e-05 - mae: 0.0035\n",
      "Epoch 678: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9063e-05 - mae: 0.0035 - val_loss: 4.5030e-05 - val_mae: 0.0055\n",
      "Epoch 679/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8401e-05 - mae: 0.0034\n",
      "Epoch 679: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8214e-05 - mae: 0.0034 - val_loss: 3.8461e-05 - val_mae: 0.0051\n",
      "Epoch 680/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8587e-05 - mae: 0.0035\n",
      "Epoch 680: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8587e-05 - mae: 0.0035 - val_loss: 3.8179e-05 - val_mae: 0.0051\n",
      "Epoch 681/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0293e-05 - mae: 0.0036\n",
      "Epoch 681: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0138e-05 - mae: 0.0036 - val_loss: 4.4018e-05 - val_mae: 0.0054\n",
      "Epoch 682/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0443e-05 - mae: 0.0036\n",
      "Epoch 682: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9740e-05 - mae: 0.0035 - val_loss: 3.9153e-05 - val_mae: 0.0051\n",
      "Epoch 683/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8059e-05 - mae: 0.0034\n",
      "Epoch 683: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8068e-05 - mae: 0.0034 - val_loss: 3.9156e-05 - val_mae: 0.0052\n",
      "Epoch 684/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.8244e-05 - mae: 0.0034\n",
      "Epoch 684: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8174e-05 - mae: 0.0034 - val_loss: 3.8621e-05 - val_mae: 0.0051\n",
      "Epoch 685/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7777e-05 - mae: 0.0034\n",
      "Epoch 685: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8410e-05 - mae: 0.0034 - val_loss: 4.1330e-05 - val_mae: 0.0053\n",
      "Epoch 686/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7325e-05 - mae: 0.0033\n",
      "Epoch 686: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8224e-05 - mae: 0.0034 - val_loss: 4.0331e-05 - val_mae: 0.0052\n",
      "Epoch 687/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8575e-05 - mae: 0.0035\n",
      "Epoch 687: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8844e-05 - mae: 0.0034 - val_loss: 3.8392e-05 - val_mae: 0.0051\n",
      "Epoch 688/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8594e-05 - mae: 0.0034\n",
      "Epoch 688: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8598e-05 - mae: 0.0034 - val_loss: 4.2735e-05 - val_mae: 0.0054\n",
      "Epoch 689/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9081e-05 - mae: 0.0035\n",
      "Epoch 689: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9081e-05 - mae: 0.0035 - val_loss: 4.3049e-05 - val_mae: 0.0053\n",
      "Epoch 690/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9049e-05 - mae: 0.0034\n",
      "Epoch 690: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9022e-05 - mae: 0.0034 - val_loss: 4.0057e-05 - val_mae: 0.0052\n",
      "Epoch 691/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6876e-05 - mae: 0.0033\n",
      "Epoch 691: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8841e-05 - mae: 0.0034 - val_loss: 4.0572e-05 - val_mae: 0.0053\n",
      "Epoch 692/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8820e-05 - mae: 0.0035\n",
      "Epoch 692: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9414e-05 - mae: 0.0035 - val_loss: 3.9153e-05 - val_mae: 0.0051\n",
      "Epoch 693/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8029e-05 - mae: 0.0033\n",
      "Epoch 693: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8202e-05 - mae: 0.0034 - val_loss: 3.8588e-05 - val_mae: 0.0051\n",
      "Epoch 694/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8271e-05 - mae: 0.0034\n",
      "Epoch 694: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8257e-05 - mae: 0.0034 - val_loss: 3.8635e-05 - val_mae: 0.0051\n",
      "Epoch 695/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7746e-05 - mae: 0.0033\n",
      "Epoch 695: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9010e-05 - mae: 0.0035 - val_loss: 4.0516e-05 - val_mae: 0.0053\n",
      "Epoch 696/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9597e-05 - mae: 0.0036\n",
      "Epoch 696: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8573e-05 - mae: 0.0034 - val_loss: 4.0527e-05 - val_mae: 0.0052\n",
      "Epoch 697/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7728e-05 - mae: 0.0033\n",
      "Epoch 697: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9017e-05 - mae: 0.0035 - val_loss: 3.9855e-05 - val_mae: 0.0051\n",
      "Epoch 698/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9803e-05 - mae: 0.0035\n",
      "Epoch 698: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.3134e-05 - mae: 0.0038 - val_loss: 5.3949e-05 - val_mae: 0.0059\n",
      "Epoch 699/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0025e-05 - mae: 0.0036\n",
      "Epoch 699: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9219e-05 - mae: 0.0035 - val_loss: 3.8183e-05 - val_mae: 0.0051\n",
      "Epoch 700/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9323e-05 - mae: 0.0035\n",
      "Epoch 700: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8681e-05 - mae: 0.0035 - val_loss: 4.6212e-05 - val_mae: 0.0054\n",
      "Epoch 701/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0005e-05 - mae: 0.0036\n",
      "Epoch 701: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9211e-05 - mae: 0.0035 - val_loss: 4.4087e-05 - val_mae: 0.0054\n",
      "Epoch 702/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7938e-05 - mae: 0.0034\n",
      "Epoch 702: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9660e-05 - mae: 0.0036 - val_loss: 5.4500e-05 - val_mae: 0.0060\n",
      "Epoch 703/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9608e-05 - mae: 0.0035\n",
      "Epoch 703: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8665e-05 - mae: 0.0034 - val_loss: 4.6183e-05 - val_mae: 0.0055\n",
      "Epoch 704/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0237e-05 - mae: 0.0036\n",
      "Epoch 704: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.0017e-05 - mae: 0.0036 - val_loss: 3.9674e-05 - val_mae: 0.0052\n",
      "Epoch 705/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7192e-05 - mae: 0.0033\n",
      "Epoch 705: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8267e-05 - mae: 0.0034 - val_loss: 3.8704e-05 - val_mae: 0.0051\n",
      "Epoch 706/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7711e-05 - mae: 0.0033\n",
      "Epoch 706: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.8969e-05 - mae: 0.0035 - val_loss: 3.7913e-05 - val_mae: 0.0050\n",
      "Epoch 707/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8528e-05 - mae: 0.0034\n",
      "Epoch 707: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.9050e-05 - mae: 0.0035 - val_loss: 4.0966e-05 - val_mae: 0.0052\n",
      "Epoch 708/1000\n",
      "620/800 [======================>.......] - ETA: 0s - loss: 1.7312e-05 - mae: 0.0033\n",
      "Epoch 708: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.8179e-05 - mae: 0.0034 - val_loss: 4.2319e-05 - val_mae: 0.0053\n",
      "Epoch 709/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.9318e-05 - mae: 0.0035\n",
      "Epoch 709: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.8763e-05 - mae: 0.0034 - val_loss: 4.3989e-05 - val_mae: 0.0053\n",
      "Epoch 710/1000\n",
      "610/800 [=====================>........] - ETA: 0s - loss: 1.7948e-05 - mae: 0.0034\n",
      "Epoch 710: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.7927e-05 - mae: 0.0034 - val_loss: 4.0356e-05 - val_mae: 0.0052\n",
      "Epoch 711/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.9138e-05 - mae: 0.0034\n",
      "Epoch 711: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.8243e-05 - mae: 0.0034 - val_loss: 4.0983e-05 - val_mae: 0.0052\n",
      "Epoch 712/1000\n",
      "620/800 [======================>.......] - ETA: 0s - loss: 1.8978e-05 - mae: 0.0034\n",
      "Epoch 712: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.8756e-05 - mae: 0.0034 - val_loss: 4.0212e-05 - val_mae: 0.0052\n",
      "Epoch 713/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/800 [====================>.........] - ETA: 0s - loss: 1.9450e-05 - mae: 0.0035\n",
      "Epoch 713: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.9404e-05 - mae: 0.0035 - val_loss: 4.0103e-05 - val_mae: 0.0052\n",
      "Epoch 714/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8395e-05 - mae: 0.0034\n",
      "Epoch 714: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.9588e-05 - mae: 0.0035 - val_loss: 3.8513e-05 - val_mae: 0.0051\n",
      "Epoch 715/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8199e-05 - mae: 0.0034\n",
      "Epoch 715: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8104e-05 - mae: 0.0034 - val_loss: 4.5990e-05 - val_mae: 0.0055\n",
      "Epoch 716/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7856e-05 - mae: 0.0033\n",
      "Epoch 716: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9291e-05 - mae: 0.0035 - val_loss: 3.9769e-05 - val_mae: 0.0052\n",
      "Epoch 717/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7994e-05 - mae: 0.0033\n",
      "Epoch 717: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7892e-05 - mae: 0.0033 - val_loss: 3.8706e-05 - val_mae: 0.0051\n",
      "Epoch 718/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6856e-05 - mae: 0.0032\n",
      "Epoch 718: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7766e-05 - mae: 0.0033 - val_loss: 3.8110e-05 - val_mae: 0.0051\n",
      "Epoch 719/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0423e-05 - mae: 0.0036\n",
      "Epoch 719: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9474e-05 - mae: 0.0035 - val_loss: 3.8037e-05 - val_mae: 0.0051\n",
      "Epoch 720/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9277e-05 - mae: 0.0035\n",
      "Epoch 720: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9599e-05 - mae: 0.0035 - val_loss: 3.8496e-05 - val_mae: 0.0051\n",
      "Epoch 721/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8764e-05 - mae: 0.0034\n",
      "Epoch 721: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8764e-05 - mae: 0.0034 - val_loss: 3.8302e-05 - val_mae: 0.0051\n",
      "Epoch 722/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0539e-05 - mae: 0.0036\n",
      "Epoch 722: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9299e-05 - mae: 0.0035 - val_loss: 3.9466e-05 - val_mae: 0.0052\n",
      "Epoch 723/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0962e-05 - mae: 0.0036\n",
      "Epoch 723: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0756e-05 - mae: 0.0036 - val_loss: 4.3805e-05 - val_mae: 0.0054\n",
      "Epoch 724/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8043e-05 - mae: 0.0034\n",
      "Epoch 724: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8218e-05 - mae: 0.0034 - val_loss: 4.1062e-05 - val_mae: 0.0053\n",
      "Epoch 725/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8767e-05 - mae: 0.0034\n",
      "Epoch 725: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8933e-05 - mae: 0.0035 - val_loss: 4.0674e-05 - val_mae: 0.0052\n",
      "Epoch 726/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6635e-05 - mae: 0.0032\n",
      "Epoch 726: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8330e-05 - mae: 0.0034 - val_loss: 3.9688e-05 - val_mae: 0.0051\n",
      "Epoch 727/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8803e-05 - mae: 0.0035\n",
      "Epoch 727: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8392e-05 - mae: 0.0034 - val_loss: 4.0087e-05 - val_mae: 0.0052\n",
      "Epoch 728/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8515e-05 - mae: 0.0035\n",
      "Epoch 728: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9030e-05 - mae: 0.0035 - val_loss: 4.1258e-05 - val_mae: 0.0052\n",
      "Epoch 729/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9955e-05 - mae: 0.0035\n",
      "Epoch 729: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9084e-05 - mae: 0.0035 - val_loss: 3.9795e-05 - val_mae: 0.0051\n",
      "Epoch 730/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1702e-05 - mae: 0.0038\n",
      "Epoch 730: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0418e-05 - mae: 0.0036 - val_loss: 4.9156e-05 - val_mae: 0.0056\n",
      "Epoch 731/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1009e-05 - mae: 0.0036\n",
      "Epoch 731: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.1009e-05 - mae: 0.0036 - val_loss: 4.1482e-05 - val_mae: 0.0053\n",
      "Epoch 732/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8277e-05 - mae: 0.0034\n",
      "Epoch 732: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8271e-05 - mae: 0.0034 - val_loss: 4.0392e-05 - val_mae: 0.0052\n",
      "Epoch 733/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7203e-05 - mae: 0.0033\n",
      "Epoch 733: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8865e-05 - mae: 0.0034 - val_loss: 4.1895e-05 - val_mae: 0.0053\n",
      "Epoch 734/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9634e-05 - mae: 0.0035\n",
      "Epoch 734: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8127e-05 - mae: 0.0034 - val_loss: 3.8359e-05 - val_mae: 0.0051\n",
      "Epoch 735/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7007e-05 - mae: 0.0032\n",
      "Epoch 735: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7606e-05 - mae: 0.0033 - val_loss: 3.9081e-05 - val_mae: 0.0051\n",
      "Epoch 736/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.8291e-05 - mae: 0.0034\n",
      "Epoch 736: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9414e-05 - mae: 0.0035 - val_loss: 3.9266e-05 - val_mae: 0.0052\n",
      "Epoch 737/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8339e-05 - mae: 0.0034\n",
      "Epoch 737: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7818e-05 - mae: 0.0033 - val_loss: 3.8278e-05 - val_mae: 0.0051\n",
      "Epoch 738/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1016e-05 - mae: 0.0036\n",
      "Epoch 738: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.0588e-05 - mae: 0.0036 - val_loss: 3.9049e-05 - val_mae: 0.0051\n",
      "Epoch 739/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8045e-05 - mae: 0.0033\n",
      "Epoch 739: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9189e-05 - mae: 0.0035 - val_loss: 3.8515e-05 - val_mae: 0.0051\n",
      "Epoch 740/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0034e-05 - mae: 0.0035\n",
      "Epoch 740: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8663e-05 - mae: 0.0034 - val_loss: 3.9698e-05 - val_mae: 0.0051\n",
      "Epoch 741/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7431e-05 - mae: 0.0034\n",
      "Epoch 741: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.7509e-05 - mae: 0.0034 - val_loss: 3.8627e-05 - val_mae: 0.0051\n",
      "Epoch 742/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7497e-05 - mae: 0.0032\n",
      "Epoch 742: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8917e-05 - mae: 0.0034 - val_loss: 4.1722e-05 - val_mae: 0.0053\n",
      "Epoch 743/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8445e-05 - mae: 0.0035\n",
      "Epoch 743: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0670e-05 - mae: 0.0036 - val_loss: 3.8230e-05 - val_mae: 0.0051\n",
      "Epoch 744/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.8930e-05 - mae: 0.0035\n",
      "Epoch 744: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.9221e-05 - mae: 0.0035 - val_loss: 4.5623e-05 - val_mae: 0.0055\n",
      "Epoch 745/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9459e-05 - mae: 0.0035\n",
      "Epoch 745: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9973e-05 - mae: 0.0035 - val_loss: 4.0819e-05 - val_mae: 0.0053\n",
      "Epoch 746/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7419e-05 - mae: 0.0033\n",
      "Epoch 746: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.7857e-05 - mae: 0.0034 - val_loss: 3.8618e-05 - val_mae: 0.0051\n",
      "Epoch 747/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8876e-05 - mae: 0.0034\n",
      "Epoch 747: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8974e-05 - mae: 0.0035 - val_loss: 3.8353e-05 - val_mae: 0.0051\n",
      "Epoch 748/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8817e-05 - mae: 0.0034\n",
      "Epoch 748: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9334e-05 - mae: 0.0035 - val_loss: 3.8085e-05 - val_mae: 0.0051\n",
      "Epoch 749/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0358e-05 - mae: 0.0036\n",
      "Epoch 749: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0519e-05 - mae: 0.0036 - val_loss: 4.0110e-05 - val_mae: 0.0052\n",
      "Epoch 750/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9683e-05 - mae: 0.0035\n",
      "Epoch 750: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8230e-05 - mae: 0.0034 - val_loss: 4.2635e-05 - val_mae: 0.0053\n",
      "Epoch 751/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9422e-05 - mae: 0.0035\n",
      "Epoch 751: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9277e-05 - mae: 0.0035 - val_loss: 3.8484e-05 - val_mae: 0.0051\n",
      "Epoch 752/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.9401e-05 - mae: 0.0035\n",
      "Epoch 752: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 1.8697e-05 - mae: 0.0034 - val_loss: 4.1747e-05 - val_mae: 0.0053\n",
      "Epoch 753/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.0145e-05 - mae: 0.0036\n",
      "Epoch 753: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.9496e-05 - mae: 0.0035 - val_loss: 3.9157e-05 - val_mae: 0.0052\n",
      "Epoch 754/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7047e-05 - mae: 0.0033\n",
      "Epoch 754: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9139e-05 - mae: 0.0035 - val_loss: 4.4791e-05 - val_mae: 0.0054\n",
      "Epoch 755/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0417e-05 - mae: 0.0037\n",
      "Epoch 755: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.0454e-05 - mae: 0.0036 - val_loss: 3.8476e-05 - val_mae: 0.0051\n",
      "Epoch 756/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6945e-05 - mae: 0.0033\n",
      "Epoch 756: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8756e-05 - mae: 0.0034 - val_loss: 3.8221e-05 - val_mae: 0.0051\n",
      "Epoch 757/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0324e-05 - mae: 0.0035\n",
      "Epoch 757: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9661e-05 - mae: 0.0035 - val_loss: 3.8317e-05 - val_mae: 0.0051\n",
      "Epoch 758/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0896e-05 - mae: 0.0037\n",
      "Epoch 758: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0470e-05 - mae: 0.0036 - val_loss: 4.4861e-05 - val_mae: 0.0055\n",
      "Epoch 759/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8658e-05 - mae: 0.0034\n",
      "Epoch 759: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8395e-05 - mae: 0.0034 - val_loss: 4.0440e-05 - val_mae: 0.0052\n",
      "Epoch 760/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8807e-05 - mae: 0.0035\n",
      "Epoch 760: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.8691e-05 - mae: 0.0034 - val_loss: 3.9457e-05 - val_mae: 0.0051\n",
      "Epoch 761/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8149e-05 - mae: 0.0034\n",
      "Epoch 761: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8217e-05 - mae: 0.0034 - val_loss: 4.5815e-05 - val_mae: 0.0055\n",
      "Epoch 762/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.2130e-05 - mae: 0.0038\n",
      "Epoch 762: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.2013e-05 - mae: 0.0038 - val_loss: 4.2925e-05 - val_mae: 0.0054\n",
      "Epoch 763/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7868e-05 - mae: 0.0033\n",
      "Epoch 763: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.8086e-05 - mae: 0.0034 - val_loss: 3.8428e-05 - val_mae: 0.0051\n",
      "Epoch 764/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.0391e-05 - mae: 0.0035\n",
      "Epoch 764: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.0783e-05 - mae: 0.0036 - val_loss: 4.5293e-05 - val_mae: 0.0055\n",
      "Epoch 765/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6882e-05 - mae: 0.0033\n",
      "Epoch 765: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7736e-05 - mae: 0.0033 - val_loss: 3.8335e-05 - val_mae: 0.0051\n",
      "Epoch 766/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7722e-05 - mae: 0.0033\n",
      "Epoch 766: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9523e-05 - mae: 0.0035 - val_loss: 4.0531e-05 - val_mae: 0.0052\n",
      "Epoch 767/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.3923e-05 - mae: 0.0039\n",
      "Epoch 767: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.2109e-05 - mae: 0.0037 - val_loss: 4.1223e-05 - val_mae: 0.0053\n",
      "Epoch 768/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8255e-05 - mae: 0.0034\n",
      "Epoch 768: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8389e-05 - mae: 0.0034 - val_loss: 3.8457e-05 - val_mae: 0.0051\n",
      "Epoch 769/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6847e-05 - mae: 0.0033\n",
      "Epoch 769: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8773e-05 - mae: 0.0035 - val_loss: 4.0948e-05 - val_mae: 0.0052\n",
      "Epoch 770/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8819e-05 - mae: 0.0034\n",
      "Epoch 770: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8326e-05 - mae: 0.0034 - val_loss: 3.8959e-05 - val_mae: 0.0051\n",
      "Epoch 771/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630/800 [======================>.......] - ETA: 0s - loss: 1.8821e-05 - mae: 0.0035\n",
      "Epoch 771: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 1.8671e-05 - mae: 0.0035 - val_loss: 4.6077e-05 - val_mae: 0.0055\n",
      "Epoch 772/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9211e-05 - mae: 0.0035\n",
      "Epoch 772: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9145e-05 - mae: 0.0035 - val_loss: 4.0172e-05 - val_mae: 0.0052\n",
      "Epoch 773/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8346e-05 - mae: 0.0034\n",
      "Epoch 773: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7636e-05 - mae: 0.0033 - val_loss: 4.2742e-05 - val_mae: 0.0053\n",
      "Epoch 774/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8064e-05 - mae: 0.0034\n",
      "Epoch 774: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0320e-05 - mae: 0.0036 - val_loss: 4.4441e-05 - val_mae: 0.0054\n",
      "Epoch 775/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9462e-05 - mae: 0.0035\n",
      "Epoch 775: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9678e-05 - mae: 0.0035 - val_loss: 4.2730e-05 - val_mae: 0.0053\n",
      "Epoch 776/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8945e-05 - mae: 0.0034\n",
      "Epoch 776: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8915e-05 - mae: 0.0035 - val_loss: 3.8623e-05 - val_mae: 0.0051\n",
      "Epoch 777/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9561e-05 - mae: 0.0035\n",
      "Epoch 777: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8988e-05 - mae: 0.0035 - val_loss: 4.0897e-05 - val_mae: 0.0052\n",
      "Epoch 778/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8927e-05 - mae: 0.0035\n",
      "Epoch 778: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8785e-05 - mae: 0.0035 - val_loss: 4.1493e-05 - val_mae: 0.0052\n",
      "Epoch 779/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9993e-05 - mae: 0.0035\n",
      "Epoch 779: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9806e-05 - mae: 0.0035 - val_loss: 4.1722e-05 - val_mae: 0.0053\n",
      "Epoch 780/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7985e-05 - mae: 0.0034\n",
      "Epoch 780: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8095e-05 - mae: 0.0034 - val_loss: 4.1272e-05 - val_mae: 0.0053\n",
      "Epoch 781/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9105e-05 - mae: 0.0034\n",
      "Epoch 781: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9040e-05 - mae: 0.0034 - val_loss: 3.8517e-05 - val_mae: 0.0051\n",
      "Epoch 782/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9927e-05 - mae: 0.0035\n",
      "Epoch 782: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.0035e-05 - mae: 0.0035 - val_loss: 3.9091e-05 - val_mae: 0.0051\n",
      "Epoch 783/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8799e-05 - mae: 0.0035\n",
      "Epoch 783: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.8715e-05 - mae: 0.0035 - val_loss: 3.8889e-05 - val_mae: 0.0051\n",
      "Epoch 784/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8215e-05 - mae: 0.0034\n",
      "Epoch 784: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8671e-05 - mae: 0.0034 - val_loss: 3.9921e-05 - val_mae: 0.0052\n",
      "Epoch 785/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0175e-05 - mae: 0.0036\n",
      "Epoch 785: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 218us/sample - loss: 1.9034e-05 - mae: 0.0035 - val_loss: 4.3661e-05 - val_mae: 0.0054\n",
      "Epoch 786/1000\n",
      "610/800 [=====================>........] - ETA: 0s - loss: 2.3050e-05 - mae: 0.0038\n",
      "Epoch 786: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 2.2142e-05 - mae: 0.0038 - val_loss: 4.4337e-05 - val_mae: 0.0054\n",
      "Epoch 787/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.1129e-05 - mae: 0.0037\n",
      "Epoch 787: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.1354e-05 - mae: 0.0037 - val_loss: 4.0113e-05 - val_mae: 0.0052\n",
      "Epoch 788/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0393e-05 - mae: 0.0036\n",
      "Epoch 788: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 2.0160e-05 - mae: 0.0036 - val_loss: 4.4927e-05 - val_mae: 0.0055\n",
      "Epoch 789/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.2852e-05 - mae: 0.0038\n",
      "Epoch 789: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.2301e-05 - mae: 0.0037 - val_loss: 3.8235e-05 - val_mae: 0.0051\n",
      "Epoch 790/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.0260e-05 - mae: 0.0036\n",
      "Epoch 790: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0399e-05 - mae: 0.0036 - val_loss: 4.1419e-05 - val_mae: 0.0052\n",
      "Epoch 791/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0055e-05 - mae: 0.0036\n",
      "Epoch 791: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0235e-05 - mae: 0.0036 - val_loss: 3.9379e-05 - val_mae: 0.0051\n",
      "Epoch 792/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9442e-05 - mae: 0.0035\n",
      "Epoch 792: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9333e-05 - mae: 0.0035 - val_loss: 3.8287e-05 - val_mae: 0.0051\n",
      "Epoch 793/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0855e-05 - mae: 0.0036\n",
      "Epoch 793: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9636e-05 - mae: 0.0035 - val_loss: 5.1141e-05 - val_mae: 0.0058\n",
      "Epoch 794/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9805e-05 - mae: 0.0035\n",
      "Epoch 794: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9513e-05 - mae: 0.0035 - val_loss: 3.8646e-05 - val_mae: 0.0051\n",
      "Epoch 795/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.8283e-05 - mae: 0.0034\n",
      "Epoch 795: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.7562e-05 - mae: 0.0033 - val_loss: 3.8197e-05 - val_mae: 0.0051\n",
      "Epoch 796/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0229e-05 - mae: 0.0036\n",
      "Epoch 796: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9720e-05 - mae: 0.0035 - val_loss: 3.9555e-05 - val_mae: 0.0052\n",
      "Epoch 797/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.8464e-05 - mae: 0.0034\n",
      "Epoch 797: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.7749e-05 - mae: 0.0033 - val_loss: 3.8628e-05 - val_mae: 0.0051\n",
      "Epoch 798/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7872e-05 - mae: 0.0034\n",
      "Epoch 798: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8504e-05 - mae: 0.0034 - val_loss: 3.9317e-05 - val_mae: 0.0052\n",
      "Epoch 799/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8384e-05 - mae: 0.0034\n",
      "Epoch 799: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8384e-05 - mae: 0.0034 - val_loss: 3.8526e-05 - val_mae: 0.0051\n",
      "Epoch 800/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0567e-05 - mae: 0.0036\n",
      "Epoch 800: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8652e-05 - mae: 0.0034 - val_loss: 3.8676e-05 - val_mae: 0.0051\n",
      "Epoch 801/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8703e-05 - mae: 0.0034\n",
      "Epoch 801: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8603e-05 - mae: 0.0034 - val_loss: 3.8566e-05 - val_mae: 0.0051\n",
      "Epoch 802/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7968e-05 - mae: 0.0033\n",
      "Epoch 802: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8333e-05 - mae: 0.0034 - val_loss: 3.9922e-05 - val_mae: 0.0051\n",
      "Epoch 803/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7258e-05 - mae: 0.0033\n",
      "Epoch 803: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7858e-05 - mae: 0.0033 - val_loss: 4.1710e-05 - val_mae: 0.0053\n",
      "Epoch 804/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7957e-05 - mae: 0.0034\n",
      "Epoch 804: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8984e-05 - mae: 0.0035 - val_loss: 4.0546e-05 - val_mae: 0.0052\n",
      "Epoch 805/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8446e-05 - mae: 0.0034\n",
      "Epoch 805: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8463e-05 - mae: 0.0034 - val_loss: 3.8076e-05 - val_mae: 0.0050\n",
      "Epoch 806/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7281e-05 - mae: 0.0033\n",
      "Epoch 806: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8402e-05 - mae: 0.0034 - val_loss: 4.2185e-05 - val_mae: 0.0053\n",
      "Epoch 807/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9736e-05 - mae: 0.0036\n",
      "Epoch 807: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9436e-05 - mae: 0.0035 - val_loss: 4.8851e-05 - val_mae: 0.0057\n",
      "Epoch 808/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9914e-05 - mae: 0.0036\n",
      "Epoch 808: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9763e-05 - mae: 0.0036 - val_loss: 3.8628e-05 - val_mae: 0.0051\n",
      "Epoch 809/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.4145e-05 - mae: 0.0039\n",
      "Epoch 809: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.4260e-05 - mae: 0.0039 - val_loss: 3.8920e-05 - val_mae: 0.0051\n",
      "Epoch 810/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7931e-05 - mae: 0.0034\n",
      "Epoch 810: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8272e-05 - mae: 0.0034 - val_loss: 4.0325e-05 - val_mae: 0.0052\n",
      "Epoch 811/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9489e-05 - mae: 0.0035\n",
      "Epoch 811: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9377e-05 - mae: 0.0035 - val_loss: 4.1826e-05 - val_mae: 0.0053\n",
      "Epoch 812/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8637e-05 - mae: 0.0035\n",
      "Epoch 812: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8546e-05 - mae: 0.0034 - val_loss: 3.9041e-05 - val_mae: 0.0051\n",
      "Epoch 813/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7640e-05 - mae: 0.0033\n",
      "Epoch 813: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.7388e-05 - mae: 0.0033 - val_loss: 4.6172e-05 - val_mae: 0.0055\n",
      "Epoch 814/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8990e-05 - mae: 0.0034\n",
      "Epoch 814: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9030e-05 - mae: 0.0035 - val_loss: 3.8039e-05 - val_mae: 0.0051\n",
      "Epoch 815/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8207e-05 - mae: 0.0034\n",
      "Epoch 815: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8207e-05 - mae: 0.0034 - val_loss: 3.7973e-05 - val_mae: 0.0050\n",
      "Epoch 816/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9360e-05 - mae: 0.0035\n",
      "Epoch 816: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9360e-05 - mae: 0.0035 - val_loss: 3.8415e-05 - val_mae: 0.0051\n",
      "Epoch 817/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8619e-05 - mae: 0.0034\n",
      "Epoch 817: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8493e-05 - mae: 0.0034 - val_loss: 4.5857e-05 - val_mae: 0.0055\n",
      "Epoch 818/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7208e-05 - mae: 0.0034\n",
      "Epoch 818: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8120e-05 - mae: 0.0034 - val_loss: 3.8320e-05 - val_mae: 0.0051\n",
      "Epoch 819/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9767e-05 - mae: 0.0035\n",
      "Epoch 819: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9291e-05 - mae: 0.0035 - val_loss: 3.8031e-05 - val_mae: 0.0051\n",
      "Epoch 820/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7462e-05 - mae: 0.0033\n",
      "Epoch 820: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8217e-05 - mae: 0.0034 - val_loss: 3.8862e-05 - val_mae: 0.0051\n",
      "Epoch 821/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0616e-05 - mae: 0.0036\n",
      "Epoch 821: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.0400e-05 - mae: 0.0036 - val_loss: 4.2487e-05 - val_mae: 0.0053\n",
      "Epoch 822/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0151e-05 - mae: 0.0035\n",
      "Epoch 822: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0159e-05 - mae: 0.0035 - val_loss: 3.8277e-05 - val_mae: 0.0051\n",
      "Epoch 823/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8664e-05 - mae: 0.0034\n",
      "Epoch 823: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8625e-05 - mae: 0.0034 - val_loss: 3.9402e-05 - val_mae: 0.0051\n",
      "Epoch 824/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0945e-05 - mae: 0.0036\n",
      "Epoch 824: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0945e-05 - mae: 0.0036 - val_loss: 3.8584e-05 - val_mae: 0.0051\n",
      "Epoch 825/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8862e-05 - mae: 0.0035\n",
      "Epoch 825: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0308e-05 - mae: 0.0037 - val_loss: 4.6167e-05 - val_mae: 0.0055\n",
      "Epoch 826/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0258e-05 - mae: 0.0036\n",
      "Epoch 826: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9918e-05 - mae: 0.0035 - val_loss: 3.8253e-05 - val_mae: 0.0051\n",
      "Epoch 827/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8014e-05 - mae: 0.0034\n",
      "Epoch 827: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8791e-05 - mae: 0.0035 - val_loss: 4.3334e-05 - val_mae: 0.0054\n",
      "Epoch 828/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8219e-05 - mae: 0.0034\n",
      "Epoch 828: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8463e-05 - mae: 0.0034 - val_loss: 5.4601e-05 - val_mae: 0.0060\n",
      "Epoch 829/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/800 [========================>.....] - ETA: 0s - loss: 1.8616e-05 - mae: 0.0034\n",
      "Epoch 829: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.8871e-05 - mae: 0.0034 - val_loss: 3.8466e-05 - val_mae: 0.0051\n",
      "Epoch 830/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9261e-05 - mae: 0.0035\n",
      "Epoch 830: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8282e-05 - mae: 0.0034 - val_loss: 4.1676e-05 - val_mae: 0.0053\n",
      "Epoch 831/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8750e-05 - mae: 0.0034\n",
      "Epoch 831: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9130e-05 - mae: 0.0035 - val_loss: 3.9298e-05 - val_mae: 0.0051\n",
      "Epoch 832/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8839e-05 - mae: 0.0034\n",
      "Epoch 832: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8763e-05 - mae: 0.0034 - val_loss: 3.8295e-05 - val_mae: 0.0051\n",
      "Epoch 833/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8900e-05 - mae: 0.0034\n",
      "Epoch 833: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8831e-05 - mae: 0.0034 - val_loss: 4.0618e-05 - val_mae: 0.0052\n",
      "Epoch 834/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8561e-05 - mae: 0.0034\n",
      "Epoch 834: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8517e-05 - mae: 0.0034 - val_loss: 3.8362e-05 - val_mae: 0.0051\n",
      "Epoch 835/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8420e-05 - mae: 0.0034\n",
      "Epoch 835: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8468e-05 - mae: 0.0034 - val_loss: 4.1717e-05 - val_mae: 0.0053\n",
      "Epoch 836/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9495e-05 - mae: 0.0035\n",
      "Epoch 836: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0236e-05 - mae: 0.0036 - val_loss: 4.3505e-05 - val_mae: 0.0054\n",
      "Epoch 837/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9225e-05 - mae: 0.0035\n",
      "Epoch 837: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9144e-05 - mae: 0.0035 - val_loss: 3.9083e-05 - val_mae: 0.0051\n",
      "Epoch 838/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7697e-05 - mae: 0.0033\n",
      "Epoch 838: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7698e-05 - mae: 0.0033 - val_loss: 4.1100e-05 - val_mae: 0.0053\n",
      "Epoch 839/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6509e-05 - mae: 0.0032\n",
      "Epoch 839: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8457e-05 - mae: 0.0034 - val_loss: 4.0056e-05 - val_mae: 0.0052\n",
      "Epoch 840/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.4397e-05 - mae: 0.0039\n",
      "Epoch 840: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.3266e-05 - mae: 0.0038 - val_loss: 4.3265e-05 - val_mae: 0.0054\n",
      "Epoch 841/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8205e-05 - mae: 0.0034\n",
      "Epoch 841: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9964e-05 - mae: 0.0036 - val_loss: 4.2331e-05 - val_mae: 0.0053\n",
      "Epoch 842/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8453e-05 - mae: 0.0034\n",
      "Epoch 842: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8467e-05 - mae: 0.0034 - val_loss: 3.8317e-05 - val_mae: 0.0051\n",
      "Epoch 843/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9094e-05 - mae: 0.0034\n",
      "Epoch 843: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8544e-05 - mae: 0.0034 - val_loss: 3.9383e-05 - val_mae: 0.0052\n",
      "Epoch 844/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8621e-05 - mae: 0.0034\n",
      "Epoch 844: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8531e-05 - mae: 0.0034 - val_loss: 3.8197e-05 - val_mae: 0.0051\n",
      "Epoch 845/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9656e-05 - mae: 0.0035\n",
      "Epoch 845: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9567e-05 - mae: 0.0035 - val_loss: 3.8319e-05 - val_mae: 0.0051\n",
      "Epoch 846/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9217e-05 - mae: 0.0035\n",
      "Epoch 846: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9061e-05 - mae: 0.0035 - val_loss: 3.8220e-05 - val_mae: 0.0051\n",
      "Epoch 847/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8382e-05 - mae: 0.0034\n",
      "Epoch 847: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8764e-05 - mae: 0.0034 - val_loss: 3.8076e-05 - val_mae: 0.0051\n",
      "Epoch 848/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8806e-05 - mae: 0.0034\n",
      "Epoch 848: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8774e-05 - mae: 0.0034 - val_loss: 4.6286e-05 - val_mae: 0.0055\n",
      "Epoch 849/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9887e-05 - mae: 0.0036\n",
      "Epoch 849: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9887e-05 - mae: 0.0036 - val_loss: 3.8130e-05 - val_mae: 0.0051\n",
      "Epoch 850/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8171e-05 - mae: 0.0034\n",
      "Epoch 850: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9220e-05 - mae: 0.0035 - val_loss: 5.0536e-05 - val_mae: 0.0057\n",
      "Epoch 851/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8934e-05 - mae: 0.0035\n",
      "Epoch 851: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8916e-05 - mae: 0.0035 - val_loss: 3.8040e-05 - val_mae: 0.0051\n",
      "Epoch 852/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7802e-05 - mae: 0.0033\n",
      "Epoch 852: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7826e-05 - mae: 0.0033 - val_loss: 3.8630e-05 - val_mae: 0.0051\n",
      "Epoch 853/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7948e-05 - mae: 0.0034\n",
      "Epoch 853: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8191e-05 - mae: 0.0034 - val_loss: 4.0545e-05 - val_mae: 0.0052\n",
      "Epoch 854/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1360e-05 - mae: 0.0036\n",
      "Epoch 854: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9460e-05 - mae: 0.0034 - val_loss: 3.8653e-05 - val_mae: 0.0051\n",
      "Epoch 855/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.6839e-05 - mae: 0.0032\n",
      "Epoch 855: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7779e-05 - mae: 0.0033 - val_loss: 4.9684e-05 - val_mae: 0.0057\n",
      "Epoch 856/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.4313e-05 - mae: 0.0039\n",
      "Epoch 856: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.3025e-05 - mae: 0.0038 - val_loss: 3.7845e-05 - val_mae: 0.0051\n",
      "Epoch 857/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0394e-05 - mae: 0.0036\n",
      "Epoch 857: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0466e-05 - mae: 0.0036 - val_loss: 4.6321e-05 - val_mae: 0.0055\n",
      "Epoch 858/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.9246e-05 - mae: 0.0034\n",
      "Epoch 858: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9144e-05 - mae: 0.0034 - val_loss: 3.7652e-05 - val_mae: 0.0050\n",
      "Epoch 859/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7586e-05 - mae: 0.0034\n",
      "Epoch 859: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8377e-05 - mae: 0.0034 - val_loss: 3.9337e-05 - val_mae: 0.0051\n",
      "Epoch 860/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9019e-05 - mae: 0.0035\n",
      "Epoch 860: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0355e-05 - mae: 0.0036 - val_loss: 3.8000e-05 - val_mae: 0.0050\n",
      "Epoch 861/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.6775e-05 - mae: 0.0042\n",
      "Epoch 861: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.2808e-05 - mae: 0.0038 - val_loss: 3.7973e-05 - val_mae: 0.0050\n",
      "Epoch 862/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4947e-05 - mae: 0.0031\n",
      "Epoch 862: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8417e-05 - mae: 0.0034 - val_loss: 3.7950e-05 - val_mae: 0.0050\n",
      "Epoch 863/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7359e-05 - mae: 0.0033\n",
      "Epoch 863: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9163e-05 - mae: 0.0035 - val_loss: 5.2513e-05 - val_mae: 0.0059\n",
      "Epoch 864/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7775e-05 - mae: 0.0033\n",
      "Epoch 864: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8518e-05 - mae: 0.0034 - val_loss: 3.8411e-05 - val_mae: 0.0051\n",
      "Epoch 865/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7862e-05 - mae: 0.0033\n",
      "Epoch 865: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7922e-05 - mae: 0.0034 - val_loss: 3.8067e-05 - val_mae: 0.0051\n",
      "Epoch 866/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9938e-05 - mae: 0.0036\n",
      "Epoch 866: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9377e-05 - mae: 0.0035 - val_loss: 3.8650e-05 - val_mae: 0.0051\n",
      "Epoch 867/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8108e-05 - mae: 0.0034\n",
      "Epoch 867: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8108e-05 - mae: 0.0034 - val_loss: 4.8408e-05 - val_mae: 0.0056\n",
      "Epoch 868/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6421e-05 - mae: 0.0033\n",
      "Epoch 868: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8028e-05 - mae: 0.0034 - val_loss: 4.4823e-05 - val_mae: 0.0055\n",
      "Epoch 869/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1071e-05 - mae: 0.0036\n",
      "Epoch 869: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9538e-05 - mae: 0.0035 - val_loss: 3.9300e-05 - val_mae: 0.0052\n",
      "Epoch 870/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0864e-05 - mae: 0.0037\n",
      "Epoch 870: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9266e-05 - mae: 0.0035 - val_loss: 4.7636e-05 - val_mae: 0.0056\n",
      "Epoch 871/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6939e-05 - mae: 0.0033\n",
      "Epoch 871: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8132e-05 - mae: 0.0034 - val_loss: 4.5987e-05 - val_mae: 0.0055\n",
      "Epoch 872/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9349e-05 - mae: 0.0035\n",
      "Epoch 872: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0357e-05 - mae: 0.0036 - val_loss: 4.0240e-05 - val_mae: 0.0052\n",
      "Epoch 873/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0308e-05 - mae: 0.0036\n",
      "Epoch 873: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9420e-05 - mae: 0.0035 - val_loss: 3.8573e-05 - val_mae: 0.0051\n",
      "Epoch 874/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0990e-05 - mae: 0.0037\n",
      "Epoch 874: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9460e-05 - mae: 0.0035 - val_loss: 4.2077e-05 - val_mae: 0.0053\n",
      "Epoch 875/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6504e-05 - mae: 0.0033\n",
      "Epoch 875: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7620e-05 - mae: 0.0034 - val_loss: 3.8397e-05 - val_mae: 0.0051\n",
      "Epoch 876/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8811e-05 - mae: 0.0034\n",
      "Epoch 876: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9571e-05 - mae: 0.0035 - val_loss: 5.2993e-05 - val_mae: 0.0059\n",
      "Epoch 877/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9762e-05 - mae: 0.0036\n",
      "Epoch 877: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9735e-05 - mae: 0.0036 - val_loss: 3.8279e-05 - val_mae: 0.0051\n",
      "Epoch 878/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0473e-05 - mae: 0.0036\n",
      "Epoch 878: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0473e-05 - mae: 0.0036 - val_loss: 3.9260e-05 - val_mae: 0.0052\n",
      "Epoch 879/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5188e-05 - mae: 0.0031\n",
      "Epoch 879: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8074e-05 - mae: 0.0034 - val_loss: 3.8234e-05 - val_mae: 0.0051\n",
      "Epoch 880/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7808e-05 - mae: 0.0034\n",
      "Epoch 880: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8266e-05 - mae: 0.0034 - val_loss: 3.8316e-05 - val_mae: 0.0051\n",
      "Epoch 881/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9226e-05 - mae: 0.0035\n",
      "Epoch 881: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8548e-05 - mae: 0.0034 - val_loss: 3.8291e-05 - val_mae: 0.0051\n",
      "Epoch 882/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0759e-05 - mae: 0.0037\n",
      "Epoch 882: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0253e-05 - mae: 0.0036 - val_loss: 4.3088e-05 - val_mae: 0.0054\n",
      "Epoch 883/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0500e-05 - mae: 0.0036\n",
      "Epoch 883: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9882e-05 - mae: 0.0035 - val_loss: 3.8385e-05 - val_mae: 0.0051\n",
      "Epoch 884/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5898e-05 - mae: 0.0032\n",
      "Epoch 884: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7620e-05 - mae: 0.0033 - val_loss: 3.9964e-05 - val_mae: 0.0052\n",
      "Epoch 885/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7284e-05 - mae: 0.0033\n",
      "Epoch 885: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7854e-05 - mae: 0.0033 - val_loss: 3.8641e-05 - val_mae: 0.0051\n",
      "Epoch 886/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9249e-05 - mae: 0.0035\n",
      "Epoch 886: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8346e-05 - mae: 0.0034 - val_loss: 3.9846e-05 - val_mae: 0.0052\n",
      "Epoch 887/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7151e-05 - mae: 0.0033\n",
      "Epoch 887: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0059e-05 - mae: 0.0036 - val_loss: 3.8792e-05 - val_mae: 0.0051\n",
      "Epoch 888/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9681e-05 - mae: 0.0035\n",
      "Epoch 888: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9681e-05 - mae: 0.0035 - val_loss: 3.8295e-05 - val_mae: 0.0051\n",
      "Epoch 889/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7533e-05 - mae: 0.0033\n",
      "Epoch 889: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8158e-05 - mae: 0.0034 - val_loss: 3.8539e-05 - val_mae: 0.0051\n",
      "Epoch 890/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8999e-05 - mae: 0.0035\n",
      "Epoch 890: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8793e-05 - mae: 0.0035 - val_loss: 3.8415e-05 - val_mae: 0.0051\n",
      "Epoch 891/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7090e-05 - mae: 0.0033\n",
      "Epoch 891: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8157e-05 - mae: 0.0034 - val_loss: 4.8054e-05 - val_mae: 0.0056\n",
      "Epoch 892/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7892e-05 - mae: 0.0033\n",
      "Epoch 892: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7895e-05 - mae: 0.0033 - val_loss: 3.8620e-05 - val_mae: 0.0051\n",
      "Epoch 893/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0293e-05 - mae: 0.0035\n",
      "Epoch 893: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.8784e-05 - mae: 0.0034 - val_loss: 3.8398e-05 - val_mae: 0.0051\n",
      "Epoch 894/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 2.0443e-05 - mae: 0.0036\n",
      "Epoch 894: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 2.0293e-05 - mae: 0.0036 - val_loss: 3.8182e-05 - val_mae: 0.0051\n",
      "Epoch 895/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.8422e-05 - mae: 0.0034\n",
      "Epoch 895: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8588e-05 - mae: 0.0034 - val_loss: 3.9695e-05 - val_mae: 0.0051\n",
      "Epoch 896/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8560e-05 - mae: 0.0034\n",
      "Epoch 896: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.9092e-05 - mae: 0.0035 - val_loss: 4.0581e-05 - val_mae: 0.0052\n",
      "Epoch 897/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7607e-05 - mae: 0.0033\n",
      "Epoch 897: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8114e-05 - mae: 0.0034 - val_loss: 3.9475e-05 - val_mae: 0.0051\n",
      "Epoch 898/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.7382e-05 - mae: 0.0032\n",
      "Epoch 898: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.8102e-05 - mae: 0.0033 - val_loss: 3.8544e-05 - val_mae: 0.0051\n",
      "Epoch 899/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.7368e-05 - mae: 0.0033\n",
      "Epoch 899: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.7856e-05 - mae: 0.0033 - val_loss: 3.8902e-05 - val_mae: 0.0051\n",
      "Epoch 900/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 2.0560e-05 - mae: 0.0036\n",
      "Epoch 900: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.9575e-05 - mae: 0.0035 - val_loss: 4.1379e-05 - val_mae: 0.0053\n",
      "Epoch 901/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.8883e-05 - mae: 0.0035\n",
      "Epoch 901: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.9324e-05 - mae: 0.0035 - val_loss: 3.8917e-05 - val_mae: 0.0051\n",
      "Epoch 902/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.9514e-05 - mae: 0.0035\n",
      "Epoch 902: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.9381e-05 - mae: 0.0035 - val_loss: 3.9758e-05 - val_mae: 0.0052\n",
      "Epoch 903/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.8108e-05 - mae: 0.0034\n",
      "Epoch 903: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.8594e-05 - mae: 0.0034 - val_loss: 4.3788e-05 - val_mae: 0.0054\n",
      "Epoch 904/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7380e-05 - mae: 0.0033\n",
      "Epoch 904: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 2.0017e-05 - mae: 0.0035 - val_loss: 4.4368e-05 - val_mae: 0.0054\n",
      "Epoch 905/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9128e-05 - mae: 0.0035\n",
      "Epoch 905: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 2.0359e-05 - mae: 0.0036 - val_loss: 3.8400e-05 - val_mae: 0.0051\n",
      "Epoch 906/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8597e-05 - mae: 0.0034\n",
      "Epoch 906: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7739e-05 - mae: 0.0033 - val_loss: 3.8196e-05 - val_mae: 0.0051\n",
      "Epoch 907/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8540e-05 - mae: 0.0034\n",
      "Epoch 907: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8538e-05 - mae: 0.0034 - val_loss: 3.9070e-05 - val_mae: 0.0051\n",
      "Epoch 908/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.9773e-05 - mae: 0.0035\n",
      "Epoch 908: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.9181e-05 - mae: 0.0035 - val_loss: 3.9579e-05 - val_mae: 0.0052\n",
      "Epoch 909/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.9355e-05 - mae: 0.0034\n",
      "Epoch 909: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 1.8416e-05 - mae: 0.0034 - val_loss: 3.8142e-05 - val_mae: 0.0051\n",
      "Epoch 910/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.8446e-05 - mae: 0.0034\n",
      "Epoch 910: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 1.8182e-05 - mae: 0.0034 - val_loss: 3.8223e-05 - val_mae: 0.0051\n",
      "Epoch 911/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 2.0497e-05 - mae: 0.0035\n",
      "Epoch 911: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.9976e-05 - mae: 0.0035 - val_loss: 3.9024e-05 - val_mae: 0.0051\n",
      "Epoch 912/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7131e-05 - mae: 0.0033\n",
      "Epoch 912: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 1.7434e-05 - mae: 0.0033 - val_loss: 3.8166e-05 - val_mae: 0.0051\n",
      "Epoch 913/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.9219e-05 - mae: 0.0035\n",
      "Epoch 913: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 1.9288e-05 - mae: 0.0035 - val_loss: 3.8932e-05 - val_mae: 0.0051\n",
      "Epoch 914/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8689e-05 - mae: 0.0035\n",
      "Epoch 914: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.8223e-05 - mae: 0.0034 - val_loss: 3.8161e-05 - val_mae: 0.0051\n",
      "Epoch 915/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 1.9487e-05 - mae: 0.0035\n",
      "Epoch 915: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.8887e-05 - mae: 0.0034 - val_loss: 3.8777e-05 - val_mae: 0.0051\n",
      "Epoch 916/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9037e-05 - mae: 0.0035\n",
      "Epoch 916: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.9354e-05 - mae: 0.0035 - val_loss: 3.8387e-05 - val_mae: 0.0051\n",
      "Epoch 917/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0192e-05 - mae: 0.0036\n",
      "Epoch 917: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9528e-05 - mae: 0.0035 - val_loss: 5.0420e-05 - val_mae: 0.0057\n",
      "Epoch 918/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7388e-05 - mae: 0.0034\n",
      "Epoch 918: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8231e-05 - mae: 0.0034 - val_loss: 3.9018e-05 - val_mae: 0.0051\n",
      "Epoch 919/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7777e-05 - mae: 0.0034\n",
      "Epoch 919: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7823e-05 - mae: 0.0034 - val_loss: 3.8880e-05 - val_mae: 0.0051\n",
      "Epoch 920/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9436e-05 - mae: 0.0035\n",
      "Epoch 920: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9548e-05 - mae: 0.0035 - val_loss: 3.8136e-05 - val_mae: 0.0051\n",
      "Epoch 921/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9900e-05 - mae: 0.0036\n",
      "Epoch 921: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9841e-05 - mae: 0.0035 - val_loss: 3.8352e-05 - val_mae: 0.0051\n",
      "Epoch 922/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7993e-05 - mae: 0.0034\n",
      "Epoch 922: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.8408e-05 - mae: 0.0034 - val_loss: 3.8679e-05 - val_mae: 0.0051\n",
      "Epoch 923/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9113e-05 - mae: 0.0035\n",
      "Epoch 923: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9537e-05 - mae: 0.0035 - val_loss: 3.8221e-05 - val_mae: 0.0051\n",
      "Epoch 924/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7683e-05 - mae: 0.0034\n",
      "Epoch 924: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7746e-05 - mae: 0.0034 - val_loss: 4.0846e-05 - val_mae: 0.0052\n",
      "Epoch 925/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0474e-05 - mae: 0.0036\n",
      "Epoch 925: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.0709e-05 - mae: 0.0036 - val_loss: 4.9460e-05 - val_mae: 0.0057\n",
      "Epoch 926/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.0406e-05 - mae: 0.0036\n",
      "Epoch 926: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.0389e-05 - mae: 0.0036 - val_loss: 3.8066e-05 - val_mae: 0.0050\n",
      "Epoch 927/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8136e-05 - mae: 0.0034\n",
      "Epoch 927: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8229e-05 - mae: 0.0034 - val_loss: 3.9394e-05 - val_mae: 0.0051\n",
      "Epoch 928/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8134e-05 - mae: 0.0034\n",
      "Epoch 928: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.7803e-05 - mae: 0.0034 - val_loss: 4.1169e-05 - val_mae: 0.0053\n",
      "Epoch 929/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6939e-05 - mae: 0.0033\n",
      "Epoch 929: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8765e-05 - mae: 0.0034 - val_loss: 3.8757e-05 - val_mae: 0.0051\n",
      "Epoch 930/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9076e-05 - mae: 0.0034\n",
      "Epoch 930: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0236e-05 - mae: 0.0035 - val_loss: 3.8837e-05 - val_mae: 0.0051\n",
      "Epoch 931/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.2723e-05 - mae: 0.0038\n",
      "Epoch 931: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0578e-05 - mae: 0.0036 - val_loss: 3.8362e-05 - val_mae: 0.0051\n",
      "Epoch 932/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9384e-05 - mae: 0.0035\n",
      "Epoch 932: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9059e-05 - mae: 0.0035 - val_loss: 3.7939e-05 - val_mae: 0.0050\n",
      "Epoch 933/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0885e-05 - mae: 0.0036\n",
      "Epoch 933: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9672e-05 - mae: 0.0035 - val_loss: 3.7690e-05 - val_mae: 0.0050\n",
      "Epoch 934/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7408e-05 - mae: 0.0033\n",
      "Epoch 934: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8598e-05 - mae: 0.0034 - val_loss: 3.9210e-05 - val_mae: 0.0051\n",
      "Epoch 935/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0093e-05 - mae: 0.0036\n",
      "Epoch 935: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1209e-05 - mae: 0.0037 - val_loss: 3.7757e-05 - val_mae: 0.0050\n",
      "Epoch 936/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7630e-05 - mae: 0.0033\n",
      "Epoch 936: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8948e-05 - mae: 0.0034 - val_loss: 3.7944e-05 - val_mae: 0.0050\n",
      "Epoch 937/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9529e-05 - mae: 0.0035\n",
      "Epoch 937: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.0563e-05 - mae: 0.0036 - val_loss: 4.3076e-05 - val_mae: 0.0053\n",
      "Epoch 938/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7329e-05 - mae: 0.0034\n",
      "Epoch 938: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8097e-05 - mae: 0.0034 - val_loss: 4.4410e-05 - val_mae: 0.0054\n",
      "Epoch 939/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1040e-05 - mae: 0.0037\n",
      "Epoch 939: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8387e-05 - mae: 0.0035 - val_loss: 4.0954e-05 - val_mae: 0.0052\n",
      "Epoch 940/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8593e-05 - mae: 0.0034\n",
      "Epoch 940: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7866e-05 - mae: 0.0034 - val_loss: 3.8963e-05 - val_mae: 0.0051\n",
      "Epoch 941/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9712e-05 - mae: 0.0035\n",
      "Epoch 941: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8454e-05 - mae: 0.0034 - val_loss: 4.4848e-05 - val_mae: 0.0054\n",
      "Epoch 942/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8885e-05 - mae: 0.0034\n",
      "Epoch 942: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8802e-05 - mae: 0.0034 - val_loss: 3.8352e-05 - val_mae: 0.0051\n",
      "Epoch 943/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5466e-05 - mae: 0.0031\n",
      "Epoch 943: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7608e-05 - mae: 0.0034 - val_loss: 4.3578e-05 - val_mae: 0.0053\n",
      "Epoch 944/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0026e-05 - mae: 0.0036\n",
      "Epoch 944: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0026e-05 - mae: 0.0036 - val_loss: 3.7812e-05 - val_mae: 0.0050\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8128e-05 - mae: 0.0033\n",
      "Epoch 945: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0918e-05 - mae: 0.0036 - val_loss: 3.8680e-05 - val_mae: 0.0051\n",
      "Epoch 946/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7800e-05 - mae: 0.0034\n",
      "Epoch 946: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7886e-05 - mae: 0.0034 - val_loss: 3.8259e-05 - val_mae: 0.0051\n",
      "Epoch 947/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9933e-05 - mae: 0.0035\n",
      "Epoch 947: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9673e-05 - mae: 0.0035 - val_loss: 4.1442e-05 - val_mae: 0.0052\n",
      "Epoch 948/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5875e-05 - mae: 0.0032\n",
      "Epoch 948: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7914e-05 - mae: 0.0033 - val_loss: 4.0036e-05 - val_mae: 0.0052\n",
      "Epoch 949/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8718e-05 - mae: 0.0035\n",
      "Epoch 949: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8834e-05 - mae: 0.0035 - val_loss: 4.1328e-05 - val_mae: 0.0052\n",
      "Epoch 950/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9031e-05 - mae: 0.0034\n",
      "Epoch 950: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9024e-05 - mae: 0.0035 - val_loss: 4.0341e-05 - val_mae: 0.0052\n",
      "Epoch 951/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6898e-05 - mae: 0.0032\n",
      "Epoch 951: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8831e-05 - mae: 0.0034 - val_loss: 4.0298e-05 - val_mae: 0.0052\n",
      "Epoch 952/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6618e-05 - mae: 0.0032\n",
      "Epoch 952: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.8599e-05 - mae: 0.0034 - val_loss: 4.6430e-05 - val_mae: 0.0055\n",
      "Epoch 953/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.9869e-05 - mae: 0.0035\n",
      "Epoch 953: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 2.0107e-05 - mae: 0.0035 - val_loss: 4.0247e-05 - val_mae: 0.0052\n",
      "Epoch 954/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.2892e-05 - mae: 0.0038\n",
      "Epoch 954: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 2.0442e-05 - mae: 0.0036 - val_loss: 3.8053e-05 - val_mae: 0.0051\n",
      "Epoch 955/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6962e-05 - mae: 0.0033\n",
      "Epoch 955: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.1519e-05 - mae: 0.0037 - val_loss: 4.1119e-05 - val_mae: 0.0052\n",
      "Epoch 956/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7400e-05 - mae: 0.0033\n",
      "Epoch 956: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7545e-05 - mae: 0.0033 - val_loss: 3.8085e-05 - val_mae: 0.0051\n",
      "Epoch 957/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1473e-05 - mae: 0.0037\n",
      "Epoch 957: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0593e-05 - mae: 0.0036 - val_loss: 3.9963e-05 - val_mae: 0.0051\n",
      "Epoch 958/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8508e-05 - mae: 0.0035\n",
      "Epoch 958: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9594e-05 - mae: 0.0035 - val_loss: 5.4445e-05 - val_mae: 0.0059\n",
      "Epoch 959/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0007e-05 - mae: 0.0035\n",
      "Epoch 959: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8763e-05 - mae: 0.0034 - val_loss: 3.8300e-05 - val_mae: 0.0051\n",
      "Epoch 960/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5875e-05 - mae: 0.0031\n",
      "Epoch 960: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8184e-05 - mae: 0.0034 - val_loss: 3.9946e-05 - val_mae: 0.0051\n",
      "Epoch 961/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6851e-05 - mae: 0.0033\n",
      "Epoch 961: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7718e-05 - mae: 0.0033 - val_loss: 3.9150e-05 - val_mae: 0.0051\n",
      "Epoch 962/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8883e-05 - mae: 0.0034\n",
      "Epoch 962: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8472e-05 - mae: 0.0034 - val_loss: 4.0298e-05 - val_mae: 0.0052\n",
      "Epoch 963/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.8587e-05 - mae: 0.0034\n",
      "Epoch 963: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.9282e-05 - mae: 0.0035 - val_loss: 3.8421e-05 - val_mae: 0.0051\n",
      "Epoch 964/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8042e-05 - mae: 0.0034\n",
      "Epoch 964: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.8295e-05 - mae: 0.0034 - val_loss: 3.8850e-05 - val_mae: 0.0051\n",
      "Epoch 965/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.0221e-05 - mae: 0.0036\n",
      "Epoch 965: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9541e-05 - mae: 0.0035 - val_loss: 3.8072e-05 - val_mae: 0.0050\n",
      "Epoch 966/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6172e-05 - mae: 0.0031\n",
      "Epoch 966: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7600e-05 - mae: 0.0033 - val_loss: 3.9094e-05 - val_mae: 0.0051\n",
      "Epoch 967/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.9322e-05 - mae: 0.0035\n",
      "Epoch 967: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.9613e-05 - mae: 0.0035 - val_loss: 4.1141e-05 - val_mae: 0.0052\n",
      "Epoch 968/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.9165e-05 - mae: 0.0035\n",
      "Epoch 968: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 2.0205e-05 - mae: 0.0036 - val_loss: 4.5159e-05 - val_mae: 0.0055\n",
      "Epoch 969/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7849e-05 - mae: 0.0034\n",
      "Epoch 969: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.8372e-05 - mae: 0.0034 - val_loss: 3.9679e-05 - val_mae: 0.0051\n",
      "Epoch 970/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.7260e-05 - mae: 0.0033\n",
      "Epoch 970: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7933e-05 - mae: 0.0034 - val_loss: 3.8190e-05 - val_mae: 0.0051\n",
      "Epoch 971/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.6823e-05 - mae: 0.0032\n",
      "Epoch 971: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8917e-05 - mae: 0.0034 - val_loss: 4.6328e-05 - val_mae: 0.0055\n",
      "Epoch 972/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8711e-05 - mae: 0.0034\n",
      "Epoch 972: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.8990e-05 - mae: 0.0035 - val_loss: 4.1789e-05 - val_mae: 0.0053\n",
      "Epoch 973/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7196e-05 - mae: 0.0033\n",
      "Epoch 973: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.7580e-05 - mae: 0.0033 - val_loss: 6.0725e-05 - val_mae: 0.0063\n",
      "Epoch 974/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/800 [===================>..........] - ETA: 0s - loss: 2.4561e-05 - mae: 0.0040\n",
      "Epoch 974: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 2.2157e-05 - mae: 0.0037 - val_loss: 3.8683e-05 - val_mae: 0.0051\n",
      "Epoch 975/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.8096e-05 - mae: 0.0034\n",
      "Epoch 975: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.9543e-05 - mae: 0.0035 - val_loss: 3.9300e-05 - val_mae: 0.0052\n",
      "Epoch 976/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.6459e-05 - mae: 0.0032\n",
      "Epoch 976: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.7788e-05 - mae: 0.0033 - val_loss: 3.9691e-05 - val_mae: 0.0052\n",
      "Epoch 977/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.9005e-05 - mae: 0.0034\n",
      "Epoch 977: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.9357e-05 - mae: 0.0035 - val_loss: 5.0041e-05 - val_mae: 0.0057\n",
      "Epoch 978/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.9137e-05 - mae: 0.0035\n",
      "Epoch 978: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 2.0082e-05 - mae: 0.0036 - val_loss: 4.4708e-05 - val_mae: 0.0055\n",
      "Epoch 979/1000\n",
      "570/800 [====================>.........] - ETA: 0s - loss: 1.9273e-05 - mae: 0.0035\n",
      "Epoch 979: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 2.0066e-05 - mae: 0.0036 - val_loss: 4.9017e-05 - val_mae: 0.0057\n",
      "Epoch 980/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 2.2510e-05 - mae: 0.0038\n",
      "Epoch 980: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 2.0833e-05 - mae: 0.0037 - val_loss: 3.8679e-05 - val_mae: 0.0051\n",
      "Epoch 981/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7705e-05 - mae: 0.0034\n",
      "Epoch 981: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.7944e-05 - mae: 0.0034 - val_loss: 3.8852e-05 - val_mae: 0.0051\n",
      "Epoch 982/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 2.1279e-05 - mae: 0.0037\n",
      "Epoch 982: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.0816e-05 - mae: 0.0036 - val_loss: 3.8267e-05 - val_mae: 0.0051\n",
      "Epoch 983/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8492e-05 - mae: 0.0035\n",
      "Epoch 983: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.8077e-05 - mae: 0.0034 - val_loss: 3.8099e-05 - val_mae: 0.0051\n",
      "Epoch 984/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9076e-05 - mae: 0.0035\n",
      "Epoch 984: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7749e-05 - mae: 0.0033 - val_loss: 4.2358e-05 - val_mae: 0.0053\n",
      "Epoch 985/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8886e-05 - mae: 0.0034\n",
      "Epoch 985: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9955e-05 - mae: 0.0036 - val_loss: 4.2993e-05 - val_mae: 0.0054\n",
      "Epoch 986/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9470e-05 - mae: 0.0036\n",
      "Epoch 986: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9680e-05 - mae: 0.0036 - val_loss: 4.3123e-05 - val_mae: 0.0053\n",
      "Epoch 987/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9262e-05 - mae: 0.0035\n",
      "Epoch 987: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8876e-05 - mae: 0.0035 - val_loss: 3.7771e-05 - val_mae: 0.0050\n",
      "Epoch 988/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9382e-05 - mae: 0.0035\n",
      "Epoch 988: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9382e-05 - mae: 0.0035 - val_loss: 3.9075e-05 - val_mae: 0.0051\n",
      "Epoch 989/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9706e-05 - mae: 0.0035\n",
      "Epoch 989: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9985e-05 - mae: 0.0036 - val_loss: 3.7909e-05 - val_mae: 0.0051\n",
      "Epoch 990/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8335e-05 - mae: 0.0034\n",
      "Epoch 990: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7848e-05 - mae: 0.0033 - val_loss: 3.7824e-05 - val_mae: 0.0050\n",
      "Epoch 991/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6349e-05 - mae: 0.0033\n",
      "Epoch 991: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8540e-05 - mae: 0.0034 - val_loss: 3.7798e-05 - val_mae: 0.0050\n",
      "Epoch 992/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7867e-05 - mae: 0.0034\n",
      "Epoch 992: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8218e-05 - mae: 0.0034 - val_loss: 3.8205e-05 - val_mae: 0.0051\n",
      "Epoch 993/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8721e-05 - mae: 0.0034\n",
      "Epoch 993: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8699e-05 - mae: 0.0034 - val_loss: 3.8076e-05 - val_mae: 0.0051\n",
      "Epoch 994/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7986e-05 - mae: 0.0034\n",
      "Epoch 994: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.7978e-05 - mae: 0.0034 - val_loss: 3.8025e-05 - val_mae: 0.0050\n",
      "Epoch 995/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1149e-05 - mae: 0.0037\n",
      "Epoch 995: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.0994e-05 - mae: 0.0036 - val_loss: 3.9674e-05 - val_mae: 0.0052\n",
      "Epoch 996/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6945e-05 - mae: 0.0033\n",
      "Epoch 996: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8159e-05 - mae: 0.0034 - val_loss: 4.2625e-05 - val_mae: 0.0053\n",
      "Epoch 997/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7418e-05 - mae: 0.0033\n",
      "Epoch 997: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8305e-05 - mae: 0.0034 - val_loss: 3.7917e-05 - val_mae: 0.0050\n",
      "Epoch 998/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7234e-05 - mae: 0.0033\n",
      "Epoch 998: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9240e-05 - mae: 0.0035 - val_loss: 4.4606e-05 - val_mae: 0.0055\n",
      "Epoch 999/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8477e-05 - mae: 0.0034\n",
      "Epoch 999: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8542e-05 - mae: 0.0034 - val_loss: 3.8087e-05 - val_mae: 0.0051\n",
      "Epoch 1000/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7766e-05 - mae: 0.0034\n",
      "Epoch 1000: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0879e-05 - mae: 0.0037 - val_loss: 4.5131e-05 - val_mae: 0.0055\n"
     ]
    }
   ],
   "source": [
    "### Try one model with non-zero last layer bias\n",
    "\n",
    "Layers = [{'size': nx+1, 'activation': None    , 'use_bias': None},\n",
    "          {'size': 10 , 'activation': 'relu'  , 'use_bias': True},\n",
    "          {'size': 1  , 'activation': 'linear', 'use_bias': True}]\n",
    "Losses = [{'kind': 'mse', 'weight': 1.0}]\n",
    "\n",
    "K = TrainFullyConnectedNN(M_samples, H_samples, \n",
    "                    Layers, Losses,\n",
    "                    'adam', ['mae'], \n",
    "                    10, 1000, 0.2, \n",
    "                    'model', os.path.abspath(''))\n",
    "\n",
    "best_model = K.quickTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0602507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.08494031513550837\r",
      "iteration 2 - y: 0.08482186606743891\r",
      "iteration 3 - y: 0.08470341699936945\r",
      "iteration 4 - y: 0.08458496793129999\r",
      "iteration 5 - y: 0.08446651886323052\r",
      "iteration 6 - y: 0.08434806979516107\r",
      "iteration 7 - y: 0.0842296207270916\r",
      "iteration 8 - y: 0.08411117165902213\r",
      "iteration 9 - y: 0.08399272259095268\r",
      "iteration 10 - y: 0.08387427352288321\r",
      "iteration 11 - y: 0.08375582445481375\r",
      "iteration 12 - y: 0.08363737538674429\r",
      "iteration 13 - y: 0.08351892631867483\r",
      "iteration 14 - y: 0.08340047725060537\r",
      "iteration 15 - y: 0.08328202818253591\r",
      "iteration 16 - y: 0.08316357911446645\r",
      "iteration 17 - y: 0.08304513004639699\r",
      "iteration 18 - y: 0.08292668097832753\r",
      "iteration 19 - y: 0.08280823191025807\r",
      "iteration 20 - y: 0.0826897828421886\r",
      "iteration 21 - y: 0.08257133377411914\r",
      "iteration 22 - y: 0.08245288470604967\r",
      "iteration 23 - y: 0.08233443563798022\r",
      "iteration 24 - y: 0.08221598656991075\r",
      "iteration 25 - y: 0.08209753750184129\r",
      "iteration 26 - y: 0.08197908843377183\r",
      "iteration 27 - y: 0.08186063936570237\r",
      "iteration 28 - y: 0.08174219029763291\r",
      "iteration 29 - y: 0.08162374122956345\r",
      "iteration 30 - y: 0.08150529216149399\r",
      "iteration 31 - y: 0.08138684309342453\r",
      "iteration 32 - y: 0.08126839402535507\r",
      "iteration 33 - y: 0.08114994495728561\r",
      "iteration 34 - y: 0.08103149588921614\r",
      "iteration 35 - y: 0.08091304682114668\r",
      "iteration 36 - y: 0.08079459775307722\r",
      "iteration 37 - y: 0.08067614868500776\r",
      "iteration 38 - y: 0.08055769961693829\r",
      "iteration 39 - y: 0.08043925054886883\r",
      "iteration 40 - y: 0.08032080148079937\r",
      "iteration 41 - y: 0.08020235241272991\r",
      "iteration 42 - y: 0.08008390334466045\r",
      "iteration 43 - y: 0.07996545427659099\r",
      "iteration 44 - y: 0.07984700520852153\r",
      "iteration 45 - y: 0.07972855614045207\r",
      "iteration 46 - y: 0.07961010707238261\r",
      "iteration 47 - y: 0.07949165800431315\r",
      "iteration 48 - y: 0.07937320893624368\r",
      "iteration 49 - y: 0.07925475986817422\r",
      "iteration 50 - y: 0.07913631080010476\r",
      "iteration 51 - y: 0.0790178617320353\r",
      "iteration 52 - y: 0.07889941266396583\r",
      "iteration 53 - y: 0.07878096359589637\r",
      "iteration 54 - y: 0.07866251452782691\r",
      "iteration 55 - y: 0.07854406545975745\r",
      "iteration 56 - y: 0.07842561639168799\r",
      "iteration 57 - y: 0.07830716732361853\r",
      "iteration 58 - y: 0.07818871825554907\r",
      "iteration 59 - y: 0.07807026918747961\r",
      "iteration 60 - y: 0.07795182011941015\r",
      "iteration 61 - y: 0.0778333710513407\r",
      "iteration 62 - y: 0.07771492198327122\r",
      "iteration 63 - y: 0.07759647291520176\r",
      "iteration 64 - y: 0.0774780238471323\r",
      "iteration 65 - y: 0.07735957477906284\r",
      "iteration 66 - y: 0.07724112571099337\r",
      "iteration 67 - y: 0.07712267664292391\r",
      "iteration 68 - y: 0.07700422757485445\r",
      "iteration 69 - y: 0.07688577850678499\r",
      "iteration 70 - y: 0.07676732943871553\r",
      "iteration 71 - y: 0.07664888037064607\r",
      "iteration 72 - y: 0.07653043130257661\r",
      "iteration 73 - y: 0.07641198223450715\r",
      "iteration 74 - y: 0.0762935331664377\r",
      "iteration 75 - y: 0.07617508409836823\r",
      "iteration 76 - y: 0.07605663503029876\r",
      "iteration 77 - y: 0.0759381859622293\r",
      "iteration 78 - y: 0.07581973689415984\r",
      "iteration 79 - y: 0.07570128782609037\r",
      "iteration 80 - y: 0.07558283875802091\r",
      "iteration 81 - y: 0.07546438968995145\r",
      "iteration 82 - y: 0.07534594062188199\r",
      "iteration 83 - y: 0.07522749155381253\r",
      "iteration 84 - y: 0.07510904248574307\r",
      "iteration 85 - y: 0.07499059341767361\r",
      "iteration 86 - y: 0.07487214434960415\r",
      "iteration 87 - y: 0.0747536952815347\r",
      "iteration 88 - y: 0.07463524621346523\r",
      "iteration 89 - y: 0.07451679714539577\r",
      "iteration 90 - y: 0.0743983480773263\r",
      "iteration 91 - y: 0.07427989900925686\r",
      "iteration 92 - y: 0.07416144994118738\r",
      "iteration 93 - y: 0.07404300087311791\r",
      "iteration 94 - y: 0.07392455180504845\r",
      "iteration 95 - y: 0.07380610273697899\r",
      "iteration 96 - y: 0.07368765366890953\r",
      "iteration 97 - y: 0.07356920460084007\r",
      "iteration 98 - y: 0.07345075553277061\r",
      "iteration 99 - y: 0.07333230646470115\r",
      "iteration 100 - y: 0.07321385739663169\r",
      "iteration 101 - y: 0.07309540832856223\r",
      "iteration 102 - y: 0.07297695926049277\r",
      "iteration 103 - y: 0.07285851019242331\r",
      "iteration 104 - y: 0.07274006112435384\r",
      "iteration 105 - y: 0.0726216120562844\r",
      "iteration 106 - y: 0.07250316298821492\r",
      "iteration 107 - y: 0.07238471392014545\r",
      "iteration 108 - y: 0.072266264852076\r",
      "iteration 109 - y: 0.07214781578400653\r",
      "iteration 110 - y: 0.07202936671593707\r",
      "iteration 111 - y: 0.07191091764786761\r",
      "iteration 112 - y: 0.07179246857979815\r",
      "iteration 113 - y: 0.07167401951172869\r",
      "iteration 114 - y: 0.07155557044365923\r",
      "iteration 115 - y: 0.07143712137558977\r",
      "iteration 116 - y: 0.07131867230752031\r",
      "iteration 117 - y: 0.07120022323945085\r",
      "iteration 118 - y: 0.07108177417138138\r",
      "iteration 119 - y: 0.07096332510331192\r",
      "iteration 120 - y: 0.07084487603524246\r",
      "iteration 121 - y: 0.070726426967173\r",
      "iteration 122 - y: 0.07060797789910354\r",
      "iteration 123 - y: 0.07048952883103407\r",
      "iteration 124 - y: 0.07037107976296461\r",
      "iteration 125 - y: 0.07025263069489515\r",
      "iteration 126 - y: 0.07013418162682569\r",
      "iteration 127 - y: 0.07001573255875623\r",
      "iteration 128 - y: 0.06989728349068677\r",
      "iteration 129 - y: 0.06977883442261731\r",
      "iteration 130 - y: 0.06966038535454785\r",
      "iteration 131 - y: 0.0695419362864784\r",
      "iteration 132 - y: 0.06942348721840892\r",
      "iteration 133 - y: 0.06930503815033946\r",
      "iteration 134 - y: 0.06918658908227\r",
      "iteration 135 - y: 0.06906814001420054\r",
      "iteration 136 - y: 0.06894969094613108\r",
      "iteration 137 - y: 0.06883124187806161\r",
      "iteration 138 - y: 0.06871279280999217\r",
      "iteration 139 - y: 0.06859434374192269\r",
      "iteration 140 - y: 0.06847589467385323\r",
      "iteration 141 - y: 0.06835744560578377\r",
      "iteration 142 - y: 0.06823899653771431\r",
      "iteration 143 - y: 0.06812054746964485\r",
      "iteration 144 - y: 0.0680020984015754\r",
      "iteration 145 - y: 0.06788364933350592\r",
      "iteration 146 - y: 0.06776520026543646\r",
      "iteration 147 - y: 0.067646751197367\r",
      "iteration 148 - y: 0.06752830212929754\r",
      "iteration 149 - y: 0.06740985306122808\r",
      "iteration 150 - y: 0.06729140399315862\r",
      "iteration 151 - y: 0.06717295492508915\r",
      "iteration 152 - y: 0.06705450585701969\r",
      "iteration 153 - y: 0.06693605678895023\r",
      "iteration 154 - y: 0.06681760772088077\r",
      "iteration 155 - y: 0.06669915865281131\r",
      "iteration 156 - y: 0.06658070958474185\r",
      "iteration 157 - y: 0.0664622605166724\r",
      "iteration 158 - y: 0.06634381144860294\r",
      "iteration 159 - y: 0.06622536238053346\r",
      "iteration 160 - y: 0.066106913312464\r",
      "iteration 161 - y: 0.06598846424439454\r",
      "iteration 162 - y: 0.06587001517632508\r",
      "iteration 163 - y: 0.06575156610825562\r",
      "iteration 164 - y: 0.06563311704018616\r",
      "iteration 165 - y: 0.06551466797211669\r",
      "iteration 166 - y: 0.06539621890404723\r",
      "iteration 167 - y: 0.06527776983597777\r",
      "iteration 168 - y: 0.06515932076790831\r",
      "iteration 169 - y: 0.06504087169983885\r",
      "iteration 170 - y: 0.0649224226317694\r",
      "iteration 171 - y: 0.06480397356369993\r",
      "iteration 172 - y: 0.06468552449563048\r",
      "iteration 173 - y: 0.064567075427561\r",
      "iteration 174 - y: 0.06444862635949156\r",
      "iteration 175 - y: 0.06433017729142208\r",
      "iteration 176 - y: 0.06421172822335262\r",
      "iteration 177 - y: 0.06409327915528316\r",
      "iteration 178 - y: 0.06397483008721369\r",
      "iteration 179 - y: 0.06385638101914425\r",
      "iteration 180 - y: 0.06373793195107477\r",
      "iteration 181 - y: 0.06361948288300531\r",
      "iteration 182 - y: 0.06350103381493585\r",
      "iteration 183 - y: 0.0633825847468664\r",
      "iteration 184 - y: 0.06326413567879693\r",
      "iteration 185 - y: 0.06314568661072748\r",
      "iteration 186 - y: 0.06302723754265802\r",
      "iteration 187 - y: 0.06290878847458854\r",
      "iteration 188 - y: 0.0627903394065191\r",
      "iteration 189 - y: 0.06267189033844962\r",
      "iteration 190 - y: 0.06255344127038016\r",
      "iteration 191 - y: 0.062434992202310705\r",
      "iteration 192 - y: 0.06231654313424124\r",
      "iteration 193 - y: 0.062198094066171786\r",
      "iteration 194 - y: 0.06207964499810231\r",
      "iteration 195 - y: 0.06196119593003286\r",
      "iteration 196 - y: 0.061842746861963394\r",
      "iteration 197 - y: 0.061724297793893934\r",
      "iteration 198 - y: 0.061605848725824475\r",
      "iteration 199 - y: 0.06148739965775501\r",
      "iteration 200 - y: 0.061368950589685556\r",
      "iteration 201 - y: 0.06125050152161608\r",
      "iteration 202 - y: 0.06113205245354663\r",
      "iteration 203 - y: 0.061013603385477164\r",
      "iteration 204 - y: 0.060895154317407704\r",
      "iteration 205 - y: 0.060776705249338245\r",
      "iteration 206 - y: 0.06065825618126878\r",
      "iteration 207 - y: 0.060539807113199326\r",
      "iteration 208 - y: 0.06042135804512985\r",
      "iteration 209 - y: 0.0603029089770604\r",
      "iteration 210 - y: 0.060184459908990934\r",
      "iteration 211 - y: 0.060066010840921474\r",
      "iteration 212 - y: 0.059947561772852015\r",
      "iteration 213 - y: 0.05982911270478255\r",
      "iteration 214 - y: 0.059710663636713096\r",
      "iteration 215 - y: 0.05959221456864363\r",
      "iteration 216 - y: 0.05947376550057417\r",
      "iteration 217 - y: 0.059355316432504704\r",
      "iteration 218 - y: 0.059236867364435244\r",
      "iteration 219 - y: 0.059118418296365785\r",
      "iteration 220 - y: 0.05899996922829632\r",
      "iteration 221 - y: 0.058881520160226866\r",
      "iteration 222 - y: 0.0587630710921574\r",
      "iteration 223 - y: 0.05864462202408794\r",
      "iteration 224 - y: 0.058526172956018474\r",
      "iteration 225 - y: 0.058407723887949015\r",
      "iteration 226 - y: 0.058289274819879555\r",
      "iteration 227 - y: 0.058170825751810096\r",
      "iteration 228 - y: 0.058052376683740636\r",
      "iteration 229 - y: 0.05793392761567117\r",
      "iteration 230 - y: 0.05781547854760171\r",
      "iteration 231 - y: 0.057697029479532244\r",
      "iteration 232 - y: 0.057578580411462785\r",
      "iteration 233 - y: 0.057460131343393325\r",
      "iteration 234 - y: 0.057341682275323866\r",
      "iteration 235 - y: 0.057223233207254406\r",
      "iteration 236 - y: 0.05710478413918494\r",
      "iteration 237 - y: 0.056986335071115474\r",
      "iteration 238 - y: 0.056867886003046014\r",
      "iteration 239 - y: 0.056749436934976555\r",
      "iteration 240 - y: 0.056630987866907095\r",
      "iteration 241 - y: 0.056512538798837636\r",
      "iteration 242 - y: 0.05639408973076818\r",
      "iteration 243 - y: 0.05627564066269871\r",
      "iteration 244 - y: 0.056157191594629244\r",
      "iteration 245 - y: 0.056038742526559784\r",
      "iteration 246 - y: 0.055920293458490325\r",
      "iteration 247 - y: 0.055801844390420866\r",
      "iteration 248 - y: 0.055683395322351406\r",
      "iteration 249 - y: 0.05556494625428195\r",
      "iteration 250 - y: 0.05544649718621248\r",
      "iteration 251 - y: 0.055328048118143014\r",
      "iteration 252 - y: 0.055209599050073555\r",
      "iteration 253 - y: 0.055091149982004095\r",
      "iteration 254 - y: 0.054972700913934636\r",
      "iteration 255 - y: 0.054854251845865176\r",
      "iteration 256 - y: 0.05473580277779571\r",
      "iteration 257 - y: 0.05461735370972625\r",
      "iteration 258 - y: 0.05449890464165679\r",
      "iteration 259 - y: 0.054380455573587325\r",
      "iteration 260 - y: 0.054262006505517865\r",
      "iteration 261 - y: 0.054143557437448406\r",
      "iteration 262 - y: 0.054025108369378946\r",
      "iteration 263 - y: 0.05390665930130949\r",
      "iteration 264 - y: 0.05378821023324002\r",
      "iteration 265 - y: 0.05366976116517056\r",
      "iteration 266 - y: 0.053551312097101095\r",
      "iteration 267 - y: 0.053432863029031635\r",
      "iteration 268 - y: 0.053314413960962176\r",
      "iteration 269 - y: 0.053195964892892716\r",
      "iteration 270 - y: 0.05307751582482326\r",
      "iteration 271 - y: 0.05295906675675379\r",
      "iteration 272 - y: 0.05284061768868433\r",
      "iteration 273 - y: 0.052722168620614865\r",
      "iteration 274 - y: 0.052603719552545405\r",
      "iteration 275 - y: 0.052485270484475946\r",
      "iteration 276 - y: 0.05236682141640649\r",
      "iteration 277 - y: 0.05224837234833703\r",
      "iteration 278 - y: 0.05212992328026756\r",
      "iteration 279 - y: 0.0520114742121981\r",
      "iteration 280 - y: 0.051893025144128635\r",
      "iteration 281 - y: 0.051774576076059176\r",
      "iteration 282 - y: 0.051656127007989716\r",
      "iteration 283 - y: 0.05153767793992026\r",
      "iteration 284 - y: 0.0514192288718508\r",
      "iteration 285 - y: 0.05130077980378133\r",
      "iteration 286 - y: 0.051182330735711865\r",
      "iteration 287 - y: 0.051063881667642405\r",
      "iteration 288 - y: 0.050945432599572946\r",
      "iteration 289 - y: 0.050826983531503486\r",
      "iteration 290 - y: 0.05070853446343403\r",
      "iteration 291 - y: 0.05059008539536457\r",
      "iteration 292 - y: 0.0504716363272951\r",
      "iteration 293 - y: 0.050353187259225635\r",
      "iteration 294 - y: 0.050234738191156175\r",
      "iteration 295 - y: 0.050116289123086716\r",
      "iteration 296 - y: 0.049997840055017256\r",
      "iteration 297 - y: 0.0498793909869478\r",
      "iteration 298 - y: 0.04976094191887834\r",
      "iteration 299 - y: 0.04964249285080888\r",
      "iteration 300 - y: 0.049524043782739405\r",
      "iteration 301 - y: 0.049405594714669945\r",
      "iteration 302 - y: 0.049287145646600486\r",
      "iteration 303 - y: 0.049168696578531026\r",
      "iteration 304 - y: 0.04905024751046157\r",
      "iteration 305 - y: 0.04893179844239211\r",
      "iteration 306 - y: 0.04881334937432264\r",
      "iteration 307 - y: 0.048694900306253175\r",
      "iteration 308 - y: 0.048576451238183715\r",
      "iteration 309 - y: 0.048458002170114256\r",
      "iteration 310 - y: 0.0483395531020448\r",
      "iteration 311 - y: 0.04822110403397534\r",
      "iteration 312 - y: 0.04810265496590588\r",
      "iteration 313 - y: 0.04798420589783641\r",
      "iteration 314 - y: 0.047865756829766945\r",
      "iteration 315 - y: 0.047747307761697486\r",
      "iteration 316 - y: 0.047628858693628026\r",
      "iteration 317 - y: 0.04751040962555857\r",
      "iteration 318 - y: 0.04739196055748911\r",
      "iteration 319 - y: 0.04727351148941964\r",
      "iteration 320 - y: 0.04715506242135018\r",
      "iteration 321 - y: 0.047036613353280715\r",
      "iteration 322 - y: 0.046918164285211256\r",
      "iteration 323 - y: 0.046799715217141796\r",
      "iteration 324 - y: 0.04668126614907234\r",
      "iteration 325 - y: 0.04656281708100288\r",
      "iteration 326 - y: 0.04644436801293341\r",
      "iteration 327 - y: 0.04632591894486395\r",
      "iteration 328 - y: 0.046207469876794485\r",
      "iteration 329 - y: 0.046089020808725026\r",
      "iteration 330 - y: 0.045970571740655566\r",
      "iteration 331 - y: 0.04585212267258611\r",
      "iteration 332 - y: 0.04573367360451665\r",
      "iteration 333 - y: 0.04561522453644718\r",
      "iteration 334 - y: 0.04549677546837772\r",
      "iteration 335 - y: 0.04537832640030826\r",
      "iteration 336 - y: 0.045259877332238796\r",
      "iteration 337 - y: 0.045141428264169337\r",
      "iteration 338 - y: 0.04502297919609988\r",
      "iteration 339 - y: 0.04490453012803042\r",
      "iteration 340 - y: 0.04478608105996096\r",
      "iteration 341 - y: 0.04466763199189149\r",
      "iteration 342 - y: 0.04454918292382203\r",
      "iteration 343 - y: 0.044430733855752566\r",
      "iteration 344 - y: 0.04431228478768311\r",
      "iteration 345 - y: 0.04419383571961365\r",
      "iteration 346 - y: 0.04407538665154419\r",
      "iteration 347 - y: 0.04395693758347473\r",
      "iteration 348 - y: 0.04383848851540526\r",
      "iteration 349 - y: 0.0437200394473358\r",
      "iteration 350 - y: 0.043601590379266336\r",
      "iteration 351 - y: 0.04348314131119688\r",
      "iteration 352 - y: 0.04336469224312742\r",
      "iteration 353 - y: 0.04324624317505796\r",
      "iteration 354 - y: 0.0431277941069885\r",
      "iteration 355 - y: 0.04300934503891903\r",
      "iteration 356 - y: 0.04289089597084957\r",
      "iteration 357 - y: 0.042772446902780106\r",
      "iteration 358 - y: 0.04265399783471065\r",
      "iteration 359 - y: 0.04253554876664119\r",
      "iteration 360 - y: 0.04241709969857173\r",
      "iteration 361 - y: 0.04229865063050227\r",
      "iteration 362 - y: 0.0421802015624328\r",
      "iteration 363 - y: 0.04206175249436334\r",
      "iteration 364 - y: 0.04194330342629388\r",
      "iteration 365 - y: 0.04182485435822442\r",
      "iteration 366 - y: 0.04170640529015496\r",
      "iteration 367 - y: 0.0415879562220855\r",
      "iteration 368 - y: 0.04146950715401604\r",
      "iteration 369 - y: 0.04135105808594657\r",
      "iteration 370 - y: 0.04123260901787711\r",
      "iteration 371 - y: 0.041114159949807653\r",
      "iteration 372 - y: 0.04099571088173819\r",
      "iteration 373 - y: 0.04087726181366873\r",
      "iteration 374 - y: 0.04075881274559927\r",
      "iteration 375 - y: 0.04064036367752981\r",
      "iteration 376 - y: 0.04052191460946035\r",
      "iteration 377 - y: 0.04040346554139088\r",
      "iteration 378 - y: 0.04028501647332142\r",
      "iteration 379 - y: 0.04016656740525196\r",
      "iteration 380 - y: 0.0400481183371825\r",
      "iteration 381 - y: 0.03992966926911304\r",
      "iteration 382 - y: 0.03981122020104358\r",
      "iteration 383 - y: 0.03969277113297412\r",
      "iteration 384 - y: 0.03957432206490465\r",
      "iteration 385 - y: 0.03945587299683519\r",
      "iteration 386 - y: 0.03933742392876573\r",
      "iteration 387 - y: 0.03921897486069627\r",
      "iteration 388 - y: 0.03910052579262681\r",
      "iteration 389 - y: 0.03898207672455735\r",
      "iteration 390 - y: 0.03886362765648789\r",
      "iteration 391 - y: 0.03874517858841842\r",
      "iteration 392 - y: 0.03862672952034896\r",
      "iteration 393 - y: 0.0385082804522795\r",
      "iteration 394 - y: 0.03838983138421004\r",
      "iteration 395 - y: 0.03827138231614058\r",
      "iteration 396 - y: 0.03815293324807112\r",
      "iteration 397 - y: 0.03803448418000166\r",
      "iteration 398 - y: 0.0379160351119322\r",
      "iteration 399 - y: 0.03779758604386274\r",
      "iteration 400 - y: 0.03767913697579328\r",
      "iteration 401 - y: 0.03756068790772382\r",
      "iteration 402 - y: 0.03744223883965436\r",
      "iteration 403 - y: 0.0373237897715849\r",
      "iteration 404 - y: 0.037205340703515444\r",
      "iteration 405 - y: 0.037086891635445984\r",
      "iteration 406 - y: 0.036968442567376525\r",
      "iteration 407 - y: 0.036849993499307065\r",
      "iteration 408 - y: 0.036731544431237606\r",
      "iteration 409 - y: 0.03661309536316815\r",
      "iteration 410 - y: 0.03649464629509869\r",
      "iteration 411 - y: 0.03637619722702922\r",
      "iteration 412 - y: 0.03625774815895976\r",
      "iteration 413 - y: 0.0361392990908903\r",
      "iteration 414 - y: 0.03602085002282085\r",
      "iteration 415 - y: 0.03590240095475139\r",
      "iteration 416 - y: 0.03578395188668193\r",
      "iteration 417 - y: 0.03566550281861247\r",
      "iteration 418 - y: 0.03554705375054302\r",
      "iteration 419 - y: 0.03542860468247356\r",
      "iteration 420 - y: 0.03531015561440411\r",
      "iteration 421 - y: 0.03519170654633465\r",
      "iteration 422 - y: 0.03507325747826519\r",
      "iteration 423 - y: 0.03495480841019573\r",
      "iteration 424 - y: 0.034836359342126276\r",
      "iteration 425 - y: 0.03471791027405682\r",
      "iteration 426 - y: 0.034599461205987364\r",
      "iteration 427 - y: 0.034481012137917905\r",
      "iteration 428 - y: 0.034362563069848445\r",
      "iteration 429 - y: 0.034244114001778986\r",
      "iteration 430 - y: 0.03412566493370953\r",
      "iteration 431 - y: 0.034007215865640074\r",
      "iteration 432 - y: 0.033888766797570614\r",
      "iteration 433 - y: 0.033770317729501155\r",
      "iteration 434 - y: 0.0336518686614317\r",
      "iteration 435 - y: 0.03353341959336224\r",
      "iteration 436 - y: 0.033414970525292784\r",
      "iteration 437 - y: 0.033296521457223324\r",
      "iteration 438 - y: 0.03317807238915387\r",
      "iteration 439 - y: 0.03305962332108441\r",
      "iteration 440 - y: 0.03294117425301496\r",
      "iteration 441 - y: 0.0328227251849455\r",
      "iteration 442 - y: 0.03270427611687604\r",
      "iteration 443 - y: 0.03258582704880658\r",
      "iteration 444 - y: 0.03246737798073713\r",
      "iteration 445 - y: 0.03234892891266768\r",
      "iteration 446 - y: 0.03223047984459822\r",
      "iteration 447 - y: 0.03211203077652876\r",
      "iteration 448 - y: 0.0319935817084593\r",
      "iteration 449 - y: 0.031875132640389846\r",
      "iteration 450 - y: 0.031756683572320386\r",
      "iteration 451 - y: 0.03163823450425093\r",
      "iteration 452 - y: 0.03151978543618147\r",
      "iteration 453 - y: 0.031401336368112015\r",
      "iteration 454 - y: 0.031282887300042556\r",
      "iteration 455 - y: 0.0311644382319731\r",
      "iteration 456 - y: 0.03104598916390364\r",
      "iteration 457 - y: 0.030927540095834184\r",
      "iteration 458 - y: 0.030809091027764725\r",
      "iteration 459 - y: 0.030690641959695272\r",
      "iteration 460 - y: 0.030572192891625813\r",
      "iteration 461 - y: 0.030453743823556353\r",
      "iteration 462 - y: 0.030335294755486894\r",
      "iteration 463 - y: 0.03021684568741744\r",
      "iteration 464 - y: 0.030098396619347982\r",
      "iteration 465 - y: 0.029979947551278526\r",
      "iteration 466 - y: 0.029861498483209067\r",
      "iteration 467 - y: 0.02974304941513961\r",
      "iteration 468 - y: 0.02962460034707015\r",
      "iteration 469 - y: 0.0295061512790007\r",
      "iteration 470 - y: 0.02938770221093124\r",
      "iteration 471 - y: 0.02926925314286178\r",
      "iteration 472 - y: 0.02915080407479232\r",
      "iteration 473 - y: 0.029032355006722868\r",
      "iteration 474 - y: 0.02891390593865341\r",
      "iteration 475 - y: 0.028795456870583953\r",
      "iteration 476 - y: 0.028677007802514493\r",
      "iteration 477 - y: 0.028558558734445037\r",
      "iteration 478 - y: 0.028440109666375578\r",
      "iteration 479 - y: 0.028321660598306125\r",
      "iteration 480 - y: 0.028203211530236666\r",
      "iteration 481 - y: 0.028084762462167207\r",
      "iteration 482 - y: 0.02796631339409775\r",
      "iteration 483 - y: 0.027847864326028295\r",
      "iteration 484 - y: 0.027729415257958835\r",
      "iteration 485 - y: 0.02761096618988938\r",
      "iteration 486 - y: 0.02749251712181992\r",
      "iteration 487 - y: 0.027374068053750464\r",
      "iteration 488 - y: 0.027255618985681004\r",
      "iteration 489 - y: 0.027137169917611552\r",
      "iteration 490 - y: 0.027018720849542092\r",
      "iteration 491 - y: 0.026900271781472633\r",
      "iteration 492 - y: 0.02678182271340318\r",
      "iteration 493 - y: 0.02666337364533372\r",
      "iteration 494 - y: 0.02654492457726426\r",
      "iteration 495 - y: 0.026426475509194806\r",
      "iteration 496 - y: 0.02630802644112535\r",
      "iteration 497 - y: 0.02618957737305589\r",
      "iteration 498 - y: 0.026071128304986434\r",
      "iteration 499 - y: 0.02595267923691698\r",
      "iteration 500 - y: 0.02583423016884752\r",
      "iteration 501 - y: 0.025715781100778063\r",
      "iteration 502 - y: 0.025597332032708607\r",
      "iteration 503 - y: 0.025478882964639148\r",
      "iteration 504 - y: 0.025360433896569688\r",
      "iteration 505 - y: 0.025241984828500232\r",
      "iteration 506 - y: 0.025123535760430776\r",
      "iteration 507 - y: 0.025005086692361317\r",
      "iteration 508 - y: 0.02488663762429186\r",
      "iteration 509 - y: 0.024768188556222405\r",
      "iteration 510 - y: 0.024649739488152946\r",
      "iteration 511 - y: 0.02453129042008349\r",
      "iteration 512 - y: 0.024412841352014034\r",
      "iteration 513 - y: 0.024294392283944574\r",
      "iteration 514 - y: 0.024175943215875118\r",
      "iteration 515 - y: 0.024057494147805662\r",
      "iteration 516 - y: 0.023939045079736203\r",
      "iteration 517 - y: 0.023820596011666747\r",
      "iteration 518 - y: 0.02370214694359729\r",
      "iteration 519 - y: 0.02358369787552783\r",
      "iteration 520 - y: 0.023465248807458376\r",
      "iteration 521 - y: 0.02334679973938892\r",
      "iteration 522 - y: 0.02322835067131946\r",
      "iteration 523 - y: 0.02310990160325\r",
      "iteration 524 - y: 0.022991452535180545\r",
      "iteration 525 - y: 0.02287300346711109\r",
      "iteration 526 - y: 0.02275455439904163\r",
      "iteration 527 - y: 0.022636105330972173\r",
      "iteration 528 - y: 0.022517656262902717\r",
      "iteration 529 - y: 0.022399207194833258\r",
      "iteration 530 - y: 0.022280758126763802\r",
      "iteration 531 - y: 0.022162309058694346\r",
      "iteration 532 - y: 0.022043859990624887\r",
      "iteration 533 - y: 0.021925410922555427\r",
      "iteration 534 - y: 0.021806961854485975\r",
      "iteration 535 - y: 0.021688512786416515\r",
      "iteration 536 - y: 0.021570063718347056\r",
      "iteration 537 - y: 0.0214516146502776\r",
      "iteration 538 - y: 0.021333165582208144\r",
      "iteration 539 - y: 0.021214716514138685\r",
      "iteration 540 - y: 0.02109626744606923\r",
      "iteration 541 - y: 0.020977818377999773\r",
      "iteration 542 - y: 0.020859369309930313\r",
      "iteration 543 - y: 0.020740920241860854\r",
      "iteration 544 - y: 0.0206224711737914\r",
      "iteration 545 - y: 0.020504022105721942\r",
      "iteration 546 - y: 0.020385573037652482\r",
      "iteration 547 - y: 0.020267123969583026\r",
      "iteration 548 - y: 0.02014867490151357\r",
      "iteration 549 - y: 0.02003022583344411\r",
      "iteration 550 - y: 0.019911776765374655\r",
      "iteration 551 - y: 0.0197933276973052\r",
      "iteration 552 - y: 0.01967487862923574\r",
      "iteration 553 - y: 0.019556429561166284\r",
      "iteration 554 - y: 0.019437980493096828\r",
      "iteration 555 - y: 0.01931953142502737\r",
      "iteration 556 - y: 0.01920108235695791\r",
      "iteration 557 - y: 0.019082633288888453\r",
      "iteration 558 - y: 0.018964184220818997\r",
      "iteration 559 - y: 0.018845735152749538\r",
      "iteration 560 - y: 0.018727286084680085\r",
      "iteration 561 - y: 0.018608837016610626\r",
      "iteration 562 - y: 0.01849038794854117\r",
      "iteration 563 - y: 0.01837193888047171\r",
      "iteration 564 - y: 0.018253489812402254\r",
      "iteration 565 - y: 0.018135040744332795\r",
      "iteration 566 - y: 0.01801659167626334\r",
      "iteration 567 - y: 0.017898142608193883\r",
      "iteration 568 - y: 0.017779693540124424\r",
      "iteration 569 - y: 0.017661244472054968\r",
      "iteration 570 - y: 0.01754279540398551\r",
      "iteration 571 - y: 0.017424346335916052\r",
      "iteration 572 - y: 0.017305897267846596\r",
      "iteration 573 - y: 0.017187448199777137\r",
      "iteration 574 - y: 0.01706899913170768\r",
      "iteration 575 - y: 0.01695055006363822\r",
      "iteration 576 - y: 0.016832100995568765\r",
      "iteration 577 - y: 0.01671365192749931\r",
      "iteration 578 - y: 0.01659520285942985\r",
      "iteration 579 - y: 0.016476753791360394\r",
      "iteration 580 - y: 0.016358304723290938\r",
      "iteration 581 - y: 0.01623985565522148\r",
      "iteration 582 - y: 0.016121406587152023\r",
      "iteration 583 - y: 0.016002957519082567\r",
      "iteration 584 - y: 0.015884508451013107\r",
      "iteration 585 - y: 0.01576605938294365\r",
      "iteration 586 - y: 0.015647610314874192\r",
      "iteration 587 - y: 0.015529161246804736\r",
      "iteration 588 - y: 0.015410712178735278\r",
      "iteration 589 - y: 0.01529226311066582\r",
      "iteration 590 - y: 0.015173814042596365\r",
      "iteration 591 - y: 0.015055364974526905\r",
      "iteration 592 - y: 0.01493691590645745\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 610 - y: 0.00039503917889402044\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.01142164]], dtype=float32), array([[0.01142811]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ref = 0.0004\n",
    "best_model = K.loadBestModel()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': y_ref}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a104b5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b663237e320>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74a85bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b663240e740>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = TrainLR(M_samples, H_samples, y_ref = y_ref, fit_intercept = False)\n",
    "regr = L.quickTrain()\n",
    "\n",
    "XL = XLR(regr, M_samples)\n",
    "a_LR, stats_LR = XL.quick_analyze()\n",
    "\n",
    "plt.plot(a_LR[0])\n",
    "plt.plot(a_LR[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
