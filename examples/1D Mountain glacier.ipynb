{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd7382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 19:35:54.229534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 19:35:54.442573: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/gnu7/openmpi/netcdf/4.5.0/lib:/opt/ohpc/pub/libs/gnu7/openmpi/netcdf-fortran/4.4.4/lib:/opt/ohpc/pub/libs/gnu7/openmpi/hdf5/1.10.1/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64:/home/shreyas/lis-1.4.43/installation/lib:/share/jdk-16.0.1/lib::\n",
      "2023-06-28 19:35:54.442618: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-28 19:35:55.618646: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/gnu7/openmpi/netcdf/4.5.0/lib:/opt/ohpc/pub/libs/gnu7/openmpi/netcdf-fortran/4.4.4/lib:/opt/ohpc/pub/libs/gnu7/openmpi/hdf5/1.10.1/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64:/home/shreyas/lis-1.4.43/installation/lib:/share/jdk-16.0.1/lib::\n",
      "2023-06-28 19:35:55.618815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/gnu7/openmpi/netcdf/4.5.0/lib:/opt/ohpc/pub/libs/gnu7/openmpi/netcdf-fortran/4.4.4/lib:/opt/ohpc/pub/libs/gnu7/openmpi/hdf5/1.10.1/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64:/home/shreyas/lis-1.4.43/installation/lib:/share/jdk-16.0.1/lib::\n",
      "2023-06-28 19:35:55.618831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import xarray as xr\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Input, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.keras.backend as kbackend\n",
    "import tensorflow.keras.models\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "import os\n",
    "import sys\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "import XAIRT\n",
    "import XAIRT.backend as backend\n",
    "from XAIRT.backend.graph import getLayerIndexByName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basal_topology_func(x):\n",
    "    b = 1.0 - 0.1*x\n",
    "    return b\n",
    "\n",
    "def solution(nx, nt, L, T, M, basal_topology_func):\n",
    "\n",
    "    if len(M) != nx + 1:\n",
    "        raise ValueError('M specified but len(M) != nx + 1')\n",
    "        \n",
    "    dx = L/nx\n",
    "    dt = T/nt\n",
    "    x = np.linspace(0,L,nx+1)\n",
    "    t = np.linspace(0,T,nt+1)\n",
    "\n",
    "    b = basal_topology_func(x)\n",
    "\n",
    "    A = 1e-16\n",
    "    rho = 920.0\n",
    "    g = 9.2 \n",
    "    n = 3\n",
    "\n",
    "    C = 2*A/(n+2) * (rho*g)**n * (1e3)**n\n",
    "\n",
    "    h = np.zeros((nx+1,nt+1))\n",
    "    H = np.zeros((nx+1,nt+1))\n",
    "    h[:,0] = b\n",
    "    h[0,:] = b[0]\n",
    "    h[-1,:] = b[-1]\n",
    "\n",
    "    H[:,0] = h[:,0] - b\n",
    "    H[0,:] = h[0,:] - b[0]\n",
    "    H[-1,:] = h[-1,:] - b[-1]\n",
    "\n",
    "    for i in range(1,len(t)):\n",
    "\n",
    "        D = C *((H[1:,i-1]+H[:nx,i-1])/2.0)**(n+2) * ((h[1:,i-1] - h[:nx,i-1])/dx)**(n-1)\n",
    "\n",
    "        phi = -D*(h[1:,i-1]-h[:nx,i-1])/dx\n",
    "\n",
    "        h[1:nx,i] = h[1:nx,i-1] + M[1:nx]*dt - dt/dx * (phi[1:]-phi[:nx-1])\n",
    "        h[1:nx,i] = (h[1:nx,i] < b[1:nx]) * b[1:nx] + (h[1:nx,i] >= b[1:nx]) * h[1:nx,i]\n",
    "        H[:,i] = np.maximum(h[:,i] - b, 0.)\n",
    "\n",
    "        if not np.any(H[:,i]>=0.0):\n",
    "            raise Exception(\"Something went wrong.\")\n",
    "            \n",
    "    Volume = np.sum(H)*dx\n",
    "    \n",
    "    return H[int(nx/2),-1], h[int(nx/2),-1], Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1038f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L = 30.\n",
    "T = 10.\n",
    "nx = 300\n",
    "nt = 12000\n",
    "samples = 10\n",
    "\n",
    "M_samples = 0.01*np.random.rand(samples, nx+1)\n",
    "H_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "Volume_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "\n",
    "for sample in range(samples):\n",
    "    if (sample+1) % 10 == 0:\n",
    "        print(f'Sample #{sample+1}', end='\\r')\n",
    "    H_samples[sample], _, Volume_samples[sample] = solution(nx, nt, L, T, M_samples[sample], basal_topology_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab204157",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "inputs = Input(shape=(nx+1,))\n",
    "dense1 = Dense(10, activation='relu')\n",
    "dense2 = Dense(1, activation='linear', use_bias=False)\n",
    "\n",
    "x = dense1(inputs)\n",
    "x = dense2(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=x) \n",
    "\n",
    "mod_h5 = 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(mod_h5, monitor='val_loss',\n",
    "                             verbose=1,save_best_only=True)\n",
    "        \n",
    "callbacks = [checkpoint]\n",
    "model.compile(optimizer='adam',\n",
    "            loss='mse', metrics=['mae'])\n",
    "\n",
    "fit = model.fit(M_samples, H_samples,\n",
    "            batch_size=10,\n",
    "            epochs=10, \n",
    "            shuffle=True,\n",
    "            validation_split = 0.2, \n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(nx+1,))\n",
    "dense1 = Dense(10, activation='relu')\n",
    "dense2 = Dense(1, activation='linear', use_bias=False)\n",
    "\n",
    "x = dense1(inputs)\n",
    "x = dense2(x)\n",
    "\n",
    "best_model = keras.Model(inputs=inputs, outputs=x) \n",
    "best_model.load_weights(mod_h5)\n",
    "best_model.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "\n",
    "H_pred = best_model.predict(M_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_gradient = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", best_model)\n",
    "a_alb0 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "for sample in range(samples):\n",
    "    a_alb0[sample] = analyzer_gradient.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a_alb0[sample] /= np.max(np.abs(a_alb0[sample]))\n",
    "\n",
    "plt.plot(a_alb0[0])\n",
    "plt.plot(a_alb0[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_gradient = innvestigate.create_analyzer(\"lrp.z\", best_model)\n",
    "a_z = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "for sample in range(samples):\n",
    "    a_z[sample] = analyzer_gradient.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a_z[sample] /= np.max(np.abs(a_z[sample]))\n",
    "\n",
    "plt.plot(a_z[0])\n",
    "plt.plot(a_z[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_letzgus(model, input_, y_ref, step_width=0.00005, max_it=10e4, method = \"flood\"):\n",
    "    \n",
    "    ### Finding a_ref for a given y_ref\n",
    "    \n",
    "    if method == \"flood\":\n",
    "        \n",
    "        model_part = Model(inputs=model.input,\n",
    "                          outputs=model.layers[-2].output)\n",
    "        a_ref = model_part.predict(input_)[0,:]\n",
    "        a_ref = a_ref[:, np.newaxis]\n",
    "        update = np.ones(a_ref.shape) * step_width\n",
    "        \n",
    "        y = model.predict(input_)\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        if y > y_ref:\n",
    "            \n",
    "            while y >= y_ref:\n",
    "                \n",
    "                a_ref = np.maximum(np.zeros(a_ref.shape),a_ref-update)\n",
    "                y = np.dot(model.layers[-1].get_weights()[0][:,0], a_ref[:,0])\n",
    "                counter +=1 \n",
    "                print(f'iteration {counter} - y: {y}', end='\\r')\n",
    "                if counter > max_it:\n",
    "                    print(f'! reference value {y_ref} was not reached within {round(max_it)} iterations!')\n",
    "                    break\n",
    "        else:\n",
    "            \n",
    "            while y <= y_ref:\n",
    "                \n",
    "                a_ref = np.maximum(np.zeros(a_ref.shape),a_ref+update)\n",
    "                y = np.dot(model.layers[-1].get_weights()[0][:,0], a_ref[:,0])\n",
    "                counter +=1 \n",
    "                print(f'iteration {counter} - y: {y}', end='\\r')\n",
    "                if counter > max_it:\n",
    "                    print(f'! reference value {y_ref} was not reached within {round(max_it)} iterations!')\n",
    "                    break\n",
    "                    \n",
    "    else: \n",
    "        \n",
    "        raise ValueError(\"The only methods available are : flood\")\n",
    "        \n",
    "    return a_ref\n",
    "\n",
    "def triplicated_model(best_model, a_ref):\n",
    "    \n",
    "    # get weights and biases\n",
    "    W_in = best_model.layers[-2].get_weights()[0]\n",
    "    W_out = best_model.layers[-1].get_weights()[0]\n",
    "    bias_in = best_model.layers[-2].get_weights()[1]\n",
    "\n",
    "    inputs = Input(shape=(nx+1,))\n",
    "\n",
    "    # layer_dict = dict([(layer.name, layer) for layer in model.layers[:-2]])\n",
    "    # weights = layer_dict['some_name'].get_weights()\n",
    "\n",
    "    dense11 = Dense(10, activation='relu', name='dense11')\n",
    "    dense12 = Dense(10, activation='relu', name='dense12')\n",
    "    dense13 = Dense(10, activation='relu', name='dense13')\n",
    "    dense21 = Dense(1, activation='linear', use_bias=False, name='dense21')\n",
    "    dense22 = Dense(1, activation='linear', use_bias=False, name='dense22')\n",
    "    dense23 = Dense(1, activation='linear', use_bias=False, name='dense23')\n",
    "\n",
    "    x1 = dense11(inputs)\n",
    "    x2 = dense12(inputs)\n",
    "    x3 = dense13(inputs)\n",
    "\n",
    "    x1 = dense21(x1)\n",
    "    x2 = dense22(x2)\n",
    "    x3 = dense23(x3)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=x1+x2+x3) \n",
    "    \n",
    "    #     def getLayerIndexByName(model, layername):\n",
    "    #         for idx, layer in enumerate(model.layers):\n",
    "    #             if layer.name == layername:\n",
    "    #                 return idx\n",
    "    #         raise Exception(f\"layername: {layername} not found.\")\n",
    "            \n",
    "    model.layers[getLayerIndexByName(model, 'dense11')].set_weights([W_in, bias_in-a_ref[:,0]])\n",
    "    model.layers[getLayerIndexByName(model, 'dense12')].set_weights([-W_in, -bias_in])                 \n",
    "    model.layers[getLayerIndexByName(model, 'dense13')].set_weights([-W_in, -bias_in+a_ref[:,0]])\n",
    "    \n",
    "    model.layers[getLayerIndexByName(model, 'dense21')].set_weights([W_out])\n",
    "    model.layers[getLayerIndexByName(model, 'dense22')].set_weights([W_out])\n",
    "    model.layers[getLayerIndexByName(model, 'dense23')].set_weights([-W_out])\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb7383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "y = best_model.predict(M_samples[np.newaxis, index])\n",
    "y_ref = y / 2.0\n",
    "\n",
    "model_reg = keras.Model(inputs=inputs, outputs=x) \n",
    "model_reg.load_weights(mod_h5)\n",
    "model_reg.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "a_ref = regression_letzgus(model_reg, M_samples[np.newaxis, index], y_ref)\n",
    "\n",
    "model_reg = keras.Model(inputs=inputs, outputs=x) \n",
    "model_reg.load_weights(mod_h5)\n",
    "model_reg.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "tri_model = triplicated_model(model_reg, a_ref)\n",
    "y_reg = tri_model.predict(M_samples[np.newaxis, index])\n",
    "\n",
    "\n",
    "y_ref, y-y_reg, y, y_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff16bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "y = best_model.predict(M_samples[np.newaxis, index])\n",
    "y_ref = y * 2.0\n",
    "\n",
    "model_reg = keras.Model(inputs=inputs, outputs=x) \n",
    "model_reg.load_weights(mod_h5)\n",
    "model_reg.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "a_ref = regression_letzgus(model_reg, M_samples[np.newaxis, index], y_ref)\n",
    "\n",
    "model_reg = keras.Model(inputs=inputs, outputs=x) \n",
    "model_reg.load_weights(mod_h5)\n",
    "model_reg.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "tri_model = triplicated_model(model_reg, a_ref)\n",
    "y_reg = tri_model.predict(M_samples[np.newaxis, index])\n",
    "\n",
    "y_ref, y-y_reg, y, y_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7611a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplicated_model_LRP_compatible(best_model, a_ref):\n",
    "    \n",
    "    # get weights and biases\n",
    "    W_in = best_model.layers[-2].get_weights()[0]\n",
    "    W_out = best_model.layers[-1].get_weights()[0]\n",
    "    bias_in = best_model.layers[-2].get_weights()[1]\n",
    "\n",
    "    inputs = Input(shape=(nx+1,))\n",
    "\n",
    "    # layer_dict = dict([(layer.name, layer) for layer in model.layers[:-2]])\n",
    "    # weights = layer_dict['some_name'].get_weights()\n",
    "\n",
    "    dense11 = Dense(10, activation='relu', name='dense11')\n",
    "    dense12 = Dense(10, activation='relu', name='dense12')\n",
    "    dense13 = Dense(10, activation='relu', name='dense13')\n",
    "    dense21 = Dense(1, activation='linear', use_bias=False, name='dense21')\n",
    "    dense22 = Dense(1, activation='linear', use_bias=False, name='dense22')\n",
    "    dense23 = Dense(1, activation='linear', use_bias=False, name='dense23')\n",
    "\n",
    "    x1 = dense11(inputs)\n",
    "    x2 = dense12(inputs)\n",
    "    x3 = dense13(inputs)\n",
    "\n",
    "    x1 = dense21(x1)\n",
    "    x2 = dense22(x2)\n",
    "    x3 = dense23(x3)\n",
    "\n",
    "    model1 = keras.Model(inputs=inputs, outputs=x1)\n",
    "    model2 = keras.Model(inputs=inputs, outputs=x2) \n",
    "    model3 = keras.Model(inputs=inputs, outputs=x3) \n",
    "            \n",
    "    model1.layers[getLayerIndexByName(model1, 'dense11')].set_weights([W_in, bias_in-a_ref[:,0]])\n",
    "    model2.layers[getLayerIndexByName(model2, 'dense12')].set_weights([-W_in, -bias_in])                 \n",
    "    model3.layers[getLayerIndexByName(model3, 'dense13')].set_weights([-W_in, -bias_in+a_ref[:,0]])\n",
    "    \n",
    "    model1.layers[getLayerIndexByName(model1, 'dense21')].set_weights([W_out])\n",
    "    model2.layers[getLayerIndexByName(model2, 'dense22')].set_weights([W_out])\n",
    "    model3.layers[getLayerIndexByName(model3, 'dense23')].set_weights([-W_out])\n",
    "    \n",
    "    model1.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "    model2.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "    model3.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "    \n",
    "    return model1, model2, model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09751739",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "y = best_model.predict(M_samples[np.newaxis, index])\n",
    "y_ref = y * 2.0\n",
    "\n",
    "model_reg = keras.Model(inputs=inputs, outputs=x) \n",
    "model_reg.load_weights(mod_h5)\n",
    "model_reg.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "a_ref = regression_letzgus(model_reg, M_samples[np.newaxis, 0], y_ref)\n",
    "\n",
    "model1, model2, model3 = triplicated_model_LRP_compatible(best_model, a_ref)\n",
    "y1 = model1.predict(M_samples[np.newaxis, index])\n",
    "y2 = model1.predict(M_samples[np.newaxis, index])\n",
    "y3 = model1.predict(M_samples[np.newaxis, index])\n",
    "\n",
    "analyzer_gradient1 = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", model1)\n",
    "analyzer_gradient2 = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", model2)\n",
    "analyzer_gradient3 = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", model3)\n",
    "a1 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "a2 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "a3 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "a  = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "\n",
    "for sample in range(samples):\n",
    "    a1[sample] = analyzer_gradient1.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a2[sample] = analyzer_gradient2.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a3[sample] = analyzer_gradient3.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a[sample]  = a1[sample] + a2[sample] + a3[sample]\n",
    "    a[sample]  = a[sample] / np.max(np.abs(a[sample]))\n",
    "    \n",
    "y, y_ref, y-(y1+y2+y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Notice how sign of H_samples[index]-y_ref matters\n",
    "\n",
    "index = 0\n",
    "plt.plot(a[index])\n",
    "H_samples[index]-y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a04b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Notice how sign of H_samples[index]-y_ref matters\n",
    "\n",
    "index = 1\n",
    "plt.plot(a[index])\n",
    "H_samples[index]-y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ae2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Notice how sign of H_samples[index]-y_ref matters\n",
    "\n",
    "index = 100\n",
    "plt.plot(a[index])\n",
    "H_samples[index]-y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499aa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "y = best_model.predict(M_samples[np.newaxis, index])\n",
    "y_ref = y / 3.0\n",
    "\n",
    "model_reg = keras.Model(inputs=inputs, outputs=x) \n",
    "model_reg.load_weights(mod_h5)\n",
    "model_reg.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "a_ref = regression_letzgus(model_reg, M_samples[np.newaxis, 0], y_ref)\n",
    "\n",
    "model1, model2, model3 = triplicated_model_LRP_compatible(best_model, a_ref)\n",
    "y1 = model1.predict(M_samples[np.newaxis, index])\n",
    "y2 = model2.predict(M_samples[np.newaxis, index])\n",
    "y3 = model3.predict(M_samples[np.newaxis, index])\n",
    "\n",
    "analyzer_gradient1 = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", model1)\n",
    "analyzer_gradient2 = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", model2)\n",
    "analyzer_gradient3 = innvestigate.create_analyzer(\"lrp.alpha_1_beta_0\", model3)\n",
    "a1 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "a2 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "a3 = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "a  = np.zeros((samples, nx+1), dtype = np.float64)\n",
    "\n",
    "for sample in range(samples):\n",
    "    a1[sample] = analyzer_gradient1.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a2[sample] = analyzer_gradient2.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a3[sample] = analyzer_gradient3.analyze(M_samples[sample][np.newaxis,:])\n",
    "    a[sample]  = a1[sample] + a2[sample] + a3[sample]\n",
    "    a[sample]  = a[sample] / np.max(np.abs(a[sample]))\n",
    "    \n",
    "y, y_ref, y-(y1+y2+y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67419074",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "plt.plot(a[index])\n",
    "H_samples[index]-y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "plt.plot(a[index])\n",
    "H_samples[index]-y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "plt.plot(a[index])\n",
    "H_samples[index]-y_ref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_LRP",
   "language": "python",
   "name": "py38_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
