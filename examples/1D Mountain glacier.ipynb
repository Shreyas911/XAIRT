{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd7382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82c18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #200\r"
     ]
    }
   ],
   "source": [
    "def basal_topology_func(x):\n",
    "    b = 1.0 - 0.1*x\n",
    "    return b\n",
    "\n",
    "def solution(nx, nt, L, T, M, basal_topology_func):\n",
    "\n",
    "    if len(M) != nx + 1:\n",
    "        raise ValueError('M specified but len(M) != nx + 1')\n",
    "        \n",
    "    dx = L/nx\n",
    "    dt = T/nt\n",
    "    x = np.linspace(0,L,nx+1)\n",
    "    t = np.linspace(0,T,nt+1)\n",
    "\n",
    "    b = basal_topology_func(x)\n",
    "\n",
    "    A = 1e-16\n",
    "    rho = 920.0\n",
    "    g = 9.2 \n",
    "    n = 3\n",
    "\n",
    "    C = 2*A/(n+2) * (rho*g)**n * (1e3)**n\n",
    "\n",
    "    h = np.zeros((nx+1,nt+1))\n",
    "    H = np.zeros((nx+1,nt+1))\n",
    "    h[:,0] = b\n",
    "    h[0,:] = b[0]\n",
    "    h[-1,:] = b[-1]\n",
    "\n",
    "    H[:,0] = h[:,0] - b\n",
    "    H[0,:] = h[0,:] - b[0]\n",
    "    H[-1,:] = h[-1,:] - b[-1]\n",
    "\n",
    "    for i in range(1,len(t)):\n",
    "\n",
    "        D = C *((H[1:,i-1]+H[:nx,i-1])/2.0)**(n+2) * ((h[1:,i-1] - h[:nx,i-1])/dx)**(n-1)\n",
    "\n",
    "        phi = -D*(h[1:,i-1]-h[:nx,i-1])/dx\n",
    "\n",
    "        h[1:nx,i] = h[1:nx,i-1] + M[1:nx]*dt - dt/dx * (phi[1:]-phi[:nx-1])\n",
    "        h[1:nx,i] = (h[1:nx,i] < b[1:nx]) * b[1:nx] + (h[1:nx,i] >= b[1:nx]) * h[1:nx,i]\n",
    "        H[:,i] = np.maximum(h[:,i] - b, 0.)\n",
    "\n",
    "        if not np.any(H[:,i]>=0.0):\n",
    "            raise Exception(\"Something went wrong.\")\n",
    "            \n",
    "    Volume = np.sum(H)*dx\n",
    "    \n",
    "    return H[int(nx/2),-1], h[int(nx/2),-1], Volume\n",
    "\n",
    "L = 30.\n",
    "T = 10.\n",
    "nx = 300\n",
    "nt = 12000\n",
    "samples = 200\n",
    "\n",
    "M_samples = 0.01*np.random.rand(samples, nx+1)\n",
    "H_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "Volume_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "\n",
    "for sample in range(samples):\n",
    "    if (sample+1) % 100 == 0:\n",
    "        print(f'Sample #{sample+1}', end='\\r')\n",
    "    H_samples[sample], _, Volume_samples[sample] = solution(nx, nt, L, T, M_samples[sample], basal_topology_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9844b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/500\n",
      " 10/160 [>.............................] - ETA: 1s - loss: 0.0026 - mae: 0.0453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 17:29:34.808261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/gnu7/openmpi/netcdf/4.5.0/lib:/opt/ohpc/pub/libs/gnu7/openmpi/netcdf-fortran/4.4.4/lib:/opt/ohpc/pub/libs/gnu7/openmpi/hdf5/1.10.1/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64:/home/shreyas/lis-1.4.43/installation/lib:/share/jdk-16.0.1/lib::\n",
      "2023-07-03 17:29:34.808310: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00047, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 0.0011 - mae: 0.0280 - val_loss: 4.6743e-04 - val_mae: 0.0189\n",
      "Epoch 2/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6393e-04 - mae: 0.0225\n",
      "Epoch 2: val_loss did not improve from 0.00047\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 5.1891e-04 - mae: 0.0190 - val_loss: 4.8512e-04 - val_mae: 0.0193\n",
      "Epoch 3/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0028e-04 - mae: 0.0167\n",
      "Epoch 3: val_loss did not improve from 0.00047\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 4.8246e-04 - mae: 0.0189 - val_loss: 4.8039e-04 - val_mae: 0.0192\n",
      "Epoch 4/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3410e-04 - mae: 0.0176\n",
      "Epoch 4: val_loss did not improve from 0.00047\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 4.7062e-04 - mae: 0.0184 - val_loss: 4.7699e-04 - val_mae: 0.0191\n",
      "Epoch 5/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6344e-04 - mae: 0.0147\n",
      "Epoch 5: val_loss did not improve from 0.00047\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 4.4942e-04 - mae: 0.0180 - val_loss: 4.7559e-04 - val_mae: 0.0191\n",
      "Epoch 6/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5113e-04 - mae: 0.0195\n",
      "Epoch 6: val_loss did not improve from 0.00047\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 4.3620e-04 - mae: 0.0178 - val_loss: 4.7054e-04 - val_mae: 0.0190\n",
      "Epoch 7/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0344e-04 - mae: 0.0180\n",
      "Epoch 7: val_loss improved from 0.00047 to 0.00047, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 321us/sample - loss: 4.3244e-04 - mae: 0.0179 - val_loss: 4.6704e-04 - val_mae: 0.0189\n",
      "Epoch 8/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0370e-04 - mae: 0.0185\n",
      "Epoch 8: val_loss did not improve from 0.00047\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 4.2252e-04 - mae: 0.0175 - val_loss: 4.6734e-04 - val_mae: 0.0190\n",
      "Epoch 9/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0991e-04 - mae: 0.0126\n",
      "Epoch 9: val_loss improved from 0.00047 to 0.00046, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 329us/sample - loss: 4.2878e-04 - mae: 0.0177 - val_loss: 4.6193e-04 - val_mae: 0.0188\n",
      "Epoch 10/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4613e-04 - mae: 0.0191\n",
      "Epoch 10: val_loss did not improve from 0.00046\n",
      "160/160 [==============================] - 0s 203us/sample - loss: 4.0225e-04 - mae: 0.0173 - val_loss: 4.6410e-04 - val_mae: 0.0189\n",
      "Epoch 11/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8901e-04 - mae: 0.0218\n",
      "Epoch 11: val_loss improved from 0.00046 to 0.00046, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 343us/sample - loss: 4.0741e-04 - mae: 0.0170 - val_loss: 4.6134e-04 - val_mae: 0.0188\n",
      "Epoch 12/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2287e-04 - mae: 0.0217\n",
      "Epoch 12: val_loss improved from 0.00046 to 0.00045, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 376us/sample - loss: 3.9037e-04 - mae: 0.0169 - val_loss: 4.5282e-04 - val_mae: 0.0186\n",
      "Epoch 13/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2427e-04 - mae: 0.0198\n",
      "Epoch 13: val_loss did not improve from 0.00045\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 3.7904e-04 - mae: 0.0165 - val_loss: 4.5441e-04 - val_mae: 0.0187\n",
      "Epoch 14/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6360e-04 - mae: 0.0194\n",
      "Epoch 14: val_loss improved from 0.00045 to 0.00045, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 339us/sample - loss: 3.6652e-04 - mae: 0.0164 - val_loss: 4.4779e-04 - val_mae: 0.0185\n",
      "Epoch 15/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7525e-04 - mae: 0.0153\n",
      "Epoch 15: val_loss did not improve from 0.00045\n",
      "160/160 [==============================] - 0s 233us/sample - loss: 3.6397e-04 - mae: 0.0164 - val_loss: 4.7734e-04 - val_mae: 0.0190\n",
      "Epoch 16/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5367e-04 - mae: 0.0175\n",
      "Epoch 16: val_loss improved from 0.00045 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 394us/sample - loss: 3.5928e-04 - mae: 0.0163 - val_loss: 4.4446e-04 - val_mae: 0.0184\n",
      "Epoch 17/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9672e-04 - mae: 0.0156\n",
      "Epoch 17: val_loss improved from 0.00044 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 374us/sample - loss: 3.3963e-04 - mae: 0.0158 - val_loss: 4.4132e-04 - val_mae: 0.0183\n",
      "Epoch 18/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7447e-04 - mae: 0.0138\n",
      "Epoch 18: val_loss did not improve from 0.00044\n",
      "160/160 [==============================] - 0s 231us/sample - loss: 3.3469e-04 - mae: 0.0157 - val_loss: 4.5100e-04 - val_mae: 0.0185\n",
      "Epoch 19/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7566e-04 - mae: 0.0151\n",
      "Epoch 19: val_loss improved from 0.00044 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 362us/sample - loss: 3.1906e-04 - mae: 0.0153 - val_loss: 4.3634e-04 - val_mae: 0.0180\n",
      "Epoch 20/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6872e-04 - mae: 0.0175\n",
      "Epoch 20: val_loss improved from 0.00044 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 379us/sample - loss: 3.2553e-04 - mae: 0.0155 - val_loss: 4.3316e-04 - val_mae: 0.0180\n",
      "Epoch 21/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0863e-04 - mae: 0.0147\n",
      "Epoch 21: val_loss did not improve from 0.00043\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 3.2854e-04 - mae: 0.0153 - val_loss: 4.4223e-04 - val_mae: 0.0183\n",
      "Epoch 22/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7530e-04 - mae: 0.0146\n",
      "Epoch 22: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 301us/sample - loss: 2.9684e-04 - mae: 0.0147 - val_loss: 4.3212e-04 - val_mae: 0.0181\n",
      "Epoch 23/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4633e-04 - mae: 0.0109\n",
      "Epoch 23: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 297us/sample - loss: 3.0193e-04 - mae: 0.0147 - val_loss: 4.2602e-04 - val_mae: 0.0178\n",
      "Epoch 24/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9201e-04 - mae: 0.0110\n",
      "Epoch 24: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 357us/sample - loss: 2.8372e-04 - mae: 0.0144 - val_loss: 4.2594e-04 - val_mae: 0.0179\n",
      "Epoch 25/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6905e-04 - mae: 0.0123\n",
      "Epoch 25: val_loss improved from 0.00043 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 415us/sample - loss: 2.7473e-04 - mae: 0.0141 - val_loss: 4.2257e-04 - val_mae: 0.0177\n",
      "Epoch 26/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9177e-04 - mae: 0.0106\n",
      "Epoch 26: val_loss did not improve from 0.00042\n",
      "160/160 [==============================] - 0s 227us/sample - loss: 2.6523e-04 - mae: 0.0138 - val_loss: 4.5365e-04 - val_mae: 0.0186\n",
      "Epoch 27/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6385e-04 - mae: 0.0177\n",
      "Epoch 27: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 359us/sample - loss: 2.5988e-04 - mae: 0.0137 - val_loss: 4.1709e-04 - val_mae: 0.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0436e-04 - mae: 0.0128\n",
      "Epoch 28: val_loss did not improve from 0.00042\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 2.5449e-04 - mae: 0.0137 - val_loss: 4.2504e-04 - val_mae: 0.0178\n",
      "Epoch 29/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3563e-04 - mae: 0.0094\n",
      "Epoch 29: val_loss improved from 0.00042 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 298us/sample - loss: 2.4511e-04 - mae: 0.0133 - val_loss: 4.1471e-04 - val_mae: 0.0175\n",
      "Epoch 30/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7386e-04 - mae: 0.0145\n",
      "Epoch 30: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 299us/sample - loss: 2.3410e-04 - mae: 0.0129 - val_loss: 4.1164e-04 - val_mae: 0.0173\n",
      "Epoch 31/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5830e-04 - mae: 0.0132\n",
      "Epoch 31: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 362us/sample - loss: 2.3106e-04 - mae: 0.0129 - val_loss: 4.0998e-04 - val_mae: 0.0173\n",
      "Epoch 32/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0369e-04 - mae: 0.0132\n",
      "Epoch 32: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 2.1890e-04 - mae: 0.0126 - val_loss: 4.2559e-04 - val_mae: 0.0179\n",
      "Epoch 33/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9337e-04 - mae: 0.0113\n",
      "Epoch 33: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 351us/sample - loss: 2.1297e-04 - mae: 0.0122 - val_loss: 4.0557e-04 - val_mae: 0.0170\n",
      "Epoch 34/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0060e-04 - mae: 0.0120\n",
      "Epoch 34: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 2.0468e-04 - mae: 0.0121 - val_loss: 4.1134e-04 - val_mae: 0.0175\n",
      "Epoch 35/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2437e-04 - mae: 0.0151\n",
      "Epoch 35: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 1.9627e-04 - mae: 0.0119 - val_loss: 4.1179e-04 - val_mae: 0.0176\n",
      "Epoch 36/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6732e-04 - mae: 0.0104\n",
      "Epoch 36: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 203us/sample - loss: 1.9258e-04 - mae: 0.0118 - val_loss: 4.0873e-04 - val_mae: 0.0175\n",
      "Epoch 37/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7715e-04 - mae: 0.0094\n",
      "Epoch 37: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 2.0506e-04 - mae: 0.0121 - val_loss: 4.1394e-04 - val_mae: 0.0167\n",
      "Epoch 38/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2454e-04 - mae: 0.0090\n",
      "Epoch 38: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 1.8531e-04 - mae: 0.0112 - val_loss: 4.1562e-04 - val_mae: 0.0177\n",
      "Epoch 39/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0910e-04 - mae: 0.0078\n",
      "Epoch 39: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 1.7653e-04 - mae: 0.0109 - val_loss: 4.8233e-04 - val_mae: 0.0192\n",
      "Epoch 40/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6269e-04 - mae: 0.0139\n",
      "Epoch 40: val_loss improved from 0.00041 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 302us/sample - loss: 1.8908e-04 - mae: 0.0112 - val_loss: 3.9549e-04 - val_mae: 0.0169\n",
      "Epoch 41/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8283e-05 - mae: 0.0084\n",
      "Epoch 41: val_loss did not improve from 0.00040\n",
      "160/160 [==============================] - 0s 170us/sample - loss: 1.7518e-04 - mae: 0.0109 - val_loss: 4.0236e-04 - val_mae: 0.0165\n",
      "Epoch 42/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8207e-04 - mae: 0.0114\n",
      "Epoch 42: val_loss did not improve from 0.00040\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 1.6337e-04 - mae: 0.0105 - val_loss: 4.1965e-04 - val_mae: 0.0179\n",
      "Epoch 43/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3149e-04 - mae: 0.0089\n",
      "Epoch 43: val_loss did not improve from 0.00040\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 1.5916e-04 - mae: 0.0105 - val_loss: 4.2957e-04 - val_mae: 0.0182\n",
      "Epoch 44/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5604e-04 - mae: 0.0109\n",
      "Epoch 44: val_loss improved from 0.00040 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 288us/sample - loss: 1.4828e-04 - mae: 0.0102 - val_loss: 3.8923e-04 - val_mae: 0.0167\n",
      "Epoch 45/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.5138e-05 - mae: 0.0081\n",
      "Epoch 45: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.4120e-04 - mae: 0.0100 - val_loss: 3.8936e-04 - val_mae: 0.0168\n",
      "Epoch 46/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3663e-04 - mae: 0.0138\n",
      "Epoch 46: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 1.3601e-04 - mae: 0.0099 - val_loss: 3.9101e-04 - val_mae: 0.0169\n",
      "Epoch 47/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1792e-04 - mae: 0.0103\n",
      "Epoch 47: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 1.3458e-04 - mae: 0.0097 - val_loss: 3.9576e-04 - val_mae: 0.0171\n",
      "Epoch 48/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4868e-04 - mae: 0.0101\n",
      "Epoch 48: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 229us/sample - loss: 1.3134e-04 - mae: 0.0097 - val_loss: 3.9981e-04 - val_mae: 0.0173\n",
      "Epoch 49/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7570e-04 - mae: 0.0157\n",
      "Epoch 49: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 1.2807e-04 - mae: 0.0093 - val_loss: 4.6200e-04 - val_mae: 0.0187\n",
      "Epoch 50/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3308e-05 - mae: 0.0061\n",
      "Epoch 50: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 1.3021e-04 - mae: 0.0092 - val_loss: 4.0952e-04 - val_mae: 0.0176\n",
      "Epoch 51/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7296e-04 - mae: 0.0110\n",
      "Epoch 51: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 1.2165e-04 - mae: 0.0092 - val_loss: 4.2618e-04 - val_mae: 0.0180\n",
      "Epoch 52/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1533e-04 - mae: 0.0091\n",
      "Epoch 52: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 1.1868e-04 - mae: 0.0089 - val_loss: 4.1817e-04 - val_mae: 0.0178\n",
      "Epoch 53/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0765e-04 - mae: 0.0089\n",
      "Epoch 53: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 215us/sample - loss: 1.1204e-04 - mae: 0.0089 - val_loss: 4.1940e-04 - val_mae: 0.0178\n",
      "Epoch 54/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7412e-05 - mae: 0.0068\n",
      "Epoch 54: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 1.0473e-04 - mae: 0.0085 - val_loss: 3.9830e-04 - val_mae: 0.0172\n",
      "Epoch 55/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1020e-04 - mae: 0.0086\n",
      "Epoch 55: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 1.0080e-04 - mae: 0.0083 - val_loss: 4.2789e-04 - val_mae: 0.0180\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1948e-04 - mae: 0.0097\n",
      "Epoch 56: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 1.0240e-04 - mae: 0.0084 - val_loss: 3.9578e-04 - val_mae: 0.0171\n",
      "Epoch 57/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6980e-05 - mae: 0.0066\n",
      "Epoch 57: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 9.4793e-05 - mae: 0.0082 - val_loss: 4.3388e-04 - val_mae: 0.0181\n",
      "Epoch 58/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5068e-04 - mae: 0.0097\n",
      "Epoch 58: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 9.6863e-05 - mae: 0.0081 - val_loss: 3.9175e-04 - val_mae: 0.0170\n",
      "Epoch 59/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.9393e-05 - mae: 0.0072\n",
      "Epoch 59: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 9.4639e-05 - mae: 0.0080 - val_loss: 4.0526e-04 - val_mae: 0.0174\n",
      "Epoch 60/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.8984e-05 - mae: 0.0056\n",
      "Epoch 60: val_loss improved from 0.00039 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 290us/sample - loss: 8.5013e-05 - mae: 0.0076 - val_loss: 3.7848e-04 - val_mae: 0.0166\n",
      "Epoch 61/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3975e-05 - mae: 0.0071\n",
      "Epoch 61: val_loss did not improve from 0.00038\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 8.5941e-05 - mae: 0.0075 - val_loss: 3.9473e-04 - val_mae: 0.0171\n",
      "Epoch 62/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.9499e-05 - mae: 0.0059\n",
      "Epoch 62: val_loss improved from 0.00038 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 373us/sample - loss: 8.4012e-05 - mae: 0.0075 - val_loss: 3.7635e-04 - val_mae: 0.0166\n",
      "Epoch 63/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1910e-05 - mae: 0.0084\n",
      "Epoch 63: val_loss did not improve from 0.00038\n",
      "160/160 [==============================] - 0s 222us/sample - loss: 7.8698e-05 - mae: 0.0073 - val_loss: 3.8220e-04 - val_mae: 0.0167\n",
      "Epoch 64/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.0721e-05 - mae: 0.0061\n",
      "Epoch 64: val_loss did not improve from 0.00038\n",
      "160/160 [==============================] - 0s 203us/sample - loss: 7.6353e-05 - mae: 0.0072 - val_loss: 4.1086e-04 - val_mae: 0.0175\n",
      "Epoch 65/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.2036e-05 - mae: 0.0075\n",
      "Epoch 65: val_loss did not improve from 0.00038\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 7.7195e-05 - mae: 0.0072 - val_loss: 4.2509e-04 - val_mae: 0.0178\n",
      "Epoch 66/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2316e-04 - mae: 0.0099\n",
      "Epoch 66: val_loss did not improve from 0.00038\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 7.5482e-05 - mae: 0.0071 - val_loss: 4.0427e-04 - val_mae: 0.0173\n",
      "Epoch 67/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3416e-05 - mae: 0.0081\n",
      "Epoch 67: val_loss improved from 0.00038 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 305us/sample - loss: 7.4079e-05 - mae: 0.0072 - val_loss: 3.7421e-04 - val_mae: 0.0165\n",
      "Epoch 68/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2336e-04 - mae: 0.0088\n",
      "Epoch 68: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 6.7837e-05 - mae: 0.0067 - val_loss: 3.8372e-04 - val_mae: 0.0167\n",
      "Epoch 69/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1837e-05 - mae: 0.0053\n",
      "Epoch 69: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 6.5798e-05 - mae: 0.0066 - val_loss: 4.0110e-04 - val_mae: 0.0172\n",
      "Epoch 70/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.8131e-05 - mae: 0.0070\n",
      "Epoch 70: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 6.8412e-05 - mae: 0.0068 - val_loss: 4.1910e-04 - val_mae: 0.0176\n",
      "Epoch 71/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7026e-05 - mae: 0.0067\n",
      "Epoch 71: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 7.1581e-05 - mae: 0.0068 - val_loss: 4.2137e-04 - val_mae: 0.0176\n",
      "Epoch 72/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2415e-05 - mae: 0.0071\n",
      "Epoch 72: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 190us/sample - loss: 6.6226e-05 - mae: 0.0067 - val_loss: 4.0255e-04 - val_mae: 0.0172\n",
      "Epoch 73/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5400e-05 - mae: 0.0040\n",
      "Epoch 73: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 6.1460e-05 - mae: 0.0064 - val_loss: 3.9629e-04 - val_mae: 0.0170\n",
      "Epoch 74/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9919e-05 - mae: 0.0037\n",
      "Epoch 74: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 5.7638e-05 - mae: 0.0062 - val_loss: 4.0792e-04 - val_mae: 0.0173\n",
      "Epoch 75/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0242e-05 - mae: 0.0062\n",
      "Epoch 75: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 6.0716e-05 - mae: 0.0063 - val_loss: 4.0154e-04 - val_mae: 0.0171\n",
      "Epoch 76/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7394e-05 - mae: 0.0083\n",
      "Epoch 76: val_loss improved from 0.00037 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 332us/sample - loss: 5.5470e-05 - mae: 0.0061 - val_loss: 3.7386e-04 - val_mae: 0.0164\n",
      "Epoch 77/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.6721e-05 - mae: 0.0085\n",
      "Epoch 77: val_loss improved from 0.00037 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 291us/sample - loss: 5.4525e-05 - mae: 0.0060 - val_loss: 3.5961e-04 - val_mae: 0.0159\n",
      "Epoch 78/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.4042e-05 - mae: 0.0091\n",
      "Epoch 78: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 6.0869e-05 - mae: 0.0065 - val_loss: 3.5986e-04 - val_mae: 0.0159\n",
      "Epoch 79/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0137e-04 - mae: 0.0082\n",
      "Epoch 79: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 5.2495e-05 - mae: 0.0059 - val_loss: 3.6694e-04 - val_mae: 0.0162\n",
      "Epoch 80/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2427e-05 - mae: 0.0072\n",
      "Epoch 80: val_loss improved from 0.00036 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 311us/sample - loss: 5.0612e-05 - mae: 0.0057 - val_loss: 3.5814e-04 - val_mae: 0.0159\n",
      "Epoch 81/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7588e-05 - mae: 0.0063\n",
      "Epoch 81: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 212us/sample - loss: 5.5623e-05 - mae: 0.0060 - val_loss: 3.6054e-04 - val_mae: 0.0156\n",
      "Epoch 82/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5674e-04 - mae: 0.0098\n",
      "Epoch 82: val_loss improved from 0.00036 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 360us/sample - loss: 6.2081e-05 - mae: 0.0064 - val_loss: 3.5806e-04 - val_mae: 0.0159\n",
      "Epoch 83/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2865e-05 - mae: 0.0064\n",
      "Epoch 83: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 221us/sample - loss: 4.8064e-05 - mae: 0.0057 - val_loss: 3.8243e-04 - val_mae: 0.0166\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9543e-05 - mae: 0.0046\n",
      "Epoch 84: val_loss improved from 0.00036 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 363us/sample - loss: 4.4373e-05 - mae: 0.0054 - val_loss: 3.5753e-04 - val_mae: 0.0159\n",
      "Epoch 85/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.5540e-05 - mae: 0.0062\n",
      "Epoch 85: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 4.6357e-05 - mae: 0.0056 - val_loss: 3.6099e-04 - val_mae: 0.0160\n",
      "Epoch 86/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2402e-05 - mae: 0.0043\n",
      "Epoch 86: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 4.4237e-05 - mae: 0.0054 - val_loss: 3.6259e-04 - val_mae: 0.0160\n",
      "Epoch 87/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1425e-05 - mae: 0.0048\n",
      "Epoch 87: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 4.0496e-05 - mae: 0.0051 - val_loss: 3.7784e-04 - val_mae: 0.0165\n",
      "Epoch 88/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6571e-05 - mae: 0.0071\n",
      "Epoch 88: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 4.2116e-05 - mae: 0.0053 - val_loss: 3.7401e-04 - val_mae: 0.0163\n",
      "Epoch 89/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9144e-05 - mae: 0.0035\n",
      "Epoch 89: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 170us/sample - loss: 4.6243e-05 - mae: 0.0054 - val_loss: 3.6010e-04 - val_mae: 0.0159\n",
      "Epoch 90/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8072e-05 - mae: 0.0048\n",
      "Epoch 90: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 4.4615e-05 - mae: 0.0054 - val_loss: 3.6906e-04 - val_mae: 0.0162\n",
      "Epoch 91/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.1805e-05 - mae: 0.0069\n",
      "Epoch 91: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 3.6412e-05 - mae: 0.0049 - val_loss: 3.6113e-04 - val_mae: 0.0159\n",
      "Epoch 92/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7073e-05 - mae: 0.0039\n",
      "Epoch 92: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 4.0716e-05 - mae: 0.0052 - val_loss: 3.9640e-04 - val_mae: 0.0169\n",
      "Epoch 93/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3113e-05 - mae: 0.0057\n",
      "Epoch 93: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 3.9365e-05 - mae: 0.0051 - val_loss: 3.9598e-04 - val_mae: 0.0168\n",
      "Epoch 94/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4840e-05 - mae: 0.0031\n",
      "Epoch 94: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 215us/sample - loss: 3.5279e-05 - mae: 0.0048 - val_loss: 3.9079e-04 - val_mae: 0.0167\n",
      "Epoch 95/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3179e-05 - mae: 0.0045\n",
      "Epoch 95: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 3.5787e-05 - mae: 0.0049 - val_loss: 3.9260e-04 - val_mae: 0.0167\n",
      "Epoch 96/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0916e-05 - mae: 0.0046\n",
      "Epoch 96: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 190us/sample - loss: 3.7631e-05 - mae: 0.0050 - val_loss: 3.7150e-04 - val_mae: 0.0162\n",
      "Epoch 97/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7487e-05 - mae: 0.0049\n",
      "Epoch 97: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 3.5213e-05 - mae: 0.0047 - val_loss: 4.0276e-04 - val_mae: 0.0170\n",
      "Epoch 98/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4582e-05 - mae: 0.0053\n",
      "Epoch 98: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 3.7755e-05 - mae: 0.0050 - val_loss: 3.8407e-04 - val_mae: 0.0165\n",
      "Epoch 99/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.7816e-05 - mae: 0.0061\n",
      "Epoch 99: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 3.7989e-05 - mae: 0.0049 - val_loss: 3.8258e-04 - val_mae: 0.0165\n",
      "Epoch 100/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6355e-05 - mae: 0.0046\n",
      "Epoch 100: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 3.3920e-05 - mae: 0.0046 - val_loss: 3.6259e-04 - val_mae: 0.0159\n",
      "Epoch 101/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6559e-05 - mae: 0.0034\n",
      "Epoch 101: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 172us/sample - loss: 2.8870e-05 - mae: 0.0045 - val_loss: 3.6474e-04 - val_mae: 0.0160\n",
      "Epoch 102/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0104e-05 - mae: 0.0041\n",
      "Epoch 102: val_loss improved from 0.00036 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 296us/sample - loss: 3.2627e-05 - mae: 0.0045 - val_loss: 3.5090e-04 - val_mae: 0.0155\n",
      "Epoch 103/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8145e-05 - mae: 0.0059\n",
      "Epoch 103: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 2.7620e-05 - mae: 0.0042 - val_loss: 3.7910e-04 - val_mae: 0.0164\n",
      "Epoch 104/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3587e-05 - mae: 0.0033\n",
      "Epoch 104: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 2.8481e-05 - mae: 0.0044 - val_loss: 3.8176e-04 - val_mae: 0.0164\n",
      "Epoch 105/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4303e-05 - mae: 0.0034\n",
      "Epoch 105: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 2.6451e-05 - mae: 0.0041 - val_loss: 3.9461e-04 - val_mae: 0.0167\n",
      "Epoch 106/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6706e-05 - mae: 0.0046\n",
      "Epoch 106: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 2.7096e-05 - mae: 0.0043 - val_loss: 4.0003e-04 - val_mae: 0.0169\n",
      "Epoch 107/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7206e-05 - mae: 0.0043\n",
      "Epoch 107: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 3.1238e-05 - mae: 0.0045 - val_loss: 3.5205e-04 - val_mae: 0.0155\n",
      "Epoch 108/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5984e-05 - mae: 0.0042\n",
      "Epoch 108: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 2.6516e-05 - mae: 0.0042 - val_loss: 3.6008e-04 - val_mae: 0.0158\n",
      "Epoch 109/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4677e-05 - mae: 0.0045\n",
      "Epoch 109: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 203us/sample - loss: 2.4600e-05 - mae: 0.0041 - val_loss: 3.8070e-04 - val_mae: 0.0164\n",
      "Epoch 110/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0604e-05 - mae: 0.0057\n",
      "Epoch 110: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 2.2770e-05 - mae: 0.0039 - val_loss: 3.7864e-04 - val_mae: 0.0163\n",
      "Epoch 111/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2383e-05 - mae: 0.0044\n",
      "Epoch 111: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 2.2074e-05 - mae: 0.0038 - val_loss: 3.6719e-04 - val_mae: 0.0160\n",
      "Epoch 112/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4903e-05 - mae: 0.0043\n",
      "Epoch 112: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 2.2129e-05 - mae: 0.0039 - val_loss: 3.5890e-04 - val_mae: 0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5451e-05 - mae: 0.0033\n",
      "Epoch 113: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 360us/sample - loss: 2.3183e-05 - mae: 0.0040 - val_loss: 3.5071e-04 - val_mae: 0.0154\n",
      "Epoch 114/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0314e-05 - mae: 0.0037\n",
      "Epoch 114: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 223us/sample - loss: 2.5842e-05 - mae: 0.0041 - val_loss: 3.5725e-04 - val_mae: 0.0157\n",
      "Epoch 115/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5279e-05 - mae: 0.0044\n",
      "Epoch 115: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 2.6211e-05 - mae: 0.0041 - val_loss: 3.7926e-04 - val_mae: 0.0163\n",
      "Epoch 116/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1944e-06 - mae: 0.0022\n",
      "Epoch 116: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 220us/sample - loss: 1.9949e-05 - mae: 0.0036 - val_loss: 3.9188e-04 - val_mae: 0.0167\n",
      "Epoch 117/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2892e-05 - mae: 0.0040\n",
      "Epoch 117: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 209us/sample - loss: 1.9978e-05 - mae: 0.0037 - val_loss: 3.7326e-04 - val_mae: 0.0162\n",
      "Epoch 118/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1786e-05 - mae: 0.0028\n",
      "Epoch 118: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 2.1623e-05 - mae: 0.0037 - val_loss: 3.5190e-04 - val_mae: 0.0155\n",
      "Epoch 119/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0608e-05 - mae: 0.0035\n",
      "Epoch 119: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 1.8694e-05 - mae: 0.0035 - val_loss: 3.7727e-04 - val_mae: 0.0163\n",
      "Epoch 120/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8587e-05 - mae: 0.0037\n",
      "Epoch 120: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 2.0249e-05 - mae: 0.0037 - val_loss: 3.5556e-04 - val_mae: 0.0156\n",
      "Epoch 121/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1635e-05 - mae: 0.0034\n",
      "Epoch 121: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 1.9037e-05 - mae: 0.0035 - val_loss: 3.5498e-04 - val_mae: 0.0156\n",
      "Epoch 122/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5474e-05 - mae: 0.0031\n",
      "Epoch 122: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 190us/sample - loss: 1.8563e-05 - mae: 0.0035 - val_loss: 3.9041e-04 - val_mae: 0.0167\n",
      "Epoch 123/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0513e-05 - mae: 0.0027\n",
      "Epoch 123: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 1.8669e-05 - mae: 0.0035 - val_loss: 3.5999e-04 - val_mae: 0.0158\n",
      "Epoch 124/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6673e-05 - mae: 0.0036\n",
      "Epoch 124: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 362us/sample - loss: 1.7563e-05 - mae: 0.0033 - val_loss: 3.5041e-04 - val_mae: 0.0154\n",
      "Epoch 125/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8150e-05 - mae: 0.0034\n",
      "Epoch 125: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 218us/sample - loss: 1.5237e-05 - mae: 0.0031 - val_loss: 3.5315e-04 - val_mae: 0.0155\n",
      "Epoch 126/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1243e-05 - mae: 0.0026\n",
      "Epoch 126: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 209us/sample - loss: 1.4586e-05 - mae: 0.0031 - val_loss: 3.5911e-04 - val_mae: 0.0158\n",
      "Epoch 127/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8914e-05 - mae: 0.0039\n",
      "Epoch 127: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 1.5694e-05 - mae: 0.0032 - val_loss: 3.8657e-04 - val_mae: 0.0166\n",
      "Epoch 128/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9199e-05 - mae: 0.0038\n",
      "Epoch 128: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 1.5429e-05 - mae: 0.0032 - val_loss: 3.8448e-04 - val_mae: 0.0165\n",
      "Epoch 129/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6002e-05 - mae: 0.0033\n",
      "Epoch 129: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 431us/sample - loss: 1.8662e-05 - mae: 0.0034 - val_loss: 3.4820e-04 - val_mae: 0.0154\n",
      "Epoch 130/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5500e-05 - mae: 0.0051\n",
      "Epoch 130: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 225us/sample - loss: 1.8531e-05 - mae: 0.0035 - val_loss: 3.5963e-04 - val_mae: 0.0158\n",
      "Epoch 131/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9589e-05 - mae: 0.0037\n",
      "Epoch 131: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 220us/sample - loss: 1.3219e-05 - mae: 0.0029 - val_loss: 3.6487e-04 - val_mae: 0.0160\n",
      "Epoch 132/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2018e-05 - mae: 0.0026\n",
      "Epoch 132: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 216us/sample - loss: 1.5089e-05 - mae: 0.0030 - val_loss: 3.7869e-04 - val_mae: 0.0164\n",
      "Epoch 133/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.5186e-06 - mae: 0.0022\n",
      "Epoch 133: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 1.2754e-05 - mae: 0.0029 - val_loss: 3.5735e-04 - val_mae: 0.0157\n",
      "Epoch 134/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.8137e-06 - mae: 0.0018\n",
      "Epoch 134: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 1.3023e-05 - mae: 0.0029 - val_loss: 3.6698e-04 - val_mae: 0.0161\n",
      "Epoch 135/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2334e-05 - mae: 0.0040\n",
      "Epoch 135: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 1.1716e-05 - mae: 0.0027 - val_loss: 3.7008e-04 - val_mae: 0.0162\n",
      "Epoch 136/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7115e-05 - mae: 0.0037\n",
      "Epoch 136: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 1.0892e-05 - mae: 0.0027 - val_loss: 3.6803e-04 - val_mae: 0.0161\n",
      "Epoch 137/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9215e-06 - mae: 0.0016\n",
      "Epoch 137: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 1.0343e-05 - mae: 0.0026 - val_loss: 3.6348e-04 - val_mae: 0.0159\n",
      "Epoch 138/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2364e-05 - mae: 0.0033\n",
      "Epoch 138: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 1.0079e-05 - mae: 0.0026 - val_loss: 3.5436e-04 - val_mae: 0.0157\n",
      "Epoch 139/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3756e-06 - mae: 0.0021\n",
      "Epoch 139: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 1.1220e-05 - mae: 0.0027 - val_loss: 3.5998e-04 - val_mae: 0.0158\n",
      "Epoch 140/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0585e-05 - mae: 0.0026\n",
      "Epoch 140: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 172us/sample - loss: 1.1322e-05 - mae: 0.0028 - val_loss: 3.6763e-04 - val_mae: 0.0161\n",
      "Epoch 141/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6247e-05 - mae: 0.0035\n",
      "Epoch 141: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 9.0766e-06 - mae: 0.0024 - val_loss: 3.6846e-04 - val_mae: 0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.6227e-06 - mae: 0.0027\n",
      "Epoch 142: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 9.0268e-06 - mae: 0.0024 - val_loss: 3.5512e-04 - val_mae: 0.0157\n",
      "Epoch 143/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6381e-06 - mae: 0.0022\n",
      "Epoch 143: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 221us/sample - loss: 9.4222e-06 - mae: 0.0025 - val_loss: 3.6441e-04 - val_mae: 0.0160\n",
      "Epoch 144/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5481e-05 - mae: 0.0037\n",
      "Epoch 144: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 203us/sample - loss: 8.2831e-06 - mae: 0.0023 - val_loss: 3.6094e-04 - val_mae: 0.0159\n",
      "Epoch 145/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3996e-06 - mae: 0.0021\n",
      "Epoch 145: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 8.1041e-06 - mae: 0.0023 - val_loss: 3.6931e-04 - val_mae: 0.0161\n",
      "Epoch 146/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7243e-06 - mae: 0.0021\n",
      "Epoch 146: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 7.9106e-06 - mae: 0.0023 - val_loss: 3.6782e-04 - val_mae: 0.0161\n",
      "Epoch 147/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7865e-06 - mae: 0.0024\n",
      "Epoch 147: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 221us/sample - loss: 8.0342e-06 - mae: 0.0023 - val_loss: 3.6194e-04 - val_mae: 0.0160\n",
      "Epoch 148/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1932e-05 - mae: 0.0030\n",
      "Epoch 148: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 7.3542e-06 - mae: 0.0022 - val_loss: 3.5740e-04 - val_mae: 0.0159\n",
      "Epoch 149/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2243e-06 - mae: 0.0021\n",
      "Epoch 149: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 8.5884e-06 - mae: 0.0023 - val_loss: 3.6256e-04 - val_mae: 0.0160\n",
      "Epoch 150/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4084e-06 - mae: 0.0020\n",
      "Epoch 150: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 9.2483e-06 - mae: 0.0025 - val_loss: 3.7150e-04 - val_mae: 0.0162\n",
      "Epoch 151/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.9945e-06 - mae: 0.0025\n",
      "Epoch 151: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 7.3649e-06 - mae: 0.0021 - val_loss: 3.5660e-04 - val_mae: 0.0158\n",
      "Epoch 152/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6107e-06 - mae: 0.0013\n",
      "Epoch 152: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 6.7911e-06 - mae: 0.0021 - val_loss: 3.5924e-04 - val_mae: 0.0159\n",
      "Epoch 153/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.3095e-06 - mae: 0.0022\n",
      "Epoch 153: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 6.5167e-06 - mae: 0.0021 - val_loss: 3.5403e-04 - val_mae: 0.0158\n",
      "Epoch 154/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.5050e-06 - mae: 0.0026\n",
      "Epoch 154: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 6.5685e-06 - mae: 0.0020 - val_loss: 3.6596e-04 - val_mae: 0.0161\n",
      "Epoch 155/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9782e-06 - mae: 0.0022\n",
      "Epoch 155: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 7.4917e-06 - mae: 0.0022 - val_loss: 3.5356e-04 - val_mae: 0.0158\n",
      "Epoch 156/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2841e-06 - mae: 0.0022\n",
      "Epoch 156: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 8.3427e-06 - mae: 0.0024 - val_loss: 3.5864e-04 - val_mae: 0.0159\n",
      "Epoch 157/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2184e-06 - mae: 0.0023\n",
      "Epoch 157: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 7.5238e-06 - mae: 0.0022 - val_loss: 3.6516e-04 - val_mae: 0.0161\n",
      "Epoch 158/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6152e-06 - mae: 0.0018\n",
      "Epoch 158: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 6.0857e-06 - mae: 0.0020 - val_loss: 3.6271e-04 - val_mae: 0.0160\n",
      "Epoch 159/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6413e-06 - mae: 0.0020\n",
      "Epoch 159: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 5.6969e-06 - mae: 0.0019 - val_loss: 3.6830e-04 - val_mae: 0.0162\n",
      "Epoch 160/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7648e-06 - mae: 0.0024\n",
      "Epoch 160: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 5.1158e-06 - mae: 0.0018 - val_loss: 3.5831e-04 - val_mae: 0.0160\n",
      "Epoch 161/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9693e-06 - mae: 0.0012\n",
      "Epoch 161: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 5.6636e-06 - mae: 0.0019 - val_loss: 3.7348e-04 - val_mae: 0.0163\n",
      "Epoch 162/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9506e-06 - mae: 0.0020\n",
      "Epoch 162: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 5.7782e-06 - mae: 0.0019 - val_loss: 3.5488e-04 - val_mae: 0.0159\n",
      "Epoch 163/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4389e-06 - mae: 0.0014\n",
      "Epoch 163: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 5.7068e-06 - mae: 0.0019 - val_loss: 3.7261e-04 - val_mae: 0.0163\n",
      "Epoch 164/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0232e-06 - mae: 0.0016\n",
      "Epoch 164: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 5.0814e-06 - mae: 0.0019 - val_loss: 3.5323e-04 - val_mae: 0.0158\n",
      "Epoch 165/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.8740e-06 - mae: 0.0016\n",
      "Epoch 165: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 5.4371e-06 - mae: 0.0018 - val_loss: 3.6321e-04 - val_mae: 0.0161\n",
      "Epoch 166/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4889e-06 - mae: 0.0018\n",
      "Epoch 166: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 4.2478e-06 - mae: 0.0016 - val_loss: 3.6300e-04 - val_mae: 0.0161\n",
      "Epoch 167/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3884e-06 - mae: 0.0017\n",
      "Epoch 167: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 5.2340e-06 - mae: 0.0018 - val_loss: 3.5897e-04 - val_mae: 0.0160\n",
      "Epoch 168/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7773e-06 - mae: 0.0021\n",
      "Epoch 168: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 4.9531e-06 - mae: 0.0018 - val_loss: 3.5636e-04 - val_mae: 0.0159\n",
      "Epoch 169/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2881e-06 - mae: 0.0012\n",
      "Epoch 169: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 4.0758e-06 - mae: 0.0016 - val_loss: 3.5882e-04 - val_mae: 0.0160\n",
      "Epoch 170/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2448e-06 - mae: 0.0016\n",
      "Epoch 170: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 4.5512e-06 - mae: 0.0017 - val_loss: 3.6819e-04 - val_mae: 0.0163\n",
      "Epoch 171/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2991e-06 - mae: 0.0011\n",
      "Epoch 171: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 3.7789e-06 - mae: 0.0016 - val_loss: 3.5889e-04 - val_mae: 0.0160\n",
      "Epoch 172/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5794e-06 - mae: 0.0016\n",
      "Epoch 172: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 3.8004e-06 - mae: 0.0015 - val_loss: 3.5919e-04 - val_mae: 0.0160\n",
      "Epoch 173/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6272e-06 - mae: 0.0011\n",
      "Epoch 173: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 3.4575e-06 - mae: 0.0015 - val_loss: 3.6658e-04 - val_mae: 0.0162\n",
      "Epoch 174/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7467e-06 - mae: 0.0018\n",
      "Epoch 174: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 3.5823e-06 - mae: 0.0015 - val_loss: 3.6701e-04 - val_mae: 0.0162\n",
      "Epoch 175/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5323e-06 - mae: 0.0018\n",
      "Epoch 175: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 3.8918e-06 - mae: 0.0016 - val_loss: 3.6538e-04 - val_mae: 0.0162\n",
      "Epoch 176/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7019e-06 - mae: 0.0013\n",
      "Epoch 176: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 3.2426e-06 - mae: 0.0014 - val_loss: 3.6440e-04 - val_mae: 0.0162\n",
      "Epoch 177/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5707e-06 - mae: 0.0014\n",
      "Epoch 177: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 3.3790e-06 - mae: 0.0014 - val_loss: 3.5601e-04 - val_mae: 0.0160\n",
      "Epoch 178/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2701e-06 - mae: 9.0094e-04\n",
      "Epoch 178: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 3.6965e-06 - mae: 0.0015 - val_loss: 3.6243e-04 - val_mae: 0.0161\n",
      "Epoch 179/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6336e-06 - mae: 0.0011\n",
      "Epoch 179: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 2.8614e-06 - mae: 0.0013 - val_loss: 3.7202e-04 - val_mae: 0.0163\n",
      "Epoch 180/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3620e-06 - mae: 0.0012\n",
      "Epoch 180: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 3.6791e-06 - mae: 0.0015 - val_loss: 3.5386e-04 - val_mae: 0.0159\n",
      "Epoch 181/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7742e-06 - mae: 0.0024\n",
      "Epoch 181: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 3.2822e-06 - mae: 0.0014 - val_loss: 3.6226e-04 - val_mae: 0.0161\n",
      "Epoch 182/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1904e-06 - mae: 0.0014\n",
      "Epoch 182: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 2.5666e-06 - mae: 0.0013 - val_loss: 3.6163e-04 - val_mae: 0.0161\n",
      "Epoch 183/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4660e-06 - mae: 0.0012\n",
      "Epoch 183: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 2.8594e-06 - mae: 0.0013 - val_loss: 3.6441e-04 - val_mae: 0.0162\n",
      "Epoch 184/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0652e-06 - mae: 0.0014\n",
      "Epoch 184: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 3.0721e-06 - mae: 0.0014 - val_loss: 3.6457e-04 - val_mae: 0.0162\n",
      "Epoch 185/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0492e-06 - mae: 8.7610e-04\n",
      "Epoch 185: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 2.6038e-06 - mae: 0.0013 - val_loss: 3.5769e-04 - val_mae: 0.0160\n",
      "Epoch 186/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.0211e-07 - mae: 7.2404e-04\n",
      "Epoch 186: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 2.5015e-06 - mae: 0.0012 - val_loss: 3.6464e-04 - val_mae: 0.0162\n",
      "Epoch 187/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3264e-06 - mae: 0.0016\n",
      "Epoch 187: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 2.1312e-06 - mae: 0.0011 - val_loss: 3.5655e-04 - val_mae: 0.0160\n",
      "Epoch 188/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3121e-06 - mae: 9.6574e-04\n",
      "Epoch 188: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 2.2381e-06 - mae: 0.0012 - val_loss: 3.6954e-04 - val_mae: 0.0163\n",
      "Epoch 189/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4577e-06 - mae: 0.0013\n",
      "Epoch 189: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 2.2766e-06 - mae: 0.0012 - val_loss: 3.6088e-04 - val_mae: 0.0161\n",
      "Epoch 190/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0093e-06 - mae: 7.6155e-04\n",
      "Epoch 190: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 2.3453e-06 - mae: 0.0012 - val_loss: 3.7186e-04 - val_mae: 0.0164\n",
      "Epoch 191/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2756e-06 - mae: 0.0020\n",
      "Epoch 191: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 2.4512e-06 - mae: 0.0012 - val_loss: 3.6293e-04 - val_mae: 0.0162\n",
      "Epoch 192/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1950e-06 - mae: 8.9551e-04\n",
      "Epoch 192: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 1.9299e-06 - mae: 0.0011 - val_loss: 3.6296e-04 - val_mae: 0.0162\n",
      "Epoch 193/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8114e-06 - mae: 0.0010\n",
      "Epoch 193: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 1.8914e-06 - mae: 0.0011 - val_loss: 3.6166e-04 - val_mae: 0.0161\n",
      "Epoch 194/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6822e-06 - mae: 0.0011\n",
      "Epoch 194: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 1.7563e-06 - mae: 0.0011 - val_loss: 3.6742e-04 - val_mae: 0.0163\n",
      "Epoch 195/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4266e-06 - mae: 0.0014\n",
      "Epoch 195: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 2.2131e-06 - mae: 0.0012 - val_loss: 3.6477e-04 - val_mae: 0.0162\n",
      "Epoch 196/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9593e-06 - mae: 0.0015\n",
      "Epoch 196: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 1.7500e-06 - mae: 0.0010 - val_loss: 3.5802e-04 - val_mae: 0.0161\n",
      "Epoch 197/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7962e-06 - mae: 0.0013\n",
      "Epoch 197: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 2.1438e-06 - mae: 0.0011 - val_loss: 3.6808e-04 - val_mae: 0.0163\n",
      "Epoch 198/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1136e-06 - mae: 0.0011\n",
      "Epoch 198: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 1.7851e-06 - mae: 0.0011 - val_loss: 3.6132e-04 - val_mae: 0.0162\n",
      "Epoch 199/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4572e-07 - mae: 6.4114e-04\n",
      "Epoch 199: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.5645e-06 - mae: 9.9418e-04 - val_loss: 3.6818e-04 - val_mae: 0.0163\n",
      "Epoch 200/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6699e-06 - mae: 9.8282e-04\n",
      "Epoch 200: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.5253e-06 - mae: 9.7004e-04 - val_loss: 3.6027e-04 - val_mae: 0.0161\n",
      "Epoch 201/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2489e-06 - mae: 8.6082e-04\n",
      "Epoch 201: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 1.6089e-06 - mae: 9.8456e-04 - val_loss: 3.6655e-04 - val_mae: 0.0163\n",
      "Epoch 202/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4085e-06 - mae: 0.0010\n",
      "Epoch 202: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 1.3565e-06 - mae: 9.1580e-04 - val_loss: 3.6446e-04 - val_mae: 0.0162\n",
      "Epoch 203/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1950e-06 - mae: 9.0752e-04\n",
      "Epoch 203: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.3230e-06 - mae: 8.9163e-04 - val_loss: 3.6061e-04 - val_mae: 0.0161\n",
      "Epoch 204/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9212e-06 - mae: 0.0011\n",
      "Epoch 204: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 1.3818e-06 - mae: 9.3161e-04 - val_loss: 3.6267e-04 - val_mae: 0.0162\n",
      "Epoch 205/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1143e-06 - mae: 8.6513e-04\n",
      "Epoch 205: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 149us/sample - loss: 1.4728e-06 - mae: 9.4367e-04 - val_loss: 3.6489e-04 - val_mae: 0.0163\n",
      "Epoch 206/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3514e-06 - mae: 0.0011\n",
      "Epoch 206: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 144us/sample - loss: 1.3514e-06 - mae: 9.4528e-04 - val_loss: 3.6975e-04 - val_mae: 0.0164\n",
      "Epoch 207/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3008e-06 - mae: 9.7609e-04\n",
      "Epoch 207: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 1.3219e-06 - mae: 8.9840e-04 - val_loss: 3.6679e-04 - val_mae: 0.0163\n",
      "Epoch 208/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0756e-06 - mae: 0.0011\n",
      "Epoch 208: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 1.2571e-06 - mae: 8.7949e-04 - val_loss: 3.6763e-04 - val_mae: 0.0163\n",
      "Epoch 209/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1097e-06 - mae: 8.3300e-04\n",
      "Epoch 209: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 1.2617e-06 - mae: 8.8589e-04 - val_loss: 3.6257e-04 - val_mae: 0.0162\n",
      "Epoch 210/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1198e-06 - mae: 8.6158e-04\n",
      "Epoch 210: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 1.1170e-06 - mae: 8.4797e-04 - val_loss: 3.6081e-04 - val_mae: 0.0161\n",
      "Epoch 211/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.5281e-07 - mae: 6.8606e-04\n",
      "Epoch 211: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 1.0380e-06 - mae: 8.2786e-04 - val_loss: 3.6283e-04 - val_mae: 0.0162\n",
      "Epoch 212/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0826e-07 - mae: 7.7202e-04\n",
      "Epoch 212: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 9.6939e-07 - mae: 7.8350e-04 - val_loss: 3.6335e-04 - val_mae: 0.0162\n",
      "Epoch 213/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.4694e-07 - mae: 7.6792e-04\n",
      "Epoch 213: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 9.0269e-07 - mae: 7.4590e-04 - val_loss: 3.6370e-04 - val_mae: 0.0162\n",
      "Epoch 214/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0656e-06 - mae: 8.1761e-04\n",
      "Epoch 214: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 9.3536e-07 - mae: 7.8613e-04 - val_loss: 3.6333e-04 - val_mae: 0.0162\n",
      "Epoch 215/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2913e-06 - mae: 9.6685e-04\n",
      "Epoch 215: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 9.6063e-07 - mae: 7.7387e-04 - val_loss: 3.6577e-04 - val_mae: 0.0163\n",
      "Epoch 216/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8605e-07 - mae: 7.0485e-04\n",
      "Epoch 216: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 8.7728e-07 - mae: 7.5742e-04 - val_loss: 3.6420e-04 - val_mae: 0.0162\n",
      "Epoch 217/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.8885e-07 - mae: 7.3442e-04\n",
      "Epoch 217: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.2489e-06 - mae: 9.2383e-04 - val_loss: 3.6371e-04 - val_mae: 0.0162\n",
      "Epoch 218/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.4603e-07 - mae: 8.1459e-04\n",
      "Epoch 218: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.5827e-06 - mae: 0.0010 - val_loss: 3.7260e-04 - val_mae: 0.0164\n",
      "Epoch 219/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3690e-06 - mae: 0.0011\n",
      "Epoch 219: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 1.0554e-06 - mae: 8.1913e-04 - val_loss: 3.6570e-04 - val_mae: 0.0163\n",
      "Epoch 220/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4186e-06 - mae: 0.0010\n",
      "Epoch 220: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 8.1622e-07 - mae: 7.1862e-04 - val_loss: 3.6905e-04 - val_mae: 0.0164\n",
      "Epoch 221/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3375e-06 - mae: 9.9033e-04\n",
      "Epoch 221: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 8.4907e-07 - mae: 7.4242e-04 - val_loss: 3.6106e-04 - val_mae: 0.0162\n",
      "Epoch 222/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7766e-07 - mae: 6.2423e-04\n",
      "Epoch 222: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 8.0956e-07 - mae: 6.9484e-04 - val_loss: 3.6549e-04 - val_mae: 0.0163\n",
      "Epoch 223/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.0460e-07 - mae: 7.0708e-04\n",
      "Epoch 223: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 8.0228e-07 - mae: 7.2130e-04 - val_loss: 3.6621e-04 - val_mae: 0.0163\n",
      "Epoch 224/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7108e-07 - mae: 5.7684e-04\n",
      "Epoch 224: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 6.6752e-07 - mae: 6.4871e-04 - val_loss: 3.6622e-04 - val_mae: 0.0163\n",
      "Epoch 225/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0758e-06 - mae: 7.4297e-04\n",
      "Epoch 225: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 7.1905e-07 - mae: 6.5577e-04 - val_loss: 3.6869e-04 - val_mae: 0.0163\n",
      "Epoch 226/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1935e-07 - mae: 7.8712e-04\n",
      "Epoch 226: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 6.9105e-07 - mae: 6.4523e-04 - val_loss: 3.6362e-04 - val_mae: 0.0162\n",
      "Epoch 227/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4213e-07 - mae: 6.7889e-04\n",
      "Epoch 227: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 6.1124e-07 - mae: 6.2525e-04 - val_loss: 3.6424e-04 - val_mae: 0.0162\n",
      "Epoch 228/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5657e-07 - mae: 5.1558e-04\n",
      "Epoch 228: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 6.1776e-07 - mae: 6.2546e-04 - val_loss: 3.6512e-04 - val_mae: 0.0163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3037e-07 - mae: 7.3414e-04\n",
      "Epoch 229: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 5.3032e-07 - mae: 5.6699e-04 - val_loss: 3.6604e-04 - val_mae: 0.0163\n",
      "Epoch 230/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5485e-07 - mae: 3.9977e-04\n",
      "Epoch 230: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 7.3980e-07 - mae: 7.2275e-04 - val_loss: 3.6279e-04 - val_mae: 0.0162\n",
      "Epoch 231/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1209e-07 - mae: 2.4402e-04\n",
      "Epoch 231: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 6.2712e-07 - mae: 6.3253e-04 - val_loss: 3.6434e-04 - val_mae: 0.0163\n",
      "Epoch 232/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2674e-07 - mae: 4.8586e-04\n",
      "Epoch 232: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 7.1515e-07 - mae: 6.9538e-04 - val_loss: 3.6616e-04 - val_mae: 0.0163\n",
      "Epoch 233/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4418e-07 - mae: 5.3527e-04\n",
      "Epoch 233: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 5.2216e-07 - mae: 5.5762e-04 - val_loss: 3.6636e-04 - val_mae: 0.0163\n",
      "Epoch 234/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4201e-07 - mae: 6.6407e-04\n",
      "Epoch 234: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 148us/sample - loss: 4.3759e-07 - mae: 5.1037e-04 - val_loss: 3.6759e-04 - val_mae: 0.0163\n",
      "Epoch 235/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2817e-07 - mae: 4.6226e-04\n",
      "Epoch 235: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 143us/sample - loss: 4.6265e-07 - mae: 5.4015e-04 - val_loss: 3.6763e-04 - val_mae: 0.0163\n",
      "Epoch 236/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3595e-07 - mae: 7.9041e-04\n",
      "Epoch 236: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 4.3532e-07 - mae: 5.0842e-04 - val_loss: 3.6326e-04 - val_mae: 0.0162\n",
      "Epoch 237/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1829e-07 - mae: 4.3325e-04\n",
      "Epoch 237: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 3.7457e-07 - mae: 4.7579e-04 - val_loss: 3.6290e-04 - val_mae: 0.0162\n",
      "Epoch 238/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0795e-07 - mae: 5.6374e-04\n",
      "Epoch 238: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 4.1453e-07 - mae: 5.1510e-04 - val_loss: 3.6809e-04 - val_mae: 0.0163\n",
      "Epoch 239/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2960e-07 - mae: 5.0840e-04\n",
      "Epoch 239: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 5.3701e-07 - mae: 5.8340e-04 - val_loss: 3.7250e-04 - val_mae: 0.0164\n",
      "Epoch 240/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0585e-06 - mae: 9.2742e-04\n",
      "Epoch 240: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 5.4523e-07 - mae: 5.9408e-04 - val_loss: 3.6982e-04 - val_mae: 0.0164\n",
      "Epoch 241/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5988e-06 - mae: 0.0011\n",
      "Epoch 241: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 3.9646e-07 - mae: 4.7721e-04 - val_loss: 3.6260e-04 - val_mae: 0.0162\n",
      "Epoch 242/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6444e-07 - mae: 5.8541e-04\n",
      "Epoch 242: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 3.5398e-07 - mae: 4.5882e-04 - val_loss: 3.6277e-04 - val_mae: 0.0162\n",
      "Epoch 243/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2808e-07 - mae: 3.9736e-04\n",
      "Epoch 243: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 144us/sample - loss: 3.3055e-07 - mae: 4.5580e-04 - val_loss: 3.6394e-04 - val_mae: 0.0162\n",
      "Epoch 244/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6368e-07 - mae: 3.7917e-04\n",
      "Epoch 244: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 147us/sample - loss: 2.8078e-07 - mae: 4.1103e-04 - val_loss: 3.6757e-04 - val_mae: 0.0163\n",
      "Epoch 245/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2720e-07 - mae: 4.2531e-04\n",
      "Epoch 245: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 147us/sample - loss: 4.3864e-07 - mae: 5.2558e-04 - val_loss: 3.6905e-04 - val_mae: 0.0164\n",
      "Epoch 246/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6695e-07 - mae: 5.7037e-04\n",
      "Epoch 246: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 4.7479e-07 - mae: 5.6610e-04 - val_loss: 3.6107e-04 - val_mae: 0.0162\n",
      "Epoch 247/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5390e-07 - mae: 5.8155e-04\n",
      "Epoch 247: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 148us/sample - loss: 5.5639e-07 - mae: 6.3403e-04 - val_loss: 3.6025e-04 - val_mae: 0.0161\n",
      "Epoch 248/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7551e-07 - mae: 7.2710e-04\n",
      "Epoch 248: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 136us/sample - loss: 4.5507e-07 - mae: 5.5738e-04 - val_loss: 3.6294e-04 - val_mae: 0.0162\n",
      "Epoch 249/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7809e-07 - mae: 4.6096e-04\n",
      "Epoch 249: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 2.7675e-07 - mae: 4.2915e-04 - val_loss: 3.6339e-04 - val_mae: 0.0162\n",
      "Epoch 250/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7400e-07 - mae: 4.0000e-04\n",
      "Epoch 250: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 134us/sample - loss: 2.7454e-07 - mae: 4.1205e-04 - val_loss: 3.6238e-04 - val_mae: 0.0162\n",
      "Epoch 251/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5276e-07 - mae: 3.3528e-04\n",
      "Epoch 251: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 137us/sample - loss: 2.3032e-07 - mae: 3.8258e-04 - val_loss: 3.6588e-04 - val_mae: 0.0163\n",
      "Epoch 252/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7321e-08 - mae: 2.1342e-04\n",
      "Epoch 252: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 137us/sample - loss: 2.0419e-07 - mae: 3.5806e-04 - val_loss: 3.6471e-04 - val_mae: 0.0163\n",
      "Epoch 253/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2720e-08 - mae: 2.0781e-04\n",
      "Epoch 253: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 2.1125e-07 - mae: 3.7006e-04 - val_loss: 3.6462e-04 - val_mae: 0.0163\n",
      "Epoch 254/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.2005e-08 - mae: 2.1548e-04\n",
      "Epoch 254: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 135us/sample - loss: 2.5987e-07 - mae: 4.0053e-04 - val_loss: 3.6117e-04 - val_mae: 0.0162\n",
      "Epoch 255/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3908e-07 - mae: 4.4901e-04\n",
      "Epoch 255: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 2.8959e-07 - mae: 4.2222e-04 - val_loss: 3.6317e-04 - val_mae: 0.0162\n",
      "Epoch 256/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2459e-07 - mae: 3.0342e-04\n",
      "Epoch 256: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 132us/sample - loss: 1.8671e-07 - mae: 3.4382e-04 - val_loss: 3.6216e-04 - val_mae: 0.0162\n",
      "Epoch 257/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7948e-07 - mae: 3.8811e-04\n",
      "Epoch 257: val_loss did not improve from 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 130us/sample - loss: 1.9396e-07 - mae: 3.5184e-04 - val_loss: 3.6265e-04 - val_mae: 0.0162\n",
      "Epoch 258/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9438e-07 - mae: 4.1098e-04\n",
      "Epoch 258: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 133us/sample - loss: 2.0714e-07 - mae: 3.6481e-04 - val_loss: 3.6273e-04 - val_mae: 0.0162\n",
      "Epoch 259/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1015e-07 - mae: 4.8784e-04\n",
      "Epoch 259: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 132us/sample - loss: 1.5549e-07 - mae: 3.2086e-04 - val_loss: 3.6574e-04 - val_mae: 0.0163\n",
      "Epoch 260/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0561e-07 - mae: 2.9387e-04\n",
      "Epoch 260: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 1.4929e-07 - mae: 3.0534e-04 - val_loss: 3.6518e-04 - val_mae: 0.0163\n",
      "Epoch 261/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9863e-07 - mae: 3.5370e-04\n",
      "Epoch 261: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 133us/sample - loss: 1.4493e-07 - mae: 3.0411e-04 - val_loss: 3.6489e-04 - val_mae: 0.0163\n",
      "Epoch 262/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3726e-07 - mae: 4.1065e-04\n",
      "Epoch 262: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 133us/sample - loss: 1.8721e-07 - mae: 3.3629e-04 - val_loss: 3.6083e-04 - val_mae: 0.0162\n",
      "Epoch 263/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1370e-07 - mae: 5.8356e-04\n",
      "Epoch 263: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 136us/sample - loss: 1.8073e-07 - mae: 3.3385e-04 - val_loss: 3.6449e-04 - val_mae: 0.0163\n",
      "Epoch 264/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2472e-07 - mae: 2.5520e-04\n",
      "Epoch 264: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 1.2063e-07 - mae: 2.7677e-04 - val_loss: 3.6450e-04 - val_mae: 0.0163\n",
      "Epoch 265/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.8573e-08 - mae: 2.2970e-04\n",
      "Epoch 265: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 130us/sample - loss: 1.2968e-07 - mae: 2.8500e-04 - val_loss: 3.6527e-04 - val_mae: 0.0163\n",
      "Epoch 266/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6943e-08 - mae: 1.6963e-04\n",
      "Epoch 266: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 143us/sample - loss: 1.2819e-07 - mae: 2.8655e-04 - val_loss: 3.6716e-04 - val_mae: 0.0163\n",
      "Epoch 267/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3962e-07 - mae: 3.0899e-04\n",
      "Epoch 267: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 139us/sample - loss: 1.1978e-07 - mae: 2.8285e-04 - val_loss: 3.6393e-04 - val_mae: 0.0162\n",
      "Epoch 268/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7926e-07 - mae: 4.4365e-04\n",
      "Epoch 268: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 143us/sample - loss: 1.0912e-07 - mae: 2.6012e-04 - val_loss: 3.6662e-04 - val_mae: 0.0163\n",
      "Epoch 269/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5188e-07 - mae: 3.3447e-04\n",
      "Epoch 269: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 141us/sample - loss: 1.2062e-07 - mae: 2.8372e-04 - val_loss: 3.6518e-04 - val_mae: 0.0163\n",
      "Epoch 270/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1024e-07 - mae: 2.9643e-04\n",
      "Epoch 270: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 141us/sample - loss: 1.0164e-07 - mae: 2.5613e-04 - val_loss: 3.6761e-04 - val_mae: 0.0163\n",
      "Epoch 271/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0491e-07 - mae: 2.2399e-04\n",
      "Epoch 271: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 136us/sample - loss: 1.0121e-07 - mae: 2.5474e-04 - val_loss: 3.6874e-04 - val_mae: 0.0164\n",
      "Epoch 272/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4492e-07 - mae: 3.1069e-04\n",
      "Epoch 272: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 146us/sample - loss: 1.1632e-07 - mae: 2.6894e-04 - val_loss: 3.6707e-04 - val_mae: 0.0163\n",
      "Epoch 273/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1372e-07 - mae: 2.4131e-04\n",
      "Epoch 273: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 137us/sample - loss: 1.0792e-07 - mae: 2.6480e-04 - val_loss: 3.6682e-04 - val_mae: 0.0163\n",
      "Epoch 274/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4192e-08 - mae: 1.7304e-04\n",
      "Epoch 274: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 147us/sample - loss: 9.5474e-08 - mae: 2.4408e-04 - val_loss: 3.6666e-04 - val_mae: 0.0163\n",
      "Epoch 275/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0306e-08 - mae: 1.8173e-04\n",
      "Epoch 275: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 145us/sample - loss: 9.3585e-08 - mae: 2.3976e-04 - val_loss: 3.6895e-04 - val_mae: 0.0164\n",
      "Epoch 276/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1506e-07 - mae: 2.9366e-04\n",
      "Epoch 276: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 134us/sample - loss: 1.0179e-07 - mae: 2.5370e-04 - val_loss: 3.6530e-04 - val_mae: 0.0163\n",
      "Epoch 277/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.5483e-08 - mae: 2.2313e-04\n",
      "Epoch 277: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 134us/sample - loss: 7.3525e-08 - mae: 2.2023e-04 - val_loss: 3.6650e-04 - val_mae: 0.0163\n",
      "Epoch 278/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8153e-08 - mae: 1.6598e-04\n",
      "Epoch 278: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 128us/sample - loss: 1.0055e-07 - mae: 2.4880e-04 - val_loss: 3.6531e-04 - val_mae: 0.0163\n",
      "Epoch 279/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1231e-08 - mae: 2.5841e-04\n",
      "Epoch 279: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 7.3265e-08 - mae: 2.1393e-04 - val_loss: 3.6740e-04 - val_mae: 0.0163\n",
      "Epoch 280/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3804e-08 - mae: 2.1173e-04\n",
      "Epoch 280: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 1.0465e-07 - mae: 2.5630e-04 - val_loss: 3.6373e-04 - val_mae: 0.0162\n",
      "Epoch 281/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9095e-07 - mae: 3.7675e-04\n",
      "Epoch 281: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 7.1088e-08 - mae: 2.1091e-04 - val_loss: 3.6745e-04 - val_mae: 0.0163\n",
      "Epoch 282/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7118e-08 - mae: 1.9606e-04\n",
      "Epoch 282: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 145us/sample - loss: 6.4668e-08 - mae: 2.0057e-04 - val_loss: 3.6566e-04 - val_mae: 0.0163\n",
      "Epoch 283/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4399e-08 - mae: 1.9197e-04\n",
      "Epoch 283: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.5324e-08 - mae: 2.0864e-04 - val_loss: 3.6525e-04 - val_mae: 0.0163\n",
      "Epoch 284/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.7327e-08 - mae: 1.7725e-04\n",
      "Epoch 284: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 147us/sample - loss: 8.2557e-08 - mae: 2.3321e-04 - val_loss: 3.6679e-04 - val_mae: 0.0163\n",
      "Epoch 285/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5932e-08 - mae: 1.2408e-04\n",
      "Epoch 285: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 8.5791e-08 - mae: 2.3829e-04 - val_loss: 3.6656e-04 - val_mae: 0.0163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2045e-08 - mae: 1.1530e-04\n",
      "Epoch 286: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 9.1735e-08 - mae: 2.4392e-04 - val_loss: 3.6433e-04 - val_mae: 0.0162\n",
      "Epoch 287/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3561e-07 - mae: 2.8699e-04\n",
      "Epoch 287: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 7.1046e-08 - mae: 2.0933e-04 - val_loss: 3.6638e-04 - val_mae: 0.0163\n",
      "Epoch 288/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2262e-08 - mae: 1.9743e-04\n",
      "Epoch 288: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 6.0650e-08 - mae: 1.9363e-04 - val_loss: 3.6714e-04 - val_mae: 0.0163\n",
      "Epoch 289/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1156e-08 - mae: 2.0036e-04\n",
      "Epoch 289: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 5.8406e-08 - mae: 1.9473e-04 - val_loss: 3.6522e-04 - val_mae: 0.0163\n",
      "Epoch 290/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4509e-08 - mae: 1.8663e-04\n",
      "Epoch 290: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 215us/sample - loss: 5.3801e-08 - mae: 1.8240e-04 - val_loss: 3.6756e-04 - val_mae: 0.0163\n",
      "Epoch 291/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7283e-08 - mae: 1.0303e-04\n",
      "Epoch 291: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 202us/sample - loss: 5.3197e-08 - mae: 1.8164e-04 - val_loss: 3.6615e-04 - val_mae: 0.0163\n",
      "Epoch 292/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9170e-08 - mae: 1.6351e-04\n",
      "Epoch 292: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 6.2958e-08 - mae: 2.0386e-04 - val_loss: 3.6760e-04 - val_mae: 0.0163\n",
      "Epoch 293/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1449e-08 - mae: 2.5383e-04\n",
      "Epoch 293: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 213us/sample - loss: 4.4353e-08 - mae: 1.6548e-04 - val_loss: 3.6670e-04 - val_mae: 0.0163\n",
      "Epoch 294/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5184e-08 - mae: 1.6440e-04\n",
      "Epoch 294: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 3.6081e-08 - mae: 1.5064e-04 - val_loss: 3.6694e-04 - val_mae: 0.0163\n",
      "Epoch 295/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9486e-08 - mae: 1.0126e-04\n",
      "Epoch 295: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 3.0138e-08 - mae: 1.4026e-04 - val_loss: 3.6648e-04 - val_mae: 0.0163\n",
      "Epoch 296/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4671e-08 - mae: 1.4057e-04\n",
      "Epoch 296: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 4.1101e-08 - mae: 1.6331e-04 - val_loss: 3.6679e-04 - val_mae: 0.0163\n",
      "Epoch 297/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2447e-08 - mae: 1.5374e-04\n",
      "Epoch 297: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 2.8541e-08 - mae: 1.3414e-04 - val_loss: 3.6675e-04 - val_mae: 0.0163\n",
      "Epoch 298/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2957e-08 - mae: 1.1356e-04\n",
      "Epoch 298: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 3.6609e-08 - mae: 1.4927e-04 - val_loss: 3.6527e-04 - val_mae: 0.0163\n",
      "Epoch 299/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2856e-08 - mae: 1.6213e-04\n",
      "Epoch 299: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 6.1876e-08 - mae: 2.0502e-04 - val_loss: 3.6919e-04 - val_mae: 0.0163\n",
      "Epoch 300/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2978e-08 - mae: 2.7178e-04\n",
      "Epoch 300: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 9.3755e-08 - mae: 2.5991e-04 - val_loss: 3.6484e-04 - val_mae: 0.0162\n",
      "Epoch 301/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0867e-07 - mae: 3.0211e-04\n",
      "Epoch 301: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 5.1137e-08 - mae: 1.8188e-04 - val_loss: 3.6750e-04 - val_mae: 0.0163\n",
      "Epoch 302/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2907e-08 - mae: 9.7984e-05\n",
      "Epoch 302: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 5.0002e-08 - mae: 1.8285e-04 - val_loss: 3.6682e-04 - val_mae: 0.0163\n",
      "Epoch 303/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0930e-08 - mae: 1.2891e-04\n",
      "Epoch 303: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 218us/sample - loss: 2.9353e-08 - mae: 1.4103e-04 - val_loss: 3.6668e-04 - val_mae: 0.0163\n",
      "Epoch 304/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2619e-08 - mae: 9.6780e-05\n",
      "Epoch 304: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 1.9909e-08 - mae: 1.1250e-04 - val_loss: 3.6660e-04 - val_mae: 0.0163\n",
      "Epoch 305/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8819e-08 - mae: 1.4033e-04\n",
      "Epoch 305: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 1.8806e-08 - mae: 1.1080e-04 - val_loss: 3.6675e-04 - val_mae: 0.0163\n",
      "Epoch 306/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6344e-08 - mae: 1.0244e-04\n",
      "Epoch 306: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 1.7979e-08 - mae: 1.0504e-04 - val_loss: 3.6789e-04 - val_mae: 0.0163\n",
      "Epoch 307/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7940e-08 - mae: 1.2340e-04\n",
      "Epoch 307: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 2.3435e-08 - mae: 1.1892e-04 - val_loss: 3.6573e-04 - val_mae: 0.0163\n",
      "Epoch 308/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8500e-08 - mae: 1.1381e-04\n",
      "Epoch 308: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 2.5934e-08 - mae: 1.2998e-04 - val_loss: 3.6823e-04 - val_mae: 0.0163\n",
      "Epoch 309/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7906e-08 - mae: 1.5091e-04\n",
      "Epoch 309: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 2.3315e-08 - mae: 1.2048e-04 - val_loss: 3.6666e-04 - val_mae: 0.0163\n",
      "Epoch 310/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.6604e-09 - mae: 8.2324e-05\n",
      "Epoch 310: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 2.1795e-08 - mae: 1.1814e-04 - val_loss: 3.6671e-04 - val_mae: 0.0163\n",
      "Epoch 311/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8355e-08 - mae: 1.0786e-04\n",
      "Epoch 311: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 1.6431e-08 - mae: 1.0551e-04 - val_loss: 3.6767e-04 - val_mae: 0.0163\n",
      "Epoch 312/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2781e-08 - mae: 9.3105e-05\n",
      "Epoch 312: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.5423e-08 - mae: 9.7544e-05 - val_loss: 3.6775e-04 - val_mae: 0.0163\n",
      "Epoch 313/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5794e-08 - mae: 1.1044e-04\n",
      "Epoch 313: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 2.4661e-08 - mae: 1.2469e-04 - val_loss: 3.6564e-04 - val_mae: 0.0163\n",
      "Epoch 314/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2362e-08 - mae: 1.4685e-04\n",
      "Epoch 314: val_loss did not improve from 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 164us/sample - loss: 1.9456e-08 - mae: 1.0963e-04 - val_loss: 3.6644e-04 - val_mae: 0.0163\n",
      "Epoch 315/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1769e-08 - mae: 9.3841e-05\n",
      "Epoch 315: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 1.2224e-08 - mae: 8.7561e-05 - val_loss: 3.6638e-04 - val_mae: 0.0163\n",
      "Epoch 316/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2071e-08 - mae: 8.4477e-05\n",
      "Epoch 316: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 1.3325e-08 - mae: 9.2159e-05 - val_loss: 3.6786e-04 - val_mae: 0.0163\n",
      "Epoch 317/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4278e-08 - mae: 9.2572e-05\n",
      "Epoch 317: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.4685e-08 - mae: 9.7487e-05 - val_loss: 3.6733e-04 - val_mae: 0.0163\n",
      "Epoch 318/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.8086e-09 - mae: 7.1929e-05\n",
      "Epoch 318: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 9.0767e-09 - mae: 7.4730e-05 - val_loss: 3.6702e-04 - val_mae: 0.0163\n",
      "Epoch 319/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.5192e-09 - mae: 6.5998e-05\n",
      "Epoch 319: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.0738e-08 - mae: 8.3585e-05 - val_loss: 3.6718e-04 - val_mae: 0.0163\n",
      "Epoch 320/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5963e-09 - mae: 5.0759e-05\n",
      "Epoch 320: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 7.9493e-09 - mae: 6.9820e-05 - val_loss: 3.6754e-04 - val_mae: 0.0163\n",
      "Epoch 321/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1034e-09 - mae: 7.8640e-05\n",
      "Epoch 321: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 9.5903e-09 - mae: 7.7080e-05 - val_loss: 3.6733e-04 - val_mae: 0.0163\n",
      "Epoch 322/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.3760e-09 - mae: 6.8078e-05\n",
      "Epoch 322: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 9.0566e-09 - mae: 7.4079e-05 - val_loss: 3.6810e-04 - val_mae: 0.0163\n",
      "Epoch 323/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1618e-09 - mae: 7.9597e-05\n",
      "Epoch 323: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 9.8358e-09 - mae: 7.6347e-05 - val_loss: 3.6669e-04 - val_mae: 0.0163\n",
      "Epoch 324/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8344e-09 - mae: 5.6996e-05\n",
      "Epoch 324: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 171us/sample - loss: 1.1246e-08 - mae: 8.5374e-05 - val_loss: 3.6634e-04 - val_mae: 0.0163\n",
      "Epoch 325/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2732e-08 - mae: 1.2498e-04\n",
      "Epoch 325: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 1.1227e-08 - mae: 8.4976e-05 - val_loss: 3.6696e-04 - val_mae: 0.0163\n",
      "Epoch 326/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.4636e-09 - mae: 7.5129e-05\n",
      "Epoch 326: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 1.3711e-08 - mae: 9.7832e-05 - val_loss: 3.6664e-04 - val_mae: 0.0163\n",
      "Epoch 327/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.4951e-09 - mae: 8.8836e-05\n",
      "Epoch 327: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 1.0952e-08 - mae: 8.3594e-05 - val_loss: 3.6673e-04 - val_mae: 0.0163\n",
      "Epoch 328/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8235e-09 - mae: 7.9533e-05\n",
      "Epoch 328: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 1.3108e-08 - mae: 9.1652e-05 - val_loss: 3.6705e-04 - val_mae: 0.0163\n",
      "Epoch 329/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.9025e-09 - mae: 8.0210e-05\n",
      "Epoch 329: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 5.5353e-09 - mae: 5.9201e-05 - val_loss: 3.6721e-04 - val_mae: 0.0163\n",
      "Epoch 330/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1925e-09 - mae: 8.1170e-05\n",
      "Epoch 330: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 5.5169e-09 - mae: 5.8823e-05 - val_loss: 3.6682e-04 - val_mae: 0.0163\n",
      "Epoch 331/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8508e-09 - mae: 3.7326e-05\n",
      "Epoch 331: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 1.5904e-08 - mae: 1.0414e-04 - val_loss: 3.6753e-04 - val_mae: 0.0163\n",
      "Epoch 332/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2703e-09 - mae: 3.1031e-05\n",
      "Epoch 332: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 6.1600e-09 - mae: 6.0409e-05 - val_loss: 3.6743e-04 - val_mae: 0.0163\n",
      "Epoch 333/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2359e-09 - mae: 2.7988e-05\n",
      "Epoch 333: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 6.7907e-09 - mae: 6.4948e-05 - val_loss: 3.6687e-04 - val_mae: 0.0163\n",
      "Epoch 334/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.7375e-09 - mae: 7.0967e-05\n",
      "Epoch 334: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 1.1797e-08 - mae: 8.8882e-05 - val_loss: 3.6647e-04 - val_mae: 0.0163\n",
      "Epoch 335/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8745e-08 - mae: 1.2039e-04\n",
      "Epoch 335: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 221us/sample - loss: 1.6728e-08 - mae: 1.0945e-04 - val_loss: 3.6643e-04 - val_mae: 0.0163\n",
      "Epoch 336/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1742e-08 - mae: 1.3156e-04\n",
      "Epoch 336: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 209us/sample - loss: 7.1320e-09 - mae: 6.5177e-05 - val_loss: 3.6763e-04 - val_mae: 0.0163\n",
      "Epoch 337/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9070e-09 - mae: 4.8202e-05\n",
      "Epoch 337: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 5.6596e-09 - mae: 6.0120e-05 - val_loss: 3.6742e-04 - val_mae: 0.0163\n",
      "Epoch 338/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0748e-09 - mae: 4.2076e-05\n",
      "Epoch 338: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 5.3992e-09 - mae: 5.6719e-05 - val_loss: 3.6798e-04 - val_mae: 0.0163\n",
      "Epoch 339/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0215e-09 - mae: 4.4385e-05\n",
      "Epoch 339: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 7.6323e-09 - mae: 7.2424e-05 - val_loss: 3.6755e-04 - val_mae: 0.0163\n",
      "Epoch 340/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6885e-09 - mae: 5.0229e-05\n",
      "Epoch 340: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 4.1000e-09 - mae: 5.1020e-05 - val_loss: 3.6724e-04 - val_mae: 0.0163\n",
      "Epoch 341/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3374e-09 - mae: 3.4017e-05\n",
      "Epoch 341: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 5.6611e-09 - mae: 5.9720e-05 - val_loss: 3.6796e-04 - val_mae: 0.0163\n",
      "Epoch 342/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7758e-09 - mae: 4.6366e-05\n",
      "Epoch 342: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 7.6463e-09 - mae: 7.3136e-05 - val_loss: 3.6835e-04 - val_mae: 0.0163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3532e-08 - mae: 1.0877e-04\n",
      "Epoch 343: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 7.7749e-09 - mae: 7.3425e-05 - val_loss: 3.6813e-04 - val_mae: 0.0163\n",
      "Epoch 344/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8020e-09 - mae: 6.8310e-05\n",
      "Epoch 344: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 5.0776e-09 - mae: 5.5429e-05 - val_loss: 3.6754e-04 - val_mae: 0.0163\n",
      "Epoch 345/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0495e-09 - mae: 2.4511e-05\n",
      "Epoch 345: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 2.7925e-09 - mae: 4.2429e-05 - val_loss: 3.6732e-04 - val_mae: 0.0163\n",
      "Epoch 346/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.6976e-10 - mae: 2.5428e-05\n",
      "Epoch 346: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 3.1844e-09 - mae: 4.4821e-05 - val_loss: 3.6689e-04 - val_mae: 0.0163\n",
      "Epoch 347/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3079e-09 - mae: 6.9007e-05\n",
      "Epoch 347: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 2.6443e-09 - mae: 4.1021e-05 - val_loss: 3.6746e-04 - val_mae: 0.0163\n",
      "Epoch 348/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1151e-09 - mae: 3.9401e-05\n",
      "Epoch 348: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 2.6769e-09 - mae: 4.1122e-05 - val_loss: 3.6717e-04 - val_mae: 0.0163\n",
      "Epoch 349/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6998e-09 - mae: 3.5394e-05\n",
      "Epoch 349: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 1.7772e-09 - mae: 3.2882e-05 - val_loss: 3.6770e-04 - val_mae: 0.0163\n",
      "Epoch 350/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7465e-09 - mae: 3.2322e-05\n",
      "Epoch 350: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 225us/sample - loss: 2.3372e-09 - mae: 3.8854e-05 - val_loss: 3.6723e-04 - val_mae: 0.0163\n",
      "Epoch 351/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0166e-09 - mae: 3.8159e-05\n",
      "Epoch 351: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 2.5216e-09 - mae: 3.9627e-05 - val_loss: 3.6727e-04 - val_mae: 0.0163\n",
      "Epoch 352/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6368e-09 - mae: 3.1001e-05\n",
      "Epoch 352: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 1.8495e-09 - mae: 3.3703e-05 - val_loss: 3.6794e-04 - val_mae: 0.0163\n",
      "Epoch 353/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3829e-09 - mae: 6.0075e-05\n",
      "Epoch 353: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 1.7420e-09 - mae: 3.2903e-05 - val_loss: 3.6749e-04 - val_mae: 0.0163\n",
      "Epoch 354/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5564e-09 - mae: 3.4249e-05\n",
      "Epoch 354: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 1.3935e-09 - mae: 2.8280e-05 - val_loss: 3.6743e-04 - val_mae: 0.0163\n",
      "Epoch 355/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3808e-10 - mae: 1.6280e-05\n",
      "Epoch 355: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.4885e-09 - mae: 3.0302e-05 - val_loss: 3.6766e-04 - val_mae: 0.0163\n",
      "Epoch 356/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8869e-10 - mae: 1.5445e-05\n",
      "Epoch 356: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 1.0677e-09 - mae: 2.6622e-05 - val_loss: 3.6773e-04 - val_mae: 0.0163\n",
      "Epoch 357/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7807e-09 - mae: 3.9540e-05\n",
      "Epoch 357: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 1.2835e-09 - mae: 2.7992e-05 - val_loss: 3.6754e-04 - val_mae: 0.0163\n",
      "Epoch 358/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2618e-09 - mae: 2.7126e-05\n",
      "Epoch 358: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 2.1790e-09 - mae: 3.9216e-05 - val_loss: 3.6731e-04 - val_mae: 0.0163\n",
      "Epoch 359/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2454e-09 - mae: 3.1639e-05\n",
      "Epoch 359: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.8226e-09 - mae: 3.5128e-05 - val_loss: 3.6784e-04 - val_mae: 0.0163\n",
      "Epoch 360/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4232e-09 - mae: 3.3062e-05\n",
      "Epoch 360: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.9384e-09 - mae: 3.4072e-05 - val_loss: 3.6763e-04 - val_mae: 0.0163\n",
      "Epoch 361/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.6852e-10 - mae: 2.4851e-05\n",
      "Epoch 361: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 2.0740e-09 - mae: 3.6638e-05 - val_loss: 3.6810e-04 - val_mae: 0.0163\n",
      "Epoch 362/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4518e-09 - mae: 6.2844e-05\n",
      "Epoch 362: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 2.4485e-09 - mae: 4.1158e-05 - val_loss: 3.6727e-04 - val_mae: 0.0163\n",
      "Epoch 363/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0640e-09 - mae: 2.9312e-05\n",
      "Epoch 363: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.0235e-09 - mae: 2.5375e-05 - val_loss: 3.6765e-04 - val_mae: 0.0163\n",
      "Epoch 364/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.8430e-10 - mae: 2.1711e-05\n",
      "Epoch 364: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 148us/sample - loss: 8.1919e-10 - mae: 2.2580e-05 - val_loss: 3.6766e-04 - val_mae: 0.0163\n",
      "Epoch 365/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7147e-10 - mae: 1.8094e-05\n",
      "Epoch 365: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 8.5342e-10 - mae: 2.1231e-05 - val_loss: 3.6751e-04 - val_mae: 0.0163\n",
      "Epoch 366/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8233e-10 - mae: 2.1207e-05\n",
      "Epoch 366: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 141us/sample - loss: 1.1152e-09 - mae: 2.5624e-05 - val_loss: 3.6768e-04 - val_mae: 0.0163\n",
      "Epoch 367/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8560e-10 - mae: 1.0521e-05\n",
      "Epoch 367: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 7.0694e-10 - mae: 2.0228e-05 - val_loss: 3.6771e-04 - val_mae: 0.0163\n",
      "Epoch 368/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3036e-10 - mae: 1.3615e-05\n",
      "Epoch 368: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 144us/sample - loss: 6.2246e-10 - mae: 1.9038e-05 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 369/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2257e-10 - mae: 2.3044e-05\n",
      "Epoch 369: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 1.1777e-09 - mae: 2.7886e-05 - val_loss: 3.6788e-04 - val_mae: 0.0163\n",
      "Epoch 370/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3315e-10 - mae: 2.5344e-05\n",
      "Epoch 370: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 1.2953e-09 - mae: 2.9738e-05 - val_loss: 3.6764e-04 - val_mae: 0.0163\n",
      "Epoch 371/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6554e-10 - mae: 1.6331e-05\n",
      "Epoch 371: val_loss did not improve from 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 146us/sample - loss: 1.0642e-09 - mae: 2.6728e-05 - val_loss: 3.6794e-04 - val_mae: 0.0163\n",
      "Epoch 372/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.7110e-10 - mae: 2.5867e-05\n",
      "Epoch 372: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 5.4742e-10 - mae: 1.8297e-05 - val_loss: 3.6749e-04 - val_mae: 0.0163\n",
      "Epoch 373/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3263e-10 - mae: 2.6144e-05\n",
      "Epoch 373: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 1.0412e-09 - mae: 2.5617e-05 - val_loss: 3.6797e-04 - val_mae: 0.0163\n",
      "Epoch 374/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4800e-09 - mae: 3.7121e-05\n",
      "Epoch 374: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 8.5683e-10 - mae: 2.3508e-05 - val_loss: 3.6744e-04 - val_mae: 0.0163\n",
      "Epoch 375/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6485e-09 - mae: 3.7726e-05\n",
      "Epoch 375: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.4502e-09 - mae: 3.1174e-05 - val_loss: 3.6776e-04 - val_mae: 0.0163\n",
      "Epoch 376/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3348e-10 - mae: 1.7312e-05\n",
      "Epoch 376: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 7.5210e-10 - mae: 2.2052e-05 - val_loss: 3.6785e-04 - val_mae: 0.0163\n",
      "Epoch 377/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4268e-10 - mae: 1.2648e-05\n",
      "Epoch 377: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 148us/sample - loss: 6.2142e-10 - mae: 1.9660e-05 - val_loss: 3.6776e-04 - val_mae: 0.0163\n",
      "Epoch 378/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0592e-10 - mae: 1.1098e-05\n",
      "Epoch 378: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 146us/sample - loss: 7.1601e-10 - mae: 2.1776e-05 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 379/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.9875e-10 - mae: 1.8507e-05\n",
      "Epoch 379: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 3.3579e-10 - mae: 1.4789e-05 - val_loss: 3.6801e-04 - val_mae: 0.0163\n",
      "Epoch 380/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4958e-09 - mae: 3.5395e-05\n",
      "Epoch 380: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 5.8311e-10 - mae: 1.9225e-05 - val_loss: 3.6772e-04 - val_mae: 0.0163\n",
      "Epoch 381/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7837e-10 - mae: 1.2796e-05\n",
      "Epoch 381: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 7.0603e-10 - mae: 2.1985e-05 - val_loss: 3.6790e-04 - val_mae: 0.0163\n",
      "Epoch 382/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8216e-10 - mae: 2.1200e-05\n",
      "Epoch 382: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 7.1040e-10 - mae: 2.1712e-05 - val_loss: 3.6726e-04 - val_mae: 0.0163\n",
      "Epoch 383/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2470e-09 - mae: 4.4958e-05\n",
      "Epoch 383: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 8.1570e-10 - mae: 2.3023e-05 - val_loss: 3.6786e-04 - val_mae: 0.0163\n",
      "Epoch 384/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1218e-10 - mae: 2.6627e-05\n",
      "Epoch 384: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 5.9057e-10 - mae: 2.0469e-05 - val_loss: 3.6752e-04 - val_mae: 0.0163\n",
      "Epoch 385/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9361e-10 - mae: 2.4810e-05\n",
      "Epoch 385: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 171us/sample - loss: 5.2661e-10 - mae: 1.8457e-05 - val_loss: 3.6782e-04 - val_mae: 0.0163\n",
      "Epoch 386/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3646e-10 - mae: 1.3747e-05\n",
      "Epoch 386: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 4.6039e-10 - mae: 1.7370e-05 - val_loss: 3.6752e-04 - val_mae: 0.0163\n",
      "Epoch 387/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0118e-10 - mae: 1.8381e-05\n",
      "Epoch 387: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 3.9347e-10 - mae: 1.6114e-05 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 388/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7348e-10 - mae: 1.1276e-05\n",
      "Epoch 388: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 3.9772e-10 - mae: 1.6205e-05 - val_loss: 3.6786e-04 - val_mae: 0.0163\n",
      "Epoch 389/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4225e-10 - mae: 2.0640e-05\n",
      "Epoch 389: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 6.1115e-10 - mae: 2.0431e-05 - val_loss: 3.6788e-04 - val_mae: 0.0163\n",
      "Epoch 390/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5696e-10 - mae: 1.2649e-05\n",
      "Epoch 390: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 4.4857e-10 - mae: 1.7321e-05 - val_loss: 3.6785e-04 - val_mae: 0.0163\n",
      "Epoch 391/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7626e-10 - mae: 1.3179e-05\n",
      "Epoch 391: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 7.8369e-10 - mae: 2.2713e-05 - val_loss: 3.6768e-04 - val_mae: 0.0163\n",
      "Epoch 392/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7334e-10 - mae: 1.0655e-05\n",
      "Epoch 392: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 170us/sample - loss: 2.8035e-10 - mae: 1.3295e-05 - val_loss: 3.6784e-04 - val_mae: 0.0163\n",
      "Epoch 393/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9627e-10 - mae: 1.5716e-05\n",
      "Epoch 393: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 3.8003e-10 - mae: 1.6526e-05 - val_loss: 3.6772e-04 - val_mae: 0.0163\n",
      "Epoch 394/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0035e-10 - mae: 1.2081e-05\n",
      "Epoch 394: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 1.4960e-10 - mae: 9.6570e-06 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 395/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7220e-10 - mae: 1.0771e-05\n",
      "Epoch 395: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 1.8568e-10 - mae: 1.0992e-05 - val_loss: 3.6773e-04 - val_mae: 0.0163\n",
      "Epoch 396/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8797e-11 - mae: 8.1574e-06\n",
      "Epoch 396: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 1.5503e-10 - mae: 9.7590e-06 - val_loss: 3.6780e-04 - val_mae: 0.0163\n",
      "Epoch 397/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8265e-11 - mae: 6.1885e-06\n",
      "Epoch 397: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 1.1699e-10 - mae: 8.7409e-06 - val_loss: 3.6760e-04 - val_mae: 0.0163\n",
      "Epoch 398/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9297e-10 - mae: 1.9114e-05\n",
      "Epoch 398: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 213us/sample - loss: 1.6431e-10 - mae: 1.0723e-05 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 399/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.2231e-11 - mae: 7.5938e-06\n",
      "Epoch 399: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 212us/sample - loss: 3.1403e-10 - mae: 1.5293e-05 - val_loss: 3.6767e-04 - val_mae: 0.0163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4704e-10 - mae: 1.0337e-05\n",
      "Epoch 400: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 2.5510e-10 - mae: 1.3217e-05 - val_loss: 3.6760e-04 - val_mae: 0.0163\n",
      "Epoch 401/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5755e-10 - mae: 1.3845e-05\n",
      "Epoch 401: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 210us/sample - loss: 3.5375e-10 - mae: 1.6607e-05 - val_loss: 3.6756e-04 - val_mae: 0.0163\n",
      "Epoch 402/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2407e-10 - mae: 2.7422e-05\n",
      "Epoch 402: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 2.7695e-10 - mae: 1.3653e-05 - val_loss: 3.6774e-04 - val_mae: 0.0163\n",
      "Epoch 403/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6009e-11 - mae: 5.8170e-06\n",
      "Epoch 403: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.7361e-10 - mae: 1.0429e-05 - val_loss: 3.6779e-04 - val_mae: 0.0163\n",
      "Epoch 404/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2239e-11 - mae: 7.0972e-06\n",
      "Epoch 404: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 1.7044e-10 - mae: 1.0796e-05 - val_loss: 3.6781e-04 - val_mae: 0.0163\n",
      "Epoch 405/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7411e-11 - mae: 5.0548e-06\n",
      "Epoch 405: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 1.0594e-10 - mae: 8.3076e-06 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 406/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2766e-10 - mae: 9.9312e-06\n",
      "Epoch 406: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 8.5146e-11 - mae: 7.2911e-06 - val_loss: 3.6778e-04 - val_mae: 0.0163\n",
      "Epoch 407/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1328e-11 - mae: 3.7875e-06\n",
      "Epoch 407: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 7.4622e-11 - mae: 6.7416e-06 - val_loss: 3.6774e-04 - val_mae: 0.0163\n",
      "Epoch 408/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.4357e-11 - mae: 7.8917e-06\n",
      "Epoch 408: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 9.4185e-11 - mae: 7.4783e-06 - val_loss: 3.6784e-04 - val_mae: 0.0163\n",
      "Epoch 409/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2073e-11 - mae: 7.3276e-06\n",
      "Epoch 409: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 3.9467e-10 - mae: 1.7024e-05 - val_loss: 3.6799e-04 - val_mae: 0.0163\n",
      "Epoch 410/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3831e-10 - mae: 2.6630e-05\n",
      "Epoch 410: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 5.8433e-10 - mae: 2.0730e-05 - val_loss: 3.6758e-04 - val_mae: 0.0163\n",
      "Epoch 411/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0553e-10 - mae: 1.1630e-05\n",
      "Epoch 411: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 172us/sample - loss: 8.2276e-10 - mae: 2.3800e-05 - val_loss: 3.6778e-04 - val_mae: 0.0163\n",
      "Epoch 412/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1216e-10 - mae: 1.1473e-05\n",
      "Epoch 412: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 8.2910e-10 - mae: 2.3907e-05 - val_loss: 3.6780e-04 - val_mae: 0.0163\n",
      "Epoch 413/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0173e-10 - mae: 1.2556e-05\n",
      "Epoch 413: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.1057e-09 - mae: 2.7775e-05 - val_loss: 3.6794e-04 - val_mae: 0.0163\n",
      "Epoch 414/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0373e-10 - mae: 2.3655e-05\n",
      "Epoch 414: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.6849e-10 - mae: 2.0763e-05 - val_loss: 3.6785e-04 - val_mae: 0.0163\n",
      "Epoch 415/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7235e-11 - mae: 8.5691e-06\n",
      "Epoch 415: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 3.2729e-10 - mae: 1.4483e-05 - val_loss: 3.6791e-04 - val_mae: 0.0163\n",
      "Epoch 416/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9543e-10 - mae: 1.7254e-05\n",
      "Epoch 416: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.9430e-10 - mae: 1.1430e-05 - val_loss: 3.6767e-04 - val_mae: 0.0163\n",
      "Epoch 417/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8785e-10 - mae: 1.4629e-05\n",
      "Epoch 417: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 1.2637e-10 - mae: 8.9939e-06 - val_loss: 3.6780e-04 - val_mae: 0.0163\n",
      "Epoch 418/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6752e-11 - mae: 6.8055e-06\n",
      "Epoch 418: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 8.7183e-11 - mae: 7.7590e-06 - val_loss: 3.6767e-04 - val_mae: 0.0163\n",
      "Epoch 419/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3682e-10 - mae: 1.1344e-05\n",
      "Epoch 419: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 1.4346e-10 - mae: 1.0185e-05 - val_loss: 3.6779e-04 - val_mae: 0.0163\n",
      "Epoch 420/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6176e-11 - mae: 4.7697e-06\n",
      "Epoch 420: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 1.3628e-10 - mae: 9.3983e-06 - val_loss: 3.6771e-04 - val_mae: 0.0163\n",
      "Epoch 421/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3533e-11 - mae: 7.6954e-06\n",
      "Epoch 421: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 147us/sample - loss: 6.6944e-11 - mae: 6.7904e-06 - val_loss: 3.6784e-04 - val_mae: 0.0163\n",
      "Epoch 422/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1387e-11 - mae: 7.3764e-06\n",
      "Epoch 422: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 1.4546e-10 - mae: 1.0032e-05 - val_loss: 3.6776e-04 - val_mae: 0.0163\n",
      "Epoch 423/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8708e-11 - mae: 6.9564e-06\n",
      "Epoch 423: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 9.6428e-11 - mae: 7.7459e-06 - val_loss: 3.6786e-04 - val_mae: 0.0163\n",
      "Epoch 424/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7486e-11 - mae: 7.4307e-06\n",
      "Epoch 424: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 1.0218e-10 - mae: 8.2410e-06 - val_loss: 3.6768e-04 - val_mae: 0.0163\n",
      "Epoch 425/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0145e-10 - mae: 1.3348e-05\n",
      "Epoch 425: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 1.1298e-10 - mae: 8.7583e-06 - val_loss: 3.6785e-04 - val_mae: 0.0163\n",
      "Epoch 426/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3815e-11 - mae: 7.7961e-06\n",
      "Epoch 426: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 5.2499e-11 - mae: 5.8790e-06 - val_loss: 3.6777e-04 - val_mae: 0.0163\n",
      "Epoch 427/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2599e-11 - mae: 3.1156e-06\n",
      "Epoch 427: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 144us/sample - loss: 6.8799e-11 - mae: 7.0260e-06 - val_loss: 3.6783e-04 - val_mae: 0.0163\n",
      "Epoch 428/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3486e-11 - mae: 3.7376e-06\n",
      "Epoch 428: val_loss did not improve from 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 148us/sample - loss: 1.0703e-10 - mae: 8.4226e-06 - val_loss: 3.6780e-04 - val_mae: 0.0163\n",
      "Epoch 429/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0113e-11 - mae: 4.7531e-06\n",
      "Epoch 429: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 3.4729e-11 - mae: 4.7107e-06 - val_loss: 3.6780e-04 - val_mae: 0.0163\n",
      "Epoch 430/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.4985e-12 - mae: 2.1387e-06\n",
      "Epoch 430: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 4.3366e-11 - mae: 5.3319e-06 - val_loss: 3.6777e-04 - val_mae: 0.0163\n",
      "Epoch 431/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4037e-11 - mae: 4.9404e-06\n",
      "Epoch 431: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.1415e-10 - mae: 9.1363e-06 - val_loss: 3.6772e-04 - val_mae: 0.0163\n",
      "Epoch 432/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1678e-10 - mae: 9.8616e-06\n",
      "Epoch 432: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 9.3065e-11 - mae: 7.9614e-06 - val_loss: 3.6782e-04 - val_mae: 0.0163\n",
      "Epoch 433/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1175e-11 - mae: 2.8523e-06\n",
      "Epoch 433: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 4.6234e-11 - mae: 5.4393e-06 - val_loss: 3.6778e-04 - val_mae: 0.0163\n",
      "Epoch 434/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9477e-11 - mae: 5.6159e-06\n",
      "Epoch 434: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 4.7551e-11 - mae: 5.6637e-06 - val_loss: 3.6781e-04 - val_mae: 0.0163\n",
      "Epoch 435/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2797e-11 - mae: 3.8378e-06\n",
      "Epoch 435: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 1.6949e-10 - mae: 1.1401e-05 - val_loss: 3.6783e-04 - val_mae: 0.0163\n",
      "Epoch 436/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6828e-11 - mae: 4.5479e-06\n",
      "Epoch 436: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 8.4353e-11 - mae: 7.4353e-06 - val_loss: 3.6787e-04 - val_mae: 0.0163\n",
      "Epoch 437/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2255e-11 - mae: 2.7224e-06\n",
      "Epoch 437: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 7.6358e-11 - mae: 6.8561e-06 - val_loss: 3.6786e-04 - val_mae: 0.0163\n",
      "Epoch 438/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3237e-11 - mae: 4.6610e-06\n",
      "Epoch 438: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 4.1967e-11 - mae: 5.2133e-06 - val_loss: 3.6785e-04 - val_mae: 0.0163\n",
      "Epoch 439/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3033e-11 - mae: 3.0077e-06\n",
      "Epoch 439: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 5.2870e-11 - mae: 6.0162e-06 - val_loss: 3.6786e-04 - val_mae: 0.0163\n",
      "Epoch 440/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0311e-11 - mae: 6.9225e-06\n",
      "Epoch 440: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 8.5966e-11 - mae: 8.1085e-06 - val_loss: 3.6778e-04 - val_mae: 0.0163\n",
      "Epoch 441/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1014e-11 - mae: 3.7895e-06\n",
      "Epoch 441: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 170us/sample - loss: 3.4039e-11 - mae: 4.7422e-06 - val_loss: 3.6771e-04 - val_mae: 0.0163\n",
      "Epoch 442/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0752e-10 - mae: 1.3893e-05\n",
      "Epoch 442: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 1.1014e-10 - mae: 9.0521e-06 - val_loss: 3.6782e-04 - val_mae: 0.0163\n",
      "Epoch 443/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8351e-11 - mae: 4.8637e-06\n",
      "Epoch 443: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 7.6603e-11 - mae: 7.1184e-06 - val_loss: 3.6763e-04 - val_mae: 0.0163\n",
      "Epoch 444/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1195e-10 - mae: 1.3874e-05\n",
      "Epoch 444: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 2.4558e-10 - mae: 1.3813e-05 - val_loss: 3.6763e-04 - val_mae: 0.0163\n",
      "Epoch 445/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8571e-10 - mae: 1.9327e-05\n",
      "Epoch 445: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 7.6356e-10 - mae: 2.3708e-05 - val_loss: 3.6778e-04 - val_mae: 0.0163\n",
      "Epoch 446/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5541e-10 - mae: 1.0281e-05\n",
      "Epoch 446: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 8.5348e-10 - mae: 2.4836e-05 - val_loss: 3.6769e-04 - val_mae: 0.0163\n",
      "Epoch 447/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4098e-10 - mae: 2.2026e-05\n",
      "Epoch 447: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 144us/sample - loss: 4.2111e-09 - mae: 5.6708e-05 - val_loss: 3.6828e-04 - val_mae: 0.0163\n",
      "Epoch 448/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.5319e-09 - mae: 7.9676e-05\n",
      "Epoch 448: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 5.9810e-09 - mae: 6.6937e-05 - val_loss: 3.6721e-04 - val_mae: 0.0163\n",
      "Epoch 449/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5053e-09 - mae: 5.9539e-05\n",
      "Epoch 449: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 145us/sample - loss: 1.1722e-08 - mae: 9.1289e-05 - val_loss: 3.6728e-04 - val_mae: 0.0163\n",
      "Epoch 450/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.9074e-09 - mae: 6.8883e-05\n",
      "Epoch 450: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 149us/sample - loss: 4.8336e-08 - mae: 1.7862e-04 - val_loss: 3.6716e-04 - val_mae: 0.0163\n",
      "Epoch 451/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7631e-08 - mae: 1.0586e-04\n",
      "Epoch 451: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 7.6888e-08 - mae: 2.3254e-04 - val_loss: 3.7044e-04 - val_mae: 0.0164\n",
      "Epoch 452/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2938e-07 - mae: 3.3932e-04\n",
      "Epoch 452: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 1.8072e-07 - mae: 3.5308e-04 - val_loss: 3.7041e-04 - val_mae: 0.0164\n",
      "Epoch 453/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2115e-07 - mae: 2.9044e-04\n",
      "Epoch 453: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 149us/sample - loss: 1.4273e-07 - mae: 3.1249e-04 - val_loss: 3.6958e-04 - val_mae: 0.0163\n",
      "Epoch 454/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7233e-08 - mae: 1.9066e-04\n",
      "Epoch 454: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 3.2200e-07 - mae: 4.6130e-04 - val_loss: 3.7075e-04 - val_mae: 0.0164\n",
      "Epoch 455/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9236e-07 - mae: 3.8254e-04\n",
      "Epoch 455: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 141us/sample - loss: 2.3667e-07 - mae: 3.9723e-04 - val_loss: 3.6455e-04 - val_mae: 0.0162\n",
      "Epoch 456/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2987e-07 - mae: 3.7747e-04\n",
      "Epoch 456: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 140us/sample - loss: 3.6026e-07 - mae: 5.0943e-04 - val_loss: 3.7178e-04 - val_mae: 0.0164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1439e-07 - mae: 5.9765e-04\n",
      "Epoch 457: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 1.3314e-07 - mae: 2.9259e-04 - val_loss: 3.6807e-04 - val_mae: 0.0163\n",
      "Epoch 458/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2241e-07 - mae: 2.8078e-04\n",
      "Epoch 458: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 133us/sample - loss: 8.5438e-08 - mae: 2.4219e-04 - val_loss: 3.6722e-04 - val_mae: 0.0163\n",
      "Epoch 459/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7858e-08 - mae: 1.4266e-04\n",
      "Epoch 459: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 1.6450e-07 - mae: 3.3870e-04 - val_loss: 3.6720e-04 - val_mae: 0.0163\n",
      "Epoch 460/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.0063e-09 - mae: 7.6388e-05\n",
      "Epoch 460: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 1.4341e-07 - mae: 3.1945e-04 - val_loss: 3.6790e-04 - val_mae: 0.0163\n",
      "Epoch 461/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4706e-08 - mae: 1.8772e-04\n",
      "Epoch 461: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 130us/sample - loss: 1.4302e-07 - mae: 3.0322e-04 - val_loss: 3.7008e-04 - val_mae: 0.0163\n",
      "Epoch 462/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6495e-08 - mae: 1.8287e-04\n",
      "Epoch 462: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 138us/sample - loss: 1.5610e-07 - mae: 3.1868e-04 - val_loss: 3.7328e-04 - val_mae: 0.0164\n",
      "Epoch 463/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5628e-07 - mae: 5.6618e-04\n",
      "Epoch 463: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 135us/sample - loss: 2.3257e-07 - mae: 4.2028e-04 - val_loss: 3.7301e-04 - val_mae: 0.0164\n",
      "Epoch 464/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2798e-07 - mae: 7.0056e-04\n",
      "Epoch 464: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 1.3570e-07 - mae: 2.9848e-04 - val_loss: 3.7104e-04 - val_mae: 0.0164\n",
      "Epoch 465/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7065e-07 - mae: 3.9114e-04\n",
      "Epoch 465: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 121us/sample - loss: 1.8987e-07 - mae: 3.6592e-04 - val_loss: 3.6695e-04 - val_mae: 0.0163\n",
      "Epoch 466/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0589e-08 - mae: 1.5586e-04\n",
      "Epoch 466: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 6.8263e-08 - mae: 2.1377e-04 - val_loss: 3.6570e-04 - val_mae: 0.0162\n",
      "Epoch 467/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1732e-08 - mae: 1.7947e-04\n",
      "Epoch 467: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 121us/sample - loss: 6.5263e-08 - mae: 2.0271e-04 - val_loss: 3.6459e-04 - val_mae: 0.0162\n",
      "Epoch 468/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.5653e-08 - mae: 2.5228e-04\n",
      "Epoch 468: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 119us/sample - loss: 5.1179e-08 - mae: 1.8620e-04 - val_loss: 3.6425e-04 - val_mae: 0.0162\n",
      "Epoch 469/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6153e-08 - mae: 1.6737e-04\n",
      "Epoch 469: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 117us/sample - loss: 4.1694e-08 - mae: 1.6498e-04 - val_loss: 3.6627e-04 - val_mae: 0.0163\n",
      "Epoch 470/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8388e-09 - mae: 7.4092e-05\n",
      "Epoch 470: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 120us/sample - loss: 6.1271e-08 - mae: 2.0420e-04 - val_loss: 3.6662e-04 - val_mae: 0.0163\n",
      "Epoch 471/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4408e-08 - mae: 1.0390e-04\n",
      "Epoch 471: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 4.8318e-08 - mae: 1.7152e-04 - val_loss: 3.6573e-04 - val_mae: 0.0163\n",
      "Epoch 472/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5326e-08 - mae: 1.3227e-04\n",
      "Epoch 472: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 118us/sample - loss: 4.4269e-08 - mae: 1.7242e-04 - val_loss: 3.6721e-04 - val_mae: 0.0163\n",
      "Epoch 473/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.9238e-09 - mae: 7.4698e-05\n",
      "Epoch 473: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 7.2183e-08 - mae: 2.1593e-04 - val_loss: 3.7127e-04 - val_mae: 0.0164\n",
      "Epoch 474/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8487e-07 - mae: 4.1707e-04\n",
      "Epoch 474: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 121us/sample - loss: 8.4777e-08 - mae: 2.3822e-04 - val_loss: 3.7065e-04 - val_mae: 0.0164\n",
      "Epoch 475/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7377e-08 - mae: 2.5544e-04\n",
      "Epoch 475: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 9.0771e-08 - mae: 2.5951e-04 - val_loss: 3.7256e-04 - val_mae: 0.0164\n",
      "Epoch 476/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3370e-07 - mae: 4.7739e-04\n",
      "Epoch 476: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 6.2540e-08 - mae: 2.0033e-04 - val_loss: 3.6852e-04 - val_mae: 0.0163\n",
      "Epoch 477/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8373e-08 - mae: 1.1295e-04\n",
      "Epoch 477: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 131us/sample - loss: 8.7835e-08 - mae: 2.4227e-04 - val_loss: 3.6978e-04 - val_mae: 0.0164\n",
      "Epoch 478/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2592e-08 - mae: 2.3136e-04\n",
      "Epoch 478: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 123us/sample - loss: 5.2600e-08 - mae: 1.8838e-04 - val_loss: 3.6812e-04 - val_mae: 0.0163\n",
      "Epoch 479/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1527e-09 - mae: 8.4409e-05\n",
      "Epoch 479: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.1068e-08 - mae: 2.0390e-04 - val_loss: 3.6827e-04 - val_mae: 0.0163\n",
      "Epoch 480/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8137e-08 - mae: 1.0939e-04\n",
      "Epoch 480: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 4.2459e-08 - mae: 1.6945e-04 - val_loss: 3.6599e-04 - val_mae: 0.0163\n",
      "Epoch 481/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0513e-08 - mae: 1.3476e-04\n",
      "Epoch 481: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 119us/sample - loss: 6.6182e-08 - mae: 2.1313e-04 - val_loss: 3.6521e-04 - val_mae: 0.0162\n",
      "Epoch 482/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0122e-07 - mae: 2.8258e-04\n",
      "Epoch 482: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 119us/sample - loss: 1.0341e-07 - mae: 2.6734e-04 - val_loss: 3.6482e-04 - val_mae: 0.0162\n",
      "Epoch 483/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7983e-08 - mae: 2.7611e-04\n",
      "Epoch 483: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 119us/sample - loss: 1.4921e-07 - mae: 3.3077e-04 - val_loss: 3.6772e-04 - val_mae: 0.0163\n",
      "Epoch 484/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4685e-08 - mae: 9.2879e-05\n",
      "Epoch 484: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 121us/sample - loss: 9.9531e-08 - mae: 2.5841e-04 - val_loss: 3.6569e-04 - val_mae: 0.0162\n",
      "Epoch 485/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1389e-07 - mae: 2.9818e-04\n",
      "Epoch 485: val_loss did not improve from 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 118us/sample - loss: 1.0116e-07 - mae: 2.6071e-04 - val_loss: 3.6980e-04 - val_mae: 0.0163\n",
      "Epoch 486/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2308e-08 - mae: 2.0214e-04\n",
      "Epoch 486: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 122us/sample - loss: 5.0763e-07 - mae: 6.0939e-04 - val_loss: 3.8034e-04 - val_mae: 0.0165\n",
      "Epoch 487/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2226e-06 - mae: 0.0011\n",
      "Epoch 487: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 6.0937e-07 - mae: 6.4269e-04 - val_loss: 3.6010e-04 - val_mae: 0.0161\n",
      "Epoch 488/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6541e-06 - mae: 0.0012\n",
      "Epoch 488: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 123us/sample - loss: 7.7499e-07 - mae: 7.3573e-04 - val_loss: 3.7031e-04 - val_mae: 0.0163\n",
      "Epoch 489/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9058e-07 - mae: 3.4801e-04\n",
      "Epoch 489: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 9.2341e-07 - mae: 8.2937e-04 - val_loss: 3.6091e-04 - val_mae: 0.0161\n",
      "Epoch 490/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.9863e-07 - mae: 8.7262e-04\n",
      "Epoch 490: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 129us/sample - loss: 3.5631e-07 - mae: 4.8727e-04 - val_loss: 3.6338e-04 - val_mae: 0.0162\n",
      "Epoch 491/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0940e-07 - mae: 6.7837e-04\n",
      "Epoch 491: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 124us/sample - loss: 5.8587e-07 - mae: 6.5161e-04 - val_loss: 3.6994e-04 - val_mae: 0.0164\n",
      "Epoch 492/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2431e-07 - mae: 4.7646e-04\n",
      "Epoch 492: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 128us/sample - loss: 3.4899e-07 - mae: 4.8149e-04 - val_loss: 3.6905e-04 - val_mae: 0.0164\n",
      "Epoch 493/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0057e-07 - mae: 4.6719e-04\n",
      "Epoch 493: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 119us/sample - loss: 2.7912e-07 - mae: 4.4547e-04 - val_loss: 3.6500e-04 - val_mae: 0.0163\n",
      "Epoch 494/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6297e-07 - mae: 3.2879e-04\n",
      "Epoch 494: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 117us/sample - loss: 1.8775e-07 - mae: 3.5927e-04 - val_loss: 3.6811e-04 - val_mae: 0.0163\n",
      "Epoch 495/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5070e-07 - mae: 3.4216e-04\n",
      "Epoch 495: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 121us/sample - loss: 1.6267e-07 - mae: 3.2588e-04 - val_loss: 3.6425e-04 - val_mae: 0.0162\n",
      "Epoch 496/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8183e-07 - mae: 4.0101e-04\n",
      "Epoch 496: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 128us/sample - loss: 5.7771e-08 - mae: 1.9245e-04 - val_loss: 3.6563e-04 - val_mae: 0.0162\n",
      "Epoch 497/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7351e-08 - mae: 1.3612e-04\n",
      "Epoch 497: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 118us/sample - loss: 9.1468e-08 - mae: 2.5541e-04 - val_loss: 3.7037e-04 - val_mae: 0.0163\n",
      "Epoch 498/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2157e-07 - mae: 4.2376e-04\n",
      "Epoch 498: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 126us/sample - loss: 8.8090e-08 - mae: 2.4413e-04 - val_loss: 3.6683e-04 - val_mae: 0.0163\n",
      "Epoch 499/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4787e-08 - mae: 9.8328e-05\n",
      "Epoch 499: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 124us/sample - loss: 4.4038e-08 - mae: 1.7086e-04 - val_loss: 3.6602e-04 - val_mae: 0.0163\n",
      "Epoch 500/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2156e-08 - mae: 9.3682e-05\n",
      "Epoch 500: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 123us/sample - loss: 6.2466e-08 - mae: 2.1398e-04 - val_loss: 3.6609e-04 - val_mae: 0.0163\n"
     ]
    }
   ],
   "source": [
    "Layers = [{'size': nx+1, 'activation': None    , 'use_bias': None},\n",
    "          {'size': 10 , 'activation': 'relu'  , 'use_bias': True},\n",
    "          {'size': 1  , 'activation': 'linear', 'use_bias': False}]\n",
    "Losses = [{'kind': 'mse', 'weight': 1.0}]\n",
    "\n",
    "K = TrainFullyConnected(M_samples, H_samples, \n",
    "                    Layers, Losses,\n",
    "                    'adam', ['mae'], \n",
    "                    10, 500, 0.2, \n",
    "                    'model', os.path.abspath(''))\n",
    "\n",
    "best_model = K.quickTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ccd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.06423315554287472\r",
      "iteration 2 - y: 0.06420611680325305\r",
      "iteration 3 - y: 0.06417907806363143\r",
      "iteration 4 - y: 0.06415203932400979\r",
      "iteration 5 - y: 0.06412500058438814\r",
      "iteration 6 - y: 0.0640979618447665\r",
      "iteration 7 - y: 0.06407092310514485\r",
      "iteration 8 - y: 0.06404388436552322\r",
      "iteration 9 - y: 0.06401684562590157\r",
      "iteration 10 - y: 0.06398980688627993\r",
      "iteration 11 - y: 0.0639627681466583\r",
      "iteration 12 - y: 0.06393572940703664\r",
      "iteration 13 - y: 0.063908690667415\r",
      "iteration 14 - y: 0.06388165192779337\r",
      "iteration 15 - y: 0.06385461318817172\r",
      "iteration 16 - y: 0.06382757444855008\r",
      "iteration 17 - y: 0.06380053570892844\r",
      "iteration 18 - y: 0.0637734969693068\r",
      "iteration 19 - y: 0.06374645822968515\r",
      "iteration 20 - y: 0.06371941949006352\r",
      "iteration 21 - y: 0.06369238075044187\r",
      "iteration 22 - y: 0.06366534201082022\r",
      "iteration 23 - y: 0.06363830327119858\r",
      "iteration 24 - y: 0.06361126453157695\r",
      "iteration 25 - y: 0.0635842257919553\r",
      "iteration 26 - y: 0.06355718705233365\r",
      "iteration 27 - y: 0.06353014831271202\r",
      "iteration 28 - y: 0.06350310957309038\r",
      "iteration 29 - y: 0.06347607083346873\r",
      "iteration 30 - y: 0.0634490320938471\r",
      "iteration 31 - y: 0.06342199335422545\r",
      "iteration 32 - y: 0.06339495461460382\r",
      "iteration 33 - y: 0.06336791587498217\r",
      "iteration 34 - y: 0.06334087713536052\r",
      "iteration 35 - y: 0.06331383839573888\r",
      "iteration 36 - y: 0.06328679965611723\r",
      "iteration 37 - y: 0.0632597609164956\r",
      "iteration 38 - y: 0.06323272217687397\r",
      "iteration 39 - y: 0.06320568343725232\r",
      "iteration 40 - y: 0.06317864469763068\r",
      "iteration 41 - y: 0.06315160595800903\r",
      "iteration 42 - y: 0.06312456721838738\r",
      "iteration 43 - y: 0.06309752847876575\r",
      "iteration 44 - y: 0.0630704897391441\r",
      "iteration 45 - y: 0.06304345099952247\r",
      "iteration 46 - y: 0.06301641225990083\r",
      "iteration 47 - y: 0.06298937352027918\r",
      "iteration 48 - y: 0.06296233478065755\r",
      "iteration 49 - y: 0.06293529604103588\r",
      "iteration 50 - y: 0.06290825730141425\r",
      "iteration 51 - y: 0.06288121856179262\r",
      "iteration 52 - y: 0.06285417982217098\r",
      "iteration 53 - y: 0.06282714108254933\r",
      "iteration 54 - y: 0.06280010234292768\r",
      "iteration 55 - y: 0.06277306360330605\r",
      "iteration 56 - y: 0.06274602486368441\r",
      "iteration 57 - y: 0.06271898612406276\r",
      "iteration 58 - y: 0.06269194738444112\r",
      "iteration 59 - y: 0.06266490864481948\r",
      "iteration 60 - y: 0.06263786990519785\r",
      "iteration 61 - y: 0.0626108311655762\r",
      "iteration 62 - y: 0.06258379242595455\r",
      "iteration 63 - y: 0.06255675368633291\r",
      "iteration 64 - y: 0.06252971494671127\r",
      "iteration 65 - y: 0.06250267620708963\r",
      "iteration 66 - y: 0.06247563746746799\r",
      "iteration 67 - y: 0.06244859872784635\r",
      "iteration 68 - y: 0.062421559988224705\r",
      "iteration 69 - y: 0.062394521248603056\r",
      "iteration 70 - y: 0.06236748250898143\r",
      "iteration 71 - y: 0.06234044376935979\r",
      "iteration 72 - y: 0.062313405029738145\r",
      "iteration 73 - y: 0.062286366290116496\r",
      "iteration 74 - y: 0.062259327550494854\r",
      "iteration 75 - y: 0.06223228881087321\r",
      "iteration 76 - y: 0.06220525007125158\r",
      "iteration 77 - y: 0.06217821133162993\r",
      "iteration 78 - y: 0.062151172592008294\r",
      "iteration 79 - y: 0.062124133852386645\r",
      "iteration 80 - y: 0.06209709511276501\r",
      "iteration 81 - y: 0.062070056373143376\r",
      "iteration 82 - y: 0.06204301763352173\r",
      "iteration 83 - y: 0.06201597889390008\r",
      "iteration 84 - y: 0.06198894015427844\r",
      "iteration 85 - y: 0.06196190141465681\r",
      "iteration 86 - y: 0.06193486267503516\r",
      "iteration 87 - y: 0.06190782393541352\r",
      "iteration 88 - y: 0.061880785195791876\r",
      "iteration 89 - y: 0.061853746456170235\r",
      "iteration 90 - y: 0.06182670771654859\r",
      "iteration 91 - y: 0.06179966897692696\r",
      "iteration 92 - y: 0.06177263023730531\r",
      "iteration 93 - y: 0.061745591497683674\r",
      "iteration 94 - y: 0.061718552758062026\r",
      "iteration 95 - y: 0.061691514018440384\r",
      "iteration 96 - y: 0.061664475278818756\r",
      "iteration 97 - y: 0.0616374365391971\r",
      "iteration 98 - y: 0.06161039779957546\r",
      "iteration 99 - y: 0.06158335905995381\r",
      "iteration 100 - y: 0.061556320320332175\r",
      "iteration 101 - y: 0.06152928158071054\r",
      "iteration 102 - y: 0.06150224284108889\r",
      "iteration 103 - y: 0.06147520410146725\r",
      "iteration 104 - y: 0.06144816536184561\r",
      "iteration 105 - y: 0.061421126622223966\r",
      "iteration 106 - y: 0.061394087882602325\r",
      "iteration 107 - y: 0.061367049142980676\r",
      "iteration 108 - y: 0.06134001040335905\r",
      "iteration 109 - y: 0.061312971663737406\r",
      "iteration 110 - y: 0.06128593292411576\r",
      "iteration 111 - y: 0.061258894184494116\r",
      "iteration 112 - y: 0.061231855444872474\r",
      "iteration 113 - y: 0.06120481670525084\r",
      "iteration 114 - y: 0.06117777796562919\r",
      "iteration 115 - y: 0.06115073922600754\r",
      "iteration 116 - y: 0.06112370048638591\r",
      "iteration 117 - y: 0.06109666174676427\r",
      "iteration 118 - y: 0.06106962300714263\r",
      "iteration 119 - y: 0.06104258426752098\r",
      "iteration 120 - y: 0.06101554552789934\r",
      "iteration 121 - y: 0.060988506788277705\r",
      "iteration 122 - y: 0.060961468048656056\r",
      "iteration 123 - y: 0.06093442930903442\r",
      "iteration 124 - y: 0.06090739056941278\r",
      "iteration 125 - y: 0.06088035182979114\r",
      "iteration 126 - y: 0.060853313090169496\r",
      "iteration 127 - y: 0.06082627435054785\r",
      "iteration 128 - y: 0.06079923561092622\r",
      "iteration 129 - y: 0.06077219687130457\r",
      "iteration 130 - y: 0.06074515813168292\r",
      "iteration 131 - y: 0.06071811939206129\r",
      "iteration 132 - y: 0.060691080652439645\r",
      "iteration 133 - y: 0.06060948511515861\r",
      "iteration 134 - y: 0.06052661943142418\r",
      "iteration 135 - y: 0.06044375374768976\r",
      "iteration 136 - y: 0.06036088806395535\r",
      "iteration 137 - y: 0.060278022380220925\r",
      "iteration 138 - y: 0.06019515669648651\r",
      "iteration 139 - y: 0.06011229101275208\r",
      "iteration 140 - y: 0.060029425329017665\r",
      "iteration 141 - y: 0.059946559645283254\r",
      "iteration 142 - y: 0.05986369396154882\r",
      "iteration 143 - y: 0.05978082827781441\r",
      "iteration 144 - y: 0.05969796259407999\r",
      "iteration 145 - y: 0.05961509691034557\r",
      "iteration 146 - y: 0.05953223122661115\r",
      "iteration 147 - y: 0.05944936554287673\r",
      "iteration 148 - y: 0.05936649985914231\r",
      "iteration 149 - y: 0.05928363417540789\r",
      "iteration 150 - y: 0.059200768491673474\r",
      "iteration 151 - y: 0.05911790280793906\r",
      "iteration 152 - y: 0.05903503712420463\r",
      "iteration 153 - y: 0.058952171440470214\r",
      "iteration 154 - y: 0.058869305756735796\r",
      "iteration 155 - y: 0.05878644007300138\r",
      "iteration 156 - y: 0.05870357438926695\r",
      "iteration 157 - y: 0.058620708705532536\r",
      "iteration 158 - y: 0.05853784302179812\r",
      "iteration 159 - y: 0.05845497733806369\r",
      "iteration 160 - y: 0.05837211165432928\r",
      "iteration 161 - y: 0.05828924597059486\r",
      "iteration 162 - y: 0.05820638028686044\r",
      "iteration 163 - y: 0.05812351460312602\r",
      "iteration 164 - y: 0.0580406489193916\r",
      "iteration 165 - y: 0.05795778323565719\r",
      "iteration 166 - y: 0.05787491755192276\r",
      "iteration 167 - y: 0.05779205186818834\r",
      "iteration 168 - y: 0.05770918618445393\r",
      "iteration 169 - y: 0.0576263205007195\r",
      "iteration 170 - y: 0.05754345481698509\r",
      "iteration 171 - y: 0.05746058913325067\r",
      "iteration 172 - y: 0.05737772344951625\r",
      "iteration 173 - y: 0.05729485776578183\r",
      "iteration 174 - y: 0.05721199208204741\r",
      "iteration 175 - y: 0.057129126398312996\r",
      "iteration 176 - y: 0.05704626071457857\r",
      "iteration 177 - y: 0.05696339503084415\r",
      "iteration 178 - y: 0.056880529347109736\r",
      "iteration 179 - y: 0.05679766366337531\r",
      "iteration 180 - y: 0.056714797979640894\r",
      "iteration 181 - y: 0.056631932295906476\r",
      "iteration 182 - y: 0.05654906661217205\r",
      "iteration 183 - y: 0.05646620092843764\r",
      "iteration 184 - y: 0.056383335244703216\r",
      "iteration 185 - y: 0.0563004695609688\r",
      "iteration 186 - y: 0.05621760387723438\r",
      "iteration 187 - y: 0.056134738193499956\r",
      "iteration 188 - y: 0.05605187250976554\r",
      "iteration 189 - y: 0.05596900682603111\r",
      "iteration 190 - y: 0.055886141142296696\r",
      "iteration 191 - y: 0.055803275458562285\r",
      "iteration 192 - y: 0.05572040977482786\r",
      "iteration 193 - y: 0.05563754409109345\r",
      "iteration 194 - y: 0.05555467840735902\r",
      "iteration 195 - y: 0.0554718127236246\r",
      "iteration 196 - y: 0.05538894703989018\r",
      "iteration 197 - y: 0.055306081356155765\r",
      "iteration 198 - y: 0.05522321567242135\r",
      "iteration 199 - y: 0.055140349988686936\r",
      "iteration 200 - y: 0.055057484304952505\r",
      "iteration 201 - y: 0.054974618621218094\r",
      "iteration 202 - y: 0.05489175293748367\r",
      "iteration 203 - y: 0.054808887253749244\r",
      "iteration 204 - y: 0.05472602157001483\r",
      "iteration 205 - y: 0.05464315588628041\r",
      "iteration 206 - y: 0.05456029020254599\r",
      "iteration 207 - y: 0.054477424518811574\r",
      "iteration 208 - y: 0.05439455883507715\r",
      "iteration 209 - y: 0.05431169315134274\r",
      "iteration 210 - y: 0.05422882746760831\r",
      "iteration 211 - y: 0.054145961783873896\r",
      "iteration 212 - y: 0.05406309610013947\r",
      "iteration 213 - y: 0.05398023041640505\r",
      "iteration 214 - y: 0.053897364732670636\r",
      "iteration 215 - y: 0.05381449904893622\r",
      "iteration 216 - y: 0.0537316333652018\r",
      "iteration 217 - y: 0.05364876768146738\r",
      "iteration 218 - y: 0.05356590199773296\r",
      "iteration 219 - y: 0.05348303631399854\r",
      "iteration 220 - y: 0.05340017063026412\r",
      "iteration 221 - y: 0.053317304946529705\r",
      "iteration 222 - y: 0.05323443926279528\r",
      "iteration 223 - y: 0.05315157357906086\r",
      "iteration 224 - y: 0.053068707895326445\r",
      "iteration 225 - y: 0.05298584221159203\r",
      "iteration 226 - y: 0.05290297652785761\r",
      "iteration 227 - y: 0.052820110844123185\r",
      "iteration 228 - y: 0.05273724516038877\r",
      "iteration 229 - y: 0.05265437947665435\r",
      "iteration 230 - y: 0.052571513792919924\r",
      "iteration 231 - y: 0.052488648109185514\r",
      "iteration 232 - y: 0.05240578242545108\r",
      "iteration 233 - y: 0.05232291674171667\r",
      "iteration 234 - y: 0.052240051057982254\r",
      "iteration 235 - y: 0.05215718537424783\r",
      "iteration 236 - y: 0.05207431969051342\r",
      "iteration 237 - y: 0.05199145400677899\r",
      "iteration 238 - y: 0.05190858832304457\r",
      "iteration 239 - y: 0.05182572263931016\r",
      "iteration 240 - y: 0.05174285695557573\r",
      "iteration 241 - y: 0.051659991271841316\r",
      "iteration 242 - y: 0.05157712558810689\r",
      "iteration 243 - y: 0.05149425990437248\r",
      "iteration 244 - y: 0.05141139422063806\r",
      "iteration 245 - y: 0.05132852853690363\r",
      "iteration 246 - y: 0.05124566285316921\r",
      "iteration 247 - y: 0.051162797169434795\r",
      "iteration 248 - y: 0.05107993148570038\r",
      "iteration 249 - y: 0.05099706580196596\r",
      "iteration 250 - y: 0.050914200118231535\r",
      "iteration 251 - y: 0.050831334434497125\r",
      "iteration 252 - y: 0.05074846875076271\r",
      "iteration 253 - y: 0.05066560306702828\r",
      "iteration 254 - y: 0.05058273738329386\r",
      "iteration 255 - y: 0.05049987169955944\r",
      "iteration 256 - y: 0.05041700601582502\r",
      "iteration 257 - y: 0.05033414033209061\r",
      "iteration 258 - y: 0.05025127464835619\r",
      "iteration 259 - y: 0.05016840896462177\r",
      "iteration 260 - y: 0.050085543280887344\r",
      "iteration 261 - y: 0.05000267759715293\r",
      "iteration 262 - y: 0.049919811913418516\r",
      "iteration 263 - y: 0.04983694622968409\r",
      "iteration 264 - y: 0.04975408054594967\r",
      "iteration 265 - y: 0.04967121486221525\r",
      "iteration 266 - y: 0.04958834917848083\r",
      "iteration 267 - y: 0.04950548349474641\r",
      "iteration 268 - y: 0.049422617811011996\r",
      "iteration 269 - y: 0.04933975212727757\r",
      "iteration 270 - y: 0.04925688644354315\r",
      "iteration 271 - y: 0.04917402075980873\r",
      "iteration 272 - y: 0.04909115507607431\r",
      "iteration 273 - y: 0.0490082893923399\r",
      "iteration 274 - y: 0.048925423708605476\r",
      "iteration 275 - y: 0.04884255802487106\r",
      "iteration 276 - y: 0.04875969234113663\r",
      "iteration 277 - y: 0.048676826657402215\r",
      "iteration 278 - y: 0.048593960973667805\r",
      "iteration 279 - y: 0.04851109528993338\r",
      "iteration 280 - y: 0.048428229606198955\r",
      "iteration 281 - y: 0.04834536392246454\r",
      "iteration 282 - y: 0.04826249823873012\r",
      "iteration 283 - y: 0.0481796325549957\r",
      "iteration 284 - y: 0.048096766871261284\r",
      "iteration 285 - y: 0.04801390118752686\r",
      "iteration 286 - y: 0.04793103550379244\r",
      "iteration 287 - y: 0.047848169820058024\r",
      "iteration 288 - y: 0.0477653041363236\r",
      "iteration 289 - y: 0.04768243845258919\r",
      "iteration 290 - y: 0.047599572768854764\r",
      "iteration 291 - y: 0.04751670708512035\r",
      "iteration 292 - y: 0.04743384140138593\r",
      "iteration 293 - y: 0.04735097571765151\r",
      "iteration 294 - y: 0.04726811003391709\r",
      "iteration 295 - y: 0.04718524435018267\r",
      "iteration 296 - y: 0.047102378666448244\r",
      "iteration 297 - y: 0.04701951298271383\r",
      "iteration 298 - y: 0.04693664729897941\r",
      "iteration 299 - y: 0.046853781615245\r",
      "iteration 300 - y: 0.04677091593151057\r",
      "iteration 301 - y: 0.046688050247776156\r",
      "iteration 302 - y: 0.04660518456404174\r",
      "iteration 303 - y: 0.04652231888030731\r",
      "iteration 304 - y: 0.0464394531965729\r",
      "iteration 305 - y: 0.04635658751283848\r",
      "iteration 306 - y: 0.04627372182910405\r",
      "iteration 307 - y: 0.04619085614536964\r",
      "iteration 308 - y: 0.04610799046163522\r",
      "iteration 309 - y: 0.0460251247779008\r",
      "iteration 310 - y: 0.04594225909416638\r",
      "iteration 311 - y: 0.04585939341043196\r",
      "iteration 312 - y: 0.04577652772669755\r",
      "iteration 313 - y: 0.04569366204296312\r",
      "iteration 314 - y: 0.0456107963592287\r",
      "iteration 315 - y: 0.04552793067549429\r",
      "iteration 316 - y: 0.04544506499175986\r",
      "iteration 317 - y: 0.045362199308025444\r",
      "iteration 318 - y: 0.04527933362429103\r",
      "iteration 319 - y: 0.0451964679405566\r",
      "iteration 320 - y: 0.04511360225682219\r",
      "iteration 321 - y: 0.045030736573087767\r",
      "iteration 322 - y: 0.044947870889353356\r",
      "iteration 323 - y: 0.04486500520561893\r",
      "iteration 324 - y: 0.044782139521884506\r",
      "iteration 325 - y: 0.04469927383815009\r",
      "iteration 326 - y: 0.04461640815441567\r",
      "iteration 327 - y: 0.044533542470681246\r",
      "iteration 328 - y: 0.044450676786946836\r",
      "iteration 329 - y: 0.04436781110321241\r",
      "iteration 330 - y: 0.044284945419477986\r",
      "iteration 331 - y: 0.044202079735743575\r",
      "iteration 332 - y: 0.04411921405200915\r",
      "iteration 333 - y: 0.04403634836827474\r",
      "iteration 334 - y: 0.043953482684540315\r",
      "iteration 335 - y: 0.0438706170008059\r",
      "iteration 336 - y: 0.04378775131707148\r",
      "iteration 337 - y: 0.04370488563333706\r",
      "iteration 338 - y: 0.043622019949602645\r",
      "iteration 339 - y: 0.04353915426586823\r",
      "iteration 340 - y: 0.04345628858213381\r",
      "iteration 341 - y: 0.04337342289839939\r",
      "iteration 342 - y: 0.043290557214664974\r",
      "iteration 343 - y: 0.043207691530930556\r",
      "iteration 344 - y: 0.04312482584719614\r",
      "iteration 345 - y: 0.043041960163461714\r",
      "iteration 346 - y: 0.0429590944797273\r",
      "iteration 347 - y: 0.04287622879599288\r",
      "iteration 348 - y: 0.04279336311225846\r",
      "iteration 349 - y: 0.04271049742852404\r",
      "iteration 350 - y: 0.042627631744789625\r",
      "iteration 351 - y: 0.04254476606105521\r",
      "iteration 352 - y: 0.04246190037732079\r",
      "iteration 353 - y: 0.042379034693586365\r",
      "iteration 354 - y: 0.042296169009851954\r",
      "iteration 355 - y: 0.04221330332611753\r",
      "iteration 356 - y: 0.04213043764238312\r",
      "iteration 357 - y: 0.042047571958648694\r",
      "iteration 358 - y: 0.041964706274914276\r",
      "iteration 359 - y: 0.04188184059117986\r",
      "iteration 360 - y: 0.04179897490744544\r",
      "iteration 361 - y: 0.04171610922371102\r",
      "iteration 362 - y: 0.041633243539976605\r",
      "iteration 363 - y: 0.04155037785624219\r",
      "iteration 364 - y: 0.04146751217250777\r",
      "iteration 365 - y: 0.04138464648877335\r",
      "iteration 366 - y: 0.041301780805038935\r",
      "iteration 367 - y: 0.04121891512130452\r",
      "iteration 368 - y: 0.04113604943757009\r",
      "iteration 369 - y: 0.041053183753835674\r",
      "iteration 370 - y: 0.04097031807010126\r",
      "iteration 371 - y: 0.040887452386366846\r",
      "iteration 372 - y: 0.04080458670263243\r",
      "iteration 373 - y: 0.040721721018898004\r",
      "iteration 374 - y: 0.04063885533516358\r",
      "iteration 375 - y: 0.04055598965142917\r",
      "iteration 376 - y: 0.04047312396769476\r",
      "iteration 377 - y: 0.04039025828396033\r",
      "iteration 378 - y: 0.04030739260022591\r",
      "iteration 379 - y: 0.04022452691649149\r",
      "iteration 380 - y: 0.04014166123275708\r",
      "iteration 381 - y: 0.04005879554902266\r",
      "iteration 382 - y: 0.039975929865288244\r",
      "iteration 383 - y: 0.03989306418155382\r",
      "iteration 384 - y: 0.0398101984978194\r",
      "iteration 385 - y: 0.039727332814084984\r",
      "iteration 386 - y: 0.039644467130350566\r",
      "iteration 387 - y: 0.03956160144661615\r",
      "iteration 388 - y: 0.03947873576288173\r",
      "iteration 389 - y: 0.03939587007914731\r",
      "iteration 390 - y: 0.039313004395412895\r",
      "iteration 391 - y: 0.03923013871167848\r",
      "iteration 392 - y: 0.03914727302794406\r",
      "iteration 393 - y: 0.03906440734420964\r",
      "iteration 394 - y: 0.038981541660475225\r",
      "iteration 395 - y: 0.03889867597674081\r",
      "iteration 396 - y: 0.03881581029300639\r",
      "iteration 397 - y: 0.03873294460927197\r",
      "iteration 398 - y: 0.038650078925537554\r",
      "iteration 399 - y: 0.038567213241803136\r",
      "iteration 400 - y: 0.03848434755806872\r",
      "iteration 401 - y: 0.0384014818743343\r",
      "iteration 402 - y: 0.03831861619059988\r",
      "iteration 403 - y: 0.038235750506865465\r",
      "iteration 404 - y: 0.03815288482313105\r",
      "iteration 405 - y: 0.03807001913939663\r",
      "iteration 406 - y: 0.03798715345566221\r",
      "iteration 407 - y: 0.037904287771927794\r",
      "iteration 408 - y: 0.03782142208819338\r",
      "iteration 409 - y: 0.03773855640445896\r",
      "iteration 410 - y: 0.03765569072072454\r",
      "iteration 411 - y: 0.03757282503699012\r",
      "iteration 412 - y: 0.037489959353255706\r",
      "iteration 413 - y: 0.03740709366952129\r",
      "iteration 414 - y: 0.03732422798578687\r",
      "iteration 415 - y: 0.03724136230205245\r",
      "iteration 416 - y: 0.037158496618318035\r",
      "iteration 417 - y: 0.03707563093458362\r",
      "iteration 418 - y: 0.0369927652508492\r",
      "iteration 419 - y: 0.03690989956711478\r",
      "iteration 420 - y: 0.036827033883380364\r",
      "iteration 421 - y: 0.036744168199645946\r",
      "iteration 422 - y: 0.03666130251591153\r",
      "iteration 423 - y: 0.03657843683217711\r",
      "iteration 424 - y: 0.03649557114844269\r",
      "iteration 425 - y: 0.036412705464708275\r",
      "iteration 426 - y: 0.03632983978097386\r",
      "iteration 427 - y: 0.03624697409723944\r",
      "iteration 428 - y: 0.03616410841350502\r",
      "iteration 429 - y: 0.036081242729770605\r",
      "iteration 430 - y: 0.03599837704603619\r",
      "iteration 431 - y: 0.03591551136230177\r",
      "iteration 432 - y: 0.03583264567856735\r",
      "iteration 433 - y: 0.035749779994832934\r",
      "iteration 434 - y: 0.035666914311098516\r",
      "iteration 435 - y: 0.0355840486273641\r",
      "iteration 436 - y: 0.03550118294362968\r",
      "iteration 437 - y: 0.03541831725989526\r",
      "iteration 438 - y: 0.035335451576160845\r",
      "iteration 439 - y: 0.03525258589242643\r",
      "iteration 440 - y: 0.03516972020869201\r",
      "iteration 441 - y: 0.03508685452495759\r",
      "iteration 442 - y: 0.035003988841223174\r",
      "iteration 443 - y: 0.034921123157488757\r",
      "iteration 444 - y: 0.03483825747375434\r",
      "iteration 445 - y: 0.03475539179001992\r",
      "iteration 446 - y: 0.0346725261062855\r",
      "iteration 447 - y: 0.034589660422551086\r",
      "iteration 448 - y: 0.03450679473881667\r",
      "iteration 449 - y: 0.03442392905508225\r",
      "iteration 450 - y: 0.03434106337134783\r",
      "iteration 451 - y: 0.034258197687613415\r",
      "iteration 452 - y: 0.034175332003879\r",
      "iteration 453 - y: 0.03409246632014458\r",
      "iteration 454 - y: 0.03400960063641016\r",
      "iteration 455 - y: 0.033926734952675744\r",
      "iteration 456 - y: 0.033843869268941326\r",
      "iteration 457 - y: 0.03376100358520691\r",
      "iteration 458 - y: 0.03367813790147249\r",
      "iteration 459 - y: 0.03359527221773807\r",
      "iteration 460 - y: 0.033512406534003655\r",
      "iteration 461 - y: 0.03342954085026924\r",
      "iteration 462 - y: 0.03334667516653482\r",
      "iteration 463 - y: 0.0332638094828004\r",
      "iteration 464 - y: 0.033180943799065984\r",
      "iteration 465 - y: 0.03309807811533157\r",
      "iteration 466 - y: 0.03301521243159715\r",
      "iteration 467 - y: 0.03293234674786273\r",
      "iteration 468 - y: 0.032849481064128314\r",
      "iteration 469 - y: 0.032766615380393896\r",
      "iteration 470 - y: 0.03268374969665948\r",
      "iteration 471 - y: 0.03260088401292506\r",
      "iteration 472 - y: 0.03251801832919064\r",
      "iteration 473 - y: 0.032435152645456225\r",
      "iteration 474 - y: 0.03235228696172181\r",
      "iteration 475 - y: 0.03226942127798739\r",
      "iteration 476 - y: 0.03218655559425298\r",
      "iteration 477 - y: 0.032103689910518554\r",
      "iteration 478 - y: 0.032020824226784136\r",
      "iteration 479 - y: 0.03193795854304972\r",
      "iteration 480 - y: 0.0318550928593153\r",
      "iteration 481 - y: 0.03177222717558088\r",
      "iteration 482 - y: 0.031689361491846466\r",
      "iteration 483 - y: 0.03160649580811205\r",
      "iteration 484 - y: 0.03152363012437763\r",
      "iteration 485 - y: 0.03144076444064321\r",
      "iteration 486 - y: 0.031357898756908795\r",
      "iteration 487 - y: 0.03127503307317438\r",
      "iteration 488 - y: 0.03119216738943996\r",
      "iteration 489 - y: 0.03110930170570554\r",
      "iteration 490 - y: 0.031026436021971124\r",
      "iteration 491 - y: 0.03094357033823671\r",
      "iteration 492 - y: 0.03086070465450229\r",
      "iteration 493 - y: 0.03077783897076787\r",
      "iteration 494 - y: 0.030694973287033453\r",
      "iteration 495 - y: 0.03061210760329904\r",
      "iteration 496 - y: 0.030529241919564618\r",
      "iteration 497 - y: 0.0304463762358302\r",
      "iteration 498 - y: 0.030363510552095782\r",
      "iteration 499 - y: 0.030280644868361364\r",
      "iteration 500 - y: 0.030197779184626947\r",
      "iteration 501 - y: 0.030114913500892532\r",
      "iteration 502 - y: 0.030032047817158115\r",
      "iteration 503 - y: 0.029949182133423694\r",
      "iteration 504 - y: 0.029866316449689276\r",
      "iteration 505 - y: 0.02978345076595486\r",
      "iteration 506 - y: 0.029700585082220444\r",
      "iteration 507 - y: 0.029617719398486023\r",
      "iteration 508 - y: 0.029534853714751605\r",
      "iteration 509 - y: 0.029451988031017187\r",
      "iteration 510 - y: 0.029369122347282773\r",
      "iteration 511 - y: 0.029286256663548352\r",
      "iteration 512 - y: 0.029203390979813934\r",
      "iteration 513 - y: 0.029120525296079516\r",
      "iteration 514 - y: 0.0290376596123451\r",
      "iteration 515 - y: 0.02895479392861068\r",
      "iteration 516 - y: 0.028871928244876263\r",
      "iteration 517 - y: 0.02878906256114185\r",
      "iteration 518 - y: 0.02870619687740743\r",
      "iteration 519 - y: 0.028623331193673014\r",
      "iteration 520 - y: 0.028540465509938596\r",
      "iteration 521 - y: 0.028457599826204178\r",
      "iteration 522 - y: 0.02837473414246976\r",
      "iteration 523 - y: 0.02829186845873534\r",
      "iteration 524 - y: 0.02820900277500092\r",
      "iteration 525 - y: 0.028126137091266507\r",
      "iteration 526 - y: 0.02804327140753209\r",
      "iteration 527 - y: 0.02796040572379767\r",
      "iteration 528 - y: 0.02787754004006325\r",
      "iteration 529 - y: 0.027794674356328833\r",
      "iteration 530 - y: 0.02771180867259442\r",
      "iteration 531 - y: 0.027628942988859997\r",
      "iteration 532 - y: 0.02754607730512558\r",
      "iteration 533 - y: 0.027463211621391165\r",
      "iteration 534 - y: 0.027380345937656748\r",
      "iteration 535 - y: 0.02729748025392233\r",
      "iteration 536 - y: 0.027214614570187912\r",
      "iteration 537 - y: 0.027131748886453495\r",
      "iteration 538 - y: 0.027048883202719077\r",
      "iteration 539 - y: 0.026966017518984656\r",
      "iteration 540 - y: 0.02688315183525024\r",
      "iteration 541 - y: 0.026800286151515824\r",
      "iteration 542 - y: 0.026717420467781406\r",
      "iteration 543 - y: 0.026634554784046985\r",
      "iteration 544 - y: 0.026551689100312567\r",
      "iteration 545 - y: 0.026468823416578153\r",
      "iteration 546 - y: 0.026385957732843735\r",
      "iteration 547 - y: 0.026303092049109314\r",
      "iteration 548 - y: 0.026220226365374896\r",
      "iteration 549 - y: 0.02613736068164048\r",
      "iteration 550 - y: 0.026054494997906068\r",
      "iteration 551 - y: 0.025971629314171647\r",
      "iteration 552 - y: 0.02588876363043723\r",
      "iteration 553 - y: 0.02580589794670281\r",
      "iteration 554 - y: 0.02572303226296839\r",
      "iteration 555 - y: 0.025640166579233976\r",
      "iteration 556 - y: 0.025557300895499558\r",
      "iteration 557 - y: 0.02547443521176514\r",
      "iteration 558 - y: 0.02539156952803072\r",
      "iteration 559 - y: 0.0253087038442963\r",
      "iteration 560 - y: 0.025225838160561887\r",
      "iteration 561 - y: 0.02514297247682747\r",
      "iteration 562 - y: 0.025060106793093048\r",
      "iteration 563 - y: 0.02497724110935863\r",
      "iteration 564 - y: 0.024894375425624213\r",
      "iteration 565 - y: 0.024811509741889802\r",
      "iteration 566 - y: 0.024728644058155377\r",
      "iteration 567 - y: 0.024645778374420963\r",
      "iteration 568 - y: 0.024562912690686545\r",
      "iteration 569 - y: 0.024480047006952128\r",
      "iteration 570 - y: 0.02439718132321771\r",
      "iteration 571 - y: 0.02431431563948329\r",
      "iteration 572 - y: 0.02423144995574887\r",
      "iteration 573 - y: 0.024148584272014453\r",
      "iteration 574 - y: 0.024065718588280036\r",
      "iteration 575 - y: 0.023982852904545618\r",
      "iteration 576 - y: 0.0238999872208112\r",
      "iteration 577 - y: 0.023817121537076782\r",
      "iteration 578 - y: 0.023734255853342365\r",
      "iteration 579 - y: 0.023651390169607947\r",
      "iteration 580 - y: 0.02356852448587353\r",
      "iteration 581 - y: 0.02348565880213911\r",
      "iteration 582 - y: 0.02340279311840469\r",
      "iteration 583 - y: 0.023319927434670273\r",
      "iteration 584 - y: 0.023237061750935855\r",
      "iteration 585 - y: 0.02315419606720144\r",
      "iteration 586 - y: 0.02307133038346702\r",
      "iteration 587 - y: 0.022988464699732602\r",
      "iteration 588 - y: 0.022905599015998184\r",
      "iteration 589 - y: 0.022822733332263766\r",
      "iteration 590 - y: 0.02273986764852935\r",
      "iteration 591 - y: 0.02265700196479493\r",
      "iteration 592 - y: 0.022574136281060513\r",
      "iteration 593 - y: 0.022491270597326096\r",
      "iteration 594 - y: 0.022408404913591674\r",
      "iteration 595 - y: 0.02232553922985726\r",
      "iteration 596 - y: 0.022242673546122842\r",
      "iteration 597 - y: 0.022159807862388425\r",
      "iteration 598 - y: 0.022076942178654003\r",
      "iteration 599 - y: 0.021994076494919586\r",
      "iteration 600 - y: 0.02191121081118517\r",
      "iteration 601 - y: 0.021828345127450754\r",
      "iteration 602 - y: 0.021745479443716333\r",
      "iteration 603 - y: 0.021662613759981915\r",
      "iteration 604 - y: 0.021579748076247497\r",
      "iteration 605 - y: 0.021496882392513083\r",
      "iteration 606 - y: 0.021414016708778662\r",
      "iteration 607 - y: 0.021331151025044244\r",
      "iteration 608 - y: 0.021248285341309826\r",
      "iteration 609 - y: 0.02116541965757541\r",
      "iteration 610 - y: 0.02108255397384099\r",
      "iteration 611 - y: 0.020999688290106573\r",
      "iteration 612 - y: 0.020916822606372155\r",
      "iteration 613 - y: 0.020833956922637734\r",
      "iteration 614 - y: 0.020751091238903317\r",
      "iteration 615 - y: 0.020668225555168902\r",
      "iteration 616 - y: 0.020585359871434485\r",
      "iteration 617 - y: 0.020502494187700063\r",
      "iteration 618 - y: 0.020419628503965646\r",
      "iteration 619 - y: 0.020336762820231228\r",
      "iteration 620 - y: 0.020253897136496814\r",
      "iteration 621 - y: 0.020171031452762393\r",
      "iteration 622 - y: 0.020088165769027975\r",
      "iteration 623 - y: 0.020005300085293557\r",
      "iteration 624 - y: 0.01992243440155914\r",
      "iteration 625 - y: 0.01983956871782472\r",
      "iteration 626 - y: 0.019756703034090304\r",
      "iteration 627 - y: 0.019673837350355886\r",
      "iteration 628 - y: 0.019590971666621465\r",
      "iteration 629 - y: 0.019508105982887047\r",
      "iteration 630 - y: 0.019425240299152637\r",
      "iteration 631 - y: 0.01934237461541822\r",
      "iteration 632 - y: 0.019259508931683794\r",
      "iteration 633 - y: 0.01917664324794938\r",
      "iteration 634 - y: 0.019093777564214962\r",
      "iteration 635 - y: 0.019010911880480545\r",
      "iteration 636 - y: 0.018928046196746127\r",
      "iteration 637 - y: 0.01884518051301171\r",
      "iteration 638 - y: 0.01876231482927729\r",
      "iteration 639 - y: 0.01867944914554287\r",
      "iteration 640 - y: 0.018596583461808456\r",
      "iteration 641 - y: 0.018513717778074038\r",
      "iteration 642 - y: 0.018430852094339617\r",
      "iteration 643 - y: 0.0183479864106052\r",
      "iteration 644 - y: 0.01826512072687078\r",
      "iteration 645 - y: 0.018182255043136364\r",
      "iteration 646 - y: 0.01811557631089025\r",
      "iteration 647 - y: 0.01806270520432525\r",
      "iteration 648 - y: 0.01800983409776025\r",
      "iteration 649 - y: 0.017956962991195247\r",
      "iteration 650 - y: 0.01790409188463025\r",
      "iteration 651 - y: 0.017851220778065252\r",
      "iteration 652 - y: 0.01779834967150025\r",
      "iteration 653 - y: 0.01774547856493525\r",
      "iteration 654 - y: 0.01769260745837025\r",
      "iteration 655 - y: 0.01763973635180525\r",
      "iteration 656 - y: 0.017586865245240252\r",
      "iteration 657 - y: 0.017533994138675252\r",
      "iteration 658 - y: 0.017481123032110253\r",
      "iteration 659 - y: 0.01742825192554525\r",
      "iteration 660 - y: 0.017375380818980254\r",
      "iteration 661 - y: 0.017322509712415255\r",
      "iteration 662 - y: 0.017269638605850252\r",
      "iteration 663 - y: 0.017216767499285253\r",
      "iteration 664 - y: 0.017163896392720254\r",
      "iteration 665 - y: 0.017111025286155258\r",
      "iteration 666 - y: 0.017058154179590255\r",
      "iteration 667 - y: 0.017005283073025255\r",
      "iteration 668 - y: 0.016952411966460256\r",
      "iteration 669 - y: 0.016899540859895253\r",
      "iteration 670 - y: 0.016846669753330257\r",
      "iteration 671 - y: 0.016793798646765258\r",
      "iteration 672 - y: 0.01674092754020026\r",
      "iteration 673 - y: 0.016688056433635256\r",
      "iteration 674 - y: 0.016635185327070257\r",
      "iteration 675 - y: 0.01658231422050526\r",
      "iteration 676 - y: 0.01652944311394026\r",
      "iteration 677 - y: 0.01647657200737526\r",
      "iteration 678 - y: 0.01642370090081026\r",
      "iteration 679 - y: 0.01637082979424526\r",
      "iteration 680 - y: 0.01631795868768026\r",
      "iteration 681 - y: 0.01626508758111526\r",
      "iteration 682 - y: 0.01621221647455026\r",
      "iteration 683 - y: 0.016159345367985262\r",
      "iteration 684 - y: 0.01610647426142026\r",
      "iteration 685 - y: 0.016053603154855264\r",
      "iteration 686 - y: 0.016000732048290264\r",
      "iteration 687 - y: 0.01594786094172526\r",
      "iteration 688 - y: 0.015894989835160262\r",
      "iteration 689 - y: 0.015842118728595263\r",
      "iteration 690 - y: 0.015789247622030263\r",
      "iteration 691 - y: 0.015736376515465264\r",
      "iteration 692 - y: 0.015683505408900265\r",
      "iteration 693 - y: 0.015630634302335265\r",
      "iteration 694 - y: 0.015577763195770266\r",
      "iteration 695 - y: 0.015524892089205265\r",
      "iteration 696 - y: 0.015472020982640267\r",
      "iteration 697 - y: 0.015419149876075266\r",
      "iteration 698 - y: 0.015366278769510269\r",
      "iteration 699 - y: 0.015313407662945267\r",
      "iteration 700 - y: 0.015260536556380266\r",
      "iteration 701 - y: 0.015207665449815269\r",
      "iteration 702 - y: 0.015154794343250268\r",
      "iteration 703 - y: 0.01511602832862475\r",
      "iteration 704 - y: 0.015080491762248063\r",
      "iteration 705 - y: 0.015044955195871375\r",
      "iteration 706 - y: 0.015009418629494689\r",
      "iteration 707 - y: 0.014973882063118001\r",
      "iteration 708 - y: 0.014938345496741315\r",
      "iteration 709 - y: 0.014902808930364627\r",
      "iteration 710 - y: 0.01486727236398794\r",
      "iteration 711 - y: 0.014831735797611253\r",
      "iteration 712 - y: 0.014796199231234566\r",
      "iteration 713 - y: 0.01476066266485788\r",
      "iteration 714 - y: 0.014725126098481192\r",
      "iteration 715 - y: 0.014689589532104504\r",
      "iteration 716 - y: 0.014654052965727818\r",
      "iteration 717 - y: 0.01461851639935113\r",
      "iteration 718 - y: 0.014582979832974444\r",
      "iteration 719 - y: 0.014547443266597756\r",
      "iteration 720 - y: 0.014511906700221068\r",
      "iteration 721 - y: 0.014476370133844382\r",
      "iteration 722 - y: 0.014440833567467695\r",
      "iteration 723 - y: 0.014405297001091008\r",
      "iteration 724 - y: 0.01436976043471432\r",
      "iteration 725 - y: 0.014334223868337633\r",
      "iteration 726 - y: 0.014298687301960947\r",
      "iteration 727 - y: 0.014263150735584259\r",
      "iteration 728 - y: 0.014227614169207573\r",
      "iteration 729 - y: 0.014192077602830885\r",
      "iteration 730 - y: 0.014156541036454197\r",
      "iteration 731 - y: 0.014121004470077511\r",
      "iteration 732 - y: 0.014085467903700823\r",
      "iteration 733 - y: 0.014049931337324137\r",
      "iteration 734 - y: 0.01401439477094745\r",
      "iteration 735 - y: 0.013978858204570762\r",
      "iteration 736 - y: 0.013943321638194076\r",
      "iteration 737 - y: 0.013907785071817388\r",
      "iteration 738 - y: 0.013872248505440702\r",
      "iteration 739 - y: 0.013836711939064014\r",
      "iteration 740 - y: 0.013801175372687326\r",
      "iteration 741 - y: 0.01376563880631064\r",
      "iteration 742 - y: 0.013730102239933952\r",
      "iteration 743 - y: 0.013694565673557266\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 744 - y: 0.013659029107180578\r",
      "iteration 745 - y: 0.013623492540803892\r",
      "iteration 746 - y: 0.013587955974427204\r",
      "iteration 747 - y: 0.013552419408050517\r",
      "iteration 748 - y: 0.01351688284167383\r",
      "iteration 749 - y: 0.013481346275297143\r",
      "iteration 750 - y: 0.013445809708920457\r",
      "iteration 751 - y: 0.013410273142543769\r",
      "iteration 752 - y: 0.013374736576167081\r",
      "iteration 753 - y: 0.013339200009790395\r",
      "iteration 754 - y: 0.013303663443413707\r",
      "iteration 755 - y: 0.013268126877037021\r",
      "iteration 756 - y: 0.013232590310660333\r",
      "iteration 757 - y: 0.013197053744283645\r",
      "iteration 758 - y: 0.01316151717790696\r",
      "iteration 759 - y: 0.013125980611530271\r",
      "iteration 760 - y: 0.013090444045153585\r",
      "iteration 761 - y: 0.013054907478776898\r",
      "iteration 762 - y: 0.01301937091240021\r",
      "iteration 763 - y: 0.012983834346023524\r",
      "iteration 764 - y: 0.012948297779646836\r",
      "iteration 765 - y: 0.01291276121327015\r",
      "iteration 766 - y: 0.012877224646893462\r",
      "iteration 767 - y: 0.012841688080516774\r",
      "iteration 768 - y: 0.012806151514140088\r",
      "iteration 769 - y: 0.0127706149477634\r",
      "iteration 770 - y: 0.012735078381386714\r",
      "iteration 771 - y: 0.012699541815010026\r",
      "iteration 772 - y: 0.012664005248633339\r",
      "iteration 773 - y: 0.012628468682256653\r",
      "iteration 774 - y: 0.012592932115879965\r",
      "iteration 775 - y: 0.012557395549503279\r",
      "iteration 776 - y: 0.01252185898312659\r",
      "iteration 777 - y: 0.012486322416749903\r",
      "iteration 778 - y: 0.012450785850373217\r",
      "iteration 779 - y: 0.01241524928399653\r",
      "iteration 780 - y: 0.012379712717619843\r",
      "iteration 781 - y: 0.012344176151243155\r",
      "iteration 782 - y: 0.012308639584866467\r",
      "iteration 783 - y: 0.012273103018489781\r",
      "iteration 784 - y: 0.012237566452113094\r",
      "iteration 785 - y: 0.012202029885736407\r",
      "iteration 786 - y: 0.01216649331935972\r",
      "iteration 787 - y: 0.012130956752983032\r",
      "iteration 788 - y: 0.012095420186606346\r",
      "iteration 789 - y: 0.012059883620229658\r",
      "iteration 790 - y: 0.012024347053852972\r",
      "iteration 791 - y: 0.011988810487476284\r",
      "iteration 792 - y: 0.011953273921099596\r",
      "iteration 793 - y: 0.01191773735472291\r",
      "iteration 794 - y: 0.011882200788346222\r",
      "iteration 795 - y: 0.011846664221969536\r",
      "iteration 796 - y: 0.011811127655592848\r",
      "iteration 797 - y: 0.01177559108921616\r",
      "iteration 798 - y: 0.011740054522839475\r",
      "iteration 799 - y: 0.011704517956462787\r",
      "iteration 800 - y: 0.0116689813900861\r",
      "iteration 801 - y: 0.011633444823709413\r",
      "iteration 802 - y: 0.011597908257332725\r",
      "iteration 803 - y: 0.011562371690956039\r",
      "iteration 804 - y: 0.011526835124579351\r",
      "iteration 805 - y: 0.011491298558202665\r",
      "iteration 806 - y: 0.011455761991825977\r",
      "iteration 807 - y: 0.01142022542544929\r",
      "iteration 808 - y: 0.011384688859072603\r",
      "iteration 809 - y: 0.011349152292695916\r",
      "iteration 810 - y: 0.01131361572631923\r",
      "iteration 811 - y: 0.011278079159942542\r",
      "iteration 812 - y: 0.011242542593565854\r",
      "iteration 813 - y: 0.011207006027189168\r",
      "iteration 814 - y: 0.01117146946081248\r",
      "iteration 815 - y: 0.011135932894435794\r",
      "iteration 816 - y: 0.011100396328059108\r",
      "iteration 817 - y: 0.011064859761682422\r",
      "iteration 818 - y: 0.011029323195305736\r",
      "iteration 819 - y: 0.01099378662892905\r",
      "iteration 820 - y: 0.010958250062552364\r",
      "iteration 821 - y: 0.010922713496175677\r",
      "iteration 822 - y: 0.010887176929798991\r",
      "iteration 823 - y: 0.010851640363422307\r",
      "iteration 824 - y: 0.010816103797045621\r",
      "iteration 825 - y: 0.010780567230668935\r",
      "iteration 826 - y: 0.010745030664292249\r",
      "iteration 827 - y: 0.010709494097915563\r",
      "iteration 828 - y: 0.010673957531538877\r",
      "iteration 829 - y: 0.01063842096516219\r",
      "iteration 830 - y: 0.010602884398785505\r",
      "iteration 831 - y: 0.010567347832408818\r",
      "iteration 832 - y: 0.010531811266032134\r",
      "iteration 833 - y: 0.010496274699655448\r",
      "iteration 834 - y: 0.010460738133278762\r",
      "iteration 835 - y: 0.010425201566902076\r",
      "iteration 836 - y: 0.01038966500052539\r",
      "iteration 837 - y: 0.010354128434148704\r",
      "iteration 838 - y: 0.010318591867772018\r",
      "iteration 839 - y: 0.010283055301395332\r",
      "iteration 840 - y: 0.010247518735018646\r",
      "iteration 841 - y: 0.010211982168641961\r",
      "iteration 842 - y: 0.010176445602265275\r",
      "iteration 843 - y: 0.010140909035888589\r",
      "iteration 844 - y: 0.010105372469511903\r",
      "iteration 845 - y: 0.010069835903135217\r",
      "iteration 846 - y: 0.01003429933675853\r",
      "iteration 847 - y: 0.009998762770381845\r",
      "iteration 848 - y: 0.009963226204005159\r",
      "iteration 849 - y: 0.009927689637628473\r",
      "iteration 850 - y: 0.009892153071251788\r",
      "iteration 851 - y: 0.009856616504875102\r",
      "iteration 852 - y: 0.009821079938498416\r",
      "iteration 853 - y: 0.00978554337212173\r",
      "iteration 854 - y: 0.009750006805745044\r",
      "iteration 855 - y: 0.009714470239368358\r",
      "iteration 856 - y: 0.009678933672991672\r",
      "iteration 857 - y: 0.009643397106614986\r",
      "iteration 858 - y: 0.0096078605402383\r",
      "iteration 859 - y: 0.009572323973861615\r",
      "iteration 860 - y: 0.00953678740748493\r",
      "iteration 861 - y: 0.009501250841108243\r",
      "iteration 862 - y: 0.009465714274731557\r",
      "iteration 863 - y: 0.009430177708354871\r",
      "iteration 864 - y: 0.009394641141978185\r",
      "iteration 865 - y: 0.009359104575601499\r",
      "iteration 866 - y: 0.009323568009224813\r",
      "iteration 867 - y: 0.009288031442848128\r",
      "iteration 868 - y: 0.009252494876471442\r",
      "iteration 869 - y: 0.009216958310094756\r",
      "iteration 870 - y: 0.00918142174371807\r",
      "iteration 871 - y: 0.009145885177341384\r",
      "iteration 872 - y: 0.009110348610964698\r",
      "iteration 873 - y: 0.009074812044588012\r",
      "iteration 874 - y: 0.009039275478211326\r",
      "iteration 875 - y: 0.00900373891183464\r",
      "iteration 876 - y: 0.008968202345457955\r",
      "iteration 877 - y: 0.00893266577908127\r",
      "iteration 878 - y: 0.008897129212704583\r",
      "iteration 879 - y: 0.008861592646327897\r",
      "iteration 880 - y: 0.008826056079951211\r",
      "iteration 881 - y: 0.008790519513574525\r",
      "iteration 882 - y: 0.008754982947197839\r",
      "iteration 883 - y: 0.008719446380821153\r",
      "iteration 884 - y: 0.008683909814444467\r",
      "iteration 885 - y: 0.008648373248067782\r",
      "iteration 886 - y: 0.008612836681691096\r",
      "iteration 887 - y: 0.00857730011531441\r",
      "iteration 888 - y: 0.008541763548937724\r",
      "iteration 889 - y: 0.008506226982561038\r",
      "iteration 890 - y: 0.008470690416184352\r",
      "iteration 891 - y: 0.008435153849807666\r",
      "iteration 892 - y: 0.00839961728343098\r",
      "iteration 893 - y: 0.008364080717054294\r",
      "iteration 894 - y: 0.00832854415067761\r",
      "iteration 895 - y: 0.008293007584300923\r",
      "iteration 896 - y: 0.008257471017924237\r",
      "iteration 897 - y: 0.008221934451547551\r",
      "iteration 898 - y: 0.008186397885170865\r",
      "iteration 899 - y: 0.008150861318794179\r",
      "iteration 900 - y: 0.008115324752417493\r",
      "iteration 901 - y: 0.008079788186040807\r",
      "iteration 902 - y: 0.008044251619664121\r",
      "iteration 903 - y: 0.008008715053287437\r",
      "iteration 904 - y: 0.00797317848691075\r",
      "iteration 905 - y: 0.007937641920534064\r",
      "iteration 906 - y: 0.007902105354157378\r",
      "iteration 907 - y: 0.007866568787780692\r",
      "iteration 908 - y: 0.007831032221404006\r",
      "iteration 909 - y: 0.00779549565502732\r",
      "iteration 910 - y: 0.007759959088650635\r",
      "iteration 911 - y: 0.007724422522273949\r",
      "iteration 912 - y: 0.007688885955897263\r",
      "iteration 913 - y: 0.007653349389520577\r",
      "iteration 914 - y: 0.0076178128231438914\r",
      "iteration 915 - y: 0.007582276256767205\r",
      "iteration 916 - y: 0.007546739690390519\r",
      "iteration 917 - y: 0.007511203124013833\r",
      "iteration 918 - y: 0.007475666557637148\r",
      "iteration 919 - y: 0.007440129991260462\r",
      "iteration 920 - y: 0.007404593424883776\r",
      "iteration 921 - y: 0.00736905685850709\r",
      "iteration 922 - y: 0.007333520292130404\r",
      "iteration 923 - y: 0.0072979837257537185\r",
      "iteration 924 - y: 0.007262447159377032\r",
      "iteration 925 - y: 0.007226910593000346\r",
      "iteration 926 - y: 0.00719137402662366\r",
      "iteration 927 - y: 0.007155837460246975\r",
      "iteration 928 - y: 0.007120300893870289\r",
      "iteration 929 - y: 0.007084764327493603\r",
      "iteration 930 - y: 0.007049227761116917\r",
      "iteration 931 - y: 0.007013691194740231\r",
      "iteration 932 - y: 0.0069781546283635455\r",
      "iteration 933 - y: 0.0069426180619868594\r",
      "iteration 934 - y: 0.006907081495610173\r",
      "iteration 935 - y: 0.006871544929233487\r",
      "iteration 936 - y: 0.006836008362856802\r",
      "iteration 937 - y: 0.006800471796480116\r",
      "iteration 938 - y: 0.00676493523010343\r",
      "iteration 939 - y: 0.006729398663726744\r",
      "iteration 940 - y: 0.006693862097350058\r",
      "iteration 941 - y: 0.006658325530973373\r",
      "iteration 942 - y: 0.0066227889645966865\r",
      "iteration 943 - y: 0.00658725239822\r",
      "iteration 944 - y: 0.006551715831843314\r",
      "iteration 945 - y: 0.006516179265466629\r",
      "iteration 946 - y: 0.006480642699089943\r",
      "iteration 947 - y: 0.006445106132713257\r",
      "iteration 948 - y: 0.006409569566336571\r",
      "iteration 949 - y: 0.006374032999959886\r",
      "iteration 950 - y: 0.0063384964335832\r",
      "iteration 951 - y: 0.0063029598672065135\r",
      "iteration 952 - y: 0.0062674233008298275\r",
      "iteration 953 - y: 0.006231886734453141\r",
      "iteration 954 - y: 0.006196350168076456\r",
      "iteration 955 - y: 0.00616081360169977\r",
      "iteration 956 - y: 0.006125277035323084\r",
      "iteration 957 - y: 0.006089740468946398\r",
      "iteration 958 - y: 0.006054203902569713\r",
      "iteration 959 - y: 0.006018667336193027\r",
      "iteration 960 - y: 0.005983130769816341\r",
      "iteration 961 - y: 0.0059475942034396545\r",
      "iteration 962 - y: 0.005912057637062968\r",
      "iteration 963 - y: 0.005876521070686283\r",
      "iteration 964 - y: 0.005840984504309597\r",
      "iteration 965 - y: 0.005805447937932911\r",
      "iteration 966 - y: 0.005769911371556225\r",
      "iteration 967 - y: 0.00573437480517954\r",
      "iteration 968 - y: 0.005698838238802854\r",
      "iteration 969 - y: 0.005663301672426168\r",
      "iteration 970 - y: 0.0056277651060494815\r",
      "iteration 971 - y: 0.0055922285396727955\r",
      "iteration 972 - y: 0.00555669197329611\r",
      "iteration 973 - y: 0.005521155406919424\r",
      "iteration 974 - y: 0.005485618840542738\r",
      "iteration 975 - y: 0.005450082274166052\r",
      "iteration 976 - y: 0.005414545707789367\r",
      "iteration 977 - y: 0.005379009141412681\r",
      "iteration 978 - y: 0.005343472575035995\r",
      "iteration 979 - y: 0.005307936008659309\r",
      "iteration 980 - y: 0.005272399442282623\r",
      "iteration 981 - y: 0.005236862875905937\r",
      "iteration 982 - y: 0.005201326309529251\r",
      "iteration 983 - y: 0.005165789743152565\r",
      "iteration 984 - y: 0.005130253176775879\r",
      "iteration 985 - y: 0.005094716610399194\r",
      "iteration 986 - y: 0.005059180044022508\r",
      "iteration 987 - y: 0.005023643477645822\r",
      "iteration 988 - y: 0.004988106911269136\r",
      "iteration 989 - y: 0.00495257034489245\r",
      "iteration 990 - y: 0.004917033778515764\r",
      "iteration 991 - y: 0.004881497212139078\r",
      "iteration 992 - y: 0.004845960645762392\r",
      "iteration 993 - y: 0.004810424079385706\r",
      "iteration 994 - y: 0.004774887513009021\r",
      "iteration 995 - y: 0.004739350946632335\r",
      "iteration 996 - y: 0.004703814380255649\r",
      "iteration 997 - y: 0.004668277813878963\r",
      "iteration 998 - y: 0.0046327412475022775\r",
      "iteration 999 - y: 0.004597204681125591\r",
      "iteration 1000 - y: 0.004561668114748905\r",
      "iteration 1001 - y: 0.004526131548372219\r",
      "iteration 1002 - y: 0.004490594981995533\r",
      "iteration 1003 - y: 0.004455058415618848\r",
      "iteration 1004 - y: 0.004419521849242162\r",
      "iteration 1005 - y: 0.004383985282865476\r",
      "iteration 1006 - y: 0.00434844871648879\r",
      "iteration 1007 - y: 0.0043129121501121045\r",
      "iteration 1008 - y: 0.0042773755837354184\r",
      "iteration 1009 - y: 0.004241839017358732\r",
      "iteration 1010 - y: 0.004206302450982046\r",
      "iteration 1011 - y: 0.00417076588460536\r",
      "iteration 1012 - y: 0.004135229318228675\r",
      "iteration 1013 - y: 0.004099692751851989\r",
      "iteration 1014 - y: 0.004064156185475303\r",
      "iteration 1015 - y: 0.004028619619098617\r",
      "iteration 1016 - y: 0.0039930830527219316\r",
      "iteration 1017 - y: 0.0039575464863452455\r",
      "iteration 1018 - y: 0.003922009919968559\r",
      "iteration 1019 - y: 0.0038864733535918738\r",
      "iteration 1020 - y: 0.0038509367872151877\r",
      "iteration 1021 - y: 0.0038154002208385016\r",
      "iteration 1022 - y: 0.003779863654461816\r",
      "iteration 1023 - y: 0.00374432708808513\r",
      "iteration 1024 - y: 0.0037087905217084442\r",
      "iteration 1025 - y: 0.003673253955331758\r",
      "iteration 1026 - y: 0.0036377173889550725\r",
      "iteration 1027 - y: 0.0036021808225783864\r",
      "iteration 1028 - y: 0.003566644256201701\r",
      "iteration 1029 - y: 0.0035311076898250147\r",
      "iteration 1030 - y: 0.003495571123448329\r",
      "iteration 1031 - y: 0.003460034557071643\r",
      "iteration 1032 - y: 0.003424497990694957\r",
      "iteration 1033 - y: 0.0033889614243182713\r",
      "iteration 1034 - y: 0.003353424857941585\r",
      "iteration 1035 - y: 0.0033178882915648996\r",
      "iteration 1036 - y: 0.0032823517251882135\r",
      "iteration 1037 - y: 0.003246815158811528\r",
      "iteration 1038 - y: 0.0032112785924348418\r",
      "iteration 1039 - y: 0.003175742026058156\r",
      "iteration 1040 - y: 0.00314020545968147\r",
      "iteration 1041 - y: 0.0031046688933047844\r",
      "iteration 1042 - y: 0.0030691323269280983\r",
      "iteration 1043 - y: 0.0030335957605514123\r",
      "iteration 1044 - y: 0.0029980591941747266\r",
      "iteration 1045 - y: 0.0029625226277980405\r",
      "iteration 1046 - y: 0.002926986061421355\r",
      "iteration 1047 - y: 0.002891449495044669\r",
      "iteration 1048 - y: 0.002855912928667983\r",
      "iteration 1049 - y: 0.002820376362291297\r",
      "iteration 1050 - y: 0.0027848397959146115\r",
      "iteration 1051 - y: 0.002749303229537925\r",
      "iteration 1052 - y: 0.002713766663161239\r",
      "iteration 1053 - y: 0.002678230096784553\r",
      "iteration 1054 - y: 0.0026426935304078667\r",
      "iteration 1055 - y: 0.00260715696403118\r",
      "iteration 1056 - y: 0.002571620397654494\r",
      "iteration 1057 - y: 0.002536083831277808\r",
      "iteration 1058 - y: 0.0025005472649011215\r",
      "iteration 1059 - y: 0.0024650106985244355\r",
      "iteration 1060 - y: 0.0024294741321477494\r",
      "iteration 1061 - y: 0.0023939375657710633\r",
      "iteration 1062 - y: 0.002358400999394377\r",
      "iteration 1063 - y: 0.0023228644330176907\r",
      "iteration 1064 - y: 0.0022873278666410046\r",
      "iteration 1065 - y: 0.0022517913002643186\r",
      "iteration 1066 - y: 0.002216254733887632\r",
      "iteration 1067 - y: 0.002180718167510946\r",
      "iteration 1068 - y: 0.00214518160113426\r",
      "iteration 1069 - y: 0.002109645034757574\r",
      "iteration 1070 - y: 0.0020741084683808873\r",
      "iteration 1071 - y: 0.0020385719020042012\r",
      "iteration 1072 - y: 0.002003035335627515\r",
      "iteration 1073 - y: 0.001967498769250829\r",
      "iteration 1074 - y: 0.0019319622028741428\r",
      "iteration 1075 - y: 0.0018964256364974565\r",
      "iteration 1076 - y: 0.0018608890701207704\r",
      "iteration 1077 - y: 0.0018253525037440841\r",
      "iteration 1078 - y: 0.001789815937367398\r",
      "iteration 1079 - y: 0.0017542793709907118\r",
      "iteration 1080 - y: 0.0017187428046140257\r",
      "iteration 1081 - y: 0.0016832062382373394\r",
      "iteration 1082 - y: 0.0016476696718606533\r",
      "iteration 1083 - y: 0.001612133105483967\r",
      "iteration 1084 - y: 0.0015765965391072807\r",
      "iteration 1085 - y: 0.0015410599727305946\r",
      "iteration 1086 - y: 0.0015055234063539084\r",
      "iteration 1087 - y: 0.0014699868399772223\r",
      "iteration 1088 - y: 0.001434450273600536\r",
      "iteration 1089 - y: 0.00139891370722385\r",
      "iteration 1090 - y: 0.0013633771408471638\r",
      "iteration 1091 - y: 0.0013278405744704778\r",
      "iteration 1092 - y: 0.0012923040080937917\r",
      "iteration 1093 - y: 0.0012567674417171058\r",
      "iteration 1094 - y: 0.0012212308753404197\r",
      "iteration 1095 - y: 0.0011856943089637337\r",
      "iteration 1096 - y: 0.0011501577425870476\r",
      "iteration 1097 - y: 0.0011146211762103615\r",
      "iteration 1098 - y: 0.0010790846098336756\r",
      "iteration 1099 - y: 0.0010435480434569896\r",
      "iteration 1100 - y: 0.0010080114770803035\r",
      "iteration 1101 - y: 0.0009724749107036174\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0758161585022845\r",
      "iteration 2 - y: 0.07578911976266284\r",
      "iteration 3 - y: 0.07576208102304122\r",
      "iteration 4 - y: 0.07573504228341957\r",
      "iteration 5 - y: 0.07570800354379792\r",
      "iteration 6 - y: 0.07568096480417628\r",
      "iteration 7 - y: 0.07565392606455464\r",
      "iteration 8 - y: 0.075626887324933\r",
      "iteration 9 - y: 0.07559984858531135\r",
      "iteration 10 - y: 0.07557280984568972\r",
      "iteration 11 - y: 0.07554577110606808\r",
      "iteration 12 - y: 0.07551873236644643\r",
      "iteration 13 - y: 0.07549169362682479\r",
      "iteration 14 - y: 0.07546465488720315\r",
      "iteration 15 - y: 0.07543761614758152\r",
      "iteration 16 - y: 0.07541057740795987\r",
      "iteration 17 - y: 0.07538353866833822\r",
      "iteration 18 - y: 0.0753564999287166\r",
      "iteration 19 - y: 0.07532946118909495\r",
      "iteration 20 - y: 0.0753024224494733\r",
      "iteration 21 - y: 0.07527538370985165\r",
      "iteration 22 - y: 0.07524834497023002\r",
      "iteration 23 - y: 0.07522130623060838\r",
      "iteration 24 - y: 0.07519426749098673\r",
      "iteration 25 - y: 0.0751672287513651\r",
      "iteration 26 - y: 0.0751026590756312\r",
      "iteration 27 - y: 0.07501979339189678\r",
      "iteration 28 - y: 0.07493692770816236\r",
      "iteration 29 - y: 0.07485406202442793\r",
      "iteration 30 - y: 0.07477119634069353\r",
      "iteration 31 - y: 0.0746883306569591\r",
      "iteration 32 - y: 0.07460546497322468\r",
      "iteration 33 - y: 0.07452259928949026\r",
      "iteration 34 - y: 0.07443973360575584\r",
      "iteration 35 - y: 0.07435686792202142\r",
      "iteration 36 - y: 0.074274002238287\r",
      "iteration 37 - y: 0.07419113655455259\r",
      "iteration 38 - y: 0.07410827087081817\r",
      "iteration 39 - y: 0.07402540518708375\r",
      "iteration 40 - y: 0.07394253950334934\r",
      "iteration 41 - y: 0.0738596738196149\r",
      "iteration 42 - y: 0.07377680813588049\r",
      "iteration 43 - y: 0.07369394245214607\r",
      "iteration 44 - y: 0.07361107676841164\r",
      "iteration 45 - y: 0.07352821108467722\r",
      "iteration 46 - y: 0.07344534540094282\r",
      "iteration 47 - y: 0.07336247971720838\r",
      "iteration 48 - y: 0.07327961403347397\r",
      "iteration 49 - y: 0.07319674834973955\r",
      "iteration 50 - y: 0.07311388266600513\r",
      "iteration 51 - y: 0.07303101698227071\r",
      "iteration 52 - y: 0.0729481512985363\r",
      "iteration 53 - y: 0.07286528561480188\r",
      "iteration 54 - y: 0.07278241993106745\r",
      "iteration 55 - y: 0.07269955424733304\r",
      "iteration 56 - y: 0.07261668856359862\r",
      "iteration 57 - y: 0.0725338228798642\r",
      "iteration 58 - y: 0.07245095719612978\r",
      "iteration 59 - y: 0.07236809151239536\r",
      "iteration 60 - y: 0.07228522582866094\r",
      "iteration 61 - y: 0.07220236014492652\r",
      "iteration 62 - y: 0.0721194944611921\r",
      "iteration 63 - y: 0.07203662877745767\r",
      "iteration 64 - y: 0.07195376309372326\r",
      "iteration 65 - y: 0.07187089740998884\r",
      "iteration 66 - y: 0.07178803172625442\r",
      "iteration 67 - y: 0.07170516604252\r",
      "iteration 68 - y: 0.07162230035878558\r",
      "iteration 69 - y: 0.07153943467505117\r",
      "iteration 70 - y: 0.07145656899131675\r",
      "iteration 71 - y: 0.07137370330758233\r",
      "iteration 72 - y: 0.07129083762384791\r",
      "iteration 73 - y: 0.0712079719401135\r",
      "iteration 74 - y: 0.07112510625637906\r",
      "iteration 75 - y: 0.07104224057264465\r",
      "iteration 76 - y: 0.07095937488891024\r",
      "iteration 77 - y: 0.07087650920517581\r",
      "iteration 78 - y: 0.0707936435214414\r",
      "iteration 79 - y: 0.07071077783770696\r",
      "iteration 80 - y: 0.07062791215397256\r",
      "iteration 81 - y: 0.07054504647023814\r",
      "iteration 82 - y: 0.07046218078650371\r",
      "iteration 83 - y: 0.07037931510276929\r",
      "iteration 84 - y: 0.07029644941903487\r",
      "iteration 85 - y: 0.07021358373530046\r",
      "iteration 86 - y: 0.07013071805156604\r",
      "iteration 87 - y: 0.07004785236783162\r",
      "iteration 88 - y: 0.0699649866840972\r",
      "iteration 89 - y: 0.06988212100036278\r",
      "iteration 90 - y: 0.06979925531662837\r",
      "iteration 91 - y: 0.06971638963289395\r",
      "iteration 92 - y: 0.06963352394915952\r",
      "iteration 93 - y: 0.0695506582654251\r",
      "iteration 94 - y: 0.06946779258169068\r",
      "iteration 95 - y: 0.06938492689795626\r",
      "iteration 96 - y: 0.06930206121422185\r",
      "iteration 97 - y: 0.06921919553048742\r",
      "iteration 98 - y: 0.06913632984675301\r",
      "iteration 99 - y: 0.06905346416301858\r",
      "iteration 100 - y: 0.06897059847928416\r",
      "iteration 101 - y: 0.06888773279554976\r",
      "iteration 102 - y: 0.06880486711181533\r",
      "iteration 103 - y: 0.06872200142808091\r",
      "iteration 104 - y: 0.06863913574434649\r",
      "iteration 105 - y: 0.06855627006061207\r",
      "iteration 106 - y: 0.06847340437687766\r",
      "iteration 107 - y: 0.06839053869314322\r",
      "iteration 108 - y: 0.06830767300940882\r",
      "iteration 109 - y: 0.06822480732567439\r",
      "iteration 110 - y: 0.06814194164193997\r",
      "iteration 111 - y: 0.06805907595820555\r",
      "iteration 112 - y: 0.06797621027447114\r",
      "iteration 113 - y: 0.06789334459073672\r",
      "iteration 114 - y: 0.0678104789070023\r",
      "iteration 115 - y: 0.06772761322326787\r",
      "iteration 116 - y: 0.06764474753953345\r",
      "iteration 117 - y: 0.06756188185579903\r",
      "iteration 118 - y: 0.06747901617206462\r",
      "iteration 119 - y: 0.0673961504883302\r",
      "iteration 120 - y: 0.06731328480459578\r",
      "iteration 121 - y: 0.06723041912086136\r",
      "iteration 122 - y: 0.06714755343712694\r",
      "iteration 123 - y: 0.06706468775339253\r",
      "iteration 124 - y: 0.06698182206965811\r",
      "iteration 125 - y: 0.06689895638592368\r",
      "iteration 126 - y: 0.06681609070218927\r",
      "iteration 127 - y: 0.06673322501845484\r",
      "iteration 128 - y: 0.06665035933472042\r",
      "iteration 129 - y: 0.066567493650986\r",
      "iteration 130 - y: 0.06648462796725159\r",
      "iteration 131 - y: 0.06640176228351717\r",
      "iteration 132 - y: 0.06631889659978274\r",
      "iteration 133 - y: 0.06623603091604834\r",
      "iteration 134 - y: 0.0661531652323139\r",
      "iteration 135 - y: 0.06607029954857949\r",
      "iteration 136 - y: 0.06598743386484507\r",
      "iteration 137 - y: 0.06590456818111065\r",
      "iteration 138 - y: 0.06582170249737623\r",
      "iteration 139 - y: 0.06573883681364182\r",
      "iteration 140 - y: 0.0656559711299074\r",
      "iteration 141 - y: 0.06557310544617298\r",
      "iteration 142 - y: 0.06549023976243855\r",
      "iteration 143 - y: 0.06540737407870414\r",
      "iteration 144 - y: 0.06532450839496971\r",
      "iteration 145 - y: 0.0652416427112353\r",
      "iteration 146 - y: 0.06515877702750088\r",
      "iteration 147 - y: 0.06507591134376645\r",
      "iteration 148 - y: 0.06499304566003204\r",
      "iteration 149 - y: 0.06491017997629761\r",
      "iteration 150 - y: 0.06482731429256319\r",
      "iteration 151 - y: 0.06474444860882879\r",
      "iteration 152 - y: 0.06466158292509436\r",
      "iteration 153 - y: 0.06457871724135994\r",
      "iteration 154 - y: 0.06449585155762552\r",
      "iteration 155 - y: 0.0644129858738911\r",
      "iteration 156 - y: 0.06433012019015669\r",
      "iteration 157 - y: 0.06424725450642225\r",
      "iteration 158 - y: 0.06416438882268785\r",
      "iteration 159 - y: 0.06408152313895342\r",
      "iteration 160 - y: 0.06399865745521902\r",
      "iteration 161 - y: 0.06391579177148458\r",
      "iteration 162 - y: 0.06383292608775017\r",
      "iteration 163 - y: 0.06375006040401575\r",
      "iteration 164 - y: 0.06366719472028133\r",
      "iteration 165 - y: 0.06358432903654691\r",
      "iteration 166 - y: 0.06350146335281248\r",
      "iteration 167 - y: 0.06341859766907806\r",
      "iteration 168 - y: 0.06333573198534365\r",
      "iteration 169 - y: 0.06325286630160923\r",
      "iteration 170 - y: 0.06317000061787481\r",
      "iteration 171 - y: 0.0630871349341404\r",
      "iteration 172 - y: 0.06300426925040598\r",
      "iteration 173 - y: 0.06292140356667156\r",
      "iteration 174 - y: 0.06283853788293714\r",
      "iteration 175 - y: 0.06275567219920272\r",
      "iteration 176 - y: 0.0626728065154683\r",
      "iteration 177 - y: 0.06258994083173387\r",
      "iteration 178 - y: 0.06250707514799946\r",
      "iteration 179 - y: 0.06242420946426504\r",
      "iteration 180 - y: 0.06234134378053062\r",
      "iteration 181 - y: 0.0622584780967962\r",
      "iteration 182 - y: 0.06217561241306178\r",
      "iteration 183 - y: 0.06209274672932737\r",
      "iteration 184 - y: 0.06200988104559294\r",
      "iteration 185 - y: 0.06192701536185853\r",
      "iteration 186 - y: 0.0618441496781241\r",
      "iteration 187 - y: 0.06176128399438968\r",
      "iteration 188 - y: 0.061678418310655264\r",
      "iteration 189 - y: 0.06159555262692084\r",
      "iteration 190 - y: 0.06151268694318642\r",
      "iteration 191 - y: 0.06142982125945201\r",
      "iteration 192 - y: 0.061346955575717586\r",
      "iteration 193 - y: 0.061264089891983176\r",
      "iteration 194 - y: 0.061181224208248744\r",
      "iteration 195 - y: 0.061098358524514326\r",
      "iteration 196 - y: 0.06101549284077991\r",
      "iteration 197 - y: 0.06093262715704549\r",
      "iteration 198 - y: 0.06084976147331107\r",
      "iteration 199 - y: 0.060766895789576655\r",
      "iteration 200 - y: 0.06068403010584223\r",
      "iteration 201 - y: 0.06060116442210782\r",
      "iteration 202 - y: 0.060518298738373395\r",
      "iteration 203 - y: 0.06043543305463897\r",
      "iteration 204 - y: 0.06035256737090455\r",
      "iteration 205 - y: 0.060269701687170135\r",
      "iteration 206 - y: 0.06018683600343572\r",
      "iteration 207 - y: 0.0601039703197013\r",
      "iteration 208 - y: 0.060021104635966875\r",
      "iteration 209 - y: 0.059938238952232464\r",
      "iteration 210 - y: 0.05985537326849804\r",
      "iteration 211 - y: 0.05977250758476362\r",
      "iteration 212 - y: 0.0596896419010292\r",
      "iteration 213 - y: 0.05960677621729478\r",
      "iteration 214 - y: 0.05952391053356036\r",
      "iteration 215 - y: 0.059441044849825944\r",
      "iteration 216 - y: 0.059358179166091526\r",
      "iteration 217 - y: 0.05927531348235711\r",
      "iteration 218 - y: 0.059192447798622684\r",
      "iteration 219 - y: 0.05910958211488827\r",
      "iteration 220 - y: 0.05902671643115384\r",
      "iteration 221 - y: 0.05894385074741943\r",
      "iteration 222 - y: 0.058860985063685006\r",
      "iteration 223 - y: 0.05877811937995059\r",
      "iteration 224 - y: 0.05869525369621617\r",
      "iteration 225 - y: 0.05861238801248175\r",
      "iteration 226 - y: 0.058529522328747335\r",
      "iteration 227 - y: 0.05844665664501292\r",
      "iteration 228 - y: 0.058363790961278486\r",
      "iteration 229 - y: 0.058280925277544075\r",
      "iteration 230 - y: 0.05819805959380965\r",
      "iteration 231 - y: 0.05811519391007524\r",
      "iteration 232 - y: 0.05803232822634081\r",
      "iteration 233 - y: 0.0579494625426064\r",
      "iteration 234 - y: 0.05786659685887198\r",
      "iteration 235 - y: 0.057783731175137555\r",
      "iteration 236 - y: 0.057700865491403144\r",
      "iteration 237 - y: 0.05761799980766871\r",
      "iteration 238 - y: 0.057535134123934295\r",
      "iteration 239 - y: 0.05745226844019988\r",
      "iteration 240 - y: 0.05736940275646546\r",
      "iteration 241 - y: 0.05728653707273104\r",
      "iteration 242 - y: 0.057203671388996624\r",
      "iteration 243 - y: 0.057120805705262206\r",
      "iteration 244 - y: 0.05703794002152778\r",
      "iteration 245 - y: 0.05695507433779336\r",
      "iteration 246 - y: 0.05687220865405894\r",
      "iteration 247 - y: 0.05678934297032453\r",
      "iteration 248 - y: 0.056706477286590104\r",
      "iteration 249 - y: 0.056623611602855686\r",
      "iteration 250 - y: 0.05654074591912126\r",
      "iteration 251 - y: 0.05645788023538685\r",
      "iteration 252 - y: 0.05637501455165243\r",
      "iteration 253 - y: 0.05629214886791801\r",
      "iteration 254 - y: 0.056209283184183584\r",
      "iteration 255 - y: 0.056126417500449166\r",
      "iteration 256 - y: 0.05604355181671475\r",
      "iteration 257 - y: 0.05596068613298034\r",
      "iteration 258 - y: 0.05587782044924591\r",
      "iteration 259 - y: 0.055794954765511495\r",
      "iteration 260 - y: 0.05571208908177707\r",
      "iteration 261 - y: 0.05562922339804265\r",
      "iteration 262 - y: 0.05554635771430824\r",
      "iteration 263 - y: 0.05546349203057382\r",
      "iteration 264 - y: 0.05538062634683939\r",
      "iteration 265 - y: 0.055297760663104975\r",
      "iteration 266 - y: 0.05521489497937055\r",
      "iteration 267 - y: 0.05513202929563614\r",
      "iteration 268 - y: 0.05504916361190172\r",
      "iteration 269 - y: 0.0549662979281673\r",
      "iteration 270 - y: 0.05488343224443288\r",
      "iteration 271 - y: 0.054800566560698455\r",
      "iteration 272 - y: 0.05471770087696404\r",
      "iteration 273 - y: 0.054634835193229626\r",
      "iteration 274 - y: 0.0545519695094952\r",
      "iteration 275 - y: 0.054469103825760784\r",
      "iteration 276 - y: 0.05438623814202636\r",
      "iteration 277 - y: 0.05430337245829194\r",
      "iteration 278 - y: 0.05422050677455753\r",
      "iteration 279 - y: 0.054137641090823106\r",
      "iteration 280 - y: 0.05405477540708868\r",
      "iteration 281 - y: 0.053971909723354264\r",
      "iteration 282 - y: 0.053889044039619846\r",
      "iteration 283 - y: 0.05380617835588543\r",
      "iteration 284 - y: 0.05372331267215101\r",
      "iteration 285 - y: 0.053640446988416586\r",
      "iteration 286 - y: 0.05355758130468217\r",
      "iteration 287 - y: 0.05347471562094775\r",
      "iteration 288 - y: 0.05339184993721334\r",
      "iteration 289 - y: 0.053308984253478915\r",
      "iteration 290 - y: 0.05322611856974449\r",
      "iteration 291 - y: 0.05314325288601007\r",
      "iteration 292 - y: 0.053060387202275655\r",
      "iteration 293 - y: 0.05297752151854124\r",
      "iteration 294 - y: 0.05289465583480682\r",
      "iteration 295 - y: 0.052811790151072395\r",
      "iteration 296 - y: 0.05272892446733797\r",
      "iteration 297 - y: 0.05264605878360356\r",
      "iteration 298 - y: 0.052563193099869135\r",
      "iteration 299 - y: 0.052480327416134724\r",
      "iteration 300 - y: 0.0523974617324003\r",
      "iteration 301 - y: 0.05231459604866588\r",
      "iteration 302 - y: 0.052231730364931464\r",
      "iteration 303 - y: 0.05214886468119704\r",
      "iteration 304 - y: 0.05206599899746263\r",
      "iteration 305 - y: 0.051983133313728204\r",
      "iteration 306 - y: 0.05190026762999378\r",
      "iteration 307 - y: 0.05181740194625937\r",
      "iteration 308 - y: 0.051734536262524944\r",
      "iteration 309 - y: 0.051651670578790526\r",
      "iteration 310 - y: 0.05156880489505611\r",
      "iteration 311 - y: 0.051485939211321684\r",
      "iteration 312 - y: 0.05140307352758727\r",
      "iteration 313 - y: 0.05132020784385285\r",
      "iteration 314 - y: 0.051237342160118424\r",
      "iteration 315 - y: 0.05115447647638401\r",
      "iteration 316 - y: 0.05107161079264959\r",
      "iteration 317 - y: 0.05098874510891517\r",
      "iteration 318 - y: 0.05090587942518075\r",
      "iteration 319 - y: 0.05082301374144633\r",
      "iteration 320 - y: 0.05074014805771192\r",
      "iteration 321 - y: 0.05065728237397749\r",
      "iteration 322 - y: 0.05057441669024307\r",
      "iteration 323 - y: 0.05049155100650866\r",
      "iteration 324 - y: 0.05040868532277423\r",
      "iteration 325 - y: 0.050325819639039815\r",
      "iteration 326 - y: 0.0502429539553054\r",
      "iteration 327 - y: 0.05016008827157097\r",
      "iteration 328 - y: 0.05007722258783656\r",
      "iteration 329 - y: 0.04999435690410214\r",
      "iteration 330 - y: 0.049911491220367726\r",
      "iteration 331 - y: 0.0498286255366333\r",
      "iteration 332 - y: 0.04974575985289888\r",
      "iteration 333 - y: 0.04966289416916446\r",
      "iteration 334 - y: 0.04958002848543004\r",
      "iteration 335 - y: 0.04949716280169562\r",
      "iteration 336 - y: 0.049414297117961206\r",
      "iteration 337 - y: 0.04933143143422678\r",
      "iteration 338 - y: 0.04924856575049237\r",
      "iteration 339 - y: 0.049165700066757946\r",
      "iteration 340 - y: 0.04908283438302352\r",
      "iteration 341 - y: 0.04899996869928911\r",
      "iteration 342 - y: 0.048917103015554686\r",
      "iteration 343 - y: 0.04883423733182027\r",
      "iteration 344 - y: 0.04875137164808585\r",
      "iteration 345 - y: 0.048668505964351426\r",
      "iteration 346 - y: 0.048585640280617015\r",
      "iteration 347 - y: 0.04850277459688259\r",
      "iteration 348 - y: 0.048419908913148166\r",
      "iteration 349 - y: 0.048337043229413755\r",
      "iteration 350 - y: 0.04825417754567933\r",
      "iteration 351 - y: 0.04817131186194491\r",
      "iteration 352 - y: 0.048088446178210495\r",
      "iteration 353 - y: 0.04800558049447607\r",
      "iteration 354 - y: 0.04792271481074166\r",
      "iteration 355 - y: 0.047839849127007235\r",
      "iteration 356 - y: 0.04775698344327282\r",
      "iteration 357 - y: 0.0476741177595384\r",
      "iteration 358 - y: 0.047591252075803975\r",
      "iteration 359 - y: 0.04750838639206956\r",
      "iteration 360 - y: 0.04742552070833514\r",
      "iteration 361 - y: 0.04734265502460072\r",
      "iteration 362 - y: 0.047259789340866304\r",
      "iteration 363 - y: 0.04717692365713188\r",
      "iteration 364 - y: 0.047094057973397455\r",
      "iteration 365 - y: 0.047011192289663044\r",
      "iteration 366 - y: 0.046928326605928626\r",
      "iteration 367 - y: 0.0468454609221942\r",
      "iteration 368 - y: 0.046762595238459784\r",
      "iteration 369 - y: 0.04667972955472536\r",
      "iteration 370 - y: 0.04659686387099095\r",
      "iteration 371 - y: 0.04651399818725653\r",
      "iteration 372 - y: 0.04643113250352211\r",
      "iteration 373 - y: 0.04634826681978769\r",
      "iteration 374 - y: 0.046265401136053264\r",
      "iteration 375 - y: 0.046182535452318846\r",
      "iteration 376 - y: 0.046099669768584435\r",
      "iteration 377 - y: 0.04601680408485001\r",
      "iteration 378 - y: 0.04593393840111559\r",
      "iteration 379 - y: 0.04585107271738117\r",
      "iteration 380 - y: 0.04576820703364676\r",
      "iteration 381 - y: 0.04568534134991234\r",
      "iteration 382 - y: 0.045602475666177915\r",
      "iteration 383 - y: 0.04551960998244349\r",
      "iteration 384 - y: 0.04543674429870907\r",
      "iteration 385 - y: 0.045353878614974655\r",
      "iteration 386 - y: 0.045271012931240244\r",
      "iteration 387 - y: 0.04518814724750582\r",
      "iteration 388 - y: 0.0451052815637714\r",
      "iteration 389 - y: 0.04502241588003698\r",
      "iteration 390 - y: 0.04493955019630255\r",
      "iteration 391 - y: 0.04485668451256814\r",
      "iteration 392 - y: 0.044773818828833724\r",
      "iteration 393 - y: 0.0446909531450993\r",
      "iteration 394 - y: 0.04460808746136488\r",
      "iteration 395 - y: 0.04452522177763046\r",
      "iteration 396 - y: 0.044442356093896046\r",
      "iteration 397 - y: 0.04435949041016163\r",
      "iteration 398 - y: 0.044276624726427204\r",
      "iteration 399 - y: 0.044193759042692786\r",
      "iteration 400 - y: 0.04411089335895836\r",
      "iteration 401 - y: 0.044028027675223944\r",
      "iteration 402 - y: 0.04394516199148953\r",
      "iteration 403 - y: 0.04386229630775511\r",
      "iteration 404 - y: 0.04377943062402069\r",
      "iteration 405 - y: 0.043696564940286266\r",
      "iteration 406 - y: 0.04361369925655185\r",
      "iteration 407 - y: 0.04353083357281744\r",
      "iteration 408 - y: 0.04344796788908301\r",
      "iteration 409 - y: 0.04336510220534859\r",
      "iteration 410 - y: 0.04328223652161417\r",
      "iteration 411 - y: 0.04319937083787975\r",
      "iteration 412 - y: 0.043116505154145335\r",
      "iteration 413 - y: 0.04303363947041092\r",
      "iteration 414 - y: 0.0429507737866765\r",
      "iteration 415 - y: 0.042867908102942075\r",
      "iteration 416 - y: 0.04278504241920766\r",
      "iteration 417 - y: 0.04270217673547323\r",
      "iteration 418 - y: 0.04261931105173882\r",
      "iteration 419 - y: 0.0425364453680044\r",
      "iteration 420 - y: 0.04245357968426998\r",
      "iteration 421 - y: 0.04237071400053556\r",
      "iteration 422 - y: 0.042287848316801144\r",
      "iteration 423 - y: 0.042204982633066726\r",
      "iteration 424 - y: 0.0421221169493323\r",
      "iteration 425 - y: 0.04203925126559788\r",
      "iteration 426 - y: 0.041956385581863466\r",
      "iteration 427 - y: 0.04187351989812904\r",
      "iteration 428 - y: 0.041790654214394624\r",
      "iteration 429 - y: 0.041707788530660206\r",
      "iteration 430 - y: 0.04162492284692579\r",
      "iteration 431 - y: 0.04154205716319137\r",
      "iteration 432 - y: 0.041459191479456946\r",
      "iteration 433 - y: 0.04137632579572252\r",
      "iteration 434 - y: 0.04129346011198811\r",
      "iteration 435 - y: 0.041210594428253686\r",
      "iteration 436 - y: 0.041127728744519275\r",
      "iteration 437 - y: 0.04104486306078485\r",
      "iteration 438 - y: 0.04096199737705043\r",
      "iteration 439 - y: 0.040879131693316015\r",
      "iteration 440 - y: 0.0407962660095816\r",
      "iteration 441 - y: 0.04071340032584718\r",
      "iteration 442 - y: 0.04063053464211276\r",
      "iteration 443 - y: 0.04054766895837834\r",
      "iteration 444 - y: 0.04046480327464392\r",
      "iteration 445 - y: 0.0403819375909095\r",
      "iteration 446 - y: 0.040299071907175084\r",
      "iteration 447 - y: 0.040216206223440666\r",
      "iteration 448 - y: 0.04013334053970625\r",
      "iteration 449 - y: 0.04005047485597183\r",
      "iteration 450 - y: 0.03996760917223741\r",
      "iteration 451 - y: 0.039884743488502995\r",
      "iteration 452 - y: 0.03980187780476858\r",
      "iteration 453 - y: 0.03971901212103415\r",
      "iteration 454 - y: 0.039636146437299735\r",
      "iteration 455 - y: 0.03955328075356532\r",
      "iteration 456 - y: 0.0394704150698309\r",
      "iteration 457 - y: 0.03938754938609648\r",
      "iteration 458 - y: 0.039304683702362064\r",
      "iteration 459 - y: 0.03922181801862765\r",
      "iteration 460 - y: 0.03913895233489323\r",
      "iteration 461 - y: 0.03905608665115881\r",
      "iteration 462 - y: 0.03897322096742439\r",
      "iteration 463 - y: 0.038890355283689976\r",
      "iteration 464 - y: 0.03880748959995555\r",
      "iteration 465 - y: 0.03872462391622113\r",
      "iteration 466 - y: 0.038641758232486716\r",
      "iteration 467 - y: 0.0385588925487523\r",
      "iteration 468 - y: 0.03847602686501788\r",
      "iteration 469 - y: 0.03839316118128346\r",
      "iteration 470 - y: 0.038310295497549045\r",
      "iteration 471 - y: 0.03822742981381463\r",
      "iteration 472 - y: 0.03814456413008021\r",
      "iteration 473 - y: 0.03806169844634579\r",
      "iteration 474 - y: 0.03797883276261137\r",
      "iteration 475 - y: 0.03789596707887695\r",
      "iteration 476 - y: 0.03781310139514253\r",
      "iteration 477 - y: 0.037730235711408114\r",
      "iteration 478 - y: 0.037647370027673696\r",
      "iteration 479 - y: 0.03756450434393928\r",
      "iteration 480 - y: 0.03748163866020486\r",
      "iteration 481 - y: 0.03739877297647044\r",
      "iteration 482 - y: 0.037315907292736025\r",
      "iteration 483 - y: 0.03723304160900161\r",
      "iteration 484 - y: 0.03715017592526719\r",
      "iteration 485 - y: 0.037067310241532765\r",
      "iteration 486 - y: 0.036984444557798354\r",
      "iteration 487 - y: 0.03690157887406393\r",
      "iteration 488 - y: 0.03681871319032952\r",
      "iteration 489 - y: 0.0367358475065951\r",
      "iteration 490 - y: 0.03665298182286068\r",
      "iteration 491 - y: 0.036570116139126266\r",
      "iteration 492 - y: 0.03648725045539184\r",
      "iteration 493 - y: 0.03640438477165742\r",
      "iteration 494 - y: 0.036321519087923006\r",
      "iteration 495 - y: 0.03623865340418859\r",
      "iteration 496 - y: 0.03615578772045417\r",
      "iteration 497 - y: 0.03607292203671975\r",
      "iteration 498 - y: 0.035990056352985335\r",
      "iteration 499 - y: 0.03590719066925092\r",
      "iteration 500 - y: 0.0358243249855165\r",
      "iteration 501 - y: 0.03574145930178208\r",
      "iteration 502 - y: 0.03565859361804766\r",
      "iteration 503 - y: 0.03557572793431324\r",
      "iteration 504 - y: 0.03549286225057882\r",
      "iteration 505 - y: 0.035409996566844404\r",
      "iteration 506 - y: 0.035327130883109986\r",
      "iteration 507 - y: 0.03524426519937557\r",
      "iteration 508 - y: 0.03516139951564115\r",
      "iteration 509 - y: 0.03507853383190673\r",
      "iteration 510 - y: 0.034995668148172315\r",
      "iteration 511 - y: 0.0349128024644379\r",
      "iteration 512 - y: 0.03482993678070348\r",
      "iteration 513 - y: 0.034747071096969055\r",
      "iteration 514 - y: 0.03466420541323464\r",
      "iteration 515 - y: 0.03458133972950022\r",
      "iteration 516 - y: 0.0344984740457658\r",
      "iteration 517 - y: 0.034415608362031384\r",
      "iteration 518 - y: 0.03433274267829697\r",
      "iteration 519 - y: 0.03424987699456255\r",
      "iteration 520 - y: 0.03416701131082813\r",
      "iteration 521 - y: 0.034084145627093713\r",
      "iteration 522 - y: 0.034001279943359296\r",
      "iteration 523 - y: 0.03391841425962487\r",
      "iteration 524 - y: 0.03383554857589045\r",
      "iteration 525 - y: 0.033752682892156036\r",
      "iteration 526 - y: 0.03366981720842162\r",
      "iteration 527 - y: 0.0335869515246872\r",
      "iteration 528 - y: 0.03350408584095278\r",
      "iteration 529 - y: 0.033421220157218365\r",
      "iteration 530 - y: 0.03333835447348395\r",
      "iteration 531 - y: 0.03325548878974953\r",
      "iteration 532 - y: 0.03317262310601511\r",
      "iteration 533 - y: 0.033089757422280694\r",
      "iteration 534 - y: 0.03300689173854627\r",
      "iteration 535 - y: 0.03292402605481186\r",
      "iteration 536 - y: 0.03284116037107744\r",
      "iteration 537 - y: 0.032758294687343016\r",
      "iteration 538 - y: 0.0326754290036086\r",
      "iteration 539 - y: 0.03259256331987418\r",
      "iteration 540 - y: 0.03250969763613977\r",
      "iteration 541 - y: 0.032426831952405345\r",
      "iteration 542 - y: 0.03234396626867093\r",
      "iteration 543 - y: 0.03226110058493651\r",
      "iteration 544 - y: 0.03217823490120209\r",
      "iteration 545 - y: 0.032095369217467674\r",
      "iteration 546 - y: 0.03201250353373326\r",
      "iteration 547 - y: 0.03192963784999884\r",
      "iteration 548 - y: 0.03184677216626442\r",
      "iteration 549 - y: 0.03176390648253\r",
      "iteration 550 - y: 0.031681040798795586\r",
      "iteration 551 - y: 0.03159817511506116\r",
      "iteration 552 - y: 0.03151530943132674\r",
      "iteration 553 - y: 0.031432443747592326\r",
      "iteration 554 - y: 0.03134957806385791\r",
      "iteration 555 - y: 0.03126671238012349\r",
      "iteration 556 - y: 0.031183846696389076\r",
      "iteration 557 - y: 0.03110098101265466\r",
      "iteration 558 - y: 0.031018115328920237\r",
      "iteration 559 - y: 0.03093524964518582\r",
      "iteration 560 - y: 0.030852383961451405\r",
      "iteration 561 - y: 0.030769518277716987\r",
      "iteration 562 - y: 0.030686652593982566\r",
      "iteration 563 - y: 0.03060378691024815\r",
      "iteration 564 - y: 0.03052092122651373\r",
      "iteration 565 - y: 0.03043805554277932\r",
      "iteration 566 - y: 0.030355189859044895\r",
      "iteration 567 - y: 0.030272324175310478\r",
      "iteration 568 - y: 0.03018945849157606\r",
      "iteration 569 - y: 0.030106592807841646\r",
      "iteration 570 - y: 0.030023727124107228\r",
      "iteration 571 - y: 0.02994086144037281\r",
      "iteration 572 - y: 0.029857995756638393\r",
      "iteration 573 - y: 0.029775130072903975\r",
      "iteration 574 - y: 0.029692264389169557\r",
      "iteration 575 - y: 0.02960939870543514\r",
      "iteration 576 - y: 0.02952653302170072\r",
      "iteration 577 - y: 0.029443667337966304\r",
      "iteration 578 - y: 0.029360801654231886\r",
      "iteration 579 - y: 0.029277935970497465\r",
      "iteration 580 - y: 0.02919507028676305\r",
      "iteration 581 - y: 0.029112204603028633\r",
      "iteration 582 - y: 0.02902933891929422\r",
      "iteration 583 - y: 0.028946473235559794\r",
      "iteration 584 - y: 0.028863607551825377\r",
      "iteration 585 - y: 0.028780741868090966\r",
      "iteration 586 - y: 0.028697876184356548\r",
      "iteration 587 - y: 0.028615010500622127\r",
      "iteration 588 - y: 0.02853214481688771\r",
      "iteration 589 - y: 0.02844927913315329\r",
      "iteration 590 - y: 0.028366413449418877\r",
      "iteration 591 - y: 0.028283547765684456\r",
      "iteration 592 - y: 0.028200682081950038\r",
      "iteration 593 - y: 0.02811781639821562\r",
      "iteration 594 - y: 0.0280349507144812\r",
      "iteration 595 - y: 0.027952085030746785\r",
      "iteration 596 - y: 0.027869219347012367\r",
      "iteration 597 - y: 0.02778635366327795\r",
      "iteration 598 - y: 0.02770348797954353\r",
      "iteration 599 - y: 0.02762062229580911\r",
      "iteration 600 - y: 0.0275377566120747\r",
      "iteration 601 - y: 0.027454890928340282\r",
      "iteration 602 - y: 0.02737202524460586\r",
      "iteration 603 - y: 0.027289159560871443\r",
      "iteration 604 - y: 0.027206293877137026\r",
      "iteration 605 - y: 0.02712342819340261\r",
      "iteration 606 - y: 0.02704056250966819\r",
      "iteration 607 - y: 0.026957696825933773\r",
      "iteration 608 - y: 0.026874831142199355\r",
      "iteration 609 - y: 0.026791965458464937\r",
      "iteration 610 - y: 0.02670909977473052\r",
      "iteration 611 - y: 0.0266262340909961\r",
      "iteration 612 - y: 0.026543368407261684\r",
      "iteration 613 - y: 0.026460502723527263\r",
      "iteration 614 - y: 0.026377637039792845\r",
      "iteration 615 - y: 0.026294771356058434\r",
      "iteration 616 - y: 0.026211905672324017\r",
      "iteration 617 - y: 0.026129039988589592\r",
      "iteration 618 - y: 0.026046174304855174\r",
      "iteration 619 - y: 0.02596330862112076\r",
      "iteration 620 - y: 0.025880442937386346\r",
      "iteration 621 - y: 0.025797577253651924\r",
      "iteration 622 - y: 0.025714711569917507\r",
      "iteration 623 - y: 0.02563184588618309\r",
      "iteration 624 - y: 0.02554898020244867\r",
      "iteration 625 - y: 0.025466114518714254\r",
      "iteration 626 - y: 0.025383248834979836\r",
      "iteration 627 - y: 0.025300383151245418\r",
      "iteration 628 - y: 0.025217517467511\r",
      "iteration 629 - y: 0.025134651783776583\r",
      "iteration 630 - y: 0.025051786100042165\r",
      "iteration 631 - y: 0.024968920416307747\r",
      "iteration 632 - y: 0.024886054732573333\r",
      "iteration 633 - y: 0.024803189048838915\r",
      "iteration 634 - y: 0.02472032336510449\r",
      "iteration 635 - y: 0.02463745768137008\r",
      "iteration 636 - y: 0.024554591997635662\r",
      "iteration 637 - y: 0.024471726313901244\r",
      "iteration 638 - y: 0.024388860630166823\r",
      "iteration 639 - y: 0.024305994946432406\r",
      "iteration 640 - y: 0.02422312926269799\r",
      "iteration 641 - y: 0.024140263578963574\r",
      "iteration 642 - y: 0.024057397895229152\r",
      "iteration 643 - y: 0.023974532211494735\r",
      "iteration 644 - y: 0.023891666527760317\r",
      "iteration 645 - y: 0.023808800844025903\r",
      "iteration 646 - y: 0.02372593516029148\r",
      "iteration 647 - y: 0.023643069476557064\r",
      "iteration 648 - y: 0.023560203792822646\r",
      "iteration 649 - y: 0.023477338109088225\r",
      "iteration 650 - y: 0.023394472425353814\r",
      "iteration 651 - y: 0.023311606741619396\r",
      "iteration 652 - y: 0.02322874105788498\r",
      "iteration 653 - y: 0.023145875374150558\r",
      "iteration 654 - y: 0.02306300969041614\r",
      "iteration 655 - y: 0.022980144006681726\r",
      "iteration 656 - y: 0.022897278322947308\r",
      "iteration 657 - y: 0.022814412639212887\r",
      "iteration 658 - y: 0.02273154695547847\r",
      "iteration 659 - y: 0.02264868127174405\r",
      "iteration 660 - y: 0.022565815588009637\r",
      "iteration 661 - y: 0.022482949904275216\r",
      "iteration 662 - y: 0.022400084220540798\r",
      "iteration 663 - y: 0.02231721853680638\r",
      "iteration 664 - y: 0.022234352853071963\r",
      "iteration 665 - y: 0.022151487169337545\r",
      "iteration 666 - y: 0.02206862148560313\r",
      "iteration 667 - y: 0.021985755801868713\r",
      "iteration 668 - y: 0.021902890118134295\r",
      "iteration 669 - y: 0.021820024434399874\r",
      "iteration 670 - y: 0.02173715875066546\r",
      "iteration 671 - y: 0.021654293066931042\r",
      "iteration 672 - y: 0.021571427383196624\r",
      "iteration 673 - y: 0.021488561699462203\r",
      "iteration 674 - y: 0.021405696015727785\r",
      "iteration 675 - y: 0.02132283033199337\r",
      "iteration 676 - y: 0.021239964648258954\r",
      "iteration 677 - y: 0.021157098964524532\r",
      "iteration 678 - y: 0.021074233280790115\r",
      "iteration 679 - y: 0.020991367597055697\r",
      "iteration 680 - y: 0.02090850191332128\r",
      "iteration 681 - y: 0.02082563622958686\r",
      "iteration 682 - y: 0.020742770545852444\r",
      "iteration 683 - y: 0.020659904862118026\r",
      "iteration 684 - y: 0.02057703917838361\r",
      "iteration 685 - y: 0.02049417349464919\r",
      "iteration 686 - y: 0.020411307810914773\r",
      "iteration 687 - y: 0.020328442127180355\r",
      "iteration 688 - y: 0.020245576443445937\r",
      "iteration 689 - y: 0.02016271075971152\r",
      "iteration 690 - y: 0.020079845075977102\r",
      "iteration 691 - y: 0.019996979392242684\r",
      "iteration 692 - y: 0.019914113708508267\r",
      "iteration 693 - y: 0.01983124802477385\r",
      "iteration 694 - y: 0.01974838234103943\r",
      "iteration 695 - y: 0.01966551665730501\r",
      "iteration 696 - y: 0.019582650973570596\r",
      "iteration 697 - y: 0.019499785289836175\r",
      "iteration 698 - y: 0.01941691960610176\r",
      "iteration 699 - y: 0.01933405392236734\r",
      "iteration 700 - y: 0.01925118823863292\r",
      "iteration 701 - y: 0.019168322554898504\r",
      "iteration 702 - y: 0.019085456871164086\r",
      "iteration 703 - y: 0.019002591187429668\r",
      "iteration 704 - y: 0.01891972550369525\r",
      "iteration 705 - y: 0.018836859819960833\r",
      "iteration 706 - y: 0.018753994136226415\r",
      "iteration 707 - y: 0.018671128452491997\r",
      "iteration 708 - y: 0.01858826276875758\r",
      "iteration 709 - y: 0.01850539708502316\r",
      "iteration 710 - y: 0.018422531401288737\r",
      "iteration 711 - y: 0.018339665717554323\r",
      "iteration 712 - y: 0.018256800033819902\r",
      "iteration 713 - y: 0.018173934350085488\r",
      "iteration 714 - y: 0.018091068666351066\r",
      "iteration 715 - y: 0.01800820298261665\r",
      "iteration 716 - y: 0.01792533729888223\r",
      "iteration 717 - y: 0.017842471615147813\r",
      "iteration 718 - y: 0.017759605931413396\r",
      "iteration 719 - y: 0.01767674024767898\r",
      "iteration 720 - y: 0.017593874563944564\r",
      "iteration 721 - y: 0.017511008880210146\r",
      "iteration 722 - y: 0.017428143196475728\r",
      "iteration 723 - y: 0.01734527751274131\r",
      "iteration 724 - y: 0.017262411829006893\r",
      "iteration 725 - y: 0.017179546145272475\r",
      "iteration 726 - y: 0.017096680461538057\r",
      "iteration 727 - y: 0.01701381477780364\r",
      "iteration 728 - y: 0.016930949094069222\r",
      "iteration 729 - y: 0.016848083410334804\r",
      "iteration 730 - y: 0.016765217726600383\r",
      "iteration 731 - y: 0.01668235204286597\r",
      "iteration 732 - y: 0.016599486359131548\r",
      "iteration 733 - y: 0.01651662067539713\r",
      "iteration 734 - y: 0.016433754991662712\r",
      "iteration 735 - y: 0.016350889307928294\r",
      "iteration 736 - y: 0.016268023624193877\r",
      "iteration 737 - y: 0.01618515794045946\r",
      "iteration 738 - y: 0.01610229225672504\r",
      "iteration 739 - y: 0.016019426572990624\r",
      "iteration 740 - y: 0.015936560889256206\r",
      "iteration 741 - y: 0.015853695205521788\r",
      "iteration 742 - y: 0.01577082952178737\r",
      "iteration 743 - y: 0.015687963838052953\r",
      "iteration 744 - y: 0.015605098154318535\r",
      "iteration 745 - y: 0.015522232470584117\r",
      "iteration 746 - y: 0.0154393667868497\r",
      "iteration 747 - y: 0.01535650110311528\r",
      "iteration 748 - y: 0.015273635419380864\r",
      "iteration 749 - y: 0.015190769735646445\r",
      "iteration 750 - y: 0.015107904051912029\r",
      "iteration 751 - y: 0.01502503836817761\r",
      "iteration 752 - y: 0.01494217268444319\r",
      "iteration 753 - y: 0.01487708888440927\r",
      "iteration 754 - y: 0.014824217777844269\r",
      "iteration 755 - y: 0.014771346671279271\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 756 - y: 0.01471847556471427\r",
      "iteration 757 - y: 0.01466560445814927\r",
      "iteration 758 - y: 0.014612733351584271\r",
      "iteration 759 - y: 0.014559862245019272\r",
      "iteration 760 - y: 0.014506991138454273\r",
      "iteration 761 - y: 0.014454120031889273\r",
      "iteration 762 - y: 0.014401248925324272\r",
      "iteration 763 - y: 0.014348377818759275\r",
      "iteration 764 - y: 0.014295506712194274\r",
      "iteration 765 - y: 0.014242635605629276\r",
      "iteration 766 - y: 0.014189764499064275\r",
      "iteration 767 - y: 0.014136893392499275\r",
      "iteration 768 - y: 0.014084022285934276\r",
      "iteration 769 - y: 0.014031151179369277\r",
      "iteration 770 - y: 0.013978280072804277\r",
      "iteration 771 - y: 0.013925408966239278\r",
      "iteration 772 - y: 0.013872537859674277\r",
      "iteration 773 - y: 0.01381966675310928\r",
      "iteration 774 - y: 0.013766795646544278\r",
      "iteration 775 - y: 0.01371392453997928\r",
      "iteration 776 - y: 0.01366105343341428\r",
      "iteration 777 - y: 0.01360818232684928\r",
      "iteration 778 - y: 0.01355531122028428\r",
      "iteration 779 - y: 0.013502440113719281\r",
      "iteration 780 - y: 0.013449569007154282\r",
      "iteration 781 - y: 0.013396697900589283\r",
      "iteration 782 - y: 0.013343826794024282\r",
      "iteration 783 - y: 0.013290955687459284\r",
      "iteration 784 - y: 0.013238084580894283\r",
      "iteration 785 - y: 0.013185213474329285\r",
      "iteration 786 - y: 0.013132342367764284\r",
      "iteration 787 - y: 0.013079471261199283\r",
      "iteration 788 - y: 0.013026600154634286\r",
      "iteration 789 - y: 0.012973729048069284\r",
      "iteration 790 - y: 0.012920857941504287\r",
      "iteration 791 - y: 0.012867986834939286\r",
      "iteration 792 - y: 0.012815115728374285\r",
      "iteration 793 - y: 0.012762244621809287\r",
      "iteration 794 - y: 0.012709373515244286\r",
      "iteration 795 - y: 0.012656502408679288\r",
      "iteration 796 - y: 0.012603631302114287\r",
      "iteration 797 - y: 0.012550760195549286\r",
      "iteration 798 - y: 0.012497889088984289\r",
      "iteration 799 - y: 0.012445017982419287\r",
      "iteration 800 - y: 0.01239214687585429\r",
      "iteration 801 - y: 0.012339275769289289\r",
      "iteration 802 - y: 0.012286404662724288\r",
      "iteration 803 - y: 0.01223353355615929\r",
      "iteration 804 - y: 0.012180662449594289\r",
      "iteration 805 - y: 0.012127791343029291\r",
      "iteration 806 - y: 0.012074920236464292\r",
      "iteration 807 - y: 0.012022049129899293\r",
      "iteration 808 - y: 0.011969178023334295\r",
      "iteration 809 - y: 0.011916306916769296\r",
      "iteration 810 - y: 0.011863435810204298\r",
      "iteration 811 - y: 0.011810564703639299\r",
      "iteration 812 - y: 0.0117576935970743\r",
      "iteration 813 - y: 0.011704822490509303\r",
      "iteration 814 - y: 0.011651951383944304\r",
      "iteration 815 - y: 0.011599080277379306\r",
      "iteration 816 - y: 0.011546209170814307\r",
      "iteration 817 - y: 0.01149333806424931\r",
      "iteration 818 - y: 0.01144046695768431\r",
      "iteration 819 - y: 0.01138759585111931\r",
      "iteration 820 - y: 0.011334724744554313\r",
      "iteration 821 - y: 0.011281853637989314\r",
      "iteration 822 - y: 0.011228982531424316\r",
      "iteration 823 - y: 0.011176111424859319\r",
      "iteration 824 - y: 0.01112324031829432\r",
      "iteration 825 - y: 0.011070369211729322\r",
      "iteration 826 - y: 0.011017498105164322\r",
      "iteration 827 - y: 0.010964626998599325\r",
      "iteration 828 - y: 0.010911755892034325\r",
      "iteration 829 - y: 0.010858884785469328\r",
      "iteration 830 - y: 0.010806013678904328\r",
      "iteration 831 - y: 0.010753142572339332\r",
      "iteration 832 - y: 0.010700271465774333\r",
      "iteration 833 - y: 0.010647400359209334\r",
      "iteration 834 - y: 0.010594529252644336\r",
      "iteration 835 - y: 0.010541658146079337\r",
      "iteration 836 - y: 0.01048878703951434\r",
      "iteration 837 - y: 0.01043591593294934\r",
      "iteration 838 - y: 0.010383044826384342\r",
      "iteration 839 - y: 0.010330173719819343\r",
      "iteration 840 - y: 0.010277302613254347\r",
      "iteration 841 - y: 0.010224431506689348\r",
      "iteration 842 - y: 0.010171560400124348\r",
      "iteration 843 - y: 0.01011868929355935\r",
      "iteration 844 - y: 0.010065818186994351\r",
      "iteration 845 - y: 0.010012947080429354\r",
      "iteration 846 - y: 0.009960075973864354\r",
      "iteration 847 - y: 0.009907204867299357\r",
      "iteration 848 - y: 0.009854333760734357\r",
      "iteration 849 - y: 0.00980146265416936\r",
      "iteration 850 - y: 0.009748591547604362\r",
      "iteration 851 - y: 0.009695720441039363\r",
      "iteration 852 - y: 0.009642849334474365\r",
      "iteration 853 - y: 0.009589978227909366\r",
      "iteration 854 - y: 0.009537107121344368\r",
      "iteration 855 - y: 0.009484236014779369\r",
      "iteration 856 - y: 0.00943136490821437\r",
      "iteration 857 - y: 0.009378493801649372\r",
      "iteration 858 - y: 0.009325622695084374\r",
      "iteration 859 - y: 0.009272751588519377\r",
      "iteration 860 - y: 0.009219880481954377\r",
      "iteration 861 - y: 0.00916700937538938\r",
      "iteration 862 - y: 0.00911413826882438\r",
      "iteration 863 - y: 0.009069624518281662\r",
      "iteration 864 - y: 0.009034087951904976\r",
      "iteration 865 - y: 0.00899855138552829\r",
      "iteration 866 - y: 0.008963014819151606\r",
      "iteration 867 - y: 0.00892747825277492\r",
      "iteration 868 - y: 0.008891941686398234\r",
      "iteration 869 - y: 0.008856405120021548\r",
      "iteration 870 - y: 0.008820868553644862\r",
      "iteration 871 - y: 0.008785331987268176\r",
      "iteration 872 - y: 0.00874979542089149\r",
      "iteration 873 - y: 0.008714258854514803\r",
      "iteration 874 - y: 0.008678722288138117\r",
      "iteration 875 - y: 0.008643185721761433\r",
      "iteration 876 - y: 0.008607649155384747\r",
      "iteration 877 - y: 0.00857211258900806\r",
      "iteration 878 - y: 0.008536576022631375\r",
      "iteration 879 - y: 0.008501039456254689\r",
      "iteration 880 - y: 0.008465502889878003\r",
      "iteration 881 - y: 0.008429966323501317\r",
      "iteration 882 - y: 0.00839442975712463\r",
      "iteration 883 - y: 0.008358893190747944\r",
      "iteration 884 - y: 0.00832335662437126\r",
      "iteration 885 - y: 0.008287820057994574\r",
      "iteration 886 - y: 0.008252283491617888\r",
      "iteration 887 - y: 0.008216746925241202\r",
      "iteration 888 - y: 0.008181210358864516\r",
      "iteration 889 - y: 0.00814567379248783\r",
      "iteration 890 - y: 0.008110137226111144\r",
      "iteration 891 - y: 0.008074600659734458\r",
      "iteration 892 - y: 0.008039064093357771\r",
      "iteration 893 - y: 0.008003527526981087\r",
      "iteration 894 - y: 0.007967990960604401\r",
      "iteration 895 - y: 0.007932454394227715\r",
      "iteration 896 - y: 0.007896917827851029\r",
      "iteration 897 - y: 0.007861381261474343\r",
      "iteration 898 - y: 0.007825844695097657\r",
      "iteration 899 - y: 0.007790308128720971\r",
      "iteration 900 - y: 0.007754771562344285\r",
      "iteration 901 - y: 0.007719234995967599\r",
      "iteration 902 - y: 0.007683698429590913\r",
      "iteration 903 - y: 0.007648161863214227\r",
      "iteration 904 - y: 0.007612625296837542\r",
      "iteration 905 - y: 0.007577088730460856\r",
      "iteration 906 - y: 0.00754155216408417\r",
      "iteration 907 - y: 0.007506015597707484\r",
      "iteration 908 - y: 0.007470479031330798\r",
      "iteration 909 - y: 0.0074349424649541125\r",
      "iteration 910 - y: 0.007399405898577426\r",
      "iteration 911 - y: 0.00736386933220074\r",
      "iteration 912 - y: 0.007328332765824054\r",
      "iteration 913 - y: 0.007292796199447369\r",
      "iteration 914 - y: 0.007257259633070683\r",
      "iteration 915 - y: 0.007221723066693997\r",
      "iteration 916 - y: 0.007186186500317311\r",
      "iteration 917 - y: 0.007150649933940626\r",
      "iteration 918 - y: 0.0071151133675639395\r",
      "iteration 919 - y: 0.0070795768011872534\r",
      "iteration 920 - y: 0.007044040234810567\r",
      "iteration 921 - y: 0.007008503668433881\r",
      "iteration 922 - y: 0.006972967102057196\r",
      "iteration 923 - y: 0.00693743053568051\r",
      "iteration 924 - y: 0.006901893969303824\r",
      "iteration 925 - y: 0.006866357402927138\r",
      "iteration 926 - y: 0.006830820836550453\r",
      "iteration 927 - y: 0.0067952842701737666\r",
      "iteration 928 - y: 0.0067597477037970805\r",
      "iteration 929 - y: 0.006724211137420394\r",
      "iteration 930 - y: 0.006688674571043708\r",
      "iteration 931 - y: 0.006653138004667023\r",
      "iteration 932 - y: 0.006617601438290337\r",
      "iteration 933 - y: 0.006582064871913651\r",
      "iteration 934 - y: 0.006546528305536965\r",
      "iteration 935 - y: 0.00651099173916028\r",
      "iteration 936 - y: 0.006475455172783594\r",
      "iteration 937 - y: 0.0064399186064069075\r",
      "iteration 938 - y: 0.0064043820400302215\r",
      "iteration 939 - y: 0.006368845473653535\r",
      "iteration 940 - y: 0.00633330890727685\r",
      "iteration 941 - y: 0.006297772340900164\r",
      "iteration 942 - y: 0.006262235774523478\r",
      "iteration 943 - y: 0.006226699208146792\r",
      "iteration 944 - y: 0.006191162641770107\r",
      "iteration 945 - y: 0.006155626075393421\r",
      "iteration 946 - y: 0.006120089509016735\r",
      "iteration 947 - y: 0.0060845529426400485\r",
      "iteration 948 - y: 0.006049016376263362\r",
      "iteration 949 - y: 0.006013479809886677\r",
      "iteration 950 - y: 0.005977943243509991\r",
      "iteration 951 - y: 0.005942406677133305\r",
      "iteration 952 - y: 0.005906870110756619\r",
      "iteration 953 - y: 0.005871333544379934\r",
      "iteration 954 - y: 0.005835796978003248\r",
      "iteration 955 - y: 0.005800260411626562\r",
      "iteration 956 - y: 0.0057647238452498755\r",
      "iteration 957 - y: 0.00572918727887319\r",
      "iteration 958 - y: 0.005693650712496504\r",
      "iteration 959 - y: 0.005658114146119818\r",
      "iteration 960 - y: 0.005622577579743132\r",
      "iteration 961 - y: 0.005587041013366446\r",
      "iteration 962 - y: 0.005551504446989761\r",
      "iteration 963 - y: 0.005515967880613075\r",
      "iteration 964 - y: 0.005480431314236389\r",
      "iteration 965 - y: 0.005444894747859703\r",
      "iteration 966 - y: 0.005409358181483017\r",
      "iteration 967 - y: 0.005373821615106331\r",
      "iteration 968 - y: 0.005338285048729645\r",
      "iteration 969 - y: 0.005302748482352959\r",
      "iteration 970 - y: 0.005267211915976273\r",
      "iteration 971 - y: 0.005231675349599588\r",
      "iteration 972 - y: 0.005196138783222902\r",
      "iteration 973 - y: 0.005160602216846216\r",
      "iteration 974 - y: 0.00512506565046953\r",
      "iteration 975 - y: 0.005089529084092844\r",
      "iteration 976 - y: 0.005053992517716158\r",
      "iteration 977 - y: 0.005018455951339472\r",
      "iteration 978 - y: 0.004982919384962786\r",
      "iteration 979 - y: 0.0049473828185861\r",
      "iteration 980 - y: 0.004911846252209415\r",
      "iteration 981 - y: 0.004876309685832729\r",
      "iteration 982 - y: 0.004840773119456043\r",
      "iteration 983 - y: 0.004805236553079357\r",
      "iteration 984 - y: 0.0047696999867026715\r",
      "iteration 985 - y: 0.004734163420325985\r",
      "iteration 986 - y: 0.004698626853949299\r",
      "iteration 987 - y: 0.004663090287572613\r",
      "iteration 988 - y: 0.004627553721195928\r",
      "iteration 989 - y: 0.004592017154819242\r",
      "iteration 990 - y: 0.004556480588442556\r",
      "iteration 991 - y: 0.00452094402206587\r",
      "iteration 992 - y: 0.004485407455689184\r",
      "iteration 993 - y: 0.0044498708893124985\r",
      "iteration 994 - y: 0.004414334322935812\r",
      "iteration 995 - y: 0.004378797756559126\r",
      "iteration 996 - y: 0.00434326119018244\r",
      "iteration 997 - y: 0.004307724623805755\r",
      "iteration 998 - y: 0.004272188057429069\r",
      "iteration 999 - y: 0.004236651491052383\r",
      "iteration 1000 - y: 0.004201114924675697\r",
      "iteration 1001 - y: 0.004165578358299011\r",
      "iteration 1002 - y: 0.0041300417919223256\r",
      "iteration 1003 - y: 0.0040945052255456395\r",
      "iteration 1004 - y: 0.004058968659168953\r",
      "iteration 1005 - y: 0.004023432092792267\r",
      "iteration 1006 - y: 0.003987895526415582\r",
      "iteration 1007 - y: 0.003952358960038896\r",
      "iteration 1008 - y: 0.00391682239366221\r",
      "iteration 1009 - y: 0.0038812858272855243\r",
      "iteration 1010 - y: 0.0038457492609088382\r",
      "iteration 1011 - y: 0.003810212694532152\r",
      "iteration 1012 - y: 0.0037746761281554665\r",
      "iteration 1013 - y: 0.0037391395617787804\r",
      "iteration 1014 - y: 0.003703602995402095\r",
      "iteration 1015 - y: 0.0036680664290254087\r",
      "iteration 1016 - y: 0.003632529862648723\r",
      "iteration 1017 - y: 0.003596993296272037\r",
      "iteration 1018 - y: 0.0035614567298953514\r",
      "iteration 1019 - y: 0.0035259201635186653\r",
      "iteration 1020 - y: 0.003490383597141979\r",
      "iteration 1021 - y: 0.0034548470307652936\r",
      "iteration 1022 - y: 0.0034193104643886075\r",
      "iteration 1023 - y: 0.003383773898011922\r",
      "iteration 1024 - y: 0.0033482373316352358\r",
      "iteration 1025 - y: 0.00331270076525855\r",
      "iteration 1026 - y: 0.003277164198881864\r",
      "iteration 1027 - y: 0.0032416276325051784\r",
      "iteration 1028 - y: 0.0032060910661284923\r",
      "iteration 1029 - y: 0.0031705544997518067\r",
      "iteration 1030 - y: 0.0031350179333751206\r",
      "iteration 1031 - y: 0.0030994813669984345\r",
      "iteration 1032 - y: 0.003063944800621749\r",
      "iteration 1033 - y: 0.003028408234245063\r",
      "iteration 1034 - y: 0.002992871667868377\r",
      "iteration 1035 - y: 0.002957335101491691\r",
      "iteration 1036 - y: 0.0029217985351150054\r",
      "iteration 1037 - y: 0.0028862619687383194\r",
      "iteration 1038 - y: 0.0028507254023616337\r",
      "iteration 1039 - y: 0.0028151888359849477\r",
      "iteration 1040 - y: 0.0027796522696082616\r",
      "iteration 1041 - y: 0.0027441157032315755\r",
      "iteration 1042 - y: 0.0027085791368548894\r",
      "iteration 1043 - y: 0.0026730425704782033\r",
      "iteration 1044 - y: 0.002637506004101517\r",
      "iteration 1045 - y: 0.0026019694377248308\r",
      "iteration 1046 - y: 0.0025664328713481447\r",
      "iteration 1047 - y: 0.0025308963049714586\r",
      "iteration 1048 - y: 0.002495359738594772\r",
      "iteration 1049 - y: 0.002459823172218086\r",
      "iteration 1050 - y: 0.0024242866058414\r",
      "iteration 1051 - y: 0.002388750039464714\r",
      "iteration 1052 - y: 0.0023532134730880274\r",
      "iteration 1053 - y: 0.0023176769067113413\r",
      "iteration 1054 - y: 0.002282140340334655\r",
      "iteration 1055 - y: 0.002246603773957969\r",
      "iteration 1056 - y: 0.0022110672075812826\r",
      "iteration 1057 - y: 0.0021755306412045965\r",
      "iteration 1058 - y: 0.0021399940748279105\r",
      "iteration 1059 - y: 0.0021044575084512244\r",
      "iteration 1060 - y: 0.002068920942074538\r",
      "iteration 1061 - y: 0.002033384375697852\r",
      "iteration 1062 - y: 0.0019978478093211657\r",
      "iteration 1063 - y: 0.0019623112429444796\r",
      "iteration 1064 - y: 0.0019267746765677933\r",
      "iteration 1065 - y: 0.001891238110191107\r",
      "iteration 1066 - y: 0.0018557015438144208\r",
      "iteration 1067 - y: 0.0018201649774377347\r",
      "iteration 1068 - y: 0.0017846284110610484\r",
      "iteration 1069 - y: 0.0017490918446843623\r",
      "iteration 1070 - y: 0.001713555278307676\r",
      "iteration 1071 - y: 0.00167801871193099\r",
      "iteration 1072 - y: 0.0016424821455543036\r",
      "iteration 1073 - y: 0.0016069455791776176\r",
      "iteration 1074 - y: 0.0015714090128009313\r",
      "iteration 1075 - y: 0.0015358724464242452\r",
      "iteration 1076 - y: 0.001500335880047559\r",
      "iteration 1077 - y: 0.0014647993136708728\r",
      "iteration 1078 - y: 0.0014292627472941865\r",
      "iteration 1079 - y: 0.0013937261809175005\r",
      "iteration 1080 - y: 0.0013581896145408144\r",
      "iteration 1081 - y: 0.0013226530481641283\r",
      "iteration 1082 - y: 0.0012871164817874422\r",
      "iteration 1083 - y: 0.0012515799154107562\r",
      "iteration 1084 - y: 0.0012160433490340703\r",
      "iteration 1085 - y: 0.0011805067826573842\r",
      "iteration 1086 - y: 0.0011449702162806981\r",
      "iteration 1087 - y: 0.001109433649904012\r",
      "iteration 1088 - y: 0.001073897083527326\r",
      "iteration 1089 - y: 0.0010383605171506401\r",
      "iteration 1090 - y: 0.001002823950773954\r",
      "iteration 1091 - y: 0.000967287384397268\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-2.7527916e-05]], dtype=float32),\n",
       " array([[-3.270607e-05]], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.001}\n",
    "X = XAIR(best_model, 'lrp.z', 'letzgus', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac0638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.06423315554287472\r",
      "iteration 2 - y: 0.06420611680325305\r",
      "iteration 3 - y: 0.06417907806363143\r",
      "iteration 4 - y: 0.06415203932400979\r",
      "iteration 5 - y: 0.06412500058438814\r",
      "iteration 6 - y: 0.0640979618447665\r",
      "iteration 7 - y: 0.06407092310514485\r",
      "iteration 8 - y: 0.06404388436552322\r",
      "iteration 9 - y: 0.06401684562590157\r",
      "iteration 10 - y: 0.06398980688627993\r",
      "iteration 11 - y: 0.0639627681466583\r",
      "iteration 12 - y: 0.06393572940703664\r",
      "iteration 13 - y: 0.063908690667415\r",
      "iteration 14 - y: 0.06388165192779337\r",
      "iteration 15 - y: 0.06385461318817172\r",
      "iteration 16 - y: 0.06382757444855008\r",
      "iteration 17 - y: 0.06380053570892844\r",
      "iteration 18 - y: 0.0637734969693068\r",
      "iteration 19 - y: 0.06374645822968515\r",
      "iteration 20 - y: 0.06371941949006352\r",
      "iteration 21 - y: 0.06369238075044187\r",
      "iteration 22 - y: 0.06366534201082022\r",
      "iteration 23 - y: 0.06363830327119858\r",
      "iteration 24 - y: 0.06361126453157695\r",
      "iteration 25 - y: 0.0635842257919553\r",
      "iteration 26 - y: 0.06355718705233365\r",
      "iteration 27 - y: 0.06353014831271202\r",
      "iteration 28 - y: 0.06350310957309038\r",
      "iteration 29 - y: 0.06347607083346873\r",
      "iteration 30 - y: 0.0634490320938471\r",
      "iteration 31 - y: 0.06342199335422545\r",
      "iteration 32 - y: 0.06339495461460382\r",
      "iteration 33 - y: 0.06336791587498217\r",
      "iteration 34 - y: 0.06334087713536052\r",
      "iteration 35 - y: 0.06331383839573888\r",
      "iteration 36 - y: 0.06328679965611723\r",
      "iteration 37 - y: 0.0632597609164956\r",
      "iteration 38 - y: 0.06323272217687397\r",
      "iteration 39 - y: 0.06320568343725232\r",
      "iteration 40 - y: 0.06317864469763068\r",
      "iteration 41 - y: 0.06315160595800903\r",
      "iteration 42 - y: 0.06312456721838738\r",
      "iteration 43 - y: 0.06309752847876575\r",
      "iteration 44 - y: 0.0630704897391441\r",
      "iteration 45 - y: 0.06304345099952247\r",
      "iteration 46 - y: 0.06301641225990083\r",
      "iteration 47 - y: 0.06298937352027918\r",
      "iteration 48 - y: 0.06296233478065755\r",
      "iteration 49 - y: 0.06293529604103588\r",
      "iteration 50 - y: 0.06290825730141425\r",
      "iteration 51 - y: 0.06288121856179262\r",
      "iteration 52 - y: 0.06285417982217098\r",
      "iteration 53 - y: 0.06282714108254933\r",
      "iteration 54 - y: 0.06280010234292768\r",
      "iteration 55 - y: 0.06277306360330605\r",
      "iteration 56 - y: 0.06274602486368441\r",
      "iteration 57 - y: 0.06271898612406276\r",
      "iteration 58 - y: 0.06269194738444112\r",
      "iteration 59 - y: 0.06266490864481948\r",
      "iteration 60 - y: 0.06263786990519785\r",
      "iteration 61 - y: 0.0626108311655762\r",
      "iteration 62 - y: 0.06258379242595455\r",
      "iteration 63 - y: 0.06255675368633291\r",
      "iteration 64 - y: 0.06252971494671127\r",
      "iteration 65 - y: 0.06250267620708963\r",
      "iteration 66 - y: 0.06247563746746799\r",
      "iteration 67 - y: 0.06244859872784635\r",
      "iteration 68 - y: 0.062421559988224705\r",
      "iteration 69 - y: 0.062394521248603056\r",
      "iteration 70 - y: 0.06236748250898143\r",
      "iteration 71 - y: 0.06234044376935979\r",
      "iteration 72 - y: 0.062313405029738145\r",
      "iteration 73 - y: 0.062286366290116496\r",
      "iteration 74 - y: 0.062259327550494854\r",
      "iteration 75 - y: 0.06223228881087321\r",
      "iteration 76 - y: 0.06220525007125158\r",
      "iteration 77 - y: 0.06217821133162993\r",
      "iteration 78 - y: 0.062151172592008294\r",
      "iteration 79 - y: 0.062124133852386645\r",
      "iteration 80 - y: 0.06209709511276501\r",
      "iteration 81 - y: 0.062070056373143376\r",
      "iteration 82 - y: 0.06204301763352173\r",
      "iteration 83 - y: 0.06201597889390008\r",
      "iteration 84 - y: 0.06198894015427844\r",
      "iteration 85 - y: 0.06196190141465681\r",
      "iteration 86 - y: 0.06193486267503516\r",
      "iteration 87 - y: 0.06190782393541352\r",
      "iteration 88 - y: 0.061880785195791876\r",
      "iteration 89 - y: 0.061853746456170235\r",
      "iteration 90 - y: 0.06182670771654859\r",
      "iteration 91 - y: 0.06179966897692696\r",
      "iteration 92 - y: 0.06177263023730531\r",
      "iteration 93 - y: 0.061745591497683674\r",
      "iteration 94 - y: 0.061718552758062026\r",
      "iteration 95 - y: 0.061691514018440384\r",
      "iteration 96 - y: 0.061664475278818756\r",
      "iteration 97 - y: 0.0616374365391971\r",
      "iteration 98 - y: 0.06161039779957546\r",
      "iteration 99 - y: 0.06158335905995381\r",
      "iteration 100 - y: 0.061556320320332175\r",
      "iteration 101 - y: 0.06152928158071054\r",
      "iteration 102 - y: 0.06150224284108889\r",
      "iteration 103 - y: 0.06147520410146725\r",
      "iteration 104 - y: 0.06144816536184561\r",
      "iteration 105 - y: 0.061421126622223966\r",
      "iteration 106 - y: 0.061394087882602325\r",
      "iteration 107 - y: 0.061367049142980676\r",
      "iteration 108 - y: 0.06134001040335905\r",
      "iteration 109 - y: 0.061312971663737406\r",
      "iteration 110 - y: 0.06128593292411576\r",
      "iteration 111 - y: 0.061258894184494116\r",
      "iteration 112 - y: 0.061231855444872474\r",
      "iteration 113 - y: 0.06120481670525084\r",
      "iteration 114 - y: 0.06117777796562919\r",
      "iteration 115 - y: 0.06115073922600754\r",
      "iteration 116 - y: 0.06112370048638591\r",
      "iteration 117 - y: 0.06109666174676427\r",
      "iteration 118 - y: 0.06106962300714263\r",
      "iteration 119 - y: 0.06104258426752098\r",
      "iteration 120 - y: 0.06101554552789934\r",
      "iteration 121 - y: 0.060988506788277705\r",
      "iteration 122 - y: 0.060961468048656056\r",
      "iteration 123 - y: 0.06093442930903442\r",
      "iteration 124 - y: 0.06090739056941278\r",
      "iteration 125 - y: 0.06088035182979114\r",
      "iteration 126 - y: 0.060853313090169496\r",
      "iteration 127 - y: 0.06082627435054785\r",
      "iteration 128 - y: 0.06079923561092622\r",
      "iteration 129 - y: 0.06077219687130457\r",
      "iteration 130 - y: 0.06074515813168292\r",
      "iteration 131 - y: 0.06071811939206129\r",
      "iteration 132 - y: 0.060691080652439645\r",
      "iteration 133 - y: 0.06060948511515861\r",
      "iteration 134 - y: 0.06052661943142418\r",
      "iteration 135 - y: 0.06044375374768976\r",
      "iteration 136 - y: 0.06036088806395535\r",
      "iteration 137 - y: 0.060278022380220925\r",
      "iteration 138 - y: 0.06019515669648651\r",
      "iteration 139 - y: 0.06011229101275208\r",
      "iteration 140 - y: 0.060029425329017665\r",
      "iteration 141 - y: 0.059946559645283254\r",
      "iteration 142 - y: 0.05986369396154882\r",
      "iteration 143 - y: 0.05978082827781441\r",
      "iteration 144 - y: 0.05969796259407999\r",
      "iteration 145 - y: 0.05961509691034557\r",
      "iteration 146 - y: 0.05953223122661115\r",
      "iteration 147 - y: 0.05944936554287673\r",
      "iteration 148 - y: 0.05936649985914231\r",
      "iteration 149 - y: 0.05928363417540789\r",
      "iteration 150 - y: 0.059200768491673474\r",
      "iteration 151 - y: 0.05911790280793906\r",
      "iteration 152 - y: 0.05903503712420463\r",
      "iteration 153 - y: 0.058952171440470214\r",
      "iteration 154 - y: 0.058869305756735796\r",
      "iteration 155 - y: 0.05878644007300138\r",
      "iteration 156 - y: 0.05870357438926695\r",
      "iteration 157 - y: 0.058620708705532536\r",
      "iteration 158 - y: 0.05853784302179812\r",
      "iteration 159 - y: 0.05845497733806369\r",
      "iteration 160 - y: 0.05837211165432928\r",
      "iteration 161 - y: 0.05828924597059486\r",
      "iteration 162 - y: 0.05820638028686044\r",
      "iteration 163 - y: 0.05812351460312602\r",
      "iteration 164 - y: 0.0580406489193916\r",
      "iteration 165 - y: 0.05795778323565719\r",
      "iteration 166 - y: 0.05787491755192276\r",
      "iteration 167 - y: 0.05779205186818834\r",
      "iteration 168 - y: 0.05770918618445393\r",
      "iteration 169 - y: 0.0576263205007195\r",
      "iteration 170 - y: 0.05754345481698509\r",
      "iteration 171 - y: 0.05746058913325067\r",
      "iteration 172 - y: 0.05737772344951625\r",
      "iteration 173 - y: 0.05729485776578183\r",
      "iteration 174 - y: 0.05721199208204741\r",
      "iteration 175 - y: 0.057129126398312996\r",
      "iteration 176 - y: 0.05704626071457857\r",
      "iteration 177 - y: 0.05696339503084415\r",
      "iteration 178 - y: 0.056880529347109736\r",
      "iteration 179 - y: 0.05679766366337531\r",
      "iteration 180 - y: 0.056714797979640894\r",
      "iteration 181 - y: 0.056631932295906476\r",
      "iteration 182 - y: 0.05654906661217205\r",
      "iteration 183 - y: 0.05646620092843764\r",
      "iteration 184 - y: 0.056383335244703216\r",
      "iteration 185 - y: 0.0563004695609688\r",
      "iteration 186 - y: 0.05621760387723438\r",
      "iteration 187 - y: 0.056134738193499956\r",
      "iteration 188 - y: 0.05605187250976554\r",
      "iteration 189 - y: 0.05596900682603111\r",
      "iteration 190 - y: 0.055886141142296696\r",
      "iteration 191 - y: 0.055803275458562285\r",
      "iteration 192 - y: 0.05572040977482786\r",
      "iteration 193 - y: 0.05563754409109345\r",
      "iteration 194 - y: 0.05555467840735902\r",
      "iteration 195 - y: 0.0554718127236246\r",
      "iteration 196 - y: 0.05538894703989018\r",
      "iteration 197 - y: 0.055306081356155765\r",
      "iteration 198 - y: 0.05522321567242135\r",
      "iteration 199 - y: 0.055140349988686936\r",
      "iteration 200 - y: 0.055057484304952505\r",
      "iteration 201 - y: 0.054974618621218094\r",
      "iteration 202 - y: 0.05489175293748367\r",
      "iteration 203 - y: 0.054808887253749244\r",
      "iteration 204 - y: 0.05472602157001483\r",
      "iteration 205 - y: 0.05464315588628041\r",
      "iteration 206 - y: 0.05456029020254599\r",
      "iteration 207 - y: 0.054477424518811574\r",
      "iteration 208 - y: 0.05439455883507715\r",
      "iteration 209 - y: 0.05431169315134274\r",
      "iteration 210 - y: 0.05422882746760831\r",
      "iteration 211 - y: 0.054145961783873896\r",
      "iteration 212 - y: 0.05406309610013947\r",
      "iteration 213 - y: 0.05398023041640505\r",
      "iteration 214 - y: 0.053897364732670636\r",
      "iteration 215 - y: 0.05381449904893622\r",
      "iteration 216 - y: 0.0537316333652018\r",
      "iteration 217 - y: 0.05364876768146738\r",
      "iteration 218 - y: 0.05356590199773296\r",
      "iteration 219 - y: 0.05348303631399854\r",
      "iteration 220 - y: 0.05340017063026412\r",
      "iteration 221 - y: 0.053317304946529705\r",
      "iteration 222 - y: 0.05323443926279528\r",
      "iteration 223 - y: 0.05315157357906086\r",
      "iteration 224 - y: 0.053068707895326445\r",
      "iteration 225 - y: 0.05298584221159203\r",
      "iteration 226 - y: 0.05290297652785761\r",
      "iteration 227 - y: 0.052820110844123185\r",
      "iteration 228 - y: 0.05273724516038877\r",
      "iteration 229 - y: 0.05265437947665435\r",
      "iteration 230 - y: 0.052571513792919924\r",
      "iteration 231 - y: 0.052488648109185514\r",
      "iteration 232 - y: 0.05240578242545108\r",
      "iteration 233 - y: 0.05232291674171667\r",
      "iteration 234 - y: 0.052240051057982254\r",
      "iteration 235 - y: 0.05215718537424783\r",
      "iteration 236 - y: 0.05207431969051342\r",
      "iteration 237 - y: 0.05199145400677899\r",
      "iteration 238 - y: 0.05190858832304457\r",
      "iteration 239 - y: 0.05182572263931016\r",
      "iteration 240 - y: 0.05174285695557573\r",
      "iteration 241 - y: 0.051659991271841316\r",
      "iteration 242 - y: 0.05157712558810689\r",
      "iteration 243 - y: 0.05149425990437248\r",
      "iteration 244 - y: 0.05141139422063806\r",
      "iteration 245 - y: 0.05132852853690363\r",
      "iteration 246 - y: 0.05124566285316921\r",
      "iteration 247 - y: 0.051162797169434795\r",
      "iteration 248 - y: 0.05107993148570038\r",
      "iteration 249 - y: 0.05099706580196596\r",
      "iteration 250 - y: 0.050914200118231535\r",
      "iteration 251 - y: 0.050831334434497125\r",
      "iteration 252 - y: 0.05074846875076271\r",
      "iteration 253 - y: 0.05066560306702828\r",
      "iteration 254 - y: 0.05058273738329386\r",
      "iteration 255 - y: 0.05049987169955944\r",
      "iteration 256 - y: 0.05041700601582502\r",
      "iteration 257 - y: 0.05033414033209061\r",
      "iteration 258 - y: 0.05025127464835619\r",
      "iteration 259 - y: 0.05016840896462177\r",
      "iteration 260 - y: 0.050085543280887344\r",
      "iteration 261 - y: 0.05000267759715293\r",
      "iteration 262 - y: 0.049919811913418516\r",
      "iteration 263 - y: 0.04983694622968409\r",
      "iteration 264 - y: 0.04975408054594967\r",
      "iteration 265 - y: 0.04967121486221525\r",
      "iteration 266 - y: 0.04958834917848083\r",
      "iteration 267 - y: 0.04950548349474641\r",
      "iteration 268 - y: 0.049422617811011996\r",
      "iteration 269 - y: 0.04933975212727757\r",
      "iteration 270 - y: 0.04925688644354315\r",
      "iteration 271 - y: 0.04917402075980873\r",
      "iteration 272 - y: 0.04909115507607431\r",
      "iteration 273 - y: 0.0490082893923399\r",
      "iteration 274 - y: 0.048925423708605476\r",
      "iteration 275 - y: 0.04884255802487106\r",
      "iteration 276 - y: 0.04875969234113663\r",
      "iteration 277 - y: 0.048676826657402215\r",
      "iteration 278 - y: 0.048593960973667805\r",
      "iteration 279 - y: 0.04851109528993338\r",
      "iteration 280 - y: 0.048428229606198955\r",
      "iteration 281 - y: 0.04834536392246454\r",
      "iteration 282 - y: 0.04826249823873012\r",
      "iteration 283 - y: 0.0481796325549957\r",
      "iteration 284 - y: 0.048096766871261284\r",
      "iteration 285 - y: 0.04801390118752686\r",
      "iteration 286 - y: 0.04793103550379244\r",
      "iteration 287 - y: 0.047848169820058024\r",
      "iteration 288 - y: 0.0477653041363236\r",
      "iteration 289 - y: 0.04768243845258919\r",
      "iteration 290 - y: 0.047599572768854764\r",
      "iteration 291 - y: 0.04751670708512035\r",
      "iteration 292 - y: 0.04743384140138593\r",
      "iteration 293 - y: 0.04735097571765151\r",
      "iteration 294 - y: 0.04726811003391709\r",
      "iteration 295 - y: 0.04718524435018267\r",
      "iteration 296 - y: 0.047102378666448244\r",
      "iteration 297 - y: 0.04701951298271383\r",
      "iteration 298 - y: 0.04693664729897941\r",
      "iteration 299 - y: 0.046853781615245\r",
      "iteration 300 - y: 0.04677091593151057\r",
      "iteration 301 - y: 0.046688050247776156\r",
      "iteration 302 - y: 0.04660518456404174\r",
      "iteration 303 - y: 0.04652231888030731\r",
      "iteration 304 - y: 0.0464394531965729\r",
      "iteration 305 - y: 0.04635658751283848\r",
      "iteration 306 - y: 0.04627372182910405\r",
      "iteration 307 - y: 0.04619085614536964\r",
      "iteration 308 - y: 0.04610799046163522\r",
      "iteration 309 - y: 0.0460251247779008\r",
      "iteration 310 - y: 0.04594225909416638\r",
      "iteration 311 - y: 0.04585939341043196\r",
      "iteration 312 - y: 0.04577652772669755\r",
      "iteration 313 - y: 0.04569366204296312\r",
      "iteration 314 - y: 0.0456107963592287\r",
      "iteration 315 - y: 0.04552793067549429\r",
      "iteration 316 - y: 0.04544506499175986\r",
      "iteration 317 - y: 0.045362199308025444\r",
      "iteration 318 - y: 0.04527933362429103\r",
      "iteration 319 - y: 0.0451964679405566\r",
      "iteration 320 - y: 0.04511360225682219\r",
      "iteration 321 - y: 0.045030736573087767\r",
      "iteration 322 - y: 0.044947870889353356\r",
      "iteration 323 - y: 0.04486500520561893\r",
      "iteration 324 - y: 0.044782139521884506\r",
      "iteration 325 - y: 0.04469927383815009\r",
      "iteration 326 - y: 0.04461640815441567\r",
      "iteration 327 - y: 0.044533542470681246\r",
      "iteration 328 - y: 0.044450676786946836\r",
      "iteration 329 - y: 0.04436781110321241\r",
      "iteration 330 - y: 0.044284945419477986\r",
      "iteration 331 - y: 0.044202079735743575\r",
      "iteration 332 - y: 0.04411921405200915\r",
      "iteration 333 - y: 0.04403634836827474\r",
      "iteration 334 - y: 0.043953482684540315\r",
      "iteration 335 - y: 0.0438706170008059\r",
      "iteration 336 - y: 0.04378775131707148\r",
      "iteration 337 - y: 0.04370488563333706\r",
      "iteration 338 - y: 0.043622019949602645\r",
      "iteration 339 - y: 0.04353915426586823\r",
      "iteration 340 - y: 0.04345628858213381\r",
      "iteration 341 - y: 0.04337342289839939\r",
      "iteration 342 - y: 0.043290557214664974\r",
      "iteration 343 - y: 0.043207691530930556\r",
      "iteration 344 - y: 0.04312482584719614\r",
      "iteration 345 - y: 0.043041960163461714\r",
      "iteration 346 - y: 0.0429590944797273\r",
      "iteration 347 - y: 0.04287622879599288\r",
      "iteration 348 - y: 0.04279336311225846\r",
      "iteration 349 - y: 0.04271049742852404\r",
      "iteration 350 - y: 0.042627631744789625\r",
      "iteration 351 - y: 0.04254476606105521\r",
      "iteration 352 - y: 0.04246190037732079\r",
      "iteration 353 - y: 0.042379034693586365\r",
      "iteration 354 - y: 0.042296169009851954\r",
      "iteration 355 - y: 0.04221330332611753\r",
      "iteration 356 - y: 0.04213043764238312\r",
      "iteration 357 - y: 0.042047571958648694\r",
      "iteration 358 - y: 0.041964706274914276\r",
      "iteration 359 - y: 0.04188184059117986\r",
      "iteration 360 - y: 0.04179897490744544\r",
      "iteration 361 - y: 0.04171610922371102\r",
      "iteration 362 - y: 0.041633243539976605\r",
      "iteration 363 - y: 0.04155037785624219\r",
      "iteration 364 - y: 0.04146751217250777\r",
      "iteration 365 - y: 0.04138464648877335\r",
      "iteration 366 - y: 0.041301780805038935\r",
      "iteration 367 - y: 0.04121891512130452\r",
      "iteration 368 - y: 0.04113604943757009\r",
      "iteration 369 - y: 0.041053183753835674\r",
      "iteration 370 - y: 0.04097031807010126\r",
      "iteration 371 - y: 0.040887452386366846\r",
      "iteration 372 - y: 0.04080458670263243\r",
      "iteration 373 - y: 0.040721721018898004\r",
      "iteration 374 - y: 0.04063885533516358\r",
      "iteration 375 - y: 0.04055598965142917\r",
      "iteration 376 - y: 0.04047312396769476\r",
      "iteration 377 - y: 0.04039025828396033\r",
      "iteration 378 - y: 0.04030739260022591\r",
      "iteration 379 - y: 0.04022452691649149\r",
      "iteration 380 - y: 0.04014166123275708\r",
      "iteration 381 - y: 0.04005879554902266\r",
      "iteration 382 - y: 0.039975929865288244\r",
      "iteration 383 - y: 0.03989306418155382\r",
      "iteration 384 - y: 0.0398101984978194\r",
      "iteration 385 - y: 0.039727332814084984\r",
      "iteration 386 - y: 0.039644467130350566\r",
      "iteration 387 - y: 0.03956160144661615\r",
      "iteration 388 - y: 0.03947873576288173\r",
      "iteration 389 - y: 0.03939587007914731\r",
      "iteration 390 - y: 0.039313004395412895\r",
      "iteration 391 - y: 0.03923013871167848\r",
      "iteration 392 - y: 0.03914727302794406\r",
      "iteration 393 - y: 0.03906440734420964\r",
      "iteration 394 - y: 0.038981541660475225\r",
      "iteration 395 - y: 0.03889867597674081\r",
      "iteration 396 - y: 0.03881581029300639\r",
      "iteration 397 - y: 0.03873294460927197\r",
      "iteration 398 - y: 0.038650078925537554\r",
      "iteration 399 - y: 0.038567213241803136\r",
      "iteration 400 - y: 0.03848434755806872\r",
      "iteration 401 - y: 0.0384014818743343\r",
      "iteration 402 - y: 0.03831861619059988\r",
      "iteration 403 - y: 0.038235750506865465\r",
      "iteration 404 - y: 0.03815288482313105\r",
      "iteration 405 - y: 0.03807001913939663\r",
      "iteration 406 - y: 0.03798715345566221\r",
      "iteration 407 - y: 0.037904287771927794\r",
      "iteration 408 - y: 0.03782142208819338\r",
      "iteration 409 - y: 0.03773855640445896\r",
      "iteration 410 - y: 0.03765569072072454\r",
      "iteration 411 - y: 0.03757282503699012\r",
      "iteration 412 - y: 0.037489959353255706\r",
      "iteration 413 - y: 0.03740709366952129\r",
      "iteration 414 - y: 0.03732422798578687\r",
      "iteration 415 - y: 0.03724136230205245\r",
      "iteration 416 - y: 0.037158496618318035\r",
      "iteration 417 - y: 0.03707563093458362\r",
      "iteration 418 - y: 0.0369927652508492\r",
      "iteration 419 - y: 0.03690989956711478\r",
      "iteration 420 - y: 0.036827033883380364\r",
      "iteration 421 - y: 0.036744168199645946\r",
      "iteration 422 - y: 0.03666130251591153\r",
      "iteration 423 - y: 0.03657843683217711\r",
      "iteration 424 - y: 0.03649557114844269\r",
      "iteration 425 - y: 0.036412705464708275\r",
      "iteration 426 - y: 0.03632983978097386\r",
      "iteration 427 - y: 0.03624697409723944\r",
      "iteration 428 - y: 0.03616410841350502\r",
      "iteration 429 - y: 0.036081242729770605\r",
      "iteration 430 - y: 0.03599837704603619\r",
      "iteration 431 - y: 0.03591551136230177\r",
      "iteration 432 - y: 0.03583264567856735\r",
      "iteration 433 - y: 0.035749779994832934\r",
      "iteration 434 - y: 0.035666914311098516\r",
      "iteration 435 - y: 0.0355840486273641\r",
      "iteration 436 - y: 0.03550118294362968\r",
      "iteration 437 - y: 0.03541831725989526\r",
      "iteration 438 - y: 0.035335451576160845\r",
      "iteration 439 - y: 0.03525258589242643\r",
      "iteration 440 - y: 0.03516972020869201\r",
      "iteration 441 - y: 0.03508685452495759\r",
      "iteration 442 - y: 0.035003988841223174\r",
      "iteration 443 - y: 0.034921123157488757\r",
      "iteration 444 - y: 0.03483825747375434\r",
      "iteration 445 - y: 0.03475539179001992\r",
      "iteration 446 - y: 0.0346725261062855\r",
      "iteration 447 - y: 0.034589660422551086\r",
      "iteration 448 - y: 0.03450679473881667\r",
      "iteration 449 - y: 0.03442392905508225\r",
      "iteration 450 - y: 0.03434106337134783\r",
      "iteration 451 - y: 0.034258197687613415\r",
      "iteration 452 - y: 0.034175332003879\r",
      "iteration 453 - y: 0.03409246632014458\r",
      "iteration 454 - y: 0.03400960063641016\r",
      "iteration 455 - y: 0.033926734952675744\r",
      "iteration 456 - y: 0.033843869268941326\r",
      "iteration 457 - y: 0.03376100358520691\r",
      "iteration 458 - y: 0.03367813790147249\r",
      "iteration 459 - y: 0.03359527221773807\r",
      "iteration 460 - y: 0.033512406534003655\r",
      "iteration 461 - y: 0.03342954085026924\r",
      "iteration 462 - y: 0.03334667516653482\r",
      "iteration 463 - y: 0.0332638094828004\r",
      "iteration 464 - y: 0.033180943799065984\r",
      "iteration 465 - y: 0.03309807811533157\r",
      "iteration 466 - y: 0.03301521243159715\r",
      "iteration 467 - y: 0.03293234674786273\r",
      "iteration 468 - y: 0.032849481064128314\r",
      "iteration 469 - y: 0.032766615380393896\r",
      "iteration 470 - y: 0.03268374969665948\r",
      "iteration 471 - y: 0.03260088401292506\r",
      "iteration 472 - y: 0.03251801832919064\r",
      "iteration 473 - y: 0.032435152645456225\r",
      "iteration 474 - y: 0.03235228696172181\r",
      "iteration 475 - y: 0.03226942127798739\r",
      "iteration 476 - y: 0.03218655559425298\r",
      "iteration 477 - y: 0.032103689910518554\r",
      "iteration 478 - y: 0.032020824226784136\r",
      "iteration 479 - y: 0.03193795854304972\r",
      "iteration 480 - y: 0.0318550928593153\r",
      "iteration 481 - y: 0.03177222717558088\r",
      "iteration 482 - y: 0.031689361491846466\r",
      "iteration 483 - y: 0.03160649580811205\r",
      "iteration 484 - y: 0.03152363012437763\r",
      "iteration 485 - y: 0.03144076444064321\r",
      "iteration 486 - y: 0.031357898756908795\r",
      "iteration 487 - y: 0.03127503307317438\r",
      "iteration 488 - y: 0.03119216738943996\r",
      "iteration 489 - y: 0.03110930170570554\r",
      "iteration 490 - y: 0.031026436021971124\r",
      "iteration 491 - y: 0.03094357033823671\r",
      "iteration 492 - y: 0.03086070465450229\r",
      "iteration 493 - y: 0.03077783897076787\r",
      "iteration 494 - y: 0.030694973287033453\r",
      "iteration 495 - y: 0.03061210760329904\r",
      "iteration 496 - y: 0.030529241919564618\r",
      "iteration 497 - y: 0.0304463762358302\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 498 - y: 0.030363510552095782\r",
      "iteration 499 - y: 0.030280644868361364\r",
      "iteration 500 - y: 0.030197779184626947\r",
      "iteration 501 - y: 0.030114913500892532\r",
      "iteration 502 - y: 0.030032047817158115\r",
      "iteration 503 - y: 0.029949182133423694\r",
      "iteration 504 - y: 0.029866316449689276\r",
      "iteration 505 - y: 0.02978345076595486\r",
      "iteration 506 - y: 0.029700585082220444\r",
      "iteration 507 - y: 0.029617719398486023\r",
      "iteration 508 - y: 0.029534853714751605\r",
      "iteration 509 - y: 0.029451988031017187\r",
      "iteration 510 - y: 0.029369122347282773\r",
      "iteration 511 - y: 0.029286256663548352\r",
      "iteration 512 - y: 0.029203390979813934\r",
      "iteration 513 - y: 0.029120525296079516\r",
      "iteration 514 - y: 0.0290376596123451\r",
      "iteration 515 - y: 0.02895479392861068\r",
      "iteration 516 - y: 0.028871928244876263\r",
      "iteration 517 - y: 0.02878906256114185\r",
      "iteration 518 - y: 0.02870619687740743\r",
      "iteration 519 - y: 0.028623331193673014\r",
      "iteration 520 - y: 0.028540465509938596\r",
      "iteration 521 - y: 0.028457599826204178\r",
      "iteration 522 - y: 0.02837473414246976\r",
      "iteration 523 - y: 0.02829186845873534\r",
      "iteration 524 - y: 0.02820900277500092\r",
      "iteration 525 - y: 0.028126137091266507\r",
      "iteration 526 - y: 0.02804327140753209\r",
      "iteration 527 - y: 0.02796040572379767\r",
      "iteration 528 - y: 0.02787754004006325\r",
      "iteration 529 - y: 0.027794674356328833\r",
      "iteration 530 - y: 0.02771180867259442\r",
      "iteration 531 - y: 0.027628942988859997\r",
      "iteration 532 - y: 0.02754607730512558\r",
      "iteration 533 - y: 0.027463211621391165\r",
      "iteration 534 - y: 0.027380345937656748\r",
      "iteration 535 - y: 0.02729748025392233\r",
      "iteration 536 - y: 0.027214614570187912\r",
      "iteration 537 - y: 0.027131748886453495\r",
      "iteration 538 - y: 0.027048883202719077\r",
      "iteration 539 - y: 0.026966017518984656\r",
      "iteration 540 - y: 0.02688315183525024\r",
      "iteration 541 - y: 0.026800286151515824\r",
      "iteration 542 - y: 0.026717420467781406\r",
      "iteration 543 - y: 0.026634554784046985\r",
      "iteration 544 - y: 0.026551689100312567\r",
      "iteration 545 - y: 0.026468823416578153\r",
      "iteration 546 - y: 0.026385957732843735\r",
      "iteration 547 - y: 0.026303092049109314\r",
      "iteration 548 - y: 0.026220226365374896\r",
      "iteration 549 - y: 0.02613736068164048\r",
      "iteration 550 - y: 0.026054494997906068\r",
      "iteration 551 - y: 0.025971629314171647\r",
      "iteration 552 - y: 0.02588876363043723\r",
      "iteration 553 - y: 0.02580589794670281\r",
      "iteration 554 - y: 0.02572303226296839\r",
      "iteration 555 - y: 0.025640166579233976\r",
      "iteration 556 - y: 0.025557300895499558\r",
      "iteration 557 - y: 0.02547443521176514\r",
      "iteration 558 - y: 0.02539156952803072\r",
      "iteration 559 - y: 0.0253087038442963\r",
      "iteration 560 - y: 0.025225838160561887\r",
      "iteration 561 - y: 0.02514297247682747\r",
      "iteration 562 - y: 0.025060106793093048\r",
      "iteration 563 - y: 0.02497724110935863\r",
      "iteration 564 - y: 0.024894375425624213\r",
      "iteration 565 - y: 0.024811509741889802\r",
      "iteration 566 - y: 0.024728644058155377\r",
      "iteration 567 - y: 0.024645778374420963\r",
      "iteration 568 - y: 0.024562912690686545\r",
      "iteration 569 - y: 0.024480047006952128\r",
      "iteration 570 - y: 0.02439718132321771\r",
      "iteration 571 - y: 0.02431431563948329\r",
      "iteration 572 - y: 0.02423144995574887\r",
      "iteration 573 - y: 0.024148584272014453\r",
      "iteration 574 - y: 0.024065718588280036\r",
      "iteration 575 - y: 0.023982852904545618\r",
      "iteration 576 - y: 0.0238999872208112\r",
      "iteration 577 - y: 0.023817121537076782\r",
      "iteration 578 - y: 0.023734255853342365\r",
      "iteration 579 - y: 0.023651390169607947\r",
      "iteration 580 - y: 0.02356852448587353\r",
      "iteration 581 - y: 0.02348565880213911\r",
      "iteration 582 - y: 0.02340279311840469\r",
      "iteration 583 - y: 0.023319927434670273\r",
      "iteration 584 - y: 0.023237061750935855\r",
      "iteration 585 - y: 0.02315419606720144\r",
      "iteration 586 - y: 0.02307133038346702\r",
      "iteration 587 - y: 0.022988464699732602\r",
      "iteration 588 - y: 0.022905599015998184\r",
      "iteration 589 - y: 0.022822733332263766\r",
      "iteration 590 - y: 0.02273986764852935\r",
      "iteration 591 - y: 0.02265700196479493\r",
      "iteration 592 - y: 0.022574136281060513\r",
      "iteration 593 - y: 0.022491270597326096\r",
      "iteration 594 - y: 0.022408404913591674\r",
      "iteration 595 - y: 0.02232553922985726\r",
      "iteration 596 - y: 0.022242673546122842\r",
      "iteration 597 - y: 0.022159807862388425\r",
      "iteration 598 - y: 0.022076942178654003\r",
      "iteration 599 - y: 0.021994076494919586\r",
      "iteration 600 - y: 0.02191121081118517\r",
      "iteration 601 - y: 0.021828345127450754\r",
      "iteration 602 - y: 0.021745479443716333\r",
      "iteration 603 - y: 0.021662613759981915\r",
      "iteration 604 - y: 0.021579748076247497\r",
      "iteration 605 - y: 0.021496882392513083\r",
      "iteration 606 - y: 0.021414016708778662\r",
      "iteration 607 - y: 0.021331151025044244\r",
      "iteration 608 - y: 0.021248285341309826\r",
      "iteration 609 - y: 0.02116541965757541\r",
      "iteration 610 - y: 0.02108255397384099\r",
      "iteration 611 - y: 0.020999688290106573\r",
      "iteration 612 - y: 0.020916822606372155\r",
      "iteration 613 - y: 0.020833956922637734\r",
      "iteration 614 - y: 0.020751091238903317\r",
      "iteration 615 - y: 0.020668225555168902\r",
      "iteration 616 - y: 0.020585359871434485\r",
      "iteration 617 - y: 0.020502494187700063\r",
      "iteration 618 - y: 0.020419628503965646\r",
      "iteration 619 - y: 0.020336762820231228\r",
      "iteration 620 - y: 0.020253897136496814\r",
      "iteration 621 - y: 0.020171031452762393\r",
      "iteration 622 - y: 0.020088165769027975\r",
      "iteration 623 - y: 0.020005300085293557\r",
      "iteration 624 - y: 0.01992243440155914\r",
      "iteration 625 - y: 0.01983956871782472\r",
      "iteration 626 - y: 0.019756703034090304\r",
      "iteration 627 - y: 0.019673837350355886\r",
      "iteration 628 - y: 0.019590971666621465\r",
      "iteration 629 - y: 0.019508105982887047\r",
      "iteration 630 - y: 0.019425240299152637\r",
      "iteration 631 - y: 0.01934237461541822\r",
      "iteration 632 - y: 0.019259508931683794\r",
      "iteration 633 - y: 0.01917664324794938\r",
      "iteration 634 - y: 0.019093777564214962\r",
      "iteration 635 - y: 0.019010911880480545\r",
      "iteration 636 - y: 0.018928046196746127\r",
      "iteration 637 - y: 0.01884518051301171\r",
      "iteration 638 - y: 0.01876231482927729\r",
      "iteration 639 - y: 0.01867944914554287\r",
      "iteration 640 - y: 0.018596583461808456\r",
      "iteration 641 - y: 0.018513717778074038\r",
      "iteration 642 - y: 0.018430852094339617\r",
      "iteration 643 - y: 0.0183479864106052\r",
      "iteration 644 - y: 0.01826512072687078\r",
      "iteration 645 - y: 0.018182255043136364\r",
      "iteration 646 - y: 0.01811557631089025\r",
      "iteration 647 - y: 0.01806270520432525\r",
      "iteration 648 - y: 0.01800983409776025\r",
      "iteration 649 - y: 0.017956962991195247\r",
      "iteration 650 - y: 0.01790409188463025\r",
      "iteration 651 - y: 0.017851220778065252\r",
      "iteration 652 - y: 0.01779834967150025\r",
      "iteration 653 - y: 0.01774547856493525\r",
      "iteration 654 - y: 0.01769260745837025\r",
      "iteration 655 - y: 0.01763973635180525\r",
      "iteration 656 - y: 0.017586865245240252\r",
      "iteration 657 - y: 0.017533994138675252\r",
      "iteration 658 - y: 0.017481123032110253\r",
      "iteration 659 - y: 0.01742825192554525\r",
      "iteration 660 - y: 0.017375380818980254\r",
      "iteration 661 - y: 0.017322509712415255\r",
      "iteration 662 - y: 0.017269638605850252\r",
      "iteration 663 - y: 0.017216767499285253\r",
      "iteration 664 - y: 0.017163896392720254\r",
      "iteration 665 - y: 0.017111025286155258\r",
      "iteration 666 - y: 0.017058154179590255\r",
      "iteration 667 - y: 0.017005283073025255\r",
      "iteration 668 - y: 0.016952411966460256\r",
      "iteration 669 - y: 0.016899540859895253\r",
      "iteration 670 - y: 0.016846669753330257\r",
      "iteration 671 - y: 0.016793798646765258\r",
      "iteration 672 - y: 0.01674092754020026\r",
      "iteration 673 - y: 0.016688056433635256\r",
      "iteration 674 - y: 0.016635185327070257\r",
      "iteration 675 - y: 0.01658231422050526\r",
      "iteration 676 - y: 0.01652944311394026\r",
      "iteration 677 - y: 0.01647657200737526\r",
      "iteration 678 - y: 0.01642370090081026\r",
      "iteration 679 - y: 0.01637082979424526\r",
      "iteration 680 - y: 0.01631795868768026\r",
      "iteration 681 - y: 0.01626508758111526\r",
      "iteration 682 - y: 0.01621221647455026\r",
      "iteration 683 - y: 0.016159345367985262\r",
      "iteration 684 - y: 0.01610647426142026\r",
      "iteration 685 - y: 0.016053603154855264\r",
      "iteration 686 - y: 0.016000732048290264\r",
      "iteration 687 - y: 0.01594786094172526\r",
      "iteration 688 - y: 0.015894989835160262\r",
      "iteration 689 - y: 0.015842118728595263\r",
      "iteration 690 - y: 0.015789247622030263\r",
      "iteration 691 - y: 0.015736376515465264\r",
      "iteration 692 - y: 0.015683505408900265\r",
      "iteration 693 - y: 0.015630634302335265\r",
      "iteration 694 - y: 0.015577763195770266\r",
      "iteration 695 - y: 0.015524892089205265\r",
      "iteration 696 - y: 0.015472020982640267\r",
      "iteration 697 - y: 0.015419149876075266\r",
      "iteration 698 - y: 0.015366278769510269\r",
      "iteration 699 - y: 0.015313407662945267\r",
      "iteration 700 - y: 0.015260536556380266\r",
      "iteration 701 - y: 0.015207665449815269\r",
      "iteration 702 - y: 0.015154794343250268\r",
      "iteration 703 - y: 0.01511602832862475\r",
      "iteration 704 - y: 0.015080491762248063\r",
      "iteration 705 - y: 0.015044955195871375\r",
      "iteration 706 - y: 0.015009418629494689\r",
      "iteration 707 - y: 0.014973882063118001\r",
      "iteration 708 - y: 0.014938345496741315\r",
      "iteration 709 - y: 0.014902808930364627\r",
      "iteration 710 - y: 0.01486727236398794\r",
      "iteration 711 - y: 0.014831735797611253\r",
      "iteration 712 - y: 0.014796199231234566\r",
      "iteration 713 - y: 0.01476066266485788\r",
      "iteration 714 - y: 0.014725126098481192\r",
      "iteration 715 - y: 0.014689589532104504\r",
      "iteration 716 - y: 0.014654052965727818\r",
      "iteration 717 - y: 0.01461851639935113\r",
      "iteration 718 - y: 0.014582979832974444\r",
      "iteration 719 - y: 0.014547443266597756\r",
      "iteration 720 - y: 0.014511906700221068\r",
      "iteration 721 - y: 0.014476370133844382\r",
      "iteration 722 - y: 0.014440833567467695\r",
      "iteration 723 - y: 0.014405297001091008\r",
      "iteration 724 - y: 0.01436976043471432\r",
      "iteration 725 - y: 0.014334223868337633\r",
      "iteration 726 - y: 0.014298687301960947\r",
      "iteration 727 - y: 0.014263150735584259\r",
      "iteration 728 - y: 0.014227614169207573\r",
      "iteration 729 - y: 0.014192077602830885\r",
      "iteration 730 - y: 0.014156541036454197\r",
      "iteration 731 - y: 0.014121004470077511\r",
      "iteration 732 - y: 0.014085467903700823\r",
      "iteration 733 - y: 0.014049931337324137\r",
      "iteration 734 - y: 0.01401439477094745\r",
      "iteration 735 - y: 0.013978858204570762\r",
      "iteration 736 - y: 0.013943321638194076\r",
      "iteration 737 - y: 0.013907785071817388\r",
      "iteration 738 - y: 0.013872248505440702\r",
      "iteration 739 - y: 0.013836711939064014\r",
      "iteration 740 - y: 0.013801175372687326\r",
      "iteration 741 - y: 0.01376563880631064\r",
      "iteration 742 - y: 0.013730102239933952\r",
      "iteration 743 - y: 0.013694565673557266\r",
      "iteration 744 - y: 0.013659029107180578\r",
      "iteration 745 - y: 0.013623492540803892\r",
      "iteration 746 - y: 0.013587955974427204\r",
      "iteration 747 - y: 0.013552419408050517\r",
      "iteration 748 - y: 0.01351688284167383\r",
      "iteration 749 - y: 0.013481346275297143\r",
      "iteration 750 - y: 0.013445809708920457\r",
      "iteration 751 - y: 0.013410273142543769\r",
      "iteration 752 - y: 0.013374736576167081\r",
      "iteration 753 - y: 0.013339200009790395\r",
      "iteration 754 - y: 0.013303663443413707\r",
      "iteration 755 - y: 0.013268126877037021\r",
      "iteration 756 - y: 0.013232590310660333\r",
      "iteration 757 - y: 0.013197053744283645\r",
      "iteration 758 - y: 0.01316151717790696\r",
      "iteration 759 - y: 0.013125980611530271\r",
      "iteration 760 - y: 0.013090444045153585\r",
      "iteration 761 - y: 0.013054907478776898\r",
      "iteration 762 - y: 0.01301937091240021\r",
      "iteration 763 - y: 0.012983834346023524\r",
      "iteration 764 - y: 0.012948297779646836\r",
      "iteration 765 - y: 0.01291276121327015\r",
      "iteration 766 - y: 0.012877224646893462\r",
      "iteration 767 - y: 0.012841688080516774\r",
      "iteration 768 - y: 0.012806151514140088\r",
      "iteration 769 - y: 0.0127706149477634\r",
      "iteration 770 - y: 0.012735078381386714\r",
      "iteration 771 - y: 0.012699541815010026\r",
      "iteration 772 - y: 0.012664005248633339\r",
      "iteration 773 - y: 0.012628468682256653\r",
      "iteration 774 - y: 0.012592932115879965\r",
      "iteration 775 - y: 0.012557395549503279\r",
      "iteration 776 - y: 0.01252185898312659\r",
      "iteration 777 - y: 0.012486322416749903\r",
      "iteration 778 - y: 0.012450785850373217\r",
      "iteration 779 - y: 0.01241524928399653\r",
      "iteration 780 - y: 0.012379712717619843\r",
      "iteration 781 - y: 0.012344176151243155\r",
      "iteration 782 - y: 0.012308639584866467\r",
      "iteration 783 - y: 0.012273103018489781\r",
      "iteration 784 - y: 0.012237566452113094\r",
      "iteration 785 - y: 0.012202029885736407\r",
      "iteration 786 - y: 0.01216649331935972\r",
      "iteration 787 - y: 0.012130956752983032\r",
      "iteration 788 - y: 0.012095420186606346\r",
      "iteration 789 - y: 0.012059883620229658\r",
      "iteration 790 - y: 0.012024347053852972\r",
      "iteration 791 - y: 0.011988810487476284\r",
      "iteration 792 - y: 0.011953273921099596\r",
      "iteration 793 - y: 0.01191773735472291\r",
      "iteration 794 - y: 0.011882200788346222\r",
      "iteration 795 - y: 0.011846664221969536\r",
      "iteration 796 - y: 0.011811127655592848\r",
      "iteration 797 - y: 0.01177559108921616\r",
      "iteration 798 - y: 0.011740054522839475\r",
      "iteration 799 - y: 0.011704517956462787\r",
      "iteration 800 - y: 0.0116689813900861\r",
      "iteration 801 - y: 0.011633444823709413\r",
      "iteration 802 - y: 0.011597908257332725\r",
      "iteration 803 - y: 0.011562371690956039\r",
      "iteration 804 - y: 0.011526835124579351\r",
      "iteration 805 - y: 0.011491298558202665\r",
      "iteration 806 - y: 0.011455761991825977\r",
      "iteration 807 - y: 0.01142022542544929\r",
      "iteration 808 - y: 0.011384688859072603\r",
      "iteration 809 - y: 0.011349152292695916\r",
      "iteration 810 - y: 0.01131361572631923\r",
      "iteration 811 - y: 0.011278079159942542\r",
      "iteration 812 - y: 0.011242542593565854\r",
      "iteration 813 - y: 0.011207006027189168\r",
      "iteration 814 - y: 0.01117146946081248\r",
      "iteration 815 - y: 0.011135932894435794\r",
      "iteration 816 - y: 0.011100396328059108\r",
      "iteration 817 - y: 0.011064859761682422\r",
      "iteration 818 - y: 0.011029323195305736\r",
      "iteration 819 - y: 0.01099378662892905\r",
      "iteration 820 - y: 0.010958250062552364\r",
      "iteration 821 - y: 0.010922713496175677\r",
      "iteration 822 - y: 0.010887176929798991\r",
      "iteration 823 - y: 0.010851640363422307\r",
      "iteration 824 - y: 0.010816103797045621\r",
      "iteration 825 - y: 0.010780567230668935\r",
      "iteration 826 - y: 0.010745030664292249\r",
      "iteration 827 - y: 0.010709494097915563\r",
      "iteration 828 - y: 0.010673957531538877\r",
      "iteration 829 - y: 0.01063842096516219\r",
      "iteration 830 - y: 0.010602884398785505\r",
      "iteration 831 - y: 0.010567347832408818\r",
      "iteration 832 - y: 0.010531811266032134\r",
      "iteration 833 - y: 0.010496274699655448\r",
      "iteration 834 - y: 0.010460738133278762\r",
      "iteration 835 - y: 0.010425201566902076\r",
      "iteration 836 - y: 0.01038966500052539\r",
      "iteration 837 - y: 0.010354128434148704\r",
      "iteration 838 - y: 0.010318591867772018\r",
      "iteration 839 - y: 0.010283055301395332\r",
      "iteration 840 - y: 0.010247518735018646\r",
      "iteration 841 - y: 0.010211982168641961\r",
      "iteration 842 - y: 0.010176445602265275\r",
      "iteration 843 - y: 0.010140909035888589\r",
      "iteration 844 - y: 0.010105372469511903\r",
      "iteration 845 - y: 0.010069835903135217\r",
      "iteration 846 - y: 0.01003429933675853\r",
      "iteration 847 - y: 0.009998762770381845\r",
      "iteration 848 - y: 0.009963226204005159\r",
      "iteration 849 - y: 0.009927689637628473\r",
      "iteration 850 - y: 0.009892153071251788\r",
      "iteration 851 - y: 0.009856616504875102\r",
      "iteration 852 - y: 0.009821079938498416\r",
      "iteration 853 - y: 0.00978554337212173\r",
      "iteration 854 - y: 0.009750006805745044\r",
      "iteration 855 - y: 0.009714470239368358\r",
      "iteration 856 - y: 0.009678933672991672\r",
      "iteration 857 - y: 0.009643397106614986\r",
      "iteration 858 - y: 0.0096078605402383\r",
      "iteration 859 - y: 0.009572323973861615\r",
      "iteration 860 - y: 0.00953678740748493\r",
      "iteration 861 - y: 0.009501250841108243\r",
      "iteration 862 - y: 0.009465714274731557\r",
      "iteration 863 - y: 0.009430177708354871\r",
      "iteration 864 - y: 0.009394641141978185\r",
      "iteration 865 - y: 0.009359104575601499\r",
      "iteration 866 - y: 0.009323568009224813\r",
      "iteration 867 - y: 0.009288031442848128\r",
      "iteration 868 - y: 0.009252494876471442\r",
      "iteration 869 - y: 0.009216958310094756\r",
      "iteration 870 - y: 0.00918142174371807\r",
      "iteration 871 - y: 0.009145885177341384\r",
      "iteration 872 - y: 0.009110348610964698\r",
      "iteration 873 - y: 0.009074812044588012\r",
      "iteration 874 - y: 0.009039275478211326\r",
      "iteration 875 - y: 0.00900373891183464\r",
      "iteration 876 - y: 0.008968202345457955\r",
      "iteration 877 - y: 0.00893266577908127\r",
      "iteration 878 - y: 0.008897129212704583\r",
      "iteration 879 - y: 0.008861592646327897\r",
      "iteration 880 - y: 0.008826056079951211\r",
      "iteration 881 - y: 0.008790519513574525\r",
      "iteration 882 - y: 0.008754982947197839\r",
      "iteration 883 - y: 0.008719446380821153\r",
      "iteration 884 - y: 0.008683909814444467\r",
      "iteration 885 - y: 0.008648373248067782\r",
      "iteration 886 - y: 0.008612836681691096\r",
      "iteration 887 - y: 0.00857730011531441\r",
      "iteration 888 - y: 0.008541763548937724\r",
      "iteration 889 - y: 0.008506226982561038\r",
      "iteration 890 - y: 0.008470690416184352\r",
      "iteration 891 - y: 0.008435153849807666\r",
      "iteration 892 - y: 0.00839961728343098\r",
      "iteration 893 - y: 0.008364080717054294\r",
      "iteration 894 - y: 0.00832854415067761\r",
      "iteration 895 - y: 0.008293007584300923\r",
      "iteration 896 - y: 0.008257471017924237\r",
      "iteration 897 - y: 0.008221934451547551\r",
      "iteration 898 - y: 0.008186397885170865\r",
      "iteration 899 - y: 0.008150861318794179\r",
      "iteration 900 - y: 0.008115324752417493\r",
      "iteration 901 - y: 0.008079788186040807\r",
      "iteration 902 - y: 0.008044251619664121\r",
      "iteration 903 - y: 0.008008715053287437\r",
      "iteration 904 - y: 0.00797317848691075\r",
      "iteration 905 - y: 0.007937641920534064\r",
      "iteration 906 - y: 0.007902105354157378\r",
      "iteration 907 - y: 0.007866568787780692\r",
      "iteration 908 - y: 0.007831032221404006\r",
      "iteration 909 - y: 0.00779549565502732\r",
      "iteration 910 - y: 0.007759959088650635\r",
      "iteration 911 - y: 0.007724422522273949\r",
      "iteration 912 - y: 0.007688885955897263\r",
      "iteration 913 - y: 0.007653349389520577\r",
      "iteration 914 - y: 0.0076178128231438914\r",
      "iteration 915 - y: 0.007582276256767205\r",
      "iteration 916 - y: 0.007546739690390519\r",
      "iteration 917 - y: 0.007511203124013833\r",
      "iteration 918 - y: 0.007475666557637148\r",
      "iteration 919 - y: 0.007440129991260462\r",
      "iteration 920 - y: 0.007404593424883776\r",
      "iteration 921 - y: 0.00736905685850709\r",
      "iteration 922 - y: 0.007333520292130404\r",
      "iteration 923 - y: 0.0072979837257537185\r",
      "iteration 924 - y: 0.007262447159377032\r",
      "iteration 925 - y: 0.007226910593000346\r",
      "iteration 926 - y: 0.00719137402662366\r",
      "iteration 927 - y: 0.007155837460246975\r",
      "iteration 928 - y: 0.007120300893870289\r",
      "iteration 929 - y: 0.007084764327493603\r",
      "iteration 930 - y: 0.007049227761116917\r",
      "iteration 931 - y: 0.007013691194740231\r",
      "iteration 932 - y: 0.0069781546283635455\r",
      "iteration 933 - y: 0.0069426180619868594\r",
      "iteration 934 - y: 0.006907081495610173\r",
      "iteration 935 - y: 0.006871544929233487\r",
      "iteration 936 - y: 0.006836008362856802\r",
      "iteration 937 - y: 0.006800471796480116\r",
      "iteration 938 - y: 0.00676493523010343\r",
      "iteration 939 - y: 0.006729398663726744\r",
      "iteration 940 - y: 0.006693862097350058\r",
      "iteration 941 - y: 0.006658325530973373\r",
      "iteration 942 - y: 0.0066227889645966865\r",
      "iteration 943 - y: 0.00658725239822\r",
      "iteration 944 - y: 0.006551715831843314\r",
      "iteration 945 - y: 0.006516179265466629\r",
      "iteration 946 - y: 0.006480642699089943\r",
      "iteration 947 - y: 0.006445106132713257\r",
      "iteration 948 - y: 0.006409569566336571\r",
      "iteration 949 - y: 0.006374032999959886\r",
      "iteration 950 - y: 0.0063384964335832\r",
      "iteration 951 - y: 0.0063029598672065135\r",
      "iteration 952 - y: 0.0062674233008298275\r",
      "iteration 953 - y: 0.006231886734453141\r",
      "iteration 954 - y: 0.006196350168076456\r",
      "iteration 955 - y: 0.00616081360169977\r",
      "iteration 956 - y: 0.006125277035323084\r",
      "iteration 957 - y: 0.006089740468946398\r",
      "iteration 958 - y: 0.006054203902569713\r",
      "iteration 959 - y: 0.006018667336193027\r",
      "iteration 960 - y: 0.005983130769816341\r",
      "iteration 961 - y: 0.0059475942034396545\r",
      "iteration 962 - y: 0.005912057637062968\r",
      "iteration 963 - y: 0.005876521070686283\r",
      "iteration 964 - y: 0.005840984504309597\r",
      "iteration 965 - y: 0.005805447937932911\r",
      "iteration 966 - y: 0.005769911371556225\r",
      "iteration 967 - y: 0.00573437480517954\r",
      "iteration 968 - y: 0.005698838238802854\r",
      "iteration 969 - y: 0.005663301672426168\r",
      "iteration 970 - y: 0.0056277651060494815\r",
      "iteration 971 - y: 0.0055922285396727955\r",
      "iteration 972 - y: 0.00555669197329611\r",
      "iteration 973 - y: 0.005521155406919424\r",
      "iteration 974 - y: 0.005485618840542738\r",
      "iteration 975 - y: 0.005450082274166052\r",
      "iteration 976 - y: 0.005414545707789367\r",
      "iteration 977 - y: 0.005379009141412681\r",
      "iteration 978 - y: 0.005343472575035995\r",
      "iteration 979 - y: 0.005307936008659309\r",
      "iteration 980 - y: 0.005272399442282623\r",
      "iteration 981 - y: 0.005236862875905937\r",
      "iteration 982 - y: 0.005201326309529251\r",
      "iteration 983 - y: 0.005165789743152565\r",
      "iteration 984 - y: 0.005130253176775879\r",
      "iteration 985 - y: 0.005094716610399194\r",
      "iteration 986 - y: 0.005059180044022508\r",
      "iteration 987 - y: 0.005023643477645822\r",
      "iteration 988 - y: 0.004988106911269136\r",
      "iteration 989 - y: 0.00495257034489245\r",
      "iteration 990 - y: 0.004917033778515764\r",
      "iteration 991 - y: 0.004881497212139078\r",
      "iteration 992 - y: 0.004845960645762392\r",
      "iteration 993 - y: 0.004810424079385706\r",
      "iteration 994 - y: 0.004774887513009021\r",
      "iteration 995 - y: 0.004739350946632335\r",
      "iteration 996 - y: 0.004703814380255649\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 914 - y: 0.00098847885161101532\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aec2c7d1150>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd63dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0471\n",
      "Epoch 1: val_loss improved from inf to 0.00128, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 0.0023 - mae: 0.0425 - val_loss: 0.0013 - val_mae: 0.0308\n",
      "Epoch 2/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1530e-04 - mae: 0.0225\n",
      "Epoch 2: val_loss improved from 0.00128 to 0.00045, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 354us/sample - loss: 6.6303e-04 - mae: 0.0213 - val_loss: 4.5029e-04 - val_mae: 0.0178\n",
      "Epoch 3/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9235e-04 - mae: 0.0162\n",
      "Epoch 3: val_loss did not improve from 0.00045\n",
      "160/160 [==============================] - 0s 218us/sample - loss: 5.1376e-04 - mae: 0.0186 - val_loss: 4.6187e-04 - val_mae: 0.0177\n",
      "Epoch 4/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6602e-04 - mae: 0.0213\n",
      "Epoch 4: val_loss improved from 0.00045 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 400us/sample - loss: 4.6491e-04 - mae: 0.0183 - val_loss: 4.3290e-04 - val_mae: 0.0179\n",
      "Epoch 5/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8007e-04 - mae: 0.0176\n",
      "Epoch 5: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 369us/sample - loss: 4.5682e-04 - mae: 0.0182 - val_loss: 4.3180e-04 - val_mae: 0.0178\n",
      "Epoch 6/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3223e-04 - mae: 0.0162\n",
      "Epoch 6: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 354us/sample - loss: 4.5602e-04 - mae: 0.0182 - val_loss: 4.3028e-04 - val_mae: 0.0178\n",
      "Epoch 7/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0219e-04 - mae: 0.0185\n",
      "Epoch 7: val_loss did not improve from 0.00043\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 4.4899e-04 - mae: 0.0180 - val_loss: 4.3210e-04 - val_mae: 0.0176\n",
      "Epoch 8/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1641e-04 - mae: 0.0173\n",
      "Epoch 8: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 392us/sample - loss: 4.4321e-04 - mae: 0.0178 - val_loss: 4.2757e-04 - val_mae: 0.0177\n",
      "Epoch 9/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3957e-04 - mae: 0.0125\n",
      "Epoch 9: val_loss improved from 0.00043 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 352us/sample - loss: 4.4260e-04 - mae: 0.0180 - val_loss: 4.2636e-04 - val_mae: 0.0177\n",
      "Epoch 10/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3909e-04 - mae: 0.0168\n",
      "Epoch 10: val_loss improved from 0.00043 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 385us/sample - loss: 4.3752e-04 - mae: 0.0178 - val_loss: 4.2476e-04 - val_mae: 0.0176\n",
      "Epoch 11/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8120e-04 - mae: 0.0221\n",
      "Epoch 11: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 348us/sample - loss: 4.3043e-04 - mae: 0.0177 - val_loss: 4.2335e-04 - val_mae: 0.0176\n",
      "Epoch 12/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8656e-04 - mae: 0.0149\n",
      "Epoch 12: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 419us/sample - loss: 4.2791e-04 - mae: 0.0176 - val_loss: 4.2195e-04 - val_mae: 0.0175\n",
      "Epoch 13/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3053e-04 - mae: 0.0158\n",
      "Epoch 13: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 362us/sample - loss: 4.2329e-04 - mae: 0.0175 - val_loss: 4.2046e-04 - val_mae: 0.0175\n",
      "Epoch 14/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3382e-04 - mae: 0.0130\n",
      "Epoch 14: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 324us/sample - loss: 4.1677e-04 - mae: 0.0174 - val_loss: 4.1973e-04 - val_mae: 0.0174\n",
      "Epoch 15/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8374e-04 - mae: 0.0119\n",
      "Epoch 15: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 382us/sample - loss: 4.1225e-04 - mae: 0.0172 - val_loss: 4.1772e-04 - val_mae: 0.0174\n",
      "Epoch 16/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2046e-04 - mae: 0.0168\n",
      "Epoch 16: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 361us/sample - loss: 4.0634e-04 - mae: 0.0172 - val_loss: 4.1673e-04 - val_mae: 0.0175\n",
      "Epoch 17/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0408e-04 - mae: 0.0212\n",
      "Epoch 17: val_loss improved from 0.00042 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 397us/sample - loss: 4.0120e-04 - mae: 0.0170 - val_loss: 4.1609e-04 - val_mae: 0.0173\n",
      "Epoch 18/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.7763e-04 - mae: 0.0180\n",
      "Epoch 18: val_loss improved from 0.00042 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 363us/sample - loss: 3.9587e-04 - mae: 0.0169 - val_loss: 4.1356e-04 - val_mae: 0.0174\n",
      "Epoch 19/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7735e-04 - mae: 0.0164\n",
      "Epoch 19: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 402us/sample - loss: 3.9186e-04 - mae: 0.0169 - val_loss: 4.1239e-04 - val_mae: 0.0172\n",
      "Epoch 20/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3716e-04 - mae: 0.0163\n",
      "Epoch 20: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 421us/sample - loss: 3.8657e-04 - mae: 0.0166 - val_loss: 4.1075e-04 - val_mae: 0.0172\n",
      "Epoch 21/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8934e-04 - mae: 0.0125\n",
      "Epoch 21: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 397us/sample - loss: 3.8290e-04 - mae: 0.0167 - val_loss: 4.0842e-04 - val_mae: 0.0172\n",
      "Epoch 22/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7776e-04 - mae: 0.0168\n",
      "Epoch 22: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 413us/sample - loss: 3.8203e-04 - mae: 0.0164 - val_loss: 4.0692e-04 - val_mae: 0.0172\n",
      "Epoch 23/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7357e-04 - mae: 0.0178\n",
      "Epoch 23: val_loss improved from 0.00041 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 409us/sample - loss: 3.7364e-04 - mae: 0.0164 - val_loss: 4.0556e-04 - val_mae: 0.0172\n",
      "Epoch 24/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2205e-04 - mae: 0.0144\n",
      "Epoch 24: val_loss did not improve from 0.00041\n",
      "160/160 [==============================] - 0s 228us/sample - loss: 3.6990e-04 - mae: 0.0164 - val_loss: 4.0761e-04 - val_mae: 0.0174\n",
      "Epoch 25/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8876e-04 - mae: 0.0143\n",
      "Epoch 25: val_loss improved from 0.00041 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 379us/sample - loss: 3.6875e-04 - mae: 0.0163 - val_loss: 4.0242e-04 - val_mae: 0.0171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2193e-04 - mae: 0.0205\n",
      "Epoch 26: val_loss did not improve from 0.00040\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 3.5374e-04 - mae: 0.0160 - val_loss: 4.0289e-04 - val_mae: 0.0168\n",
      "Epoch 27/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5786e-04 - mae: 0.0141\n",
      "Epoch 27: val_loss did not improve from 0.00040\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 3.5732e-04 - mae: 0.0157 - val_loss: 4.0378e-04 - val_mae: 0.0173\n",
      "Epoch 28/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6318e-04 - mae: 0.0169\n",
      "Epoch 28: val_loss improved from 0.00040 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 397us/sample - loss: 3.3978e-04 - mae: 0.0157 - val_loss: 4.0058e-04 - val_mae: 0.0167\n",
      "Epoch 29/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5833e-04 - mae: 0.0169\n",
      "Epoch 29: val_loss improved from 0.00040 to 0.00040, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 391us/sample - loss: 3.3535e-04 - mae: 0.0155 - val_loss: 3.9740e-04 - val_mae: 0.0170\n",
      "Epoch 30/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0532e-04 - mae: 0.0215\n",
      "Epoch 30: val_loss improved from 0.00040 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 343us/sample - loss: 3.2653e-04 - mae: 0.0153 - val_loss: 3.9380e-04 - val_mae: 0.0168\n",
      "Epoch 31/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0191e-04 - mae: 0.0202\n",
      "Epoch 31: val_loss improved from 0.00039 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 352us/sample - loss: 3.2278e-04 - mae: 0.0152 - val_loss: 3.9364e-04 - val_mae: 0.0169\n",
      "Epoch 32/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0456e-04 - mae: 0.0177\n",
      "Epoch 32: val_loss improved from 0.00039 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 371us/sample - loss: 3.2105e-04 - mae: 0.0152 - val_loss: 3.9039e-04 - val_mae: 0.0167\n",
      "Epoch 33/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3066e-04 - mae: 0.0132\n",
      "Epoch 33: val_loss improved from 0.00039 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 330us/sample - loss: 3.1203e-04 - mae: 0.0148 - val_loss: 3.8855e-04 - val_mae: 0.0167\n",
      "Epoch 34/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5059e-04 - mae: 0.0110\n",
      "Epoch 34: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 3.0218e-04 - mae: 0.0147 - val_loss: 3.9555e-04 - val_mae: 0.0170\n",
      "Epoch 35/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1248e-04 - mae: 0.0122\n",
      "Epoch 35: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 202us/sample - loss: 2.8937e-04 - mae: 0.0144 - val_loss: 3.8888e-04 - val_mae: 0.0163\n",
      "Epoch 36/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8317e-04 - mae: 0.0145\n",
      "Epoch 36: val_loss did not improve from 0.00039\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 3.0391e-04 - mae: 0.0146 - val_loss: 3.9155e-04 - val_mae: 0.0169\n",
      "Epoch 37/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3031e-04 - mae: 0.0115\n",
      "Epoch 37: val_loss improved from 0.00039 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 322us/sample - loss: 2.8397e-04 - mae: 0.0143 - val_loss: 3.8519e-04 - val_mae: 0.0162\n",
      "Epoch 38/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6198e-04 - mae: 0.0090\n",
      "Epoch 38: val_loss improved from 0.00039 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 414us/sample - loss: 2.7980e-04 - mae: 0.0139 - val_loss: 3.8247e-04 - val_mae: 0.0166\n",
      "Epoch 39/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8036e-04 - mae: 0.0136\n",
      "Epoch 39: val_loss improved from 0.00038 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 417us/sample - loss: 2.6745e-04 - mae: 0.0138 - val_loss: 3.7948e-04 - val_mae: 0.0162\n",
      "Epoch 40/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9138e-04 - mae: 0.0128\n",
      "Epoch 40: val_loss did not improve from 0.00038\n",
      "160/160 [==============================] - 0s 227us/sample - loss: 2.7092e-04 - mae: 0.0140 - val_loss: 3.8960e-04 - val_mae: 0.0169\n",
      "Epoch 41/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8579e-04 - mae: 0.0105\n",
      "Epoch 41: val_loss improved from 0.00038 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 419us/sample - loss: 2.5641e-04 - mae: 0.0134 - val_loss: 3.7547e-04 - val_mae: 0.0160\n",
      "Epoch 42/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6807e-04 - mae: 0.0140\n",
      "Epoch 42: val_loss improved from 0.00038 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 421us/sample - loss: 2.5107e-04 - mae: 0.0133 - val_loss: 3.7290e-04 - val_mae: 0.0161\n",
      "Epoch 43/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8475e-04 - mae: 0.0182\n",
      "Epoch 43: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 2.4340e-04 - mae: 0.0130 - val_loss: 3.7781e-04 - val_mae: 0.0165\n",
      "Epoch 44/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8619e-04 - mae: 0.0113\n",
      "Epoch 44: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 2.4366e-04 - mae: 0.0131 - val_loss: 3.7463e-04 - val_mae: 0.0158\n",
      "Epoch 45/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2325e-04 - mae: 0.0170\n",
      "Epoch 45: val_loss improved from 0.00037 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 368us/sample - loss: 2.3129e-04 - mae: 0.0126 - val_loss: 3.7006e-04 - val_mae: 0.0162\n",
      "Epoch 46/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5334e-04 - mae: 0.0166\n",
      "Epoch 46: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 2.2455e-04 - mae: 0.0125 - val_loss: 3.8081e-04 - val_mae: 0.0166\n",
      "Epoch 47/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0284e-04 - mae: 0.0156\n",
      "Epoch 47: val_loss improved from 0.00037 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 363us/sample - loss: 2.1979e-04 - mae: 0.0124 - val_loss: 3.6552e-04 - val_mae: 0.0160\n",
      "Epoch 48/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4448e-04 - mae: 0.0103\n",
      "Epoch 48: val_loss improved from 0.00037 to 0.00037, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 402us/sample - loss: 2.1349e-04 - mae: 0.0123 - val_loss: 3.6536e-04 - val_mae: 0.0156\n",
      "Epoch 49/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3775e-04 - mae: 0.0131\n",
      "Epoch 49: val_loss did not improve from 0.00037\n",
      "160/160 [==============================] - 0s 228us/sample - loss: 2.0374e-04 - mae: 0.0118 - val_loss: 3.7058e-04 - val_mae: 0.0162\n",
      "Epoch 50/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0806e-04 - mae: 0.0136\n",
      "Epoch 50: val_loss improved from 0.00037 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 367us/sample - loss: 1.9913e-04 - mae: 0.0118 - val_loss: 3.6101e-04 - val_mae: 0.0158\n",
      "Epoch 51/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0025e-04 - mae: 0.0115\n",
      "Epoch 51: val_loss improved from 0.00036 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 378us/sample - loss: 1.8937e-04 - mae: 0.0115 - val_loss: 3.5946e-04 - val_mae: 0.0157\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0335e-04 - mae: 0.0094\n",
      "Epoch 52: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 272us/sample - loss: 1.8401e-04 - mae: 0.0112 - val_loss: 3.6090e-04 - val_mae: 0.0159\n",
      "Epoch 53/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3218e-04 - mae: 0.0107\n",
      "Epoch 53: val_loss did not improve from 0.00036\n",
      "160/160 [==============================] - 0s 202us/sample - loss: 1.7768e-04 - mae: 0.0110 - val_loss: 3.6102e-04 - val_mae: 0.0159\n",
      "Epoch 54/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7710e-04 - mae: 0.0117\n",
      "Epoch 54: val_loss improved from 0.00036 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 346us/sample - loss: 1.7267e-04 - mae: 0.0108 - val_loss: 3.5491e-04 - val_mae: 0.0156\n",
      "Epoch 55/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4762e-04 - mae: 0.0134\n",
      "Epoch 55: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 358us/sample - loss: 1.6949e-04 - mae: 0.0107 - val_loss: 3.5268e-04 - val_mae: 0.0154\n",
      "Epoch 56/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3400e-04 - mae: 0.0136\n",
      "Epoch 56: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 1.6416e-04 - mae: 0.0105 - val_loss: 3.6128e-04 - val_mae: 0.0158\n",
      "Epoch 57/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6950e-04 - mae: 0.0107\n",
      "Epoch 57: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 389us/sample - loss: 1.5354e-04 - mae: 0.0103 - val_loss: 3.5031e-04 - val_mae: 0.0153\n",
      "Epoch 58/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1295e-04 - mae: 0.0088\n",
      "Epoch 58: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 224us/sample - loss: 1.5223e-04 - mae: 0.0101 - val_loss: 3.5652e-04 - val_mae: 0.0157\n",
      "Epoch 59/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9953e-04 - mae: 0.0108\n",
      "Epoch 59: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 1.4906e-04 - mae: 0.0100 - val_loss: 3.6707e-04 - val_mae: 0.0149\n",
      "Epoch 60/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4325e-04 - mae: 0.0158\n",
      "Epoch 60: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 1.4687e-04 - mae: 0.0097 - val_loss: 3.9590e-04 - val_mae: 0.0167\n",
      "Epoch 61/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8046e-05 - mae: 0.0078\n",
      "Epoch 61: val_loss improved from 0.00035 to 0.00035, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 354us/sample - loss: 1.4550e-04 - mae: 0.0096 - val_loss: 3.4684e-04 - val_mae: 0.0150\n",
      "Epoch 62/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4232e-05 - mae: 0.0072\n",
      "Epoch 62: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 1.3309e-04 - mae: 0.0094 - val_loss: 3.5223e-04 - val_mae: 0.0155\n",
      "Epoch 63/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3298e-05 - mae: 0.0081\n",
      "Epoch 63: val_loss did not improve from 0.00035\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.3477e-04 - mae: 0.0095 - val_loss: 3.5073e-04 - val_mae: 0.0154\n",
      "Epoch 64/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4145e-04 - mae: 0.0100\n",
      "Epoch 64: val_loss improved from 0.00035 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 342us/sample - loss: 1.2388e-04 - mae: 0.0091 - val_loss: 3.4367e-04 - val_mae: 0.0151\n",
      "Epoch 65/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8312e-04 - mae: 0.0115\n",
      "Epoch 65: val_loss improved from 0.00034 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 438us/sample - loss: 1.2021e-04 - mae: 0.0089 - val_loss: 3.4296e-04 - val_mae: 0.0151\n",
      "Epoch 66/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4157e-04 - mae: 0.0091\n",
      "Epoch 66: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 1.1955e-04 - mae: 0.0089 - val_loss: 3.4754e-04 - val_mae: 0.0153\n",
      "Epoch 67/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5551e-04 - mae: 0.0105\n",
      "Epoch 67: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 202us/sample - loss: 1.2386e-04 - mae: 0.0089 - val_loss: 3.8709e-04 - val_mae: 0.0164\n",
      "Epoch 68/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5787e-04 - mae: 0.0108\n",
      "Epoch 68: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 220us/sample - loss: 1.1789e-04 - mae: 0.0089 - val_loss: 3.6714e-04 - val_mae: 0.0159\n",
      "Epoch 69/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2001e-04 - mae: 0.0089\n",
      "Epoch 69: val_loss improved from 0.00034 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 391us/sample - loss: 1.1151e-04 - mae: 0.0086 - val_loss: 3.3955e-04 - val_mae: 0.0148\n",
      "Epoch 70/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4392e-05 - mae: 0.0053\n",
      "Epoch 70: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 212us/sample - loss: 1.0309e-04 - mae: 0.0082 - val_loss: 3.5244e-04 - val_mae: 0.0154\n",
      "Epoch 71/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1095e-04 - mae: 0.0086\n",
      "Epoch 71: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.0444e-04 - mae: 0.0081 - val_loss: 3.4597e-04 - val_mae: 0.0152\n",
      "Epoch 72/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.8426e-05 - mae: 0.0062\n",
      "Epoch 72: val_loss improved from 0.00034 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 378us/sample - loss: 9.8717e-05 - mae: 0.0080 - val_loss: 3.3782e-04 - val_mae: 0.0147\n",
      "Epoch 73/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0774e-04 - mae: 0.0096\n",
      "Epoch 73: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 216us/sample - loss: 9.4156e-05 - mae: 0.0078 - val_loss: 3.3969e-04 - val_mae: 0.0148\n",
      "Epoch 74/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.7157e-05 - mae: 0.0059\n",
      "Epoch 74: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 224us/sample - loss: 9.1932e-05 - mae: 0.0077 - val_loss: 3.4086e-04 - val_mae: 0.0149\n",
      "Epoch 75/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.5803e-05 - mae: 0.0054\n",
      "Epoch 75: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 8.9088e-05 - mae: 0.0075 - val_loss: 3.4340e-04 - val_mae: 0.0151\n",
      "Epoch 76/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4460e-05 - mae: 0.0066\n",
      "Epoch 76: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 8.6592e-05 - mae: 0.0075 - val_loss: 3.6770e-04 - val_mae: 0.0159\n",
      "Epoch 77/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2089e-04 - mae: 0.0092\n",
      "Epoch 77: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 8.5858e-05 - mae: 0.0075 - val_loss: 3.4132e-04 - val_mae: 0.0149\n",
      "Epoch 78/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.9133e-05 - mae: 0.0091\n",
      "Epoch 78: val_loss improved from 0.00034 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 316us/sample - loss: 8.0271e-05 - mae: 0.0071 - val_loss: 3.3778e-04 - val_mae: 0.0147\n",
      "Epoch 79/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.1953e-05 - mae: 0.0061\n",
      "Epoch 79: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 7.9536e-05 - mae: 0.0070 - val_loss: 3.4190e-04 - val_mae: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.7022e-05 - mae: 0.0070\n",
      "Epoch 80: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 7.6649e-05 - mae: 0.0070 - val_loss: 3.4252e-04 - val_mae: 0.0150\n",
      "Epoch 81/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0585e-04 - mae: 0.0095\n",
      "Epoch 81: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 212us/sample - loss: 7.6972e-05 - mae: 0.0071 - val_loss: 3.5893e-04 - val_mae: 0.0156\n",
      "Epoch 82/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1284e-05 - mae: 0.0051\n",
      "Epoch 82: val_loss did not improve from 0.00034\n",
      "160/160 [==============================] - 0s 210us/sample - loss: 7.4807e-05 - mae: 0.0069 - val_loss: 3.4988e-04 - val_mae: 0.0153\n",
      "Epoch 83/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2885e-05 - mae: 0.0052\n",
      "Epoch 83: val_loss improved from 0.00034 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 367us/sample - loss: 6.8620e-05 - mae: 0.0066 - val_loss: 3.3317e-04 - val_mae: 0.0143\n",
      "Epoch 84/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.0355e-05 - mae: 0.0056\n",
      "Epoch 84: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 7.5920e-05 - mae: 0.0069 - val_loss: 3.4424e-04 - val_mae: 0.0150\n",
      "Epoch 85/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0873e-05 - mae: 0.0039\n",
      "Epoch 85: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 353us/sample - loss: 7.1679e-05 - mae: 0.0068 - val_loss: 3.3260e-04 - val_mae: 0.0143\n",
      "Epoch 86/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.1828e-05 - mae: 0.0076\n",
      "Epoch 86: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 7.7345e-05 - mae: 0.0069 - val_loss: 3.3963e-04 - val_mae: 0.0142\n",
      "Epoch 87/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9658e-05 - mae: 0.0050\n",
      "Epoch 87: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 209us/sample - loss: 7.0495e-05 - mae: 0.0068 - val_loss: 3.3269e-04 - val_mae: 0.0144\n",
      "Epoch 88/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.5898e-05 - mae: 0.0058\n",
      "Epoch 88: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 6.6067e-05 - mae: 0.0064 - val_loss: 3.3574e-04 - val_mae: 0.0146\n",
      "Epoch 89/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8107e-05 - mae: 0.0067\n",
      "Epoch 89: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 6.1372e-05 - mae: 0.0063 - val_loss: 3.4418e-04 - val_mae: 0.0150\n",
      "Epoch 90/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1594e-04 - mae: 0.0091\n",
      "Epoch 90: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 5.9685e-05 - mae: 0.0062 - val_loss: 3.4840e-04 - val_mae: 0.0151\n",
      "Epoch 91/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8614e-05 - mae: 0.0056\n",
      "Epoch 91: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 5.8205e-05 - mae: 0.0061 - val_loss: 3.4344e-04 - val_mae: 0.0150\n",
      "Epoch 92/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4101e-05 - mae: 0.0052\n",
      "Epoch 92: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 5.7744e-05 - mae: 0.0061 - val_loss: 3.4058e-04 - val_mae: 0.0149\n",
      "Epoch 93/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.5763e-05 - mae: 0.0078\n",
      "Epoch 93: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 5.6558e-05 - mae: 0.0060 - val_loss: 3.3284e-04 - val_mae: 0.0145\n",
      "Epoch 94/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9790e-05 - mae: 0.0058\n",
      "Epoch 94: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 5.6272e-05 - mae: 0.0060 - val_loss: 3.4246e-04 - val_mae: 0.0150\n",
      "Epoch 95/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0122e-05 - mae: 0.0057\n",
      "Epoch 95: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 364us/sample - loss: 5.3928e-05 - mae: 0.0059 - val_loss: 3.3167e-04 - val_mae: 0.0144\n",
      "Epoch 96/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1892e-05 - mae: 0.0046\n",
      "Epoch 96: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 331us/sample - loss: 5.5056e-05 - mae: 0.0060 - val_loss: 3.3025e-04 - val_mae: 0.0141\n",
      "Epoch 97/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2369e-05 - mae: 0.0054\n",
      "Epoch 97: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 5.3561e-05 - mae: 0.0059 - val_loss: 3.5385e-04 - val_mae: 0.0153\n",
      "Epoch 98/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6835e-05 - mae: 0.0071\n",
      "Epoch 98: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 5.2950e-05 - mae: 0.0059 - val_loss: 3.3509e-04 - val_mae: 0.0146\n",
      "Epoch 99/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.0254e-05 - mae: 0.0054\n",
      "Epoch 99: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 364us/sample - loss: 4.9459e-05 - mae: 0.0057 - val_loss: 3.2949e-04 - val_mae: 0.0142\n",
      "Epoch 100/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9801e-05 - mae: 0.0040\n",
      "Epoch 100: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 4.8930e-05 - mae: 0.0056 - val_loss: 3.3513e-04 - val_mae: 0.0146\n",
      "Epoch 101/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.4251e-05 - mae: 0.0060\n",
      "Epoch 101: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 4.9491e-05 - mae: 0.0056 - val_loss: 3.5003e-04 - val_mae: 0.0152\n",
      "Epoch 102/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7057e-05 - mae: 0.0071\n",
      "Epoch 102: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 4.5476e-05 - mae: 0.0055 - val_loss: 3.3325e-04 - val_mae: 0.0145\n",
      "Epoch 103/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5234e-05 - mae: 0.0045\n",
      "Epoch 103: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 362us/sample - loss: 4.5466e-05 - mae: 0.0056 - val_loss: 3.2922e-04 - val_mae: 0.0142\n",
      "Epoch 104/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1704e-05 - mae: 0.0054\n",
      "Epoch 104: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 413us/sample - loss: 4.6081e-05 - mae: 0.0054 - val_loss: 3.2884e-04 - val_mae: 0.0142\n",
      "Epoch 105/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6478e-05 - mae: 0.0059\n",
      "Epoch 105: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 230us/sample - loss: 4.3038e-05 - mae: 0.0052 - val_loss: 3.4098e-04 - val_mae: 0.0150\n",
      "Epoch 106/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2934e-05 - mae: 0.0055\n",
      "Epoch 106: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 229us/sample - loss: 4.0635e-05 - mae: 0.0052 - val_loss: 3.4058e-04 - val_mae: 0.0149\n",
      "Epoch 107/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1233e-05 - mae: 0.0048\n",
      "Epoch 107: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 215us/sample - loss: 4.3133e-05 - mae: 0.0053 - val_loss: 3.3870e-04 - val_mae: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3639e-05 - mae: 0.0052\n",
      "Epoch 108: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 216us/sample - loss: 3.8685e-05 - mae: 0.0050 - val_loss: 3.2974e-04 - val_mae: 0.0143\n",
      "Epoch 109/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2531e-05 - mae: 0.0057\n",
      "Epoch 109: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 225us/sample - loss: 4.0646e-05 - mae: 0.0051 - val_loss: 3.4596e-04 - val_mae: 0.0152\n",
      "Epoch 110/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0691e-05 - mae: 0.0059\n",
      "Epoch 110: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 3.8587e-05 - mae: 0.0050 - val_loss: 3.3822e-04 - val_mae: 0.0149\n",
      "Epoch 111/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8923e-05 - mae: 0.0035\n",
      "Epoch 111: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 3.6874e-05 - mae: 0.0049 - val_loss: 3.6085e-04 - val_mae: 0.0156\n",
      "Epoch 112/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3417e-05 - mae: 0.0077\n",
      "Epoch 112: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 3.9818e-05 - mae: 0.0051 - val_loss: 3.3962e-04 - val_mae: 0.0149\n",
      "Epoch 113/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6146e-05 - mae: 0.0052\n",
      "Epoch 113: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 218us/sample - loss: 3.9007e-05 - mae: 0.0051 - val_loss: 3.3303e-04 - val_mae: 0.0146\n",
      "Epoch 114/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0642e-05 - mae: 0.0053\n",
      "Epoch 114: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 3.6285e-05 - mae: 0.0049 - val_loss: 3.5414e-04 - val_mae: 0.0155\n",
      "Epoch 115/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5133e-05 - mae: 0.0044\n",
      "Epoch 115: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 3.4327e-05 - mae: 0.0047 - val_loss: 3.4218e-04 - val_mae: 0.0150\n",
      "Epoch 116/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0510e-05 - mae: 0.0035\n",
      "Epoch 116: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 325us/sample - loss: 3.2951e-05 - mae: 0.0047 - val_loss: 3.2754e-04 - val_mae: 0.0143\n",
      "Epoch 117/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0736e-05 - mae: 0.0043\n",
      "Epoch 117: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 3.3835e-05 - mae: 0.0048 - val_loss: 3.4571e-04 - val_mae: 0.0152\n",
      "Epoch 118/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0856e-05 - mae: 0.0050\n",
      "Epoch 118: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 3.3140e-05 - mae: 0.0047 - val_loss: 3.3861e-04 - val_mae: 0.0149\n",
      "Epoch 119/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5187e-05 - mae: 0.0038\n",
      "Epoch 119: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 3.0481e-05 - mae: 0.0045 - val_loss: 3.4096e-04 - val_mae: 0.0150\n",
      "Epoch 120/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3912e-05 - mae: 0.0049\n",
      "Epoch 120: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 219us/sample - loss: 3.0470e-05 - mae: 0.0045 - val_loss: 3.4416e-04 - val_mae: 0.0151\n",
      "Epoch 121/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9402e-05 - mae: 0.0048\n",
      "Epoch 121: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 3.0053e-05 - mae: 0.0044 - val_loss: 3.4469e-04 - val_mae: 0.0151\n",
      "Epoch 122/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1639e-05 - mae: 0.0042\n",
      "Epoch 122: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 2.9626e-05 - mae: 0.0044 - val_loss: 3.5342e-04 - val_mae: 0.0155\n",
      "Epoch 123/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1777e-05 - mae: 0.0043\n",
      "Epoch 123: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 3.2947e-05 - mae: 0.0048 - val_loss: 3.4617e-04 - val_mae: 0.0152\n",
      "Epoch 124/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3468e-05 - mae: 0.0045\n",
      "Epoch 124: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 3.0328e-05 - mae: 0.0044 - val_loss: 3.3293e-04 - val_mae: 0.0147\n",
      "Epoch 125/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8616e-05 - mae: 0.0049\n",
      "Epoch 125: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 2.8479e-05 - mae: 0.0044 - val_loss: 3.3188e-04 - val_mae: 0.0147\n",
      "Epoch 126/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4772e-05 - mae: 0.0034\n",
      "Epoch 126: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 2.7188e-05 - mae: 0.0043 - val_loss: 3.3179e-04 - val_mae: 0.0147\n",
      "Epoch 127/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0374e-05 - mae: 0.0027\n",
      "Epoch 127: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 2.8558e-05 - mae: 0.0044 - val_loss: 3.2756e-04 - val_mae: 0.0145\n",
      "Epoch 128/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7086e-05 - mae: 0.0033\n",
      "Epoch 128: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 2.9178e-05 - mae: 0.0043 - val_loss: 3.2780e-04 - val_mae: 0.0145\n",
      "Epoch 129/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3782e-05 - mae: 0.0047\n",
      "Epoch 129: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 315us/sample - loss: 2.6983e-05 - mae: 0.0042 - val_loss: 3.2623e-04 - val_mae: 0.0144\n",
      "Epoch 130/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7272e-05 - mae: 0.0046\n",
      "Epoch 130: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 2.6307e-05 - mae: 0.0042 - val_loss: 3.4017e-04 - val_mae: 0.0150\n",
      "Epoch 131/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0159e-05 - mae: 0.0036\n",
      "Epoch 131: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 2.3965e-05 - mae: 0.0039 - val_loss: 3.4094e-04 - val_mae: 0.0151\n",
      "Epoch 132/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1157e-05 - mae: 0.0038\n",
      "Epoch 132: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 307us/sample - loss: 2.4665e-05 - mae: 0.0041 - val_loss: 3.2552e-04 - val_mae: 0.0143\n",
      "Epoch 133/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6919e-06 - mae: 0.0019\n",
      "Epoch 133: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 392us/sample - loss: 2.5384e-05 - mae: 0.0040 - val_loss: 3.2548e-04 - val_mae: 0.0144\n",
      "Epoch 134/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1756e-05 - mae: 0.0062\n",
      "Epoch 134: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 247us/sample - loss: 2.2518e-05 - mae: 0.0039 - val_loss: 3.3546e-04 - val_mae: 0.0149\n",
      "Epoch 135/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5887e-05 - mae: 0.0054\n",
      "Epoch 135: val_loss did not improve from 0.00033\n",
      "160/160 [==============================] - 0s 241us/sample - loss: 2.1802e-05 - mae: 0.0039 - val_loss: 3.3278e-04 - val_mae: 0.0148\n",
      "Epoch 136/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6330e-05 - mae: 0.0051\n",
      "Epoch 136: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 424us/sample - loss: 2.0669e-05 - mae: 0.0037 - val_loss: 3.2540e-04 - val_mae: 0.0144\n",
      "Epoch 137/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0431e-05 - mae: 0.0035\n",
      "Epoch 137: val_loss improved from 0.00033 to 0.00032, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "160/160 [==============================] - 0s 340us/sample - loss: 2.2093e-05 - mae: 0.0038 - val_loss: 3.2447e-04 - val_mae: 0.0144\n",
      "Epoch 138/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0554e-05 - mae: 0.0023\n",
      "Epoch 138: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 2.2375e-05 - mae: 0.0036 - val_loss: 3.2722e-04 - val_mae: 0.0145\n",
      "Epoch 139/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3059e-05 - mae: 0.0038\n",
      "Epoch 139: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 1.9115e-05 - mae: 0.0035 - val_loss: 3.3850e-04 - val_mae: 0.0150\n",
      "Epoch 140/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5638e-05 - mae: 0.0041\n",
      "Epoch 140: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 1.9700e-05 - mae: 0.0036 - val_loss: 3.3971e-04 - val_mae: 0.0151\n",
      "Epoch 141/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8748e-05 - mae: 0.0034\n",
      "Epoch 141: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 172us/sample - loss: 1.8419e-05 - mae: 0.0035 - val_loss: 3.5001e-04 - val_mae: 0.0154\n",
      "Epoch 142/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2760e-05 - mae: 0.0041\n",
      "Epoch 142: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 1.8449e-05 - mae: 0.0035 - val_loss: 3.3291e-04 - val_mae: 0.0148\n",
      "Epoch 143/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1024e-05 - mae: 0.0029\n",
      "Epoch 143: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 1.8274e-05 - mae: 0.0034 - val_loss: 3.3465e-04 - val_mae: 0.0149\n",
      "Epoch 144/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7947e-06 - mae: 0.0024\n",
      "Epoch 144: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 1.7262e-05 - mae: 0.0034 - val_loss: 3.2657e-04 - val_mae: 0.0145\n",
      "Epoch 145/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5121e-05 - mae: 0.0043\n",
      "Epoch 145: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 1.9371e-05 - mae: 0.0036 - val_loss: 3.2734e-04 - val_mae: 0.0145\n",
      "Epoch 146/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5459e-05 - mae: 0.0027\n",
      "Epoch 146: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 1.7016e-05 - mae: 0.0033 - val_loss: 3.2971e-04 - val_mae: 0.0147\n",
      "Epoch 147/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.7366e-06 - mae: 0.0020\n",
      "Epoch 147: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 1.6840e-05 - mae: 0.0033 - val_loss: 3.2771e-04 - val_mae: 0.0146\n",
      "Epoch 148/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.5824e-06 - mae: 0.0023\n",
      "Epoch 148: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 216us/sample - loss: 1.5699e-05 - mae: 0.0032 - val_loss: 3.4702e-04 - val_mae: 0.0154\n",
      "Epoch 149/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4559e-05 - mae: 0.0031\n",
      "Epoch 149: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 210us/sample - loss: 1.8840e-05 - mae: 0.0035 - val_loss: 3.3789e-04 - val_mae: 0.0150\n",
      "Epoch 150/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7632e-05 - mae: 0.0036\n",
      "Epoch 150: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 1.4983e-05 - mae: 0.0030 - val_loss: 3.4612e-04 - val_mae: 0.0153\n",
      "Epoch 151/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4924e-05 - mae: 0.0034\n",
      "Epoch 151: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 215us/sample - loss: 1.6204e-05 - mae: 0.0032 - val_loss: 3.3097e-04 - val_mae: 0.0147\n",
      "Epoch 152/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1466e-06 - mae: 0.0021\n",
      "Epoch 152: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 1.4130e-05 - mae: 0.0030 - val_loss: 3.4002e-04 - val_mae: 0.0151\n",
      "Epoch 153/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2395e-05 - mae: 0.0033\n",
      "Epoch 153: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.5119e-05 - mae: 0.0031 - val_loss: 3.3403e-04 - val_mae: 0.0149\n",
      "Epoch 154/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0244e-05 - mae: 0.0027\n",
      "Epoch 154: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 1.3250e-05 - mae: 0.0029 - val_loss: 3.3256e-04 - val_mae: 0.0148\n",
      "Epoch 155/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0852e-05 - mae: 0.0029\n",
      "Epoch 155: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 1.2902e-05 - mae: 0.0029 - val_loss: 3.3087e-04 - val_mae: 0.0147\n",
      "Epoch 156/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2897e-05 - mae: 0.0027\n",
      "Epoch 156: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 1.2713e-05 - mae: 0.0028 - val_loss: 3.3619e-04 - val_mae: 0.0150\n",
      "Epoch 157/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1997e-06 - mae: 0.0021\n",
      "Epoch 157: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 1.3265e-05 - mae: 0.0030 - val_loss: 3.2922e-04 - val_mae: 0.0147\n",
      "Epoch 158/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2188e-05 - mae: 0.0028\n",
      "Epoch 158: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 247us/sample - loss: 1.3020e-05 - mae: 0.0029 - val_loss: 3.3119e-04 - val_mae: 0.0147\n",
      "Epoch 159/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.1068e-06 - mae: 0.0024\n",
      "Epoch 159: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.2976e-05 - mae: 0.0029 - val_loss: 3.3110e-04 - val_mae: 0.0147\n",
      "Epoch 160/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2430e-05 - mae: 0.0029\n",
      "Epoch 160: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 1.1539e-05 - mae: 0.0028 - val_loss: 3.2826e-04 - val_mae: 0.0147\n",
      "Epoch 161/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7992e-05 - mae: 0.0031\n",
      "Epoch 161: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 190us/sample - loss: 1.2349e-05 - mae: 0.0028 - val_loss: 3.3240e-04 - val_mae: 0.0148\n",
      "Epoch 162/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8780e-06 - mae: 0.0025\n",
      "Epoch 162: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 1.1148e-05 - mae: 0.0026 - val_loss: 3.3568e-04 - val_mae: 0.0149\n",
      "Epoch 163/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0581e-05 - mae: 0.0027\n",
      "Epoch 163: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 1.1169e-05 - mae: 0.0027 - val_loss: 3.3839e-04 - val_mae: 0.0151\n",
      "Epoch 164/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0371e-06 - mae: 0.0025\n",
      "Epoch 164: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 1.1228e-05 - mae: 0.0027 - val_loss: 3.4583e-04 - val_mae: 0.0153\n",
      "Epoch 165/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.5337e-06 - mae: 0.0020\n",
      "Epoch 165: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 1.5298e-05 - mae: 0.0032 - val_loss: 3.4369e-04 - val_mae: 0.0152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.9017e-06 - mae: 0.0027\n",
      "Epoch 166: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 1.0635e-05 - mae: 0.0026 - val_loss: 3.3176e-04 - val_mae: 0.0148\n",
      "Epoch 167/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3490e-06 - mae: 0.0022\n",
      "Epoch 167: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 9.5659e-06 - mae: 0.0025 - val_loss: 3.4178e-04 - val_mae: 0.0152\n",
      "Epoch 168/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.5108e-06 - mae: 0.0024\n",
      "Epoch 168: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 1.0922e-05 - mae: 0.0026 - val_loss: 3.3649e-04 - val_mae: 0.0150\n",
      "Epoch 169/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8287e-06 - mae: 0.0017\n",
      "Epoch 169: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 1.1172e-05 - mae: 0.0026 - val_loss: 3.3059e-04 - val_mae: 0.0148\n",
      "Epoch 170/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.0176e-06 - mae: 0.0028\n",
      "Epoch 170: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 9.6743e-06 - mae: 0.0025 - val_loss: 3.3451e-04 - val_mae: 0.0149\n",
      "Epoch 171/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9288e-06 - mae: 0.0024\n",
      "Epoch 171: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 9.4073e-06 - mae: 0.0025 - val_loss: 3.3094e-04 - val_mae: 0.0148\n",
      "Epoch 172/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2481e-06 - mae: 0.0019\n",
      "Epoch 172: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 218us/sample - loss: 9.0887e-06 - mae: 0.0024 - val_loss: 3.3371e-04 - val_mae: 0.0149\n",
      "Epoch 173/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0936e-05 - mae: 0.0028\n",
      "Epoch 173: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 226us/sample - loss: 9.4293e-06 - mae: 0.0024 - val_loss: 3.3600e-04 - val_mae: 0.0150\n",
      "Epoch 174/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4307e-06 - mae: 0.0014\n",
      "Epoch 174: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 9.0692e-06 - mae: 0.0024 - val_loss: 3.3401e-04 - val_mae: 0.0149\n",
      "Epoch 175/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5413e-06 - mae: 0.0010\n",
      "Epoch 175: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 8.4185e-06 - mae: 0.0023 - val_loss: 3.3761e-04 - val_mae: 0.0150\n",
      "Epoch 176/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9105e-06 - mae: 0.0018\n",
      "Epoch 176: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 7.8584e-06 - mae: 0.0022 - val_loss: 3.3745e-04 - val_mae: 0.0150\n",
      "Epoch 177/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.9065e-06 - mae: 0.0020\n",
      "Epoch 177: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 214us/sample - loss: 8.1007e-06 - mae: 0.0023 - val_loss: 3.3516e-04 - val_mae: 0.0150\n",
      "Epoch 178/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.4450e-06 - mae: 0.0025\n",
      "Epoch 178: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 8.5347e-06 - mae: 0.0024 - val_loss: 3.3190e-04 - val_mae: 0.0149\n",
      "Epoch 179/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3185e-06 - mae: 0.0020\n",
      "Epoch 179: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 8.4358e-06 - mae: 0.0022 - val_loss: 3.4028e-04 - val_mae: 0.0151\n",
      "Epoch 180/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6489e-06 - mae: 0.0019\n",
      "Epoch 180: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 8.0004e-06 - mae: 0.0022 - val_loss: 3.3464e-04 - val_mae: 0.0150\n",
      "Epoch 181/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.3094e-06 - mae: 0.0024\n",
      "Epoch 181: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 287us/sample - loss: 8.5023e-06 - mae: 0.0023 - val_loss: 3.3217e-04 - val_mae: 0.0149\n",
      "Epoch 182/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3901e-06 - mae: 0.0025\n",
      "Epoch 182: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 202us/sample - loss: 7.8503e-06 - mae: 0.0022 - val_loss: 3.3696e-04 - val_mae: 0.0150\n",
      "Epoch 183/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2164e-06 - mae: 0.0022\n",
      "Epoch 183: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 6.4874e-06 - mae: 0.0020 - val_loss: 3.3756e-04 - val_mae: 0.0151\n",
      "Epoch 184/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4798e-06 - mae: 0.0017\n",
      "Epoch 184: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 6.5608e-06 - mae: 0.0021 - val_loss: 3.3689e-04 - val_mae: 0.0150\n",
      "Epoch 185/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7485e-06 - mae: 0.0019\n",
      "Epoch 185: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 6.2781e-06 - mae: 0.0020 - val_loss: 3.3752e-04 - val_mae: 0.0151\n",
      "Epoch 186/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7828e-06 - mae: 0.0015\n",
      "Epoch 186: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 7.5469e-06 - mae: 0.0021 - val_loss: 3.3565e-04 - val_mae: 0.0150\n",
      "Epoch 187/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2992e-06 - mae: 0.0019\n",
      "Epoch 187: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 216us/sample - loss: 9.4149e-06 - mae: 0.0024 - val_loss: 3.4661e-04 - val_mae: 0.0153\n",
      "Epoch 188/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0060e-05 - mae: 0.0027\n",
      "Epoch 188: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 8.7184e-06 - mae: 0.0024 - val_loss: 3.3337e-04 - val_mae: 0.0150\n",
      "Epoch 189/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5830e-05 - mae: 0.0031\n",
      "Epoch 189: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 7.8657e-06 - mae: 0.0022 - val_loss: 3.4905e-04 - val_mae: 0.0154\n",
      "Epoch 190/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.9960e-06 - mae: 0.0020\n",
      "Epoch 190: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 7.5517e-06 - mae: 0.0022 - val_loss: 3.3629e-04 - val_mae: 0.0151\n",
      "Epoch 191/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1100e-06 - mae: 0.0017\n",
      "Epoch 191: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 5.4749e-06 - mae: 0.0019 - val_loss: 3.3776e-04 - val_mae: 0.0151\n",
      "Epoch 192/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6942e-06 - mae: 0.0019\n",
      "Epoch 192: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 4.9982e-06 - mae: 0.0018 - val_loss: 3.3586e-04 - val_mae: 0.0151\n",
      "Epoch 193/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.9403e-06 - mae: 0.0025\n",
      "Epoch 193: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 5.0362e-06 - mae: 0.0018 - val_loss: 3.3991e-04 - val_mae: 0.0151\n",
      "Epoch 194/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8806e-06 - mae: 0.0018\n",
      "Epoch 194: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 5.0466e-06 - mae: 0.0018 - val_loss: 3.3999e-04 - val_mae: 0.0152\n",
      "Epoch 195/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1761e-06 - mae: 0.0017\n",
      "Epoch 195: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 4.7665e-06 - mae: 0.0017 - val_loss: 3.4124e-04 - val_mae: 0.0152\n",
      "Epoch 196/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0113e-06 - mae: 0.0015\n",
      "Epoch 196: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 4.6459e-06 - mae: 0.0017 - val_loss: 3.4087e-04 - val_mae: 0.0152\n",
      "Epoch 197/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.9030e-06 - mae: 0.0020\n",
      "Epoch 197: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 171us/sample - loss: 4.7096e-06 - mae: 0.0017 - val_loss: 3.4173e-04 - val_mae: 0.0152\n",
      "Epoch 198/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9542e-06 - mae: 0.0018\n",
      "Epoch 198: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 4.9395e-06 - mae: 0.0018 - val_loss: 3.3940e-04 - val_mae: 0.0152\n",
      "Epoch 199/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7429e-06 - mae: 0.0013\n",
      "Epoch 199: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 4.3775e-06 - mae: 0.0017 - val_loss: 3.4099e-04 - val_mae: 0.0152\n",
      "Epoch 200/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0664e-06 - mae: 0.0016\n",
      "Epoch 200: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 4.7461e-06 - mae: 0.0017 - val_loss: 3.3685e-04 - val_mae: 0.0152\n",
      "Epoch 201/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.1509e-06 - mae: 0.0019\n",
      "Epoch 201: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 5.6800e-06 - mae: 0.0018 - val_loss: 3.4300e-04 - val_mae: 0.0152\n",
      "Epoch 202/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.7936e-06 - mae: 0.0017\n",
      "Epoch 202: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 224us/sample - loss: 4.6143e-06 - mae: 0.0017 - val_loss: 3.4104e-04 - val_mae: 0.0152\n",
      "Epoch 203/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8446e-06 - mae: 0.0018\n",
      "Epoch 203: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 213us/sample - loss: 4.4553e-06 - mae: 0.0017 - val_loss: 3.3892e-04 - val_mae: 0.0152\n",
      "Epoch 204/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4545e-06 - mae: 0.0013\n",
      "Epoch 204: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 4.2651e-06 - mae: 0.0016 - val_loss: 3.4466e-04 - val_mae: 0.0153\n",
      "Epoch 205/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1610e-06 - mae: 0.0020\n",
      "Epoch 205: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 4.8994e-06 - mae: 0.0018 - val_loss: 3.3736e-04 - val_mae: 0.0152\n",
      "Epoch 206/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.9959e-06 - mae: 0.0021\n",
      "Epoch 206: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 4.4253e-06 - mae: 0.0016 - val_loss: 3.4313e-04 - val_mae: 0.0153\n",
      "Epoch 207/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2601e-06 - mae: 0.0025\n",
      "Epoch 207: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 214us/sample - loss: 4.0757e-06 - mae: 0.0016 - val_loss: 3.4557e-04 - val_mae: 0.0153\n",
      "Epoch 208/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8538e-06 - mae: 0.0017\n",
      "Epoch 208: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 209us/sample - loss: 4.5488e-06 - mae: 0.0017 - val_loss: 3.3694e-04 - val_mae: 0.0152\n",
      "Epoch 209/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6027e-06 - mae: 0.0014\n",
      "Epoch 209: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 3.9405e-06 - mae: 0.0016 - val_loss: 3.3991e-04 - val_mae: 0.0153\n",
      "Epoch 210/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2115e-06 - mae: 0.0016\n",
      "Epoch 210: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 3.4615e-06 - mae: 0.0015 - val_loss: 3.4501e-04 - val_mae: 0.0153\n",
      "Epoch 211/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0103e-06 - mae: 0.0014\n",
      "Epoch 211: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 3.9436e-06 - mae: 0.0016 - val_loss: 3.3933e-04 - val_mae: 0.0153\n",
      "Epoch 212/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3253e-06 - mae: 0.0015\n",
      "Epoch 212: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 4.0098e-06 - mae: 0.0015 - val_loss: 3.4394e-04 - val_mae: 0.0153\n",
      "Epoch 213/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3106e-06 - mae: 0.0012\n",
      "Epoch 213: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 3.1399e-06 - mae: 0.0014 - val_loss: 3.3897e-04 - val_mae: 0.0153\n",
      "Epoch 214/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6481e-06 - mae: 0.0017\n",
      "Epoch 214: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 3.3192e-06 - mae: 0.0014 - val_loss: 3.4005e-04 - val_mae: 0.0153\n",
      "Epoch 215/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.5370e-06 - mae: 0.0019\n",
      "Epoch 215: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 261us/sample - loss: 3.8442e-06 - mae: 0.0016 - val_loss: 3.3886e-04 - val_mae: 0.0153\n",
      "Epoch 216/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7748e-06 - mae: 0.0012\n",
      "Epoch 216: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 213us/sample - loss: 3.1275e-06 - mae: 0.0014 - val_loss: 3.4193e-04 - val_mae: 0.0153\n",
      "Epoch 217/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2159e-06 - mae: 0.0013\n",
      "Epoch 217: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 2.8206e-06 - mae: 0.0013 - val_loss: 3.4272e-04 - val_mae: 0.0153\n",
      "Epoch 218/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2112e-06 - mae: 0.0011\n",
      "Epoch 218: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 2.7906e-06 - mae: 0.0013 - val_loss: 3.4317e-04 - val_mae: 0.0153\n",
      "Epoch 219/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2696e-06 - mae: 0.0012\n",
      "Epoch 219: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 2.9205e-06 - mae: 0.0014 - val_loss: 3.4339e-04 - val_mae: 0.0154\n",
      "Epoch 220/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8268e-06 - mae: 9.9046e-04\n",
      "Epoch 220: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 2.6923e-06 - mae: 0.0013 - val_loss: 3.4371e-04 - val_mae: 0.0154\n",
      "Epoch 221/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8064e-06 - mae: 0.0016\n",
      "Epoch 221: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 2.8620e-06 - mae: 0.0014 - val_loss: 3.4191e-04 - val_mae: 0.0153\n",
      "Epoch 222/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3037e-06 - mae: 0.0010\n",
      "Epoch 222: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 2.3798e-06 - mae: 0.0012 - val_loss: 3.4261e-04 - val_mae: 0.0154\n",
      "Epoch 223/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3567e-06 - mae: 0.0010\n",
      "Epoch 223: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 2.6201e-06 - mae: 0.0013 - val_loss: 3.4236e-04 - val_mae: 0.0154\n",
      "Epoch 224/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4329e-06 - mae: 9.0651e-04\n",
      "Epoch 224: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 3.0011e-06 - mae: 0.0013 - val_loss: 3.4523e-04 - val_mae: 0.0154\n",
      "Epoch 225/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0189e-06 - mae: 8.2339e-04\n",
      "Epoch 225: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 2.6834e-06 - mae: 0.0013 - val_loss: 3.4061e-04 - val_mae: 0.0153\n",
      "Epoch 226/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.5305e-06 - mae: 0.0022\n",
      "Epoch 226: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 2.8827e-06 - mae: 0.0014 - val_loss: 3.4588e-04 - val_mae: 0.0154\n",
      "Epoch 227/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2098e-06 - mae: 0.0013\n",
      "Epoch 227: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 2.2740e-06 - mae: 0.0012 - val_loss: 3.4460e-04 - val_mae: 0.0154\n",
      "Epoch 228/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5994e-06 - mae: 0.0010\n",
      "Epoch 228: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 2.0626e-06 - mae: 0.0011 - val_loss: 3.4291e-04 - val_mae: 0.0154\n",
      "Epoch 229/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3263e-06 - mae: 0.0011\n",
      "Epoch 229: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 2.0413e-06 - mae: 0.0011 - val_loss: 3.4478e-04 - val_mae: 0.0154\n",
      "Epoch 230/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9841e-06 - mae: 0.0017\n",
      "Epoch 230: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 2.1875e-06 - mae: 0.0012 - val_loss: 3.4321e-04 - val_mae: 0.0154\n",
      "Epoch 231/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4941e-06 - mae: 0.0013\n",
      "Epoch 231: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 1.9270e-06 - mae: 0.0011 - val_loss: 3.4731e-04 - val_mae: 0.0154\n",
      "Epoch 232/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1510e-06 - mae: 0.0014\n",
      "Epoch 232: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 1.9860e-06 - mae: 0.0011 - val_loss: 3.4534e-04 - val_mae: 0.0154\n",
      "Epoch 233/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2806e-06 - mae: 7.5778e-04\n",
      "Epoch 233: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 210us/sample - loss: 1.9286e-06 - mae: 0.0011 - val_loss: 3.4559e-04 - val_mae: 0.0154\n",
      "Epoch 234/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5856e-06 - mae: 0.0010\n",
      "Epoch 234: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 219us/sample - loss: 1.7423e-06 - mae: 0.0010 - val_loss: 3.4378e-04 - val_mae: 0.0154\n",
      "Epoch 235/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3828e-07 - mae: 7.7710e-04\n",
      "Epoch 235: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 2.1805e-06 - mae: 0.0012 - val_loss: 3.4733e-04 - val_mae: 0.0155\n",
      "Epoch 236/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3763e-06 - mae: 0.0010\n",
      "Epoch 236: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 215us/sample - loss: 1.6923e-06 - mae: 0.0010 - val_loss: 3.4673e-04 - val_mae: 0.0155\n",
      "Epoch 237/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5220e-06 - mae: 0.0011\n",
      "Epoch 237: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 1.7454e-06 - mae: 0.0011 - val_loss: 3.4326e-04 - val_mae: 0.0154\n",
      "Epoch 238/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2570e-07 - mae: 6.7314e-04\n",
      "Epoch 238: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 2.0130e-06 - mae: 0.0011 - val_loss: 3.4621e-04 - val_mae: 0.0155\n",
      "Epoch 239/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8082e-06 - mae: 0.0011\n",
      "Epoch 239: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 249us/sample - loss: 1.9135e-06 - mae: 0.0011 - val_loss: 3.4449e-04 - val_mae: 0.0155\n",
      "Epoch 240/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4487e-06 - mae: 8.6917e-04\n",
      "Epoch 240: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 1.5678e-06 - mae: 0.0010 - val_loss: 3.4443e-04 - val_mae: 0.0155\n",
      "Epoch 241/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7380e-07 - mae: 6.2961e-04\n",
      "Epoch 241: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 1.8406e-06 - mae: 0.0011 - val_loss: 3.4693e-04 - val_mae: 0.0155\n",
      "Epoch 242/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.6777e-07 - mae: 7.5200e-04\n",
      "Epoch 242: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 1.5908e-06 - mae: 9.9736e-04 - val_loss: 3.4712e-04 - val_mae: 0.0155\n",
      "Epoch 243/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0412e-06 - mae: 7.9163e-04\n",
      "Epoch 243: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 1.6018e-06 - mae: 0.0010 - val_loss: 3.4738e-04 - val_mae: 0.0155\n",
      "Epoch 244/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0891e-06 - mae: 0.0012\n",
      "Epoch 244: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 1.5741e-06 - mae: 0.0010 - val_loss: 3.4427e-04 - val_mae: 0.0155\n",
      "Epoch 245/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7562e-07 - mae: 6.2216e-04\n",
      "Epoch 245: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 1.3394e-06 - mae: 9.2420e-04 - val_loss: 3.4911e-04 - val_mae: 0.0155\n",
      "Epoch 246/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9510e-06 - mae: 0.0013\n",
      "Epoch 246: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 237us/sample - loss: 1.7823e-06 - mae: 0.0011 - val_loss: 3.4543e-04 - val_mae: 0.0155\n",
      "Epoch 247/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3371e-07 - mae: 6.4763e-04\n",
      "Epoch 247: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 226us/sample - loss: 1.3581e-06 - mae: 9.3173e-04 - val_loss: 3.4661e-04 - val_mae: 0.0155\n",
      "Epoch 248/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.9383e-07 - mae: 7.6735e-04\n",
      "Epoch 248: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 1.4414e-06 - mae: 9.4823e-04 - val_loss: 3.4803e-04 - val_mae: 0.0155\n",
      "Epoch 249/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.5467e-07 - mae: 7.4231e-04\n",
      "Epoch 249: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 1.2684e-06 - mae: 9.0672e-04 - val_loss: 3.4687e-04 - val_mae: 0.0155\n",
      "Epoch 250/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7494e-07 - mae: 8.1969e-04\n",
      "Epoch 250: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 1.2494e-06 - mae: 8.9237e-04 - val_loss: 3.4929e-04 - val_mae: 0.0155\n",
      "Epoch 251/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7197e-06 - mae: 0.0012\n",
      "Epoch 251: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 1.4363e-06 - mae: 9.7483e-04 - val_loss: 3.4570e-04 - val_mae: 0.0155\n",
      "Epoch 252/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.2824e-07 - mae: 6.7759e-04\n",
      "Epoch 252: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 1.3120e-06 - mae: 8.9383e-04 - val_loss: 3.4415e-04 - val_mae: 0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9731e-06 - mae: 0.0011\n",
      "Epoch 253: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 1.3359e-06 - mae: 9.1906e-04 - val_loss: 3.4810e-04 - val_mae: 0.0155\n",
      "Epoch 254/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6743e-07 - mae: 6.2447e-04\n",
      "Epoch 254: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 1.1457e-06 - mae: 8.2445e-04 - val_loss: 3.4659e-04 - val_mae: 0.0155\n",
      "Epoch 255/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.1604e-07 - mae: 4.9941e-04\n",
      "Epoch 255: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 9.9223e-07 - mae: 8.0504e-04 - val_loss: 3.4922e-04 - val_mae: 0.0155\n",
      "Epoch 256/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3511e-06 - mae: 0.0010\n",
      "Epoch 256: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 1.3578e-06 - mae: 9.3997e-04 - val_loss: 3.4730e-04 - val_mae: 0.0155\n",
      "Epoch 257/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.3736e-07 - mae: 7.5941e-04\n",
      "Epoch 257: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 1.1225e-06 - mae: 8.8158e-04 - val_loss: 3.4583e-04 - val_mae: 0.0155\n",
      "Epoch 258/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3197e-06 - mae: 0.0012\n",
      "Epoch 258: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 1.0425e-06 - mae: 8.0632e-04 - val_loss: 3.4611e-04 - val_mae: 0.0155\n",
      "Epoch 259/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9146e-07 - mae: 4.6901e-04\n",
      "Epoch 259: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 9.5570e-07 - mae: 7.5428e-04 - val_loss: 3.4689e-04 - val_mae: 0.0155\n",
      "Epoch 260/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8417e-07 - mae: 7.7313e-04\n",
      "Epoch 260: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 9.2378e-07 - mae: 7.5650e-04 - val_loss: 3.4662e-04 - val_mae: 0.0155\n",
      "Epoch 261/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2565e-06 - mae: 8.9507e-04\n",
      "Epoch 261: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 218us/sample - loss: 8.6089e-07 - mae: 7.3506e-04 - val_loss: 3.4845e-04 - val_mae: 0.0155\n",
      "Epoch 262/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3124e-07 - mae: 5.6166e-04\n",
      "Epoch 262: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 9.4121e-07 - mae: 7.6940e-04 - val_loss: 3.4961e-04 - val_mae: 0.0155\n",
      "Epoch 263/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.8141e-07 - mae: 8.6867e-04\n",
      "Epoch 263: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 1.1340e-06 - mae: 8.5393e-04 - val_loss: 3.4993e-04 - val_mae: 0.0156\n",
      "Epoch 264/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0380e-07 - mae: 8.1232e-04\n",
      "Epoch 264: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 8.5149e-07 - mae: 7.2080e-04 - val_loss: 3.4647e-04 - val_mae: 0.0155\n",
      "Epoch 265/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.8279e-07 - mae: 5.4940e-04\n",
      "Epoch 265: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 170us/sample - loss: 8.1145e-07 - mae: 7.1161e-04 - val_loss: 3.4747e-04 - val_mae: 0.0155\n",
      "Epoch 266/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2890e-07 - mae: 6.1903e-04\n",
      "Epoch 266: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 7.4433e-07 - mae: 6.8001e-04 - val_loss: 3.4745e-04 - val_mae: 0.0156\n",
      "Epoch 267/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7678e-07 - mae: 6.1564e-04\n",
      "Epoch 267: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 7.3292e-07 - mae: 6.7332e-04 - val_loss: 3.4845e-04 - val_mae: 0.0156\n",
      "Epoch 268/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7023e-07 - mae: 6.7661e-04\n",
      "Epoch 268: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 6.5798e-07 - mae: 6.4793e-04 - val_loss: 3.4795e-04 - val_mae: 0.0156\n",
      "Epoch 269/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1048e-06 - mae: 9.0541e-04\n",
      "Epoch 269: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 6.6396e-07 - mae: 6.5593e-04 - val_loss: 3.4802e-04 - val_mae: 0.0156\n",
      "Epoch 270/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7945e-07 - mae: 7.6431e-04\n",
      "Epoch 270: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 6.3592e-07 - mae: 6.3425e-04 - val_loss: 3.4807e-04 - val_mae: 0.0156\n",
      "Epoch 271/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9145e-07 - mae: 7.9711e-04\n",
      "Epoch 271: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 5.9039e-07 - mae: 6.0936e-04 - val_loss: 3.4882e-04 - val_mae: 0.0156\n",
      "Epoch 272/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0351e-07 - mae: 4.7550e-04\n",
      "Epoch 272: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 5.6434e-07 - mae: 6.0837e-04 - val_loss: 3.4750e-04 - val_mae: 0.0156\n",
      "Epoch 273/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7850e-07 - mae: 7.3693e-04\n",
      "Epoch 273: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 6.2002e-07 - mae: 6.3713e-04 - val_loss: 3.4970e-04 - val_mae: 0.0156\n",
      "Epoch 274/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3881e-07 - mae: 7.7395e-04\n",
      "Epoch 274: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 152us/sample - loss: 5.5869e-07 - mae: 5.9479e-04 - val_loss: 3.4769e-04 - val_mae: 0.0156\n",
      "Epoch 275/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0482e-07 - mae: 5.8346e-04\n",
      "Epoch 275: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 146us/sample - loss: 6.9343e-07 - mae: 6.4722e-04 - val_loss: 3.4779e-04 - val_mae: 0.0156\n",
      "Epoch 276/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2318e-07 - mae: 7.2295e-04\n",
      "Epoch 276: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 149us/sample - loss: 5.4633e-07 - mae: 5.9342e-04 - val_loss: 3.4848e-04 - val_mae: 0.0156\n",
      "Epoch 277/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3451e-07 - mae: 5.5525e-04\n",
      "Epoch 277: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 5.3176e-07 - mae: 5.8372e-04 - val_loss: 3.4849e-04 - val_mae: 0.0156\n",
      "Epoch 278/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0402e-07 - mae: 5.6266e-04\n",
      "Epoch 278: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 5.0167e-07 - mae: 5.7261e-04 - val_loss: 3.4998e-04 - val_mae: 0.0156\n",
      "Epoch 279/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8383e-07 - mae: 4.9648e-04\n",
      "Epoch 279: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 5.7607e-07 - mae: 6.2826e-04 - val_loss: 3.4907e-04 - val_mae: 0.0156\n",
      "Epoch 280/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9123e-07 - mae: 5.6825e-04\n",
      "Epoch 280: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 4.3979e-07 - mae: 5.4437e-04 - val_loss: 3.4873e-04 - val_mae: 0.0156\n",
      "Epoch 281/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2781e-07 - mae: 6.1094e-04\n",
      "Epoch 281: val_loss did not improve from 0.00032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 153us/sample - loss: 5.0991e-07 - mae: 5.7427e-04 - val_loss: 3.4822e-04 - val_mae: 0.0156\n",
      "Epoch 282/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8208e-07 - mae: 7.0312e-04\n",
      "Epoch 282: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 4.8495e-07 - mae: 5.4331e-04 - val_loss: 3.4918e-04 - val_mae: 0.0156\n",
      "Epoch 283/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9816e-07 - mae: 4.3164e-04\n",
      "Epoch 283: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 145us/sample - loss: 5.2325e-07 - mae: 5.6749e-04 - val_loss: 3.4836e-04 - val_mae: 0.0156\n",
      "Epoch 284/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6448e-07 - mae: 6.7251e-04\n",
      "Epoch 284: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 4.3677e-07 - mae: 5.2280e-04 - val_loss: 3.5101e-04 - val_mae: 0.0156\n",
      "Epoch 285/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1279e-07 - mae: 4.6613e-04\n",
      "Epoch 285: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 4.2241e-07 - mae: 5.2336e-04 - val_loss: 3.5019e-04 - val_mae: 0.0156\n",
      "Epoch 286/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.7191e-07 - mae: 5.9188e-04\n",
      "Epoch 286: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 4.6465e-07 - mae: 5.5234e-04 - val_loss: 3.5077e-04 - val_mae: 0.0156\n",
      "Epoch 287/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2768e-07 - mae: 4.2516e-04\n",
      "Epoch 287: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 4.7391e-07 - mae: 5.5619e-04 - val_loss: 3.5226e-04 - val_mae: 0.0156\n",
      "Epoch 288/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.5674e-07 - mae: 9.0098e-04\n",
      "Epoch 288: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 4.4930e-07 - mae: 5.4150e-04 - val_loss: 3.4948e-04 - val_mae: 0.0156\n",
      "Epoch 289/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9823e-07 - mae: 4.5591e-04\n",
      "Epoch 289: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 4.3540e-07 - mae: 5.2339e-04 - val_loss: 3.5047e-04 - val_mae: 0.0156\n",
      "Epoch 290/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2047e-07 - mae: 4.9888e-04\n",
      "Epoch 290: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 3.2110e-07 - mae: 4.5391e-04 - val_loss: 3.5090e-04 - val_mae: 0.0156\n",
      "Epoch 291/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5975e-07 - mae: 3.0327e-04\n",
      "Epoch 291: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 3.6423e-07 - mae: 4.8642e-04 - val_loss: 3.5026e-04 - val_mae: 0.0156\n",
      "Epoch 292/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0310e-07 - mae: 4.8935e-04\n",
      "Epoch 292: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 3.8480e-07 - mae: 5.0462e-04 - val_loss: 3.5096e-04 - val_mae: 0.0156\n",
      "Epoch 293/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6207e-07 - mae: 5.7464e-04\n",
      "Epoch 293: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 158us/sample - loss: 3.5745e-07 - mae: 4.8611e-04 - val_loss: 3.5171e-04 - val_mae: 0.0156\n",
      "Epoch 294/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0356e-07 - mae: 4.7741e-04\n",
      "Epoch 294: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 3.0704e-07 - mae: 4.5364e-04 - val_loss: 3.5144e-04 - val_mae: 0.0156\n",
      "Epoch 295/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9497e-07 - mae: 4.9967e-04\n",
      "Epoch 295: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 3.1097e-07 - mae: 4.4925e-04 - val_loss: 3.5086e-04 - val_mae: 0.0156\n",
      "Epoch 296/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5794e-07 - mae: 3.6695e-04\n",
      "Epoch 296: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 3.4712e-07 - mae: 4.7678e-04 - val_loss: 3.5164e-04 - val_mae: 0.0156\n",
      "Epoch 297/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8249e-07 - mae: 5.0576e-04\n",
      "Epoch 297: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 3.7627e-07 - mae: 4.9144e-04 - val_loss: 3.5020e-04 - val_mae: 0.0156\n",
      "Epoch 298/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1993e-07 - mae: 4.0418e-04\n",
      "Epoch 298: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 2.9776e-07 - mae: 4.3272e-04 - val_loss: 3.5003e-04 - val_mae: 0.0156\n",
      "Epoch 299/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5426e-07 - mae: 4.6230e-04\n",
      "Epoch 299: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 3.0792e-07 - mae: 4.4473e-04 - val_loss: 3.4970e-04 - val_mae: 0.0156\n",
      "Epoch 300/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.3118e-07 - mae: 7.2067e-04\n",
      "Epoch 300: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 3.7779e-07 - mae: 4.9047e-04 - val_loss: 3.5061e-04 - val_mae: 0.0156\n",
      "Epoch 301/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6436e-07 - mae: 4.0419e-04\n",
      "Epoch 301: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 2.9717e-07 - mae: 4.3898e-04 - val_loss: 3.5160e-04 - val_mae: 0.0156\n",
      "Epoch 302/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1539e-07 - mae: 4.8080e-04\n",
      "Epoch 302: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 2.4392e-07 - mae: 3.9414e-04 - val_loss: 3.5033e-04 - val_mae: 0.0156\n",
      "Epoch 303/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7083e-07 - mae: 3.7822e-04\n",
      "Epoch 303: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 2.7995e-07 - mae: 4.2902e-04 - val_loss: 3.5168e-04 - val_mae: 0.0156\n",
      "Epoch 304/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0494e-07 - mae: 3.8476e-04\n",
      "Epoch 304: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 3.4508e-07 - mae: 4.7551e-04 - val_loss: 3.5263e-04 - val_mae: 0.0156\n",
      "Epoch 305/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5878e-07 - mae: 4.0551e-04\n",
      "Epoch 305: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 2.7334e-07 - mae: 4.1421e-04 - val_loss: 3.5268e-04 - val_mae: 0.0156\n",
      "Epoch 306/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9341e-07 - mae: 3.8800e-04\n",
      "Epoch 306: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 3.0215e-07 - mae: 4.6132e-04 - val_loss: 3.5175e-04 - val_mae: 0.0156\n",
      "Epoch 307/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6122e-07 - mae: 5.2123e-04\n",
      "Epoch 307: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 1.8059e-07 - mae: 3.3984e-04 - val_loss: 3.5106e-04 - val_mae: 0.0156\n",
      "Epoch 308/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.8554e-07 - mae: 5.0922e-04\n",
      "Epoch 308: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 2.1759e-07 - mae: 3.7205e-04 - val_loss: 3.5133e-04 - val_mae: 0.0156\n",
      "Epoch 309/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7101e-07 - mae: 3.4371e-04\n",
      "Epoch 309: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 2.0353e-07 - mae: 3.4660e-04 - val_loss: 3.5143e-04 - val_mae: 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1290e-07 - mae: 2.7621e-04\n",
      "Epoch 310: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 2.3690e-07 - mae: 4.0134e-04 - val_loss: 3.5120e-04 - val_mae: 0.0156\n",
      "Epoch 311/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9200e-07 - mae: 3.6543e-04\n",
      "Epoch 311: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 1.5307e-07 - mae: 3.1623e-04 - val_loss: 3.5167e-04 - val_mae: 0.0156\n",
      "Epoch 312/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1961e-08 - mae: 2.5700e-04\n",
      "Epoch 312: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 1.9016e-07 - mae: 3.5903e-04 - val_loss: 3.5126e-04 - val_mae: 0.0156\n",
      "Epoch 313/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.1477e-08 - mae: 1.9150e-04\n",
      "Epoch 313: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 210us/sample - loss: 1.8738e-07 - mae: 3.5325e-04 - val_loss: 3.5059e-04 - val_mae: 0.0156\n",
      "Epoch 314/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3879e-07 - mae: 2.9747e-04\n",
      "Epoch 314: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 206us/sample - loss: 1.5045e-07 - mae: 3.0515e-04 - val_loss: 3.5154e-04 - val_mae: 0.0156\n",
      "Epoch 315/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9224e-07 - mae: 3.7278e-04\n",
      "Epoch 315: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 1.7860e-07 - mae: 3.4367e-04 - val_loss: 3.5129e-04 - val_mae: 0.0156\n",
      "Epoch 316/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3636e-08 - mae: 1.3037e-04\n",
      "Epoch 316: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 1.8097e-07 - mae: 3.4061e-04 - val_loss: 3.5108e-04 - val_mae: 0.0156\n",
      "Epoch 317/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3439e-07 - mae: 3.3103e-04\n",
      "Epoch 317: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 171us/sample - loss: 1.4020e-07 - mae: 3.1203e-04 - val_loss: 3.5131e-04 - val_mae: 0.0156\n",
      "Epoch 318/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.5150e-08 - mae: 2.0974e-04\n",
      "Epoch 318: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 1.4793e-07 - mae: 3.0685e-04 - val_loss: 3.5141e-04 - val_mae: 0.0156\n",
      "Epoch 319/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9484e-07 - mae: 3.8413e-04\n",
      "Epoch 319: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 1.8257e-07 - mae: 3.4460e-04 - val_loss: 3.5163e-04 - val_mae: 0.0156\n",
      "Epoch 320/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0572e-07 - mae: 2.8551e-04\n",
      "Epoch 320: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.9313e-07 - mae: 3.5986e-04 - val_loss: 3.5226e-04 - val_mae: 0.0156\n",
      "Epoch 321/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0542e-08 - mae: 2.4646e-04\n",
      "Epoch 321: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 1.1699e-07 - mae: 2.7701e-04 - val_loss: 3.5205e-04 - val_mae: 0.0156\n",
      "Epoch 322/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.6211e-08 - mae: 1.8109e-04\n",
      "Epoch 322: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 1.1818e-07 - mae: 2.7469e-04 - val_loss: 3.5278e-04 - val_mae: 0.0156\n",
      "Epoch 323/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3521e-07 - mae: 4.0668e-04\n",
      "Epoch 323: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 1.9576e-07 - mae: 3.5728e-04 - val_loss: 3.5127e-04 - val_mae: 0.0156\n",
      "Epoch 324/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5582e-07 - mae: 3.2387e-04\n",
      "Epoch 324: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 1.6087e-07 - mae: 3.2796e-04 - val_loss: 3.5190e-04 - val_mae: 0.0156\n",
      "Epoch 325/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.2500e-08 - mae: 1.9567e-04\n",
      "Epoch 325: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 1.1708e-07 - mae: 2.7153e-04 - val_loss: 3.5270e-04 - val_mae: 0.0156\n",
      "Epoch 326/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1396e-08 - mae: 2.0623e-04\n",
      "Epoch 326: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 170us/sample - loss: 1.2074e-07 - mae: 2.7948e-04 - val_loss: 3.5256e-04 - val_mae: 0.0156\n",
      "Epoch 327/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.7141e-08 - mae: 2.1360e-04\n",
      "Epoch 327: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.2232e-07 - mae: 2.8465e-04 - val_loss: 3.5314e-04 - val_mae: 0.0156\n",
      "Epoch 328/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.8282e-08 - mae: 2.6004e-04\n",
      "Epoch 328: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 153us/sample - loss: 1.4905e-07 - mae: 3.2361e-04 - val_loss: 3.5253e-04 - val_mae: 0.0156\n",
      "Epoch 329/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3718e-08 - mae: 2.0840e-04\n",
      "Epoch 329: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 1.2604e-07 - mae: 2.9267e-04 - val_loss: 3.5113e-04 - val_mae: 0.0156\n",
      "Epoch 330/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0708e-07 - mae: 3.0664e-04\n",
      "Epoch 330: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.2292e-07 - mae: 2.8126e-04 - val_loss: 3.5227e-04 - val_mae: 0.0156\n",
      "Epoch 331/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.1421e-08 - mae: 2.0567e-04\n",
      "Epoch 331: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 161us/sample - loss: 1.1064e-07 - mae: 2.6863e-04 - val_loss: 3.5202e-04 - val_mae: 0.0156\n",
      "Epoch 332/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4442e-08 - mae: 1.8528e-04\n",
      "Epoch 332: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 9.7431e-08 - mae: 2.5000e-04 - val_loss: 3.5150e-04 - val_mae: 0.0156\n",
      "Epoch 333/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0476e-08 - mae: 2.5451e-04\n",
      "Epoch 333: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 171us/sample - loss: 1.1528e-07 - mae: 2.7257e-04 - val_loss: 3.5330e-04 - val_mae: 0.0156\n",
      "Epoch 334/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1790e-07 - mae: 2.7674e-04\n",
      "Epoch 334: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 160us/sample - loss: 1.2121e-07 - mae: 2.8231e-04 - val_loss: 3.5290e-04 - val_mae: 0.0156\n",
      "Epoch 335/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3223e-08 - mae: 1.9510e-04\n",
      "Epoch 335: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 163us/sample - loss: 8.9837e-08 - mae: 2.4187e-04 - val_loss: 3.5272e-04 - val_mae: 0.0156\n",
      "Epoch 336/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.6411e-08 - mae: 1.8990e-04\n",
      "Epoch 336: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 8.0052e-08 - mae: 2.1974e-04 - val_loss: 3.5212e-04 - val_mae: 0.0156\n",
      "Epoch 337/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4273e-07 - mae: 3.3503e-04\n",
      "Epoch 337: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 9.6061e-08 - mae: 2.5373e-04 - val_loss: 3.5359e-04 - val_mae: 0.0156\n",
      "Epoch 338/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.6034e-08 - mae: 2.0906e-04\n",
      "Epoch 338: val_loss did not improve from 0.00032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 155us/sample - loss: 7.8336e-08 - mae: 2.1834e-04 - val_loss: 3.5256e-04 - val_mae: 0.0156\n",
      "Epoch 339/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.8003e-08 - mae: 2.3901e-04\n",
      "Epoch 339: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 154us/sample - loss: 6.7461e-08 - mae: 2.1474e-04 - val_loss: 3.5254e-04 - val_mae: 0.0156\n",
      "Epoch 340/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.9428e-08 - mae: 2.3082e-04\n",
      "Epoch 340: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 7.8554e-08 - mae: 2.2570e-04 - val_loss: 3.5326e-04 - val_mae: 0.0156\n",
      "Epoch 341/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1023e-07 - mae: 2.9612e-04\n",
      "Epoch 341: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 6.0269e-08 - mae: 2.0042e-04 - val_loss: 3.5255e-04 - val_mae: 0.0156\n",
      "Epoch 342/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0319e-07 - mae: 2.7323e-04\n",
      "Epoch 342: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 141us/sample - loss: 6.6293e-08 - mae: 2.0445e-04 - val_loss: 3.5289e-04 - val_mae: 0.0156\n",
      "Epoch 343/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3980e-08 - mae: 1.7974e-04\n",
      "Epoch 343: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 143us/sample - loss: 5.5398e-08 - mae: 1.8899e-04 - val_loss: 3.5248e-04 - val_mae: 0.0156\n",
      "Epoch 344/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.4368e-08 - mae: 1.3203e-04\n",
      "Epoch 344: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 146us/sample - loss: 6.2330e-08 - mae: 1.8893e-04 - val_loss: 3.5345e-04 - val_mae: 0.0156\n",
      "Epoch 345/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.2658e-08 - mae: 2.1291e-04\n",
      "Epoch 345: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 5.5584e-08 - mae: 1.9142e-04 - val_loss: 3.5321e-04 - val_mae: 0.0156\n",
      "Epoch 346/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7366e-08 - mae: 2.2239e-04\n",
      "Epoch 346: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 5.7614e-08 - mae: 1.9154e-04 - val_loss: 3.5395e-04 - val_mae: 0.0156\n",
      "Epoch 347/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1287e-07 - mae: 2.9351e-04\n",
      "Epoch 347: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 168us/sample - loss: 1.0182e-07 - mae: 2.6167e-04 - val_loss: 3.5197e-04 - val_mae: 0.0156\n",
      "Epoch 348/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.1157e-08 - mae: 2.5056e-04\n",
      "Epoch 348: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 157us/sample - loss: 8.3177e-08 - mae: 2.3187e-04 - val_loss: 3.5283e-04 - val_mae: 0.0156\n",
      "Epoch 349/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.3058e-08 - mae: 2.4106e-04\n",
      "Epoch 349: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 4.7525e-08 - mae: 1.7475e-04 - val_loss: 3.5247e-04 - val_mae: 0.0156\n",
      "Epoch 350/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8733e-08 - mae: 1.1261e-04\n",
      "Epoch 350: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 4.0863e-08 - mae: 1.6065e-04 - val_loss: 3.5266e-04 - val_mae: 0.0156\n",
      "Epoch 351/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7625e-08 - mae: 1.5521e-04\n",
      "Epoch 351: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 146us/sample - loss: 5.0661e-08 - mae: 1.8071e-04 - val_loss: 3.5223e-04 - val_mae: 0.0156\n",
      "Epoch 352/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1014e-08 - mae: 1.6878e-04\n",
      "Epoch 352: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 4.2922e-08 - mae: 1.6625e-04 - val_loss: 3.5302e-04 - val_mae: 0.0156\n",
      "Epoch 353/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5093e-08 - mae: 1.5393e-04\n",
      "Epoch 353: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 149us/sample - loss: 6.3148e-08 - mae: 1.9784e-04 - val_loss: 3.5239e-04 - val_mae: 0.0156\n",
      "Epoch 354/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8371e-08 - mae: 1.9608e-04\n",
      "Epoch 354: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 156us/sample - loss: 4.1021e-08 - mae: 1.6491e-04 - val_loss: 3.5305e-04 - val_mae: 0.0156\n",
      "Epoch 355/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5344e-08 - mae: 9.6040e-05\n",
      "Epoch 355: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 3.8419e-08 - mae: 1.5694e-04 - val_loss: 3.5308e-04 - val_mae: 0.0156\n",
      "Epoch 356/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4032e-08 - mae: 8.2322e-05\n",
      "Epoch 356: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 3.3305e-08 - mae: 1.4692e-04 - val_loss: 3.5278e-04 - val_mae: 0.0156\n",
      "Epoch 357/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1988e-08 - mae: 1.7329e-04\n",
      "Epoch 357: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 4.5953e-08 - mae: 1.7474e-04 - val_loss: 3.5292e-04 - val_mae: 0.0156\n",
      "Epoch 358/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3906e-08 - mae: 1.0116e-04\n",
      "Epoch 358: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 3.2715e-08 - mae: 1.4473e-04 - val_loss: 3.5351e-04 - val_mae: 0.0156\n",
      "Epoch 359/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5965e-08 - mae: 1.6569e-04\n",
      "Epoch 359: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 212us/sample - loss: 5.9956e-08 - mae: 2.0494e-04 - val_loss: 3.5378e-04 - val_mae: 0.0156\n",
      "Epoch 360/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.5788e-08 - mae: 1.5583e-04\n",
      "Epoch 360: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 3.9575e-08 - mae: 1.5644e-04 - val_loss: 3.5329e-04 - val_mae: 0.0156\n",
      "Epoch 361/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5724e-08 - mae: 1.3087e-04\n",
      "Epoch 361: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 4.1866e-08 - mae: 1.6292e-04 - val_loss: 3.5359e-04 - val_mae: 0.0156\n",
      "Epoch 362/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2821e-08 - mae: 1.4707e-04\n",
      "Epoch 362: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 3.9566e-08 - mae: 1.5634e-04 - val_loss: 3.5396e-04 - val_mae: 0.0156\n",
      "Epoch 363/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8234e-08 - mae: 1.5129e-04\n",
      "Epoch 363: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 3.3147e-08 - mae: 1.4968e-04 - val_loss: 3.5288e-04 - val_mae: 0.0156\n",
      "Epoch 364/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2447e-07 - mae: 2.8619e-04\n",
      "Epoch 364: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 5.1276e-08 - mae: 1.8361e-04 - val_loss: 3.5402e-04 - val_mae: 0.0156\n",
      "Epoch 365/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0380e-08 - mae: 1.5392e-04\n",
      "Epoch 365: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 3.1245e-08 - mae: 1.4166e-04 - val_loss: 3.5361e-04 - val_mae: 0.0156\n",
      "Epoch 366/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.9384e-08 - mae: 1.4655e-04\n",
      "Epoch 366: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 2.6499e-08 - mae: 1.2438e-04 - val_loss: 3.5411e-04 - val_mae: 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4014e-08 - mae: 1.7675e-04\n",
      "Epoch 367: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 3.9665e-08 - mae: 1.6072e-04 - val_loss: 3.5335e-04 - val_mae: 0.0156\n",
      "Epoch 368/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6443e-08 - mae: 1.3816e-04\n",
      "Epoch 368: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 3.9389e-08 - mae: 1.5828e-04 - val_loss: 3.5360e-04 - val_mae: 0.0156\n",
      "Epoch 369/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5635e-08 - mae: 1.2905e-04\n",
      "Epoch 369: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 3.4636e-08 - mae: 1.4797e-04 - val_loss: 3.5304e-04 - val_mae: 0.0156\n",
      "Epoch 370/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7646e-08 - mae: 2.1994e-04\n",
      "Epoch 370: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 3.6779e-08 - mae: 1.5337e-04 - val_loss: 3.5311e-04 - val_mae: 0.0156\n",
      "Epoch 371/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.3417e-08 - mae: 1.4263e-04\n",
      "Epoch 371: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 3.2153e-08 - mae: 1.3901e-04 - val_loss: 3.5393e-04 - val_mae: 0.0156\n",
      "Epoch 372/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.9826e-08 - mae: 1.4938e-04\n",
      "Epoch 372: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 3.6453e-08 - mae: 1.5531e-04 - val_loss: 3.5374e-04 - val_mae: 0.0156\n",
      "Epoch 373/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6179e-08 - mae: 1.0471e-04\n",
      "Epoch 373: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 2.2015e-08 - mae: 1.1901e-04 - val_loss: 3.5341e-04 - val_mae: 0.0156\n",
      "Epoch 374/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0964e-08 - mae: 1.3366e-04\n",
      "Epoch 374: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 2.3540e-08 - mae: 1.2275e-04 - val_loss: 3.5385e-04 - val_mae: 0.0156\n",
      "Epoch 375/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6440e-08 - mae: 9.4768e-05\n",
      "Epoch 375: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 3.4466e-08 - mae: 1.5080e-04 - val_loss: 3.5435e-04 - val_mae: 0.0156\n",
      "Epoch 376/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6249e-08 - mae: 1.6935e-04\n",
      "Epoch 376: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 3.4399e-08 - mae: 1.4879e-04 - val_loss: 3.5392e-04 - val_mae: 0.0156\n",
      "Epoch 377/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6986e-08 - mae: 1.3884e-04\n",
      "Epoch 377: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 2.2141e-08 - mae: 1.2132e-04 - val_loss: 3.5342e-04 - val_mae: 0.0156\n",
      "Epoch 378/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0706e-08 - mae: 1.2515e-04\n",
      "Epoch 378: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 2.0478e-08 - mae: 1.1910e-04 - val_loss: 3.5330e-04 - val_mae: 0.0156\n",
      "Epoch 379/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.2654e-08 - mae: 1.6182e-04\n",
      "Epoch 379: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 203us/sample - loss: 2.4121e-08 - mae: 1.2229e-04 - val_loss: 3.5318e-04 - val_mae: 0.0156\n",
      "Epoch 380/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.8941e-08 - mae: 2.0117e-04\n",
      "Epoch 380: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 207us/sample - loss: 2.8527e-08 - mae: 1.3653e-04 - val_loss: 3.5345e-04 - val_mae: 0.0156\n",
      "Epoch 381/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6820e-08 - mae: 1.1466e-04\n",
      "Epoch 381: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 3.8247e-08 - mae: 1.6017e-04 - val_loss: 3.5395e-04 - val_mae: 0.0156\n",
      "Epoch 382/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2810e-08 - mae: 1.0407e-04\n",
      "Epoch 382: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 1.5308e-08 - mae: 9.8345e-05 - val_loss: 3.5397e-04 - val_mae: 0.0156\n",
      "Epoch 383/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5907e-08 - mae: 1.1570e-04\n",
      "Epoch 383: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 1.8218e-08 - mae: 1.0910e-04 - val_loss: 3.5347e-04 - val_mae: 0.0156\n",
      "Epoch 384/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4267e-08 - mae: 1.4275e-04\n",
      "Epoch 384: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 3.3087e-08 - mae: 1.5204e-04 - val_loss: 3.5336e-04 - val_mae: 0.0156\n",
      "Epoch 385/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8053e-08 - mae: 2.4213e-04\n",
      "Epoch 385: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 2.5779e-08 - mae: 1.2349e-04 - val_loss: 3.5326e-04 - val_mae: 0.0156\n",
      "Epoch 386/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8938e-08 - mae: 1.2133e-04\n",
      "Epoch 386: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 219us/sample - loss: 1.7024e-08 - mae: 1.0810e-04 - val_loss: 3.5437e-04 - val_mae: 0.0156\n",
      "Epoch 387/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0679e-08 - mae: 1.1193e-04\n",
      "Epoch 387: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 219us/sample - loss: 2.7645e-08 - mae: 1.3353e-04 - val_loss: 3.5400e-04 - val_mae: 0.0156\n",
      "Epoch 388/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5211e-08 - mae: 9.3094e-05\n",
      "Epoch 388: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 208us/sample - loss: 1.1352e-08 - mae: 8.3531e-05 - val_loss: 3.5366e-04 - val_mae: 0.0156\n",
      "Epoch 389/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2253e-08 - mae: 1.0067e-04\n",
      "Epoch 389: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 1.0983e-08 - mae: 8.4034e-05 - val_loss: 3.5413e-04 - val_mae: 0.0156\n",
      "Epoch 390/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1270e-08 - mae: 8.4793e-05\n",
      "Epoch 390: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 1.0104e-08 - mae: 7.7546e-05 - val_loss: 3.5386e-04 - val_mae: 0.0156\n",
      "Epoch 391/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.9667e-09 - mae: 4.9949e-05\n",
      "Epoch 391: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 1.1051e-08 - mae: 8.3765e-05 - val_loss: 3.5427e-04 - val_mae: 0.0156\n",
      "Epoch 392/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6276e-08 - mae: 1.1594e-04\n",
      "Epoch 392: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.8031e-08 - mae: 1.0818e-04 - val_loss: 3.5429e-04 - val_mae: 0.0156\n",
      "Epoch 393/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0247e-08 - mae: 1.2026e-04\n",
      "Epoch 393: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 2.0965e-08 - mae: 1.2206e-04 - val_loss: 3.5378e-04 - val_mae: 0.0156\n",
      "Epoch 394/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.0208e-09 - mae: 8.8155e-05\n",
      "Epoch 394: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 213us/sample - loss: 1.2393e-08 - mae: 8.8945e-05 - val_loss: 3.5373e-04 - val_mae: 0.0156\n",
      "Epoch 395/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3841e-08 - mae: 1.1240e-04\n",
      "Epoch 395: val_loss did not improve from 0.00032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 198us/sample - loss: 1.4449e-08 - mae: 9.6188e-05 - val_loss: 3.5378e-04 - val_mae: 0.0156\n",
      "Epoch 396/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2742e-08 - mae: 1.3059e-04\n",
      "Epoch 396: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 1.6886e-08 - mae: 1.0560e-04 - val_loss: 3.5434e-04 - val_mae: 0.0156\n",
      "Epoch 397/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3346e-08 - mae: 9.6200e-05\n",
      "Epoch 397: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 1.1406e-08 - mae: 8.6764e-05 - val_loss: 3.5434e-04 - val_mae: 0.0156\n",
      "Epoch 398/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.6168e-09 - mae: 5.4902e-05\n",
      "Epoch 398: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 8.6957e-09 - mae: 7.1045e-05 - val_loss: 3.5409e-04 - val_mae: 0.0156\n",
      "Epoch 399/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.9598e-09 - mae: 8.1167e-05\n",
      "Epoch 399: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 169us/sample - loss: 8.8145e-09 - mae: 7.5269e-05 - val_loss: 3.5412e-04 - val_mae: 0.0156\n",
      "Epoch 400/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.4506e-09 - mae: 7.8036e-05\n",
      "Epoch 400: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 6.6602e-09 - mae: 6.3907e-05 - val_loss: 3.5415e-04 - val_mae: 0.0156\n",
      "Epoch 401/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0603e-09 - mae: 5.3757e-05\n",
      "Epoch 401: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 6.6700e-09 - mae: 6.2958e-05 - val_loss: 3.5442e-04 - val_mae: 0.0156\n",
      "Epoch 402/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3929e-09 - mae: 5.6945e-05\n",
      "Epoch 402: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 7.0421e-09 - mae: 6.5764e-05 - val_loss: 3.5406e-04 - val_mae: 0.0156\n",
      "Epoch 403/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.4804e-09 - mae: 8.3649e-05\n",
      "Epoch 403: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 1.0625e-08 - mae: 8.4801e-05 - val_loss: 3.5436e-04 - val_mae: 0.0156\n",
      "Epoch 404/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6609e-09 - mae: 3.2144e-05\n",
      "Epoch 404: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 1.4323e-08 - mae: 1.0007e-04 - val_loss: 3.5471e-04 - val_mae: 0.0156\n",
      "Epoch 405/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2530e-08 - mae: 1.9924e-04\n",
      "Epoch 405: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 2.2170e-08 - mae: 1.2377e-04 - val_loss: 3.5440e-04 - val_mae: 0.0156\n",
      "Epoch 406/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3090e-09 - mae: 5.3260e-05\n",
      "Epoch 406: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 9.7729e-09 - mae: 7.8961e-05 - val_loss: 3.5413e-04 - val_mae: 0.0156\n",
      "Epoch 407/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6186e-09 - mae: 4.9710e-05\n",
      "Epoch 407: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 1.2105e-08 - mae: 8.8880e-05 - val_loss: 3.5399e-04 - val_mae: 0.0156\n",
      "Epoch 408/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1657e-08 - mae: 1.3698e-04\n",
      "Epoch 408: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 8.3280e-09 - mae: 7.6431e-05 - val_loss: 3.5415e-04 - val_mae: 0.0156\n",
      "Epoch 409/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3532e-09 - mae: 7.0186e-05\n",
      "Epoch 409: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 6.6746e-09 - mae: 6.5927e-05 - val_loss: 3.5395e-04 - val_mae: 0.0156\n",
      "Epoch 410/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0613e-08 - mae: 1.3116e-04\n",
      "Epoch 410: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 211us/sample - loss: 7.5520e-09 - mae: 6.9315e-05 - val_loss: 3.5447e-04 - val_mae: 0.0156\n",
      "Epoch 411/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.4534e-09 - mae: 7.8533e-05\n",
      "Epoch 411: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 5.3780e-09 - mae: 5.7897e-05 - val_loss: 3.5407e-04 - val_mae: 0.0156\n",
      "Epoch 412/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.2473e-08 - mae: 1.0349e-04\n",
      "Epoch 412: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 190us/sample - loss: 8.4781e-09 - mae: 7.5188e-05 - val_loss: 3.5415e-04 - val_mae: 0.0156\n",
      "Epoch 413/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.3351e-09 - mae: 5.9816e-05\n",
      "Epoch 413: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 189us/sample - loss: 5.4986e-09 - mae: 5.6742e-05 - val_loss: 3.5471e-04 - val_mae: 0.0156\n",
      "Epoch 414/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.4678e-09 - mae: 7.7969e-05\n",
      "Epoch 414: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 5.9597e-09 - mae: 6.4017e-05 - val_loss: 3.5438e-04 - val_mae: 0.0156\n",
      "Epoch 415/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4837e-09 - mae: 5.5826e-05\n",
      "Epoch 415: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 3.3592e-09 - mae: 4.4316e-05 - val_loss: 3.5440e-04 - val_mae: 0.0156\n",
      "Epoch 416/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7455e-09 - mae: 3.4229e-05\n",
      "Epoch 416: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 162us/sample - loss: 6.4672e-09 - mae: 6.6931e-05 - val_loss: 3.5402e-04 - val_mae: 0.0156\n",
      "Epoch 417/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.0329e-09 - mae: 7.7125e-05\n",
      "Epoch 417: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 172us/sample - loss: 8.5642e-09 - mae: 7.4389e-05 - val_loss: 3.5429e-04 - val_mae: 0.0156\n",
      "Epoch 418/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7938e-09 - mae: 5.2478e-05\n",
      "Epoch 418: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 4.5065e-09 - mae: 5.3722e-05 - val_loss: 3.5437e-04 - val_mae: 0.0156\n",
      "Epoch 419/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8833e-09 - mae: 6.1310e-05\n",
      "Epoch 419: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 176us/sample - loss: 2.8832e-09 - mae: 4.2722e-05 - val_loss: 3.5422e-04 - val_mae: 0.0156\n",
      "Epoch 420/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9068e-09 - mae: 3.8510e-05\n",
      "Epoch 420: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 3.3292e-09 - mae: 4.3524e-05 - val_loss: 3.5426e-04 - val_mae: 0.0156\n",
      "Epoch 421/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0800e-09 - mae: 3.7325e-05\n",
      "Epoch 421: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 4.2253e-09 - mae: 5.2057e-05 - val_loss: 3.5442e-04 - val_mae: 0.0156\n",
      "Epoch 422/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0171e-09 - mae: 3.7163e-05\n",
      "Epoch 422: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 3.8652e-09 - mae: 5.0497e-05 - val_loss: 3.5419e-04 - val_mae: 0.0156\n",
      "Epoch 423/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7723e-09 - mae: 5.8622e-05\n",
      "Epoch 423: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 184us/sample - loss: 2.5652e-09 - mae: 3.9495e-05 - val_loss: 3.5443e-04 - val_mae: 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 424/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8549e-10 - mae: 2.3972e-05\n",
      "Epoch 424: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 3.4698e-09 - mae: 4.7618e-05 - val_loss: 3.5428e-04 - val_mae: 0.0156\n",
      "Epoch 425/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.2487e-10 - mae: 2.3527e-05\n",
      "Epoch 425: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 214us/sample - loss: 2.2853e-09 - mae: 3.7070e-05 - val_loss: 3.5435e-04 - val_mae: 0.0156\n",
      "Epoch 426/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.1375e-09 - mae: 3.3967e-05\n",
      "Epoch 426: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 204us/sample - loss: 2.4883e-09 - mae: 3.8925e-05 - val_loss: 3.5414e-04 - val_mae: 0.0156\n",
      "Epoch 427/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7738e-09 - mae: 5.3629e-05\n",
      "Epoch 427: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 2.8593e-09 - mae: 4.3623e-05 - val_loss: 3.5416e-04 - val_mae: 0.0156\n",
      "Epoch 428/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9117e-09 - mae: 3.5406e-05\n",
      "Epoch 428: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 2.2598e-09 - mae: 3.7033e-05 - val_loss: 3.5438e-04 - val_mae: 0.0156\n",
      "Epoch 429/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3089e-09 - mae: 2.7893e-05\n",
      "Epoch 429: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.9115e-09 - mae: 3.2994e-05 - val_loss: 3.5423e-04 - val_mae: 0.0156\n",
      "Epoch 430/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.6045e-10 - mae: 2.3841e-05\n",
      "Epoch 430: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 2.4059e-09 - mae: 4.0604e-05 - val_loss: 3.5443e-04 - val_mae: 0.0156\n",
      "Epoch 431/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3888e-09 - mae: 2.8790e-05\n",
      "Epoch 431: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 195us/sample - loss: 1.7728e-09 - mae: 3.3023e-05 - val_loss: 3.5439e-04 - val_mae: 0.0156\n",
      "Epoch 432/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.8660e-10 - mae: 1.8211e-05\n",
      "Epoch 432: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 1.4372e-09 - mae: 2.9501e-05 - val_loss: 3.5440e-04 - val_mae: 0.0156\n",
      "Epoch 433/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5811e-09 - mae: 3.2006e-05\n",
      "Epoch 433: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 174us/sample - loss: 1.7759e-09 - mae: 3.3138e-05 - val_loss: 3.5447e-04 - val_mae: 0.0156\n",
      "Epoch 434/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0432e-09 - mae: 3.6448e-05\n",
      "Epoch 434: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 2.1253e-09 - mae: 3.8243e-05 - val_loss: 3.5430e-04 - val_mae: 0.0156\n",
      "Epoch 435/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.3688e-10 - mae: 2.5912e-05\n",
      "Epoch 435: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 3.3256e-09 - mae: 4.8926e-05 - val_loss: 3.5467e-04 - val_mae: 0.0156\n",
      "Epoch 436/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.2275e-09 - mae: 5.8738e-05\n",
      "Epoch 436: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 7.1132e-09 - mae: 7.2441e-05 - val_loss: 3.5406e-04 - val_mae: 0.0156\n",
      "Epoch 437/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5210e-09 - mae: 4.3940e-05\n",
      "Epoch 437: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 2.6330e-09 - mae: 4.0428e-05 - val_loss: 3.5457e-04 - val_mae: 0.0156\n",
      "Epoch 438/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4098e-08 - mae: 1.0972e-04\n",
      "Epoch 438: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 3.2381e-09 - mae: 4.4262e-05 - val_loss: 3.5424e-04 - val_mae: 0.0156\n",
      "Epoch 439/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7787e-09 - mae: 3.4322e-05\n",
      "Epoch 439: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 231us/sample - loss: 2.5928e-09 - mae: 4.2000e-05 - val_loss: 3.5457e-04 - val_mae: 0.0156\n",
      "Epoch 440/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.4569e-10 - mae: 1.8086e-05\n",
      "Epoch 440: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 186us/sample - loss: 2.3313e-09 - mae: 3.8576e-05 - val_loss: 3.5452e-04 - val_mae: 0.0156\n",
      "Epoch 441/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4211e-09 - mae: 3.1272e-05\n",
      "Epoch 441: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 1.5932e-09 - mae: 3.1600e-05 - val_loss: 3.5460e-04 - val_mae: 0.0156\n",
      "Epoch 442/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.4140e-09 - mae: 5.4838e-05\n",
      "Epoch 442: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 2.3568e-09 - mae: 4.0537e-05 - val_loss: 3.5438e-04 - val_mae: 0.0156\n",
      "Epoch 443/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.7775e-10 - mae: 1.5643e-05\n",
      "Epoch 443: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 178us/sample - loss: 2.7056e-09 - mae: 4.1450e-05 - val_loss: 3.5441e-04 - val_mae: 0.0156\n",
      "Epoch 444/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3533e-09 - mae: 3.0062e-05\n",
      "Epoch 444: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 180us/sample - loss: 1.6402e-09 - mae: 3.3600e-05 - val_loss: 3.5428e-04 - val_mae: 0.0156\n",
      "Epoch 445/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5271e-09 - mae: 3.2766e-05\n",
      "Epoch 445: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 187us/sample - loss: 3.1652e-09 - mae: 4.6728e-05 - val_loss: 3.5445e-04 - val_mae: 0.0156\n",
      "Epoch 446/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5632e-09 - mae: 3.2783e-05\n",
      "Epoch 446: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 2.6879e-09 - mae: 4.3517e-05 - val_loss: 3.5442e-04 - val_mae: 0.0156\n",
      "Epoch 447/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.4477e-10 - mae: 1.5062e-05\n",
      "Epoch 447: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 1.4392e-09 - mae: 2.9599e-05 - val_loss: 3.5431e-04 - val_mae: 0.0156\n",
      "Epoch 448/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.6386e-10 - mae: 1.6726e-05\n",
      "Epoch 448: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 191us/sample - loss: 1.0109e-09 - mae: 2.6144e-05 - val_loss: 3.5433e-04 - val_mae: 0.0156\n",
      "Epoch 449/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9233e-10 - mae: 2.3414e-05\n",
      "Epoch 449: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 7.4240e-10 - mae: 2.2032e-05 - val_loss: 3.5456e-04 - val_mae: 0.0156\n",
      "Epoch 450/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9077e-10 - mae: 2.2989e-05\n",
      "Epoch 450: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 4.9121e-10 - mae: 1.7602e-05 - val_loss: 3.5443e-04 - val_mae: 0.0156\n",
      "Epoch 451/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4163e-10 - mae: 9.0944e-06\n",
      "Epoch 451: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 182us/sample - loss: 5.5956e-10 - mae: 1.8702e-05 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 452/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0738e-09 - mae: 2.8397e-05\n",
      "Epoch 452: val_loss did not improve from 0.00032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 185us/sample - loss: 6.9959e-10 - mae: 2.1126e-05 - val_loss: 3.5442e-04 - val_mae: 0.0156\n",
      "Epoch 453/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1691e-10 - mae: 1.5765e-05\n",
      "Epoch 453: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 5.7324e-10 - mae: 1.8377e-05 - val_loss: 3.5453e-04 - val_mae: 0.0156\n",
      "Epoch 454/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7150e-09 - mae: 3.8147e-05\n",
      "Epoch 454: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 6.5598e-10 - mae: 1.9908e-05 - val_loss: 3.5435e-04 - val_mae: 0.0156\n",
      "Epoch 455/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.7117e-10 - mae: 1.2972e-05\n",
      "Epoch 455: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 193us/sample - loss: 1.2882e-09 - mae: 3.0239e-05 - val_loss: 3.5428e-04 - val_mae: 0.0156\n",
      "Epoch 456/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4231e-09 - mae: 3.5055e-05\n",
      "Epoch 456: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 181us/sample - loss: 7.4124e-10 - mae: 2.2505e-05 - val_loss: 3.5440e-04 - val_mae: 0.0156\n",
      "Epoch 457/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 9.1914e-10 - mae: 2.6579e-05\n",
      "Epoch 457: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 2.0638e-09 - mae: 4.0261e-05 - val_loss: 3.5442e-04 - val_mae: 0.0156\n",
      "Epoch 458/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.4112e-10 - mae: 2.5650e-05\n",
      "Epoch 458: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 6.9891e-10 - mae: 2.1105e-05 - val_loss: 3.5469e-04 - val_mae: 0.0156\n",
      "Epoch 459/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.6321e-09 - mae: 3.9106e-05\n",
      "Epoch 459: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 185us/sample - loss: 9.8110e-10 - mae: 2.5850e-05 - val_loss: 3.5443e-04 - val_mae: 0.0156\n",
      "Epoch 460/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3540e-10 - mae: 2.0516e-05\n",
      "Epoch 460: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 179us/sample - loss: 6.1091e-10 - mae: 2.0203e-05 - val_loss: 3.5454e-04 - val_mae: 0.0156\n",
      "Epoch 461/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.9122e-10 - mae: 1.2156e-05\n",
      "Epoch 461: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 198us/sample - loss: 5.2496e-10 - mae: 1.8532e-05 - val_loss: 3.5444e-04 - val_mae: 0.0156\n",
      "Epoch 462/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1530e-10 - mae: 1.7980e-05\n",
      "Epoch 462: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 9.4734e-10 - mae: 2.5440e-05 - val_loss: 3.5452e-04 - val_mae: 0.0156\n",
      "Epoch 463/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.7803e-10 - mae: 2.6504e-05\n",
      "Epoch 463: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 196us/sample - loss: 7.4871e-10 - mae: 2.2042e-05 - val_loss: 3.5461e-04 - val_mae: 0.0156\n",
      "Epoch 464/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.2420e-09 - mae: 4.2718e-05\n",
      "Epoch 464: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 9.3708e-10 - mae: 2.5685e-05 - val_loss: 3.5443e-04 - val_mae: 0.0156\n",
      "Epoch 465/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.7200e-10 - mae: 2.0077e-05\n",
      "Epoch 465: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 1.8739e-09 - mae: 3.7601e-05 - val_loss: 3.5423e-04 - val_mae: 0.0156\n",
      "Epoch 466/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.0485e-09 - mae: 8.8417e-05\n",
      "Epoch 466: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 1.7285e-09 - mae: 3.3822e-05 - val_loss: 3.5436e-04 - val_mae: 0.0156\n",
      "Epoch 467/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.3762e-10 - mae: 1.7473e-05\n",
      "Epoch 467: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 183us/sample - loss: 8.0775e-10 - mae: 2.2745e-05 - val_loss: 3.5458e-04 - val_mae: 0.0156\n",
      "Epoch 468/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5685e-09 - mae: 3.6358e-05\n",
      "Epoch 468: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 6.6506e-10 - mae: 2.0535e-05 - val_loss: 3.5441e-04 - val_mae: 0.0156\n",
      "Epoch 469/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.0657e-10 - mae: 1.6844e-05\n",
      "Epoch 469: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 4.2091e-10 - mae: 1.5291e-05 - val_loss: 3.5452e-04 - val_mae: 0.0156\n",
      "Epoch 470/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7460e-10 - mae: 1.0293e-05\n",
      "Epoch 470: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 4.3095e-10 - mae: 1.6683e-05 - val_loss: 3.5450e-04 - val_mae: 0.0156\n",
      "Epoch 471/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8000e-10 - mae: 1.4607e-05\n",
      "Epoch 471: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 164us/sample - loss: 3.5137e-10 - mae: 1.5591e-05 - val_loss: 3.5448e-04 - val_mae: 0.0156\n",
      "Epoch 472/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.9891e-11 - mae: 8.0801e-06\n",
      "Epoch 472: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 172us/sample - loss: 2.7980e-10 - mae: 1.2844e-05 - val_loss: 3.5464e-04 - val_mae: 0.0156\n",
      "Epoch 473/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1799e-09 - mae: 3.2264e-05\n",
      "Epoch 473: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 173us/sample - loss: 4.8448e-10 - mae: 1.8418e-05 - val_loss: 3.5447e-04 - val_mae: 0.0156\n",
      "Epoch 474/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.6239e-10 - mae: 1.3513e-05\n",
      "Epoch 474: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 177us/sample - loss: 2.3974e-10 - mae: 1.2668e-05 - val_loss: 3.5452e-04 - val_mae: 0.0156\n",
      "Epoch 475/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.9616e-11 - mae: 6.2248e-06\n",
      "Epoch 475: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 166us/sample - loss: 4.5559e-10 - mae: 1.7482e-05 - val_loss: 3.5459e-04 - val_mae: 0.0156\n",
      "Epoch 476/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 5.8549e-10 - mae: 2.2419e-05\n",
      "Epoch 476: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 188us/sample - loss: 8.9323e-10 - mae: 2.5426e-05 - val_loss: 3.5446e-04 - val_mae: 0.0156\n",
      "Epoch 477/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.6412e-10 - mae: 2.4322e-05\n",
      "Epoch 477: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 167us/sample - loss: 8.6368e-10 - mae: 2.4414e-05 - val_loss: 3.5444e-04 - val_mae: 0.0156\n",
      "Epoch 478/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3739e-09 - mae: 4.7103e-05\n",
      "Epoch 478: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 155us/sample - loss: 1.3471e-09 - mae: 3.0700e-05 - val_loss: 3.5456e-04 - val_mae: 0.0156\n",
      "Epoch 479/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 8.3227e-10 - mae: 2.4838e-05\n",
      "Epoch 479: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 205us/sample - loss: 3.3175e-10 - mae: 1.4551e-05 - val_loss: 3.5457e-04 - val_mae: 0.0156\n",
      "Epoch 480/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.1910e-10 - mae: 8.1899e-06\n",
      "Epoch 480: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 197us/sample - loss: 2.9214e-10 - mae: 1.3546e-05 - val_loss: 3.5460e-04 - val_mae: 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0200e-10 - mae: 7.9164e-06\n",
      "Epoch 481: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 199us/sample - loss: 2.0149e-10 - mae: 1.1117e-05 - val_loss: 3.5446e-04 - val_mae: 0.0156\n",
      "Epoch 482/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0288e-10 - mae: 1.5165e-05\n",
      "Epoch 482: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 1.0129e-10 - mae: 8.1941e-06 - val_loss: 3.5454e-04 - val_mae: 0.0156\n",
      "Epoch 483/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 7.6487e-11 - mae: 7.7460e-06\n",
      "Epoch 483: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 1.0217e-10 - mae: 8.2809e-06 - val_loss: 3.5453e-04 - val_mae: 0.0156\n",
      "Epoch 484/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.3508e-11 - mae: 7.2301e-06\n",
      "Epoch 484: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 190us/sample - loss: 9.7751e-11 - mae: 8.3200e-06 - val_loss: 3.5446e-04 - val_mae: 0.0156\n",
      "Epoch 485/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 6.0084e-11 - mae: 6.5949e-06\n",
      "Epoch 485: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 219us/sample - loss: 8.3071e-11 - mae: 7.3071e-06 - val_loss: 3.5451e-04 - val_mae: 0.0156\n",
      "Epoch 486/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.5729e-11 - mae: 3.4566e-06\n",
      "Epoch 486: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 232us/sample - loss: 2.6538e-10 - mae: 1.3497e-05 - val_loss: 3.5443e-04 - val_mae: 0.0156\n",
      "Epoch 487/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.3094e-10 - mae: 1.3688e-05\n",
      "Epoch 487: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 224us/sample - loss: 1.4048e-10 - mae: 9.3342e-06 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 488/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.8488e-11 - mae: 4.3424e-06\n",
      "Epoch 488: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 212us/sample - loss: 5.8659e-11 - mae: 6.0369e-06 - val_loss: 3.5447e-04 - val_mae: 0.0156\n",
      "Epoch 489/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.0002e-10 - mae: 1.1886e-05\n",
      "Epoch 489: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 224us/sample - loss: 9.0636e-11 - mae: 7.3675e-06 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 490/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0970e-11 - mae: 4.3074e-06\n",
      "Epoch 490: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 216us/sample - loss: 3.6095e-11 - mae: 4.6101e-06 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 491/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.8768e-11 - mae: 3.6566e-06\n",
      "Epoch 491: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 201us/sample - loss: 3.6034e-11 - mae: 4.9230e-06 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 492/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.7997e-11 - mae: 3.7973e-06\n",
      "Epoch 492: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 194us/sample - loss: 4.0682e-11 - mae: 5.1034e-06 - val_loss: 3.5451e-04 - val_mae: 0.0156\n",
      "Epoch 493/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.1013e-11 - mae: 4.5124e-06\n",
      "Epoch 493: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 192us/sample - loss: 4.8208e-11 - mae: 5.5855e-06 - val_loss: 3.5457e-04 - val_mae: 0.0156\n",
      "Epoch 494/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3820e-10 - mae: 1.1140e-05\n",
      "Epoch 494: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 210us/sample - loss: 4.6622e-11 - mae: 5.4912e-06 - val_loss: 3.5451e-04 - val_mae: 0.0156\n",
      "Epoch 495/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 3.0765e-11 - mae: 4.2852e-06\n",
      "Epoch 495: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 3.0604e-11 - mae: 4.4310e-06 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 496/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.4648e-11 - mae: 3.1952e-06\n",
      "Epoch 496: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 2.4045e-11 - mae: 3.8984e-06 - val_loss: 3.5449e-04 - val_mae: 0.0156\n",
      "Epoch 497/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.3014e-11 - mae: 2.9169e-06\n",
      "Epoch 497: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 214us/sample - loss: 1.4499e-11 - mae: 3.0087e-06 - val_loss: 3.5450e-04 - val_mae: 0.0156\n",
      "Epoch 498/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 1.0386e-11 - mae: 2.5474e-06\n",
      "Epoch 498: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 200us/sample - loss: 5.8794e-11 - mae: 6.1418e-06 - val_loss: 3.5448e-04 - val_mae: 0.0156\n",
      "Epoch 499/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 4.1404e-11 - mae: 5.4378e-06\n",
      "Epoch 499: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 228us/sample - loss: 9.1758e-11 - mae: 8.0025e-06 - val_loss: 3.5447e-04 - val_mae: 0.0156\n",
      "Epoch 500/500\n",
      " 10/160 [>.............................] - ETA: 0s - loss: 2.5610e-11 - mae: 3.8868e-06\n",
      "Epoch 500: val_loss did not improve from 0.00032\n",
      "160/160 [==============================] - 0s 217us/sample - loss: 1.0216e-10 - mae: 8.2901e-06 - val_loss: 3.5452e-04 - val_mae: 0.0156\n",
      "iteration 885 - y: 0.0009925772517053089\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-6.6811917e-06]], dtype=float32),\n",
       " array([[-7.42625e-06]], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = K.quickTrain()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.001}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d19e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aec2c6ca590>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
