{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd7382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82c18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1000\r"
     ]
    }
   ],
   "source": [
    "def basal_topology_func(x):\n",
    "    b = 1.0 - 0.1*x\n",
    "    return b\n",
    "\n",
    "def solution(nx, nt, L, T, M, basal_topology_func):\n",
    "\n",
    "    if len(M) != nx + 1:\n",
    "        raise ValueError('M specified but len(M) != nx + 1')\n",
    "        \n",
    "    dx = L/nx\n",
    "    dt = T/nt\n",
    "    x = np.linspace(0,L,nx+1)\n",
    "    t = np.linspace(0,T,nt+1)\n",
    "\n",
    "    b = basal_topology_func(x)\n",
    "\n",
    "    A = 1e-16\n",
    "    rho = 920.0\n",
    "    g = 9.2 \n",
    "    n = 3\n",
    "\n",
    "    C = 2*A/(n+2) * (rho*g)**n * (1e3)**n\n",
    "\n",
    "    h = np.zeros((nx+1,nt+1))\n",
    "    H = np.zeros((nx+1,nt+1))\n",
    "    h[:,0] = b\n",
    "    h[0,:] = b[0]\n",
    "    h[-1,:] = b[-1]\n",
    "\n",
    "    H[:,0] = h[:,0] - b\n",
    "    H[0,:] = h[0,:] - b[0]\n",
    "    H[-1,:] = h[-1,:] - b[-1]\n",
    "\n",
    "    for i in range(1,len(t)):\n",
    "\n",
    "        D = C *((H[1:,i-1]+H[:nx,i-1])/2.0)**(n+2) * ((h[1:,i-1] - h[:nx,i-1])/dx)**(n-1)\n",
    "\n",
    "        phi = -D*(h[1:,i-1]-h[:nx,i-1])/dx\n",
    "\n",
    "        h[1:nx,i] = h[1:nx,i-1] + M[1:nx]*dt - dt/dx * (phi[1:]-phi[:nx-1])\n",
    "        h[1:nx,i] = (h[1:nx,i] < b[1:nx]) * b[1:nx] + (h[1:nx,i] >= b[1:nx]) * h[1:nx,i]\n",
    "        H[:,i] = np.maximum(h[:,i] - b, 0.)\n",
    "\n",
    "        if not np.any(H[:,i]>=0.0):\n",
    "            raise Exception(\"Something went wrong.\")\n",
    "            \n",
    "    Volume = np.sum(H)*dx\n",
    "    \n",
    "    return H[int(nx/2),-1], h[int(nx/2),-1], Volume\n",
    "\n",
    "L = 30.\n",
    "T = 10.\n",
    "nx = 300\n",
    "nt = 12000\n",
    "samples = 1000\n",
    "\n",
    "M_samples = 0.01*np.random.rand(samples, nx+1)\n",
    "H_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "Volume_samples = np.zeros((samples,1), dtype = np.float64)\n",
    "\n",
    "for sample in range(samples):\n",
    "    if (sample+1) % 100 == 0:\n",
    "        print(f'Sample #{sample+1}', end='\\r')\n",
    "    H_samples[sample], _, Volume_samples[sample] = solution(nx, nt, L, T, M_samples[sample], basal_topology_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9844b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 7.3417e-04 - mae: 0.0218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 19:54:25.524006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/gnu7/openmpi/netcdf/4.5.0/lib:/opt/ohpc/pub/libs/gnu7/openmpi/netcdf-fortran/4.4.4/lib:/opt/ohpc/pub/libs/gnu7/openmpi/hdf5/1.10.1/lib:/opt/ohpc/pub/mpi/openmpi-gnu7/1.10.7/lib:/opt/ohpc/pub/compiler/gcc/7.3.0/lib64:/home/shreyas/lis-1.4.43/installation/lib:/share/jdk-16.0.1/lib::\n",
      "2023-07-03 19:54:25.524068: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00056, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 6.0848e-04 - mae: 0.0202 - val_loss: 5.5585e-04 - val_mae: 0.0204\n",
      "Epoch 2/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 4.6314e-04 - mae: 0.0181\n",
      "Epoch 2: val_loss improved from 0.00056 to 0.00051, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 4.8160e-04 - mae: 0.0185 - val_loss: 5.0896e-04 - val_mae: 0.0196\n",
      "Epoch 3/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 4.4980e-04 - mae: 0.0180\n",
      "Epoch 3: val_loss improved from 0.00051 to 0.00045, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 4.4166e-04 - mae: 0.0178 - val_loss: 4.4736e-04 - val_mae: 0.0178\n",
      "Epoch 4/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 4.4318e-04 - mae: 0.0179\n",
      "Epoch 4: val_loss improved from 0.00045 to 0.00044, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 4.1706e-04 - mae: 0.0173 - val_loss: 4.4464e-04 - val_mae: 0.0183\n",
      "Epoch 5/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 3.8932e-04 - mae: 0.0166\n",
      "Epoch 5: val_loss improved from 0.00044 to 0.00042, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 3.9276e-04 - mae: 0.0167 - val_loss: 4.2165e-04 - val_mae: 0.0178\n",
      "Epoch 6/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 3.9449e-04 - mae: 0.0166\n",
      "Epoch 6: val_loss improved from 0.00042 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 3.9037e-04 - mae: 0.0165 - val_loss: 4.1202e-04 - val_mae: 0.0166\n",
      "Epoch 7/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 3.4159e-04 - mae: 0.0156\n",
      "Epoch 7: val_loss improved from 0.00041 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 3.3896e-04 - mae: 0.0156 - val_loss: 3.7657e-04 - val_mae: 0.0165\n",
      "Epoch 8/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 3.1458e-04 - mae: 0.0149\n",
      "Epoch 8: val_loss did not improve from 0.00038\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 3.1446e-04 - mae: 0.0149 - val_loss: 3.8240e-04 - val_mae: 0.0170\n",
      "Epoch 9/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 3.1649e-04 - mae: 0.0148\n",
      "Epoch 9: val_loss improved from 0.00038 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 3.0143e-04 - mae: 0.0146 - val_loss: 3.6088e-04 - val_mae: 0.0165\n",
      "Epoch 10/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.6539e-04 - mae: 0.0136\n",
      "Epoch 10: val_loss improved from 0.00036 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 2.7117e-04 - mae: 0.0137 - val_loss: 3.4045e-04 - val_mae: 0.0160\n",
      "Epoch 11/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.7016e-04 - mae: 0.0139\n",
      "Epoch 11: val_loss improved from 0.00034 to 0.00032, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 2.6057e-04 - mae: 0.0134 - val_loss: 3.2053e-04 - val_mae: 0.0145\n",
      "Epoch 12/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.2349e-04 - mae: 0.0124\n",
      "Epoch 12: val_loss improved from 0.00032 to 0.00030, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 2.2450e-04 - mae: 0.0125 - val_loss: 2.9617e-04 - val_mae: 0.0147\n",
      "Epoch 13/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.0323e-04 - mae: 0.0118\n",
      "Epoch 13: val_loss improved from 0.00030 to 0.00028, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 1.9943e-04 - mae: 0.0117 - val_loss: 2.7597e-04 - val_mae: 0.0140\n",
      "Epoch 14/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8214e-04 - mae: 0.0111\n",
      "Epoch 14: val_loss improved from 0.00028 to 0.00026, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.8090e-04 - mae: 0.0111 - val_loss: 2.5943e-04 - val_mae: 0.0130\n",
      "Epoch 15/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7407e-04 - mae: 0.0108\n",
      "Epoch 15: val_loss improved from 0.00026 to 0.00024, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 1.7393e-04 - mae: 0.0107 - val_loss: 2.3833e-04 - val_mae: 0.0127\n",
      "Epoch 16/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.4906e-04 - mae: 0.0100\n",
      "Epoch 16: val_loss improved from 0.00024 to 0.00023, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 1.4580e-04 - mae: 0.0099 - val_loss: 2.2699e-04 - val_mae: 0.0127\n",
      "Epoch 17/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.3193e-04 - mae: 0.0093\n",
      "Epoch 17: val_loss improved from 0.00023 to 0.00021, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 1.2960e-04 - mae: 0.0092 - val_loss: 2.0715e-04 - val_mae: 0.0119\n",
      "Epoch 18/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.1337e-04 - mae: 0.0086\n",
      "Epoch 18: val_loss did not improve from 0.00021\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.1505e-04 - mae: 0.0087 - val_loss: 2.3385e-04 - val_mae: 0.0132\n",
      "Epoch 19/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.0969e-04 - mae: 0.0085\n",
      "Epoch 19: val_loss improved from 0.00021 to 0.00019, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 203us/sample - loss: 1.1080e-04 - mae: 0.0085 - val_loss: 1.8781e-04 - val_mae: 0.0115\n",
      "Epoch 20/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 9.1485e-05 - mae: 0.0076\n",
      "Epoch 20: val_loss improved from 0.00019 to 0.00017, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 9.4816e-05 - mae: 0.0078 - val_loss: 1.7145e-04 - val_mae: 0.0109\n",
      "Epoch 21/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 9.8716e-05 - mae: 0.0080\n",
      "Epoch 21: val_loss improved from 0.00017 to 0.00015, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 9.0506e-05 - mae: 0.0076 - val_loss: 1.5454e-04 - val_mae: 0.0102\n",
      "Epoch 22/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 7.7920e-05 - mae: 0.0070\n",
      "Epoch 22: val_loss improved from 0.00015 to 0.00014, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 7.8031e-05 - mae: 0.0071 - val_loss: 1.4038e-04 - val_mae: 0.0095\n",
      "Epoch 23/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 6.9250e-05 - mae: 0.0066\n",
      "Epoch 23: val_loss improved from 0.00014 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 6.8696e-05 - mae: 0.0066 - val_loss: 1.3487e-04 - val_mae: 0.0095\n",
      "Epoch 24/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 6.6195e-05 - mae: 0.0064\n",
      "Epoch 24: val_loss improved from 0.00013 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 7.3394e-05 - mae: 0.0068 - val_loss: 1.2537e-04 - val_mae: 0.0092\n",
      "Epoch 25/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 5.5712e-05 - mae: 0.0059\n",
      "Epoch 25: val_loss improved from 0.00013 to 0.00011, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 5.8202e-05 - mae: 0.0060 - val_loss: 1.1235e-04 - val_mae: 0.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 5.8370e-05 - mae: 0.0060\n",
      "Epoch 26: val_loss improved from 0.00011 to 0.00011, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 5.8497e-05 - mae: 0.0061 - val_loss: 1.0770e-04 - val_mae: 0.0084\n",
      "Epoch 27/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 5.5634e-05 - mae: 0.0059\n",
      "Epoch 27: val_loss improved from 0.00011 to 0.00011, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 5.4530e-05 - mae: 0.0059 - val_loss: 1.0509e-04 - val_mae: 0.0080\n",
      "Epoch 28/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 4.7780e-05 - mae: 0.0055\n",
      "Epoch 28: val_loss improved from 0.00011 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 4.7624e-05 - mae: 0.0055 - val_loss: 9.9270e-05 - val_mae: 0.0078\n",
      "Epoch 29/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 4.4872e-05 - mae: 0.0053\n",
      "Epoch 29: val_loss improved from 0.00010 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 4.4831e-05 - mae: 0.0053 - val_loss: 8.4657e-05 - val_mae: 0.0074\n",
      "Epoch 30/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 4.5115e-05 - mae: 0.0053\n",
      "Epoch 30: val_loss improved from 0.00008 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 188us/sample - loss: 4.5217e-05 - mae: 0.0054 - val_loss: 8.3234e-05 - val_mae: 0.0072\n",
      "Epoch 31/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 3.6020e-05 - mae: 0.0048\n",
      "Epoch 31: val_loss improved from 0.00008 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 3.6390e-05 - mae: 0.0048 - val_loss: 7.4411e-05 - val_mae: 0.0069\n",
      "Epoch 32/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 3.5455e-05 - mae: 0.0048\n",
      "Epoch 32: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 3.4917e-05 - mae: 0.0047 - val_loss: 6.9502e-05 - val_mae: 0.0067\n",
      "Epoch 33/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 3.0175e-05 - mae: 0.0044\n",
      "Epoch 33: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 3.3479e-05 - mae: 0.0046 - val_loss: 6.8395e-05 - val_mae: 0.0067\n",
      "Epoch 34/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 3.5082e-05 - mae: 0.0048\n",
      "Epoch 34: val_loss improved from 0.00007 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 3.4834e-05 - mae: 0.0048 - val_loss: 6.3024e-05 - val_mae: 0.0064\n",
      "Epoch 35/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.1954e-05 - mae: 0.0045\n",
      "Epoch 35: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 3.1458e-05 - mae: 0.0045 - val_loss: 7.2565e-05 - val_mae: 0.0070\n",
      "Epoch 36/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.6189e-05 - mae: 0.0040\n",
      "Epoch 36: val_loss did not improve from 0.00006\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 3.2505e-05 - mae: 0.0045 - val_loss: 1.0217e-04 - val_mae: 0.0084\n",
      "Epoch 37/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.2928e-05 - mae: 0.0046\n",
      "Epoch 37: val_loss improved from 0.00006 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 3.2380e-05 - mae: 0.0046 - val_loss: 5.4673e-05 - val_mae: 0.0059\n",
      "Epoch 38/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.7047e-05 - mae: 0.0042\n",
      "Epoch 38: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 2.7104e-05 - mae: 0.0042 - val_loss: 5.2511e-05 - val_mae: 0.0058\n",
      "Epoch 39/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.4681e-05 - mae: 0.0040\n",
      "Epoch 39: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 2.5870e-05 - mae: 0.0040 - val_loss: 5.1271e-05 - val_mae: 0.0057\n",
      "Epoch 40/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 2.8461e-05 - mae: 0.0043\n",
      "Epoch 40: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 2.8744e-05 - mae: 0.0043 - val_loss: 5.1532e-05 - val_mae: 0.0058\n",
      "Epoch 41/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.5066e-05 - mae: 0.0040\n",
      "Epoch 41: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 2.4990e-05 - mae: 0.0040 - val_loss: 5.3248e-05 - val_mae: 0.0059\n",
      "Epoch 42/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.5180e-05 - mae: 0.0040\n",
      "Epoch 42: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 2.5568e-05 - mae: 0.0040 - val_loss: 4.6398e-05 - val_mae: 0.0054\n",
      "Epoch 43/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.1925e-05 - mae: 0.0038\n",
      "Epoch 43: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.1960e-05 - mae: 0.0038 - val_loss: 4.6550e-05 - val_mae: 0.0055\n",
      "Epoch 44/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.2384e-05 - mae: 0.0038\n",
      "Epoch 44: val_loss improved from 0.00005 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 2.3621e-05 - mae: 0.0039 - val_loss: 4.3303e-05 - val_mae: 0.0052\n",
      "Epoch 45/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.2039e-05 - mae: 0.0038\n",
      "Epoch 45: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.2117e-05 - mae: 0.0038 - val_loss: 4.5186e-05 - val_mae: 0.0053\n",
      "Epoch 46/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1439e-05 - mae: 0.0037\n",
      "Epoch 46: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 2.1468e-05 - mae: 0.0037 - val_loss: 4.1051e-05 - val_mae: 0.0051\n",
      "Epoch 47/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.3304e-05 - mae: 0.0038\n",
      "Epoch 47: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.3535e-05 - mae: 0.0039 - val_loss: 4.1296e-05 - val_mae: 0.0051\n",
      "Epoch 48/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0332e-05 - mae: 0.0036\n",
      "Epoch 48: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.2359e-05 - mae: 0.0038 - val_loss: 4.3634e-05 - val_mae: 0.0052\n",
      "Epoch 49/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1405e-05 - mae: 0.0037\n",
      "Epoch 49: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.1196e-05 - mae: 0.0037 - val_loss: 4.0254e-05 - val_mae: 0.0051\n",
      "Epoch 50/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1806e-05 - mae: 0.0038\n",
      "Epoch 50: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 2.1674e-05 - mae: 0.0038 - val_loss: 3.9269e-05 - val_mae: 0.0050\n",
      "Epoch 51/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.4260e-05 - mae: 0.0039\n",
      "Epoch 51: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.3958e-05 - mae: 0.0039 - val_loss: 3.8546e-05 - val_mae: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9946e-05 - mae: 0.0036\n",
      "Epoch 52: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0081e-05 - mae: 0.0036 - val_loss: 4.1109e-05 - val_mae: 0.0051\n",
      "Epoch 53/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9599e-05 - mae: 0.0036\n",
      "Epoch 53: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.0031e-05 - mae: 0.0036 - val_loss: 3.7576e-05 - val_mae: 0.0049\n",
      "Epoch 54/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.0337e-05 - mae: 0.0036\n",
      "Epoch 54: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.0499e-05 - mae: 0.0036 - val_loss: 3.7712e-05 - val_mae: 0.0049\n",
      "Epoch 55/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0600e-05 - mae: 0.0036\n",
      "Epoch 55: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.1605e-05 - mae: 0.0037 - val_loss: 4.7882e-05 - val_mae: 0.0056\n",
      "Epoch 56/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9780e-05 - mae: 0.0036\n",
      "Epoch 56: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0179e-05 - mae: 0.0036 - val_loss: 4.0505e-05 - val_mae: 0.0050\n",
      "Epoch 57/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.1935e-05 - mae: 0.0038\n",
      "Epoch 57: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.1829e-05 - mae: 0.0037 - val_loss: 3.6112e-05 - val_mae: 0.0047\n",
      "Epoch 58/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8603e-05 - mae: 0.0035\n",
      "Epoch 58: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 1.8478e-05 - mae: 0.0035 - val_loss: 3.5573e-05 - val_mae: 0.0047\n",
      "Epoch 59/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9032e-05 - mae: 0.0035\n",
      "Epoch 59: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.9032e-05 - mae: 0.0035 - val_loss: 3.5345e-05 - val_mae: 0.0047\n",
      "Epoch 60/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.8096e-05 - mae: 0.0033\n",
      "Epoch 60: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0497e-05 - mae: 0.0036 - val_loss: 3.5633e-05 - val_mae: 0.0047\n",
      "Epoch 61/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7469e-05 - mae: 0.0034\n",
      "Epoch 61: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8967e-05 - mae: 0.0035 - val_loss: 5.1026e-05 - val_mae: 0.0057\n",
      "Epoch 62/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1194e-05 - mae: 0.0037\n",
      "Epoch 62: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0249e-05 - mae: 0.0036 - val_loss: 3.5975e-05 - val_mae: 0.0047\n",
      "Epoch 63/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6679e-05 - mae: 0.0033\n",
      "Epoch 63: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0394e-05 - mae: 0.0036 - val_loss: 3.9566e-05 - val_mae: 0.0049\n",
      "Epoch 64/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.3771e-05 - mae: 0.0039\n",
      "Epoch 64: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 2.2139e-05 - mae: 0.0038 - val_loss: 4.4829e-05 - val_mae: 0.0054\n",
      "Epoch 65/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8741e-05 - mae: 0.0035\n",
      "Epoch 65: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.8449e-05 - mae: 0.0035 - val_loss: 3.7046e-05 - val_mae: 0.0048\n",
      "Epoch 66/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9685e-05 - mae: 0.0036\n",
      "Epoch 66: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9881e-05 - mae: 0.0036 - val_loss: 3.5178e-05 - val_mae: 0.0047\n",
      "Epoch 67/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8573e-05 - mae: 0.0035\n",
      "Epoch 67: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0125e-05 - mae: 0.0036 - val_loss: 4.4872e-05 - val_mae: 0.0054\n",
      "Epoch 68/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 2.0399e-05 - mae: 0.0036\n",
      "Epoch 68: val_loss improved from 0.00004 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.0625e-05 - mae: 0.0036 - val_loss: 3.4611e-05 - val_mae: 0.0046\n",
      "Epoch 69/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1673e-05 - mae: 0.0037\n",
      "Epoch 69: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1673e-05 - mae: 0.0037 - val_loss: 4.7466e-05 - val_mae: 0.0056\n",
      "Epoch 70/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.3478e-05 - mae: 0.0039\n",
      "Epoch 70: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 2.5046e-05 - mae: 0.0040 - val_loss: 3.8880e-05 - val_mae: 0.0050\n",
      "Epoch 71/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7779e-05 - mae: 0.0034\n",
      "Epoch 71: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9542e-05 - mae: 0.0036 - val_loss: 4.6439e-05 - val_mae: 0.0055\n",
      "Epoch 72/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3320e-05 - mae: 0.0038\n",
      "Epoch 72: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.1312e-05 - mae: 0.0037 - val_loss: 3.4269e-05 - val_mae: 0.0046\n",
      "Epoch 73/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0518e-05 - mae: 0.0036\n",
      "Epoch 73: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 2.0044e-05 - mae: 0.0035 - val_loss: 3.5817e-05 - val_mae: 0.0046\n",
      "Epoch 74/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8415e-05 - mae: 0.0034\n",
      "Epoch 74: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.8273e-05 - mae: 0.0034 - val_loss: 4.1887e-05 - val_mae: 0.0052\n",
      "Epoch 75/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 2.0214e-05 - mae: 0.0036\n",
      "Epoch 75: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.9532e-05 - mae: 0.0035 - val_loss: 3.4871e-05 - val_mae: 0.0046\n",
      "Epoch 76/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0963e-05 - mae: 0.0036\n",
      "Epoch 76: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.1302e-05 - mae: 0.0037 - val_loss: 3.7890e-05 - val_mae: 0.0048\n",
      "Epoch 77/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8388e-05 - mae: 0.0035\n",
      "Epoch 77: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8167e-05 - mae: 0.0034 - val_loss: 3.4320e-05 - val_mae: 0.0046\n",
      "Epoch 78/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6207e-05 - mae: 0.0032\n",
      "Epoch 78: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7520e-05 - mae: 0.0034 - val_loss: 3.6424e-05 - val_mae: 0.0048\n",
      "Epoch 79/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0208e-05 - mae: 0.0037\n",
      "Epoch 79: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1234e-05 - mae: 0.0037 - val_loss: 5.1459e-05 - val_mae: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8858e-05 - mae: 0.0035\n",
      "Epoch 80: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8563e-05 - mae: 0.0034 - val_loss: 3.4241e-05 - val_mae: 0.0046\n",
      "Epoch 81/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9706e-05 - mae: 0.0035\n",
      "Epoch 81: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8364e-05 - mae: 0.0034 - val_loss: 3.8222e-05 - val_mae: 0.0050\n",
      "Epoch 82/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0561e-05 - mae: 0.0037\n",
      "Epoch 82: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9570e-05 - mae: 0.0036 - val_loss: 3.5819e-05 - val_mae: 0.0047\n",
      "Epoch 83/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9089e-05 - mae: 0.0035\n",
      "Epoch 83: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8966e-05 - mae: 0.0035 - val_loss: 4.1462e-05 - val_mae: 0.0050\n",
      "Epoch 84/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9783e-05 - mae: 0.0035\n",
      "Epoch 84: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9053e-05 - mae: 0.0035 - val_loss: 3.4504e-05 - val_mae: 0.0046\n",
      "Epoch 85/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9069e-05 - mae: 0.0035\n",
      "Epoch 85: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0718e-05 - mae: 0.0037 - val_loss: 3.9618e-05 - val_mae: 0.0051\n",
      "Epoch 86/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7782e-05 - mae: 0.0033\n",
      "Epoch 86: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9846e-05 - mae: 0.0035 - val_loss: 3.4702e-05 - val_mae: 0.0047\n",
      "Epoch 87/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6399e-05 - mae: 0.0032\n",
      "Epoch 87: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7428e-05 - mae: 0.0033 - val_loss: 3.4275e-05 - val_mae: 0.0046\n",
      "Epoch 88/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9383e-05 - mae: 0.0036\n",
      "Epoch 88: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 2.0274e-05 - mae: 0.0036 - val_loss: 3.4509e-05 - val_mae: 0.0046\n",
      "Epoch 89/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7501e-05 - mae: 0.0033\n",
      "Epoch 89: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8335e-05 - mae: 0.0034 - val_loss: 5.1802e-05 - val_mae: 0.0057\n",
      "Epoch 90/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8642e-05 - mae: 0.0035\n",
      "Epoch 90: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8686e-05 - mae: 0.0034 - val_loss: 3.4905e-05 - val_mae: 0.0046\n",
      "Epoch 91/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6497e-05 - mae: 0.0032\n",
      "Epoch 91: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7898e-05 - mae: 0.0034 - val_loss: 3.7685e-05 - val_mae: 0.0047\n",
      "Epoch 92/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.5873e-05 - mae: 0.0041\n",
      "Epoch 92: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.5873e-05 - mae: 0.0041 - val_loss: 6.5778e-05 - val_mae: 0.0068\n",
      "Epoch 93/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9936e-05 - mae: 0.0036\n",
      "Epoch 93: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9397e-05 - mae: 0.0035 - val_loss: 3.6556e-05 - val_mae: 0.0047\n",
      "Epoch 94/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8213e-05 - mae: 0.0034\n",
      "Epoch 94: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8587e-05 - mae: 0.0035 - val_loss: 3.4311e-05 - val_mae: 0.0046\n",
      "Epoch 95/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8550e-05 - mae: 0.0034\n",
      "Epoch 95: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.0773e-05 - mae: 0.0036 - val_loss: 4.2911e-05 - val_mae: 0.0051\n",
      "Epoch 96/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9183e-05 - mae: 0.0035\n",
      "Epoch 96: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8373e-05 - mae: 0.0034 - val_loss: 3.4804e-05 - val_mae: 0.0047\n",
      "Epoch 97/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8227e-05 - mae: 0.0033\n",
      "Epoch 97: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8051e-05 - mae: 0.0033 - val_loss: 3.4414e-05 - val_mae: 0.0046\n",
      "Epoch 98/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1739e-05 - mae: 0.0038\n",
      "Epoch 98: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.7933e-05 - mae: 0.0042 - val_loss: 3.5352e-05 - val_mae: 0.0047\n",
      "Epoch 99/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0875e-05 - mae: 0.0036\n",
      "Epoch 99: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0277e-05 - mae: 0.0036 - val_loss: 3.4537e-05 - val_mae: 0.0046\n",
      "Epoch 100/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0574e-05 - mae: 0.0036\n",
      "Epoch 100: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.1992e-05 - mae: 0.0037 - val_loss: 6.3027e-05 - val_mae: 0.0066\n",
      "Epoch 101/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9168e-05 - mae: 0.0035\n",
      "Epoch 101: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9168e-05 - mae: 0.0035 - val_loss: 3.5794e-05 - val_mae: 0.0046\n",
      "Epoch 102/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8430e-05 - mae: 0.0034\n",
      "Epoch 102: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8970e-05 - mae: 0.0035 - val_loss: 3.5921e-05 - val_mae: 0.0048\n",
      "Epoch 103/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9212e-05 - mae: 0.0035\n",
      "Epoch 103: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9496e-05 - mae: 0.0035 - val_loss: 4.7353e-05 - val_mae: 0.0056\n",
      "Epoch 104/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8070e-05 - mae: 0.0033\n",
      "Epoch 104: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8133e-05 - mae: 0.0034 - val_loss: 3.3876e-05 - val_mae: 0.0045\n",
      "Epoch 105/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8813e-05 - mae: 0.0034\n",
      "Epoch 105: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.9457e-05 - mae: 0.0035 - val_loss: 4.6990e-05 - val_mae: 0.0056\n",
      "Epoch 106/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9580e-05 - mae: 0.0036\n",
      "Epoch 106: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0339e-05 - mae: 0.0036 - val_loss: 3.4231e-05 - val_mae: 0.0046\n",
      "Epoch 107/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1249e-05 - mae: 0.0037\n",
      "Epoch 107: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9896e-05 - mae: 0.0036 - val_loss: 4.1862e-05 - val_mae: 0.0050\n",
      "Epoch 108/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9433e-05 - mae: 0.0035\n",
      "Epoch 108: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9286e-05 - mae: 0.0035 - val_loss: 4.9940e-05 - val_mae: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8697e-05 - mae: 0.0035\n",
      "Epoch 109: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8697e-05 - mae: 0.0035 - val_loss: 3.9245e-05 - val_mae: 0.0048\n",
      "Epoch 110/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9328e-05 - mae: 0.0035\n",
      "Epoch 110: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9408e-05 - mae: 0.0035 - val_loss: 3.5712e-05 - val_mae: 0.0048\n",
      "Epoch 111/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8094e-05 - mae: 0.0034\n",
      "Epoch 111: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7896e-05 - mae: 0.0033 - val_loss: 3.4372e-05 - val_mae: 0.0046\n",
      "Epoch 112/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6957e-05 - mae: 0.0033\n",
      "Epoch 112: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8141e-05 - mae: 0.0034 - val_loss: 3.4889e-05 - val_mae: 0.0046\n",
      "Epoch 113/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6221e-05 - mae: 0.0032\n",
      "Epoch 113: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8348e-05 - mae: 0.0034 - val_loss: 4.1814e-05 - val_mae: 0.0052\n",
      "Epoch 114/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8785e-05 - mae: 0.0035\n",
      "Epoch 114: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8327e-05 - mae: 0.0034 - val_loss: 3.7858e-05 - val_mae: 0.0048\n",
      "Epoch 115/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7213e-05 - mae: 0.0034\n",
      "Epoch 115: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7323e-05 - mae: 0.0034 - val_loss: 3.5367e-05 - val_mae: 0.0047\n",
      "Epoch 116/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6661e-05 - mae: 0.0032\n",
      "Epoch 116: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8138e-05 - mae: 0.0034 - val_loss: 3.9863e-05 - val_mae: 0.0049\n",
      "Epoch 117/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1031e-05 - mae: 0.0036\n",
      "Epoch 117: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0613e-05 - mae: 0.0036 - val_loss: 4.3256e-05 - val_mae: 0.0053\n",
      "Epoch 118/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.4980e-05 - mae: 0.0040\n",
      "Epoch 118: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.3795e-05 - mae: 0.0039 - val_loss: 3.7159e-05 - val_mae: 0.0047\n",
      "Epoch 119/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.2069e-05 - mae: 0.0037\n",
      "Epoch 119: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.4616e-05 - mae: 0.0040 - val_loss: 3.6505e-05 - val_mae: 0.0048\n",
      "Epoch 120/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.3378e-05 - mae: 0.0038\n",
      "Epoch 120: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0352e-05 - mae: 0.0036 - val_loss: 4.2778e-05 - val_mae: 0.0053\n",
      "Epoch 121/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.3755e-05 - mae: 0.0040\n",
      "Epoch 121: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.2976e-05 - mae: 0.0039 - val_loss: 3.8056e-05 - val_mae: 0.0048\n",
      "Epoch 122/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6458e-05 - mae: 0.0032\n",
      "Epoch 122: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7520e-05 - mae: 0.0033 - val_loss: 3.4727e-05 - val_mae: 0.0046\n",
      "Epoch 123/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6957e-05 - mae: 0.0033\n",
      "Epoch 123: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6981e-05 - mae: 0.0033 - val_loss: 3.4928e-05 - val_mae: 0.0046\n",
      "Epoch 124/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7413e-05 - mae: 0.0033\n",
      "Epoch 124: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8419e-05 - mae: 0.0034 - val_loss: 3.6399e-05 - val_mae: 0.0048\n",
      "Epoch 125/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6436e-05 - mae: 0.0032\n",
      "Epoch 125: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7290e-05 - mae: 0.0033 - val_loss: 3.4640e-05 - val_mae: 0.0046\n",
      "Epoch 126/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8986e-05 - mae: 0.0035\n",
      "Epoch 126: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9257e-05 - mae: 0.0035 - val_loss: 3.6346e-05 - val_mae: 0.0047\n",
      "Epoch 127/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9062e-05 - mae: 0.0035\n",
      "Epoch 127: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9276e-05 - mae: 0.0035 - val_loss: 3.8680e-05 - val_mae: 0.0050\n",
      "Epoch 128/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0202e-05 - mae: 0.0035\n",
      "Epoch 128: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0276e-05 - mae: 0.0036 - val_loss: 4.5227e-05 - val_mae: 0.0055\n",
      "Epoch 129/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8210e-05 - mae: 0.0034\n",
      "Epoch 129: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8210e-05 - mae: 0.0034 - val_loss: 4.1013e-05 - val_mae: 0.0052\n",
      "Epoch 130/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8123e-05 - mae: 0.0034\n",
      "Epoch 130: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8230e-05 - mae: 0.0034 - val_loss: 3.5641e-05 - val_mae: 0.0046\n",
      "Epoch 131/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9139e-05 - mae: 0.0035\n",
      "Epoch 131: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9009e-05 - mae: 0.0035 - val_loss: 3.4694e-05 - val_mae: 0.0046\n",
      "Epoch 132/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.5459e-05 - mae: 0.0031\n",
      "Epoch 132: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9510e-05 - mae: 0.0035 - val_loss: 3.7388e-05 - val_mae: 0.0049\n",
      "Epoch 133/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5528e-05 - mae: 0.0031\n",
      "Epoch 133: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0207e-05 - mae: 0.0035 - val_loss: 3.5285e-05 - val_mae: 0.0046\n",
      "Epoch 134/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.6889e-05 - mae: 0.0032\n",
      "Epoch 134: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.6759e-05 - mae: 0.0032 - val_loss: 3.4725e-05 - val_mae: 0.0046\n",
      "Epoch 135/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6560e-05 - mae: 0.0033\n",
      "Epoch 135: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.6682e-05 - mae: 0.0033 - val_loss: 3.8022e-05 - val_mae: 0.0048\n",
      "Epoch 136/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9424e-05 - mae: 0.0035\n",
      "Epoch 136: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9278e-05 - mae: 0.0035 - val_loss: 3.5481e-05 - val_mae: 0.0047\n",
      "Epoch 137/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8491e-05 - mae: 0.0034\n",
      "Epoch 137: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8814e-05 - mae: 0.0035 - val_loss: 3.4875e-05 - val_mae: 0.0046\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1865e-05 - mae: 0.0037\n",
      "Epoch 138: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.3911e-05 - mae: 0.0039 - val_loss: 3.7266e-05 - val_mae: 0.0047\n",
      "Epoch 139/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7684e-05 - mae: 0.0034\n",
      "Epoch 139: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7640e-05 - mae: 0.0034 - val_loss: 3.8519e-05 - val_mae: 0.0048\n",
      "Epoch 140/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1648e-05 - mae: 0.0037\n",
      "Epoch 140: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0772e-05 - mae: 0.0036 - val_loss: 3.5938e-05 - val_mae: 0.0046\n",
      "Epoch 141/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8796e-05 - mae: 0.0034\n",
      "Epoch 141: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8779e-05 - mae: 0.0035 - val_loss: 3.7071e-05 - val_mae: 0.0047\n",
      "Epoch 142/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7674e-05 - mae: 0.0033\n",
      "Epoch 142: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8605e-05 - mae: 0.0034 - val_loss: 3.6359e-05 - val_mae: 0.0047\n",
      "Epoch 143/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.7570e-05 - mae: 0.0033\n",
      "Epoch 143: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.9302e-05 - mae: 0.0035 - val_loss: 4.2349e-05 - val_mae: 0.0053\n",
      "Epoch 144/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6311e-05 - mae: 0.0032\n",
      "Epoch 144: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8536e-05 - mae: 0.0034 - val_loss: 3.6314e-05 - val_mae: 0.0048\n",
      "Epoch 145/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6824e-05 - mae: 0.0032\n",
      "Epoch 145: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6831e-05 - mae: 0.0032 - val_loss: 4.4123e-05 - val_mae: 0.0052\n",
      "Epoch 146/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7787e-05 - mae: 0.0033\n",
      "Epoch 146: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7777e-05 - mae: 0.0033 - val_loss: 3.6368e-05 - val_mae: 0.0048\n",
      "Epoch 147/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6323e-05 - mae: 0.0033\n",
      "Epoch 147: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6866e-05 - mae: 0.0033 - val_loss: 3.8735e-05 - val_mae: 0.0048\n",
      "Epoch 148/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9266e-05 - mae: 0.0035\n",
      "Epoch 148: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0502e-05 - mae: 0.0036 - val_loss: 5.5981e-05 - val_mae: 0.0062\n",
      "Epoch 149/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.4294e-05 - mae: 0.0041\n",
      "Epoch 149: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.2210e-05 - mae: 0.0038 - val_loss: 4.0552e-05 - val_mae: 0.0050\n",
      "Epoch 150/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8959e-05 - mae: 0.0035\n",
      "Epoch 150: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8897e-05 - mae: 0.0035 - val_loss: 3.8645e-05 - val_mae: 0.0048\n",
      "Epoch 151/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9587e-05 - mae: 0.0035\n",
      "Epoch 151: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1153e-05 - mae: 0.0037 - val_loss: 4.6243e-05 - val_mae: 0.0054\n",
      "Epoch 152/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0166e-05 - mae: 0.0036\n",
      "Epoch 152: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9958e-05 - mae: 0.0036 - val_loss: 3.4860e-05 - val_mae: 0.0046\n",
      "Epoch 153/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2509e-05 - mae: 0.0038\n",
      "Epoch 153: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0984e-05 - mae: 0.0037 - val_loss: 4.8147e-05 - val_mae: 0.0057\n",
      "Epoch 154/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7936e-05 - mae: 0.0034\n",
      "Epoch 154: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8032e-05 - mae: 0.0034 - val_loss: 3.6510e-05 - val_mae: 0.0047\n",
      "Epoch 155/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7210e-05 - mae: 0.0033\n",
      "Epoch 155: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8836e-05 - mae: 0.0034 - val_loss: 3.6858e-05 - val_mae: 0.0047\n",
      "Epoch 156/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3531e-05 - mae: 0.0039\n",
      "Epoch 156: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0442e-05 - mae: 0.0036 - val_loss: 3.4962e-05 - val_mae: 0.0046\n",
      "Epoch 157/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6139e-05 - mae: 0.0032\n",
      "Epoch 157: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8071e-05 - mae: 0.0034 - val_loss: 4.1981e-05 - val_mae: 0.0051\n",
      "Epoch 158/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1626e-05 - mae: 0.0036\n",
      "Epoch 158: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0822e-05 - mae: 0.0036 - val_loss: 4.5534e-05 - val_mae: 0.0053\n",
      "Epoch 159/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9527e-05 - mae: 0.0035\n",
      "Epoch 159: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9600e-05 - mae: 0.0035 - val_loss: 3.5367e-05 - val_mae: 0.0047\n",
      "Epoch 160/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4664e-05 - mae: 0.0030\n",
      "Epoch 160: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9425e-05 - mae: 0.0035 - val_loss: 4.8580e-05 - val_mae: 0.0055\n",
      "Epoch 161/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5898e-05 - mae: 0.0031\n",
      "Epoch 161: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8152e-05 - mae: 0.0034 - val_loss: 3.4813e-05 - val_mae: 0.0047\n",
      "Epoch 162/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0993e-05 - mae: 0.0036\n",
      "Epoch 162: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0383e-05 - mae: 0.0035 - val_loss: 3.9510e-05 - val_mae: 0.0049\n",
      "Epoch 163/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9945e-05 - mae: 0.0036\n",
      "Epoch 163: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9945e-05 - mae: 0.0036 - val_loss: 3.6863e-05 - val_mae: 0.0047\n",
      "Epoch 164/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0175e-05 - mae: 0.0036\n",
      "Epoch 164: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9072e-05 - mae: 0.0035 - val_loss: 4.1162e-05 - val_mae: 0.0050\n",
      "Epoch 165/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9037e-05 - mae: 0.0035\n",
      "Epoch 165: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9222e-05 - mae: 0.0035 - val_loss: 3.5089e-05 - val_mae: 0.0046\n",
      "Epoch 166/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6058e-05 - mae: 0.0032\n",
      "Epoch 166: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9426e-05 - mae: 0.0035 - val_loss: 3.6317e-05 - val_mae: 0.0047\n",
      "Epoch 167/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7055e-05 - mae: 0.0033\n",
      "Epoch 167: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7153e-05 - mae: 0.0033 - val_loss: 3.4380e-05 - val_mae: 0.0046\n",
      "Epoch 168/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6319e-05 - mae: 0.0032\n",
      "Epoch 168: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7355e-05 - mae: 0.0033 - val_loss: 4.9686e-05 - val_mae: 0.0058\n",
      "Epoch 169/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9498e-05 - mae: 0.0035\n",
      "Epoch 169: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9558e-05 - mae: 0.0035 - val_loss: 3.5833e-05 - val_mae: 0.0048\n",
      "Epoch 170/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6620e-05 - mae: 0.0033\n",
      "Epoch 170: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9167e-05 - mae: 0.0035 - val_loss: 3.5591e-05 - val_mae: 0.0046\n",
      "Epoch 171/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6641e-05 - mae: 0.0032\n",
      "Epoch 171: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7412e-05 - mae: 0.0033 - val_loss: 3.5242e-05 - val_mae: 0.0046\n",
      "Epoch 172/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0888e-05 - mae: 0.0036\n",
      "Epoch 172: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1250e-05 - mae: 0.0036 - val_loss: 3.5289e-05 - val_mae: 0.0046\n",
      "Epoch 173/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5838e-05 - mae: 0.0031\n",
      "Epoch 173: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9558e-05 - mae: 0.0035 - val_loss: 3.7005e-05 - val_mae: 0.0047\n",
      "Epoch 174/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9838e-05 - mae: 0.0036\n",
      "Epoch 174: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9773e-05 - mae: 0.0035 - val_loss: 3.5279e-05 - val_mae: 0.0047\n",
      "Epoch 175/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8625e-05 - mae: 0.0034\n",
      "Epoch 175: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8490e-05 - mae: 0.0034 - val_loss: 3.8145e-05 - val_mae: 0.0049\n",
      "Epoch 176/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7644e-05 - mae: 0.0033\n",
      "Epoch 176: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8974e-05 - mae: 0.0035 - val_loss: 3.6375e-05 - val_mae: 0.0047\n",
      "Epoch 177/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7635e-05 - mae: 0.0034\n",
      "Epoch 177: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7635e-05 - mae: 0.0034 - val_loss: 3.4879e-05 - val_mae: 0.0046\n",
      "Epoch 178/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8084e-05 - mae: 0.0034\n",
      "Epoch 178: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9637e-05 - mae: 0.0035 - val_loss: 5.5542e-05 - val_mae: 0.0062\n",
      "Epoch 179/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7104e-05 - mae: 0.0033\n",
      "Epoch 179: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7402e-05 - mae: 0.0033 - val_loss: 4.1401e-05 - val_mae: 0.0050\n",
      "Epoch 180/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7066e-05 - mae: 0.0033\n",
      "Epoch 180: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7989e-05 - mae: 0.0034 - val_loss: 3.6652e-05 - val_mae: 0.0047\n",
      "Epoch 181/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7018e-05 - mae: 0.0033\n",
      "Epoch 181: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9030e-05 - mae: 0.0035 - val_loss: 3.5477e-05 - val_mae: 0.0047\n",
      "Epoch 182/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8449e-05 - mae: 0.0034\n",
      "Epoch 182: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9282e-05 - mae: 0.0035 - val_loss: 3.5085e-05 - val_mae: 0.0047\n",
      "Epoch 183/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6272e-05 - mae: 0.0032\n",
      "Epoch 183: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7627e-05 - mae: 0.0034 - val_loss: 5.2494e-05 - val_mae: 0.0060\n",
      "Epoch 184/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8586e-05 - mae: 0.0035\n",
      "Epoch 184: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.9337e-05 - mae: 0.0035 - val_loss: 3.5371e-05 - val_mae: 0.0047\n",
      "Epoch 185/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0973e-05 - mae: 0.0037\n",
      "Epoch 185: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9939e-05 - mae: 0.0036 - val_loss: 3.5846e-05 - val_mae: 0.0047\n",
      "Epoch 186/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7701e-05 - mae: 0.0033\n",
      "Epoch 186: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8122e-05 - mae: 0.0033 - val_loss: 3.6353e-05 - val_mae: 0.0048\n",
      "Epoch 187/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7257e-05 - mae: 0.0033\n",
      "Epoch 187: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9116e-05 - mae: 0.0035 - val_loss: 3.5425e-05 - val_mae: 0.0047\n",
      "Epoch 188/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6556e-05 - mae: 0.0032\n",
      "Epoch 188: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8943e-05 - mae: 0.0035 - val_loss: 3.5118e-05 - val_mae: 0.0046\n",
      "Epoch 189/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8528e-05 - mae: 0.0033\n",
      "Epoch 189: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8636e-05 - mae: 0.0034 - val_loss: 3.5484e-05 - val_mae: 0.0047\n",
      "Epoch 190/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8866e-05 - mae: 0.0034\n",
      "Epoch 190: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8180e-05 - mae: 0.0034 - val_loss: 3.7088e-05 - val_mae: 0.0049\n",
      "Epoch 191/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.9254e-05 - mae: 0.0034\n",
      "Epoch 191: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.8354e-05 - mae: 0.0034 - val_loss: 3.5177e-05 - val_mae: 0.0047\n",
      "Epoch 192/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.8008e-05 - mae: 0.0034\n",
      "Epoch 192: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.8031e-05 - mae: 0.0034 - val_loss: 3.9343e-05 - val_mae: 0.0051\n",
      "Epoch 193/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7819e-05 - mae: 0.0033\n",
      "Epoch 193: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.7804e-05 - mae: 0.0034 - val_loss: 3.8318e-05 - val_mae: 0.0050\n",
      "Epoch 194/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0164e-05 - mae: 0.0036\n",
      "Epoch 194: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 2.0319e-05 - mae: 0.0036 - val_loss: 3.7636e-05 - val_mae: 0.0049\n",
      "Epoch 195/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8041e-05 - mae: 0.0034\n",
      "Epoch 195: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.7707e-05 - mae: 0.0033 - val_loss: 3.7828e-05 - val_mae: 0.0048\n",
      "Epoch 196/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8011e-05 - mae: 0.0033\n",
      "Epoch 196: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.7926e-05 - mae: 0.0033 - val_loss: 3.5556e-05 - val_mae: 0.0046\n",
      "Epoch 197/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 2.0805e-05 - mae: 0.0037\n",
      "Epoch 197: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.9757e-05 - mae: 0.0036 - val_loss: 3.5583e-05 - val_mae: 0.0046\n",
      "Epoch 198/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7976e-05 - mae: 0.0033\n",
      "Epoch 198: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.7575e-05 - mae: 0.0033 - val_loss: 3.5988e-05 - val_mae: 0.0048\n",
      "Epoch 199/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.6633e-05 - mae: 0.0033\n",
      "Epoch 199: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 2.0197e-05 - mae: 0.0036 - val_loss: 3.5727e-05 - val_mae: 0.0047\n",
      "Epoch 200/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7985e-05 - mae: 0.0034\n",
      "Epoch 200: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9144e-05 - mae: 0.0035 - val_loss: 3.6580e-05 - val_mae: 0.0048\n",
      "Epoch 201/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6493e-05 - mae: 0.0032\n",
      "Epoch 201: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6298e-05 - mae: 0.0032 - val_loss: 3.5408e-05 - val_mae: 0.0047\n",
      "Epoch 202/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.7055e-05 - mae: 0.0032\n",
      "Epoch 202: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.7733e-05 - mae: 0.0033 - val_loss: 3.5481e-05 - val_mae: 0.0047\n",
      "Epoch 203/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6577e-05 - mae: 0.0032\n",
      "Epoch 203: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5898e-05 - mae: 0.0032 - val_loss: 3.5492e-05 - val_mae: 0.0046\n",
      "Epoch 204/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6876e-05 - mae: 0.0032\n",
      "Epoch 204: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.7136e-05 - mae: 0.0033 - val_loss: 4.3729e-05 - val_mae: 0.0054\n",
      "Epoch 205/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.2815e-05 - mae: 0.0038\n",
      "Epoch 205: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 2.1337e-05 - mae: 0.0037 - val_loss: 3.5104e-05 - val_mae: 0.0046\n",
      "Epoch 206/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8054e-05 - mae: 0.0034\n",
      "Epoch 206: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8817e-05 - mae: 0.0035 - val_loss: 4.0255e-05 - val_mae: 0.0051\n",
      "Epoch 207/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.4404e-05 - mae: 0.0040\n",
      "Epoch 207: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.1743e-05 - mae: 0.0037 - val_loss: 4.3148e-05 - val_mae: 0.0053\n",
      "Epoch 208/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7534e-05 - mae: 0.0033\n",
      "Epoch 208: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8482e-05 - mae: 0.0034 - val_loss: 3.6961e-05 - val_mae: 0.0047\n",
      "Epoch 209/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4221e-05 - mae: 0.0030\n",
      "Epoch 209: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.6305e-05 - mae: 0.0032 - val_loss: 5.6943e-05 - val_mae: 0.0060\n",
      "Epoch 210/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1524e-05 - mae: 0.0037\n",
      "Epoch 210: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.2446e-05 - mae: 0.0038 - val_loss: 3.6254e-05 - val_mae: 0.0048\n",
      "Epoch 211/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.1500e-05 - mae: 0.0037\n",
      "Epoch 211: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.1168e-05 - mae: 0.0037 - val_loss: 4.1012e-05 - val_mae: 0.0052\n",
      "Epoch 212/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8215e-05 - mae: 0.0034\n",
      "Epoch 212: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8356e-05 - mae: 0.0034 - val_loss: 3.5238e-05 - val_mae: 0.0047\n",
      "Epoch 213/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5622e-05 - mae: 0.0032\n",
      "Epoch 213: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.6652e-05 - mae: 0.0032 - val_loss: 3.5370e-05 - val_mae: 0.0047\n",
      "Epoch 214/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0281e-05 - mae: 0.0036\n",
      "Epoch 214: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.0188e-05 - mae: 0.0035 - val_loss: 3.5744e-05 - val_mae: 0.0047\n",
      "Epoch 215/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7575e-05 - mae: 0.0033\n",
      "Epoch 215: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.7819e-05 - mae: 0.0033 - val_loss: 3.6229e-05 - val_mae: 0.0048\n",
      "Epoch 216/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8130e-05 - mae: 0.0034\n",
      "Epoch 216: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8480e-05 - mae: 0.0034 - val_loss: 4.7997e-05 - val_mae: 0.0054\n",
      "Epoch 217/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9033e-05 - mae: 0.0034\n",
      "Epoch 217: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9044e-05 - mae: 0.0035 - val_loss: 3.5996e-05 - val_mae: 0.0047\n",
      "Epoch 218/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7727e-05 - mae: 0.0034\n",
      "Epoch 218: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7830e-05 - mae: 0.0034 - val_loss: 3.5328e-05 - val_mae: 0.0047\n",
      "Epoch 219/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9025e-05 - mae: 0.0035\n",
      "Epoch 219: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7325e-05 - mae: 0.0033 - val_loss: 3.5382e-05 - val_mae: 0.0046\n",
      "Epoch 220/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.1204e-05 - mae: 0.0037\n",
      "Epoch 220: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0638e-05 - mae: 0.0036 - val_loss: 3.6921e-05 - val_mae: 0.0047\n",
      "Epoch 221/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.6068e-05 - mae: 0.0041\n",
      "Epoch 221: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.2337e-05 - mae: 0.0038 - val_loss: 4.0444e-05 - val_mae: 0.0051\n",
      "Epoch 222/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7557e-05 - mae: 0.0033\n",
      "Epoch 222: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7557e-05 - mae: 0.0033 - val_loss: 3.7573e-05 - val_mae: 0.0049\n",
      "Epoch 223/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7897e-05 - mae: 0.0034\n",
      "Epoch 223: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7553e-05 - mae: 0.0034 - val_loss: 3.5451e-05 - val_mae: 0.0047\n",
      "Epoch 224/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5076e-05 - mae: 0.0031\n",
      "Epoch 224: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6233e-05 - mae: 0.0032 - val_loss: 3.5850e-05 - val_mae: 0.0047\n",
      "Epoch 225/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.6831e-05 - mae: 0.0033\n",
      "Epoch 225: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8107e-05 - mae: 0.0033 - val_loss: 3.5057e-05 - val_mae: 0.0047\n",
      "Epoch 226/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6962e-05 - mae: 0.0033\n",
      "Epoch 226: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7113e-05 - mae: 0.0033 - val_loss: 4.2496e-05 - val_mae: 0.0053\n",
      "Epoch 227/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8579e-05 - mae: 0.0033\n",
      "Epoch 227: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8955e-05 - mae: 0.0034 - val_loss: 3.5903e-05 - val_mae: 0.0047\n",
      "Epoch 228/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5531e-05 - mae: 0.0031\n",
      "Epoch 228: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6272e-05 - mae: 0.0032 - val_loss: 3.6585e-05 - val_mae: 0.0048\n",
      "Epoch 229/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6228e-05 - mae: 0.0032\n",
      "Epoch 229: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6807e-05 - mae: 0.0032 - val_loss: 3.6900e-05 - val_mae: 0.0048\n",
      "Epoch 230/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7328e-05 - mae: 0.0033\n",
      "Epoch 230: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8057e-05 - mae: 0.0034 - val_loss: 3.5346e-05 - val_mae: 0.0047\n",
      "Epoch 231/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5712e-05 - mae: 0.0031\n",
      "Epoch 231: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7551e-05 - mae: 0.0033 - val_loss: 4.5180e-05 - val_mae: 0.0053\n",
      "Epoch 232/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0247e-05 - mae: 0.0036\n",
      "Epoch 232: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0712e-05 - mae: 0.0036 - val_loss: 3.5428e-05 - val_mae: 0.0047\n",
      "Epoch 233/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0985e-05 - mae: 0.0037\n",
      "Epoch 233: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0229e-05 - mae: 0.0036 - val_loss: 4.5951e-05 - val_mae: 0.0053\n",
      "Epoch 234/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6530e-05 - mae: 0.0032\n",
      "Epoch 234: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7799e-05 - mae: 0.0033 - val_loss: 5.4064e-05 - val_mae: 0.0058\n",
      "Epoch 235/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9170e-05 - mae: 0.0035\n",
      "Epoch 235: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9019e-05 - mae: 0.0034 - val_loss: 3.5931e-05 - val_mae: 0.0047\n",
      "Epoch 236/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8177e-05 - mae: 0.0033\n",
      "Epoch 236: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.7955e-05 - mae: 0.0033 - val_loss: 3.5776e-05 - val_mae: 0.0047\n",
      "Epoch 237/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7664e-05 - mae: 0.0033\n",
      "Epoch 237: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7600e-05 - mae: 0.0033 - val_loss: 4.3542e-05 - val_mae: 0.0054\n",
      "Epoch 238/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.6089e-05 - mae: 0.0032\n",
      "Epoch 238: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.7949e-05 - mae: 0.0033 - val_loss: 3.8706e-05 - val_mae: 0.0050\n",
      "Epoch 239/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.8777e-05 - mae: 0.0034\n",
      "Epoch 239: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8080e-05 - mae: 0.0034 - val_loss: 3.5524e-05 - val_mae: 0.0047\n",
      "Epoch 240/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.6573e-05 - mae: 0.0032\n",
      "Epoch 240: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.7197e-05 - mae: 0.0033 - val_loss: 4.3723e-05 - val_mae: 0.0054\n",
      "Epoch 241/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.7965e-05 - mae: 0.0034\n",
      "Epoch 241: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.7197e-05 - mae: 0.0033 - val_loss: 3.6120e-05 - val_mae: 0.0048\n",
      "Epoch 242/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 1.6764e-05 - mae: 0.0033\n",
      "Epoch 242: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 1.8119e-05 - mae: 0.0034 - val_loss: 3.7244e-05 - val_mae: 0.0047\n",
      "Epoch 243/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 2.1220e-05 - mae: 0.0036\n",
      "Epoch 243: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 2.0358e-05 - mae: 0.0036 - val_loss: 3.5974e-05 - val_mae: 0.0047\n",
      "Epoch 244/1000\n",
      "610/800 [=====================>........] - ETA: 0s - loss: 1.5831e-05 - mae: 0.0032\n",
      "Epoch 244: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 1.6172e-05 - mae: 0.0032 - val_loss: 3.8868e-05 - val_mae: 0.0050\n",
      "Epoch 245/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8989e-05 - mae: 0.0035\n",
      "Epoch 245: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7540e-05 - mae: 0.0033 - val_loss: 3.5789e-05 - val_mae: 0.0047\n",
      "Epoch 246/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1380e-05 - mae: 0.0037\n",
      "Epoch 246: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9852e-05 - mae: 0.0036 - val_loss: 3.6016e-05 - val_mae: 0.0047\n",
      "Epoch 247/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.4184e-05 - mae: 0.0040\n",
      "Epoch 247: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.4274e-05 - mae: 0.0040 - val_loss: 3.6546e-05 - val_mae: 0.0047\n",
      "Epoch 248/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7307e-05 - mae: 0.0033\n",
      "Epoch 248: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.7270e-05 - mae: 0.0033 - val_loss: 3.5689e-05 - val_mae: 0.0047\n",
      "Epoch 249/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1402e-05 - mae: 0.0037\n",
      "Epoch 249: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.1402e-05 - mae: 0.0037 - val_loss: 4.1324e-05 - val_mae: 0.0050\n",
      "Epoch 250/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7798e-05 - mae: 0.0033\n",
      "Epoch 250: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9456e-05 - mae: 0.0035 - val_loss: 3.6068e-05 - val_mae: 0.0047\n",
      "Epoch 251/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.3434e-05 - mae: 0.0039\n",
      "Epoch 251: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.2537e-05 - mae: 0.0038 - val_loss: 3.6312e-05 - val_mae: 0.0047\n",
      "Epoch 252/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6612e-05 - mae: 0.0033\n",
      "Epoch 252: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6585e-05 - mae: 0.0032 - val_loss: 3.7219e-05 - val_mae: 0.0048\n",
      "Epoch 253/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.4487e-05 - mae: 0.0040\n",
      "Epoch 253: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.5097e-05 - mae: 0.0040 - val_loss: 4.3380e-05 - val_mae: 0.0051\n",
      "Epoch 254/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 2.3431e-05 - mae: 0.0039\n",
      "Epoch 254: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0741e-05 - mae: 0.0036 - val_loss: 3.7516e-05 - val_mae: 0.0048\n",
      "Epoch 255/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5579e-05 - mae: 0.0032\n",
      "Epoch 255: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6194e-05 - mae: 0.0032 - val_loss: 3.6966e-05 - val_mae: 0.0048\n",
      "Epoch 256/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8147e-05 - mae: 0.0034\n",
      "Epoch 256: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9442e-05 - mae: 0.0035 - val_loss: 3.5657e-05 - val_mae: 0.0047\n",
      "Epoch 257/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6146e-05 - mae: 0.0032\n",
      "Epoch 257: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7397e-05 - mae: 0.0033 - val_loss: 3.6173e-05 - val_mae: 0.0047\n",
      "Epoch 258/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6243e-05 - mae: 0.0032\n",
      "Epoch 258: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7662e-05 - mae: 0.0033 - val_loss: 3.6525e-05 - val_mae: 0.0048\n",
      "Epoch 259/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5332e-05 - mae: 0.0031\n",
      "Epoch 259: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.6635e-05 - mae: 0.0033 - val_loss: 3.8544e-05 - val_mae: 0.0048\n",
      "Epoch 260/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6629e-05 - mae: 0.0033\n",
      "Epoch 260: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6469e-05 - mae: 0.0032 - val_loss: 3.5916e-05 - val_mae: 0.0047\n",
      "Epoch 261/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5759e-05 - mae: 0.0031\n",
      "Epoch 261: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5640e-05 - mae: 0.0031 - val_loss: 3.5958e-05 - val_mae: 0.0047\n",
      "Epoch 262/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0477e-05 - mae: 0.0036\n",
      "Epoch 262: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7834e-05 - mae: 0.0033 - val_loss: 3.7027e-05 - val_mae: 0.0048\n",
      "Epoch 263/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7407e-05 - mae: 0.0034\n",
      "Epoch 263: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6321e-05 - mae: 0.0032 - val_loss: 3.6166e-05 - val_mae: 0.0047\n",
      "Epoch 264/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7107e-05 - mae: 0.0033\n",
      "Epoch 264: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6261e-05 - mae: 0.0032 - val_loss: 5.3299e-05 - val_mae: 0.0060\n",
      "Epoch 265/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.2894e-05 - mae: 0.0038\n",
      "Epoch 265: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0280e-05 - mae: 0.0036 - val_loss: 3.7160e-05 - val_mae: 0.0049\n",
      "Epoch 266/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7300e-05 - mae: 0.0032\n",
      "Epoch 266: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7996e-05 - mae: 0.0033 - val_loss: 4.1730e-05 - val_mae: 0.0052\n",
      "Epoch 267/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7012e-05 - mae: 0.0033\n",
      "Epoch 267: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7199e-05 - mae: 0.0033 - val_loss: 3.8353e-05 - val_mae: 0.0049\n",
      "Epoch 268/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8600e-05 - mae: 0.0034\n",
      "Epoch 268: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8257e-05 - mae: 0.0034 - val_loss: 3.6071e-05 - val_mae: 0.0047\n",
      "Epoch 269/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5207e-05 - mae: 0.0031\n",
      "Epoch 269: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6366e-05 - mae: 0.0032 - val_loss: 4.1732e-05 - val_mae: 0.0052\n",
      "Epoch 270/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7165e-05 - mae: 0.0033\n",
      "Epoch 270: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.6969e-05 - mae: 0.0033 - val_loss: 3.7146e-05 - val_mae: 0.0048\n",
      "Epoch 271/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4125e-05 - mae: 0.0029\n",
      "Epoch 271: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7649e-05 - mae: 0.0033 - val_loss: 4.6378e-05 - val_mae: 0.0056\n",
      "Epoch 272/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8106e-05 - mae: 0.0033\n",
      "Epoch 272: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7548e-05 - mae: 0.0033 - val_loss: 4.0484e-05 - val_mae: 0.0051\n",
      "Epoch 273/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9663e-05 - mae: 0.0036\n",
      "Epoch 273: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9393e-05 - mae: 0.0035 - val_loss: 3.7108e-05 - val_mae: 0.0049\n",
      "Epoch 274/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6174e-05 - mae: 0.0032\n",
      "Epoch 274: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.6253e-05 - mae: 0.0032 - val_loss: 3.9968e-05 - val_mae: 0.0051\n",
      "Epoch 275/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8462e-05 - mae: 0.0034\n",
      "Epoch 275: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7793e-05 - mae: 0.0034 - val_loss: 4.1989e-05 - val_mae: 0.0053\n",
      "Epoch 276/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9153e-05 - mae: 0.0034\n",
      "Epoch 276: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9360e-05 - mae: 0.0035 - val_loss: 3.7376e-05 - val_mae: 0.0048\n",
      "Epoch 277/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9846e-05 - mae: 0.0036\n",
      "Epoch 277: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8264e-05 - mae: 0.0034 - val_loss: 3.5985e-05 - val_mae: 0.0047\n",
      "Epoch 278/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6154e-05 - mae: 0.0031\n",
      "Epoch 278: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6692e-05 - mae: 0.0032 - val_loss: 3.6388e-05 - val_mae: 0.0048\n",
      "Epoch 279/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7001e-05 - mae: 0.0032\n",
      "Epoch 279: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6292e-05 - mae: 0.0032 - val_loss: 3.6250e-05 - val_mae: 0.0047\n",
      "Epoch 280/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6049e-05 - mae: 0.0032\n",
      "Epoch 280: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8750e-05 - mae: 0.0034 - val_loss: 3.8433e-05 - val_mae: 0.0048\n",
      "Epoch 281/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6906e-05 - mae: 0.0032\n",
      "Epoch 281: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.6846e-05 - mae: 0.0032 - val_loss: 4.1617e-05 - val_mae: 0.0050\n",
      "Epoch 282/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8645e-05 - mae: 0.0034\n",
      "Epoch 282: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.0051e-05 - mae: 0.0036 - val_loss: 3.6241e-05 - val_mae: 0.0047\n",
      "Epoch 283/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/800 [=================>............] - ETA: 0s - loss: 1.5165e-05 - mae: 0.0031\n",
      "Epoch 283: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.6112e-05 - mae: 0.0032 - val_loss: 3.6303e-05 - val_mae: 0.0048\n",
      "Epoch 284/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4900e-05 - mae: 0.0031\n",
      "Epoch 284: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7711e-05 - mae: 0.0033 - val_loss: 3.6499e-05 - val_mae: 0.0048\n",
      "Epoch 285/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7088e-05 - mae: 0.0033\n",
      "Epoch 285: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 2.0388e-05 - mae: 0.0036 - val_loss: 5.5297e-05 - val_mae: 0.0059\n",
      "Epoch 286/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.8050e-05 - mae: 0.0034\n",
      "Epoch 286: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.8380e-05 - mae: 0.0035 - val_loss: 3.6711e-05 - val_mae: 0.0047\n",
      "Epoch 287/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8096e-05 - mae: 0.0034\n",
      "Epoch 287: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 1.8076e-05 - mae: 0.0034 - val_loss: 3.7705e-05 - val_mae: 0.0048\n",
      "Epoch 288/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.5711e-05 - mae: 0.0031\n",
      "Epoch 288: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.6549e-05 - mae: 0.0032 - val_loss: 4.3639e-05 - val_mae: 0.0053\n",
      "Epoch 289/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7911e-05 - mae: 0.0034\n",
      "Epoch 289: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8294e-05 - mae: 0.0034 - val_loss: 3.8230e-05 - val_mae: 0.0048\n",
      "Epoch 290/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.5578e-05 - mae: 0.0031\n",
      "Epoch 290: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.6681e-05 - mae: 0.0032 - val_loss: 4.0463e-05 - val_mae: 0.0049\n",
      "Epoch 291/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9915e-05 - mae: 0.0035\n",
      "Epoch 291: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.9785e-05 - mae: 0.0035 - val_loss: 3.7461e-05 - val_mae: 0.0048\n",
      "Epoch 292/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.5141e-05 - mae: 0.0031\n",
      "Epoch 292: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.5851e-05 - mae: 0.0031 - val_loss: 3.7495e-05 - val_mae: 0.0048\n",
      "Epoch 293/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.5214e-05 - mae: 0.0031\n",
      "Epoch 293: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.6945e-05 - mae: 0.0032 - val_loss: 4.0053e-05 - val_mae: 0.0051\n",
      "Epoch 294/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.5560e-05 - mae: 0.0031\n",
      "Epoch 294: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.6166e-05 - mae: 0.0032 - val_loss: 3.6257e-05 - val_mae: 0.0047\n",
      "Epoch 295/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.6558e-05 - mae: 0.0032\n",
      "Epoch 295: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.6836e-05 - mae: 0.0033 - val_loss: 3.8704e-05 - val_mae: 0.0050\n",
      "Epoch 296/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.7445e-05 - mae: 0.0033\n",
      "Epoch 296: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.8555e-05 - mae: 0.0034 - val_loss: 3.6665e-05 - val_mae: 0.0048\n",
      "Epoch 297/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.7900e-05 - mae: 0.0033\n",
      "Epoch 297: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.7827e-05 - mae: 0.0034 - val_loss: 3.6530e-05 - val_mae: 0.0047\n",
      "Epoch 298/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.6491e-05 - mae: 0.0032\n",
      "Epoch 298: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.7428e-05 - mae: 0.0033 - val_loss: 4.4504e-05 - val_mae: 0.0052\n",
      "Epoch 299/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.6896e-05 - mae: 0.0032\n",
      "Epoch 299: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.7063e-05 - mae: 0.0032 - val_loss: 3.7806e-05 - val_mae: 0.0049\n",
      "Epoch 300/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4397e-05 - mae: 0.0030\n",
      "Epoch 300: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6118e-05 - mae: 0.0032 - val_loss: 3.6644e-05 - val_mae: 0.0048\n",
      "Epoch 301/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7448e-05 - mae: 0.0033\n",
      "Epoch 301: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8258e-05 - mae: 0.0034 - val_loss: 5.7295e-05 - val_mae: 0.0062\n",
      "Epoch 302/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9685e-05 - mae: 0.0035\n",
      "Epoch 302: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8926e-05 - mae: 0.0034 - val_loss: 3.7731e-05 - val_mae: 0.0048\n",
      "Epoch 303/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7489e-05 - mae: 0.0033\n",
      "Epoch 303: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9215e-05 - mae: 0.0035 - val_loss: 3.6937e-05 - val_mae: 0.0048\n",
      "Epoch 304/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7363e-05 - mae: 0.0033\n",
      "Epoch 304: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7294e-05 - mae: 0.0033 - val_loss: 3.8224e-05 - val_mae: 0.0050\n",
      "Epoch 305/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7615e-05 - mae: 0.0033\n",
      "Epoch 305: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7532e-05 - mae: 0.0033 - val_loss: 3.6165e-05 - val_mae: 0.0048\n",
      "Epoch 306/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7606e-05 - mae: 0.0033\n",
      "Epoch 306: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7606e-05 - mae: 0.0033 - val_loss: 4.1449e-05 - val_mae: 0.0052\n",
      "Epoch 307/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8801e-05 - mae: 0.0035\n",
      "Epoch 307: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8314e-05 - mae: 0.0034 - val_loss: 3.7533e-05 - val_mae: 0.0049\n",
      "Epoch 308/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6528e-05 - mae: 0.0032\n",
      "Epoch 308: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7976e-05 - mae: 0.0033 - val_loss: 4.1127e-05 - val_mae: 0.0052\n",
      "Epoch 309/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7373e-05 - mae: 0.0033\n",
      "Epoch 309: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7077e-05 - mae: 0.0033 - val_loss: 4.2024e-05 - val_mae: 0.0050\n",
      "Epoch 310/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2908e-05 - mae: 0.0038\n",
      "Epoch 310: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.3338e-05 - mae: 0.0039 - val_loss: 4.0364e-05 - val_mae: 0.0049\n",
      "Epoch 311/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7133e-05 - mae: 0.0033\n",
      "Epoch 311: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0238e-05 - mae: 0.0036 - val_loss: 4.0602e-05 - val_mae: 0.0050\n",
      "Epoch 312/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/800 [============================>.] - ETA: 0s - loss: 1.7575e-05 - mae: 0.0033\n",
      "Epoch 312: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7701e-05 - mae: 0.0033 - val_loss: 3.7314e-05 - val_mae: 0.0049\n",
      "Epoch 313/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4965e-05 - mae: 0.0030\n",
      "Epoch 313: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5459e-05 - mae: 0.0031 - val_loss: 3.7382e-05 - val_mae: 0.0048\n",
      "Epoch 314/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6778e-05 - mae: 0.0032\n",
      "Epoch 314: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7001e-05 - mae: 0.0033 - val_loss: 4.2819e-05 - val_mae: 0.0053\n",
      "Epoch 315/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7849e-05 - mae: 0.0033\n",
      "Epoch 315: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7593e-05 - mae: 0.0033 - val_loss: 3.6675e-05 - val_mae: 0.0048\n",
      "Epoch 316/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4896e-05 - mae: 0.0031\n",
      "Epoch 316: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.6048e-05 - mae: 0.0032 - val_loss: 3.7313e-05 - val_mae: 0.0048\n",
      "Epoch 317/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4538e-05 - mae: 0.0030\n",
      "Epoch 317: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7712e-05 - mae: 0.0033 - val_loss: 4.9548e-05 - val_mae: 0.0058\n",
      "Epoch 318/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6524e-05 - mae: 0.0032\n",
      "Epoch 318: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.6733e-05 - mae: 0.0032 - val_loss: 4.0202e-05 - val_mae: 0.0049\n",
      "Epoch 319/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8960e-05 - mae: 0.0035\n",
      "Epoch 319: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0386e-05 - mae: 0.0037 - val_loss: 4.1213e-05 - val_mae: 0.0052\n",
      "Epoch 320/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6811e-05 - mae: 0.0033\n",
      "Epoch 320: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.6437e-05 - mae: 0.0032 - val_loss: 3.6761e-05 - val_mae: 0.0048\n",
      "Epoch 321/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5496e-05 - mae: 0.0031\n",
      "Epoch 321: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.5447e-05 - mae: 0.0031 - val_loss: 3.8584e-05 - val_mae: 0.0048\n",
      "Epoch 322/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4393e-05 - mae: 0.0030\n",
      "Epoch 322: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.5881e-05 - mae: 0.0032 - val_loss: 3.6616e-05 - val_mae: 0.0048\n",
      "Epoch 323/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4500e-05 - mae: 0.0030\n",
      "Epoch 323: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7589e-05 - mae: 0.0033 - val_loss: 3.6819e-05 - val_mae: 0.0048\n",
      "Epoch 324/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6173e-05 - mae: 0.0032\n",
      "Epoch 324: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6294e-05 - mae: 0.0032 - val_loss: 4.1316e-05 - val_mae: 0.0052\n",
      "Epoch 325/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6880e-05 - mae: 0.0032\n",
      "Epoch 325: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8067e-05 - mae: 0.0033 - val_loss: 3.9769e-05 - val_mae: 0.0049\n",
      "Epoch 326/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6943e-05 - mae: 0.0032\n",
      "Epoch 326: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6770e-05 - mae: 0.0032 - val_loss: 3.7167e-05 - val_mae: 0.0048\n",
      "Epoch 327/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.6916e-05 - mae: 0.0032\n",
      "Epoch 327: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.6916e-05 - mae: 0.0032 - val_loss: 3.9139e-05 - val_mae: 0.0049\n",
      "Epoch 328/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7511e-05 - mae: 0.0033\n",
      "Epoch 328: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7507e-05 - mae: 0.0033 - val_loss: 3.8255e-05 - val_mae: 0.0049\n",
      "Epoch 329/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6404e-05 - mae: 0.0032\n",
      "Epoch 329: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.6624e-05 - mae: 0.0032 - val_loss: 3.6485e-05 - val_mae: 0.0048\n",
      "Epoch 330/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4035e-05 - mae: 0.0030\n",
      "Epoch 330: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.5780e-05 - mae: 0.0032 - val_loss: 4.7964e-05 - val_mae: 0.0057\n",
      "Epoch 331/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8816e-05 - mae: 0.0034\n",
      "Epoch 331: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8316e-05 - mae: 0.0034 - val_loss: 4.4355e-05 - val_mae: 0.0052\n",
      "Epoch 332/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7396e-05 - mae: 0.0033\n",
      "Epoch 332: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6581e-05 - mae: 0.0032 - val_loss: 3.8188e-05 - val_mae: 0.0049\n",
      "Epoch 333/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6377e-05 - mae: 0.0032\n",
      "Epoch 333: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8556e-05 - mae: 0.0035 - val_loss: 5.1526e-05 - val_mae: 0.0056\n",
      "Epoch 334/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9840e-05 - mae: 0.0035\n",
      "Epoch 334: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.9101e-05 - mae: 0.0035 - val_loss: 3.6392e-05 - val_mae: 0.0047\n",
      "Epoch 335/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7126e-05 - mae: 0.0033\n",
      "Epoch 335: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8994e-05 - mae: 0.0035 - val_loss: 5.2558e-05 - val_mae: 0.0059\n",
      "Epoch 336/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8248e-05 - mae: 0.0034\n",
      "Epoch 336: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7557e-05 - mae: 0.0034 - val_loss: 3.7821e-05 - val_mae: 0.0049\n",
      "Epoch 337/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5359e-05 - mae: 0.0031\n",
      "Epoch 337: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.6806e-05 - mae: 0.0032 - val_loss: 4.1048e-05 - val_mae: 0.0050\n",
      "Epoch 338/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4670e-05 - mae: 0.0030\n",
      "Epoch 338: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6980e-05 - mae: 0.0033 - val_loss: 4.3224e-05 - val_mae: 0.0051\n",
      "Epoch 339/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6791e-05 - mae: 0.0033\n",
      "Epoch 339: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.0030e-05 - mae: 0.0035 - val_loss: 5.3805e-05 - val_mae: 0.0060\n",
      "Epoch 340/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7569e-05 - mae: 0.0032\n",
      "Epoch 340: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.6917e-05 - mae: 0.0032 - val_loss: 3.7127e-05 - val_mae: 0.0048\n",
      "Epoch 341/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6436e-05 - mae: 0.0032\n",
      "Epoch 341: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8362e-05 - mae: 0.0034 - val_loss: 3.7459e-05 - val_mae: 0.0048\n",
      "Epoch 342/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.8688e-05 - mae: 0.0043\n",
      "Epoch 342: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.3947e-05 - mae: 0.0039 - val_loss: 4.0309e-05 - val_mae: 0.0049\n",
      "Epoch 343/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6853e-05 - mae: 0.0033\n",
      "Epoch 343: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7280e-05 - mae: 0.0033 - val_loss: 3.6817e-05 - val_mae: 0.0048\n",
      "Epoch 344/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7190e-05 - mae: 0.0033\n",
      "Epoch 344: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7059e-05 - mae: 0.0033 - val_loss: 3.9161e-05 - val_mae: 0.0049\n",
      "Epoch 345/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6842e-05 - mae: 0.0032\n",
      "Epoch 345: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7304e-05 - mae: 0.0033 - val_loss: 3.8162e-05 - val_mae: 0.0049\n",
      "Epoch 346/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7264e-05 - mae: 0.0033\n",
      "Epoch 346: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7677e-05 - mae: 0.0033 - val_loss: 3.7152e-05 - val_mae: 0.0048\n",
      "Epoch 347/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6575e-05 - mae: 0.0032\n",
      "Epoch 347: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6372e-05 - mae: 0.0032 - val_loss: 3.6985e-05 - val_mae: 0.0048\n",
      "Epoch 348/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7085e-05 - mae: 0.0033\n",
      "Epoch 348: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9313e-05 - mae: 0.0035 - val_loss: 4.0337e-05 - val_mae: 0.0049\n",
      "Epoch 349/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8422e-05 - mae: 0.0034\n",
      "Epoch 349: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7026e-05 - mae: 0.0032 - val_loss: 3.7165e-05 - val_mae: 0.0048\n",
      "Epoch 350/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7310e-05 - mae: 0.0033\n",
      "Epoch 350: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.6806e-05 - mae: 0.0033 - val_loss: 3.7872e-05 - val_mae: 0.0048\n",
      "Epoch 351/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5689e-05 - mae: 0.0030\n",
      "Epoch 351: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.6566e-05 - mae: 0.0032 - val_loss: 3.7806e-05 - val_mae: 0.0048\n",
      "Epoch 352/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7240e-05 - mae: 0.0033\n",
      "Epoch 352: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7101e-05 - mae: 0.0033 - val_loss: 3.6894e-05 - val_mae: 0.0048\n",
      "Epoch 353/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.8621e-05 - mae: 0.0034\n",
      "Epoch 353: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7239e-05 - mae: 0.0033 - val_loss: 4.7827e-05 - val_mae: 0.0054\n",
      "Epoch 354/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.1637e-05 - mae: 0.0037\n",
      "Epoch 354: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9369e-05 - mae: 0.0035 - val_loss: 4.6256e-05 - val_mae: 0.0055\n",
      "Epoch 355/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1006e-05 - mae: 0.0036\n",
      "Epoch 355: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1821e-05 - mae: 0.0037 - val_loss: 3.6524e-05 - val_mae: 0.0048\n",
      "Epoch 356/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5301e-05 - mae: 0.0031\n",
      "Epoch 356: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5495e-05 - mae: 0.0031 - val_loss: 3.8395e-05 - val_mae: 0.0048\n",
      "Epoch 357/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6271e-05 - mae: 0.0032\n",
      "Epoch 357: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7029e-05 - mae: 0.0033 - val_loss: 3.6495e-05 - val_mae: 0.0047\n",
      "Epoch 358/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5972e-05 - mae: 0.0031\n",
      "Epoch 358: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6886e-05 - mae: 0.0032 - val_loss: 3.6728e-05 - val_mae: 0.0048\n",
      "Epoch 359/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7737e-05 - mae: 0.0033\n",
      "Epoch 359: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7782e-05 - mae: 0.0033 - val_loss: 3.6563e-05 - val_mae: 0.0048\n",
      "Epoch 360/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1601e-05 - mae: 0.0036\n",
      "Epoch 360: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9945e-05 - mae: 0.0035 - val_loss: 3.6442e-05 - val_mae: 0.0047\n",
      "Epoch 361/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4460e-05 - mae: 0.0030\n",
      "Epoch 361: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5633e-05 - mae: 0.0031 - val_loss: 4.8590e-05 - val_mae: 0.0055\n",
      "Epoch 362/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9382e-05 - mae: 0.0035\n",
      "Epoch 362: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9382e-05 - mae: 0.0035 - val_loss: 4.0228e-05 - val_mae: 0.0049\n",
      "Epoch 363/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9965e-05 - mae: 0.0036\n",
      "Epoch 363: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8451e-05 - mae: 0.0034 - val_loss: 3.6908e-05 - val_mae: 0.0048\n",
      "Epoch 364/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9318e-05 - mae: 0.0035\n",
      "Epoch 364: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7593e-05 - mae: 0.0033 - val_loss: 4.3953e-05 - val_mae: 0.0054\n",
      "Epoch 365/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4318e-05 - mae: 0.0030\n",
      "Epoch 365: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5544e-05 - mae: 0.0031 - val_loss: 3.8068e-05 - val_mae: 0.0049\n",
      "Epoch 366/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4556e-05 - mae: 0.0030\n",
      "Epoch 366: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7436e-05 - mae: 0.0033 - val_loss: 5.0687e-05 - val_mae: 0.0058\n",
      "Epoch 367/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8958e-05 - mae: 0.0035\n",
      "Epoch 367: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7634e-05 - mae: 0.0033 - val_loss: 4.0611e-05 - val_mae: 0.0049\n",
      "Epoch 368/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5799e-05 - mae: 0.0032\n",
      "Epoch 368: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.6725e-05 - mae: 0.0033 - val_loss: 3.7970e-05 - val_mae: 0.0049\n",
      "Epoch 369/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6985e-05 - mae: 0.0032\n",
      "Epoch 369: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8239e-05 - mae: 0.0034 - val_loss: 3.6637e-05 - val_mae: 0.0047\n",
      "Epoch 370/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1369e-05 - mae: 0.0037\n",
      "Epoch 370: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9854e-05 - mae: 0.0036 - val_loss: 3.7641e-05 - val_mae: 0.0048\n",
      "Epoch 371/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5324e-05 - mae: 0.0031\n",
      "Epoch 371: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7050e-05 - mae: 0.0033 - val_loss: 4.4350e-05 - val_mae: 0.0054\n",
      "Epoch 372/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0968e-05 - mae: 0.0036\n",
      "Epoch 372: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0994e-05 - mae: 0.0036 - val_loss: 5.9168e-05 - val_mae: 0.0063\n",
      "Epoch 373/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5537e-05 - mae: 0.0031\n",
      "Epoch 373: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5919e-05 - mae: 0.0032 - val_loss: 3.6532e-05 - val_mae: 0.0047\n",
      "Epoch 374/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4245e-05 - mae: 0.0030\n",
      "Epoch 374: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.6407e-05 - mae: 0.0032 - val_loss: 3.9497e-05 - val_mae: 0.0050\n",
      "Epoch 375/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6550e-05 - mae: 0.0031\n",
      "Epoch 375: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7854e-05 - mae: 0.0033 - val_loss: 3.9564e-05 - val_mae: 0.0050\n",
      "Epoch 376/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8455e-05 - mae: 0.0034\n",
      "Epoch 376: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9718e-05 - mae: 0.0035 - val_loss: 3.6116e-05 - val_mae: 0.0047\n",
      "Epoch 377/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6022e-05 - mae: 0.0031\n",
      "Epoch 377: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5954e-05 - mae: 0.0032 - val_loss: 3.6922e-05 - val_mae: 0.0047\n",
      "Epoch 378/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5683e-05 - mae: 0.0031\n",
      "Epoch 378: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5484e-05 - mae: 0.0031 - val_loss: 4.4472e-05 - val_mae: 0.0054\n",
      "Epoch 379/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7058e-05 - mae: 0.0033\n",
      "Epoch 379: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7216e-05 - mae: 0.0032 - val_loss: 3.9601e-05 - val_mae: 0.0050\n",
      "Epoch 380/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.5625e-05 - mae: 0.0031\n",
      "Epoch 380: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.5533e-05 - mae: 0.0031 - val_loss: 3.7745e-05 - val_mae: 0.0048\n",
      "Epoch 381/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7385e-05 - mae: 0.0033\n",
      "Epoch 381: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6917e-05 - mae: 0.0032 - val_loss: 4.2712e-05 - val_mae: 0.0053\n",
      "Epoch 382/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7106e-05 - mae: 0.0033\n",
      "Epoch 382: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7104e-05 - mae: 0.0033 - val_loss: 4.2791e-05 - val_mae: 0.0051\n",
      "Epoch 383/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7793e-05 - mae: 0.0034\n",
      "Epoch 383: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7873e-05 - mae: 0.0034 - val_loss: 4.2036e-05 - val_mae: 0.0050\n",
      "Epoch 384/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6138e-05 - mae: 0.0032\n",
      "Epoch 384: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7169e-05 - mae: 0.0033 - val_loss: 3.6975e-05 - val_mae: 0.0048\n",
      "Epoch 385/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7181e-05 - mae: 0.0033\n",
      "Epoch 385: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.6269e-05 - mae: 0.0032 - val_loss: 3.7892e-05 - val_mae: 0.0048\n",
      "Epoch 386/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9854e-05 - mae: 0.0036\n",
      "Epoch 386: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7739e-05 - mae: 0.0033 - val_loss: 4.3306e-05 - val_mae: 0.0053\n",
      "Epoch 387/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6198e-05 - mae: 0.0032\n",
      "Epoch 387: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6729e-05 - mae: 0.0032 - val_loss: 3.5985e-05 - val_mae: 0.0047\n",
      "Epoch 388/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8464e-05 - mae: 0.0034\n",
      "Epoch 388: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.9777e-05 - mae: 0.0035 - val_loss: 3.7040e-05 - val_mae: 0.0049\n",
      "Epoch 389/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4628e-05 - mae: 0.0030\n",
      "Epoch 389: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5051e-05 - mae: 0.0031 - val_loss: 3.6599e-05 - val_mae: 0.0048\n",
      "Epoch 390/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4707e-05 - mae: 0.0030\n",
      "Epoch 390: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6470e-05 - mae: 0.0032 - val_loss: 4.3769e-05 - val_mae: 0.0054\n",
      "Epoch 391/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7261e-05 - mae: 0.0033\n",
      "Epoch 391: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7234e-05 - mae: 0.0033 - val_loss: 3.7525e-05 - val_mae: 0.0049\n",
      "Epoch 392/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6802e-05 - mae: 0.0033\n",
      "Epoch 392: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8213e-05 - mae: 0.0034 - val_loss: 4.5432e-05 - val_mae: 0.0055\n",
      "Epoch 393/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8898e-05 - mae: 0.0034\n",
      "Epoch 393: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.2442e-05 - mae: 0.0038 - val_loss: 3.7965e-05 - val_mae: 0.0049\n",
      "Epoch 394/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4832e-05 - mae: 0.0031\n",
      "Epoch 394: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8871e-05 - mae: 0.0035 - val_loss: 3.8957e-05 - val_mae: 0.0050\n",
      "Epoch 395/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7665e-05 - mae: 0.0034\n",
      "Epoch 395: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7690e-05 - mae: 0.0034 - val_loss: 4.0641e-05 - val_mae: 0.0049\n",
      "Epoch 396/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7905e-05 - mae: 0.0033\n",
      "Epoch 396: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7575e-05 - mae: 0.0033 - val_loss: 4.2501e-05 - val_mae: 0.0050\n",
      "Epoch 397/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8650e-05 - mae: 0.0035\n",
      "Epoch 397: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7029e-05 - mae: 0.0033 - val_loss: 3.8218e-05 - val_mae: 0.0048\n",
      "Epoch 398/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6714e-05 - mae: 0.0032\n",
      "Epoch 398: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6802e-05 - mae: 0.0032 - val_loss: 3.7798e-05 - val_mae: 0.0048\n",
      "Epoch 399/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4076e-05 - mae: 0.0030\n",
      "Epoch 399: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.4938e-05 - mae: 0.0030 - val_loss: 3.7292e-05 - val_mae: 0.0048\n",
      "Epoch 400/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5835e-05 - mae: 0.0032\n",
      "Epoch 400: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.6117e-05 - mae: 0.0032 - val_loss: 3.7934e-05 - val_mae: 0.0049\n",
      "Epoch 401/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4772e-05 - mae: 0.0030\n",
      "Epoch 401: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5003e-05 - mae: 0.0030 - val_loss: 3.7492e-05 - val_mae: 0.0048\n",
      "Epoch 402/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4565e-05 - mae: 0.0031\n",
      "Epoch 402: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6499e-05 - mae: 0.0032 - val_loss: 3.7828e-05 - val_mae: 0.0048\n",
      "Epoch 403/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5404e-05 - mae: 0.0031\n",
      "Epoch 403: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.6118e-05 - mae: 0.0032 - val_loss: 4.1310e-05 - val_mae: 0.0050\n",
      "Epoch 404/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8457e-05 - mae: 0.0034\n",
      "Epoch 404: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7038e-05 - mae: 0.0032 - val_loss: 3.9468e-05 - val_mae: 0.0049\n",
      "Epoch 405/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6556e-05 - mae: 0.0032\n",
      "Epoch 405: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7664e-05 - mae: 0.0034 - val_loss: 4.2413e-05 - val_mae: 0.0050\n",
      "Epoch 406/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7878e-05 - mae: 0.0033\n",
      "Epoch 406: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7027e-05 - mae: 0.0032 - val_loss: 3.7386e-05 - val_mae: 0.0049\n",
      "Epoch 407/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4998e-05 - mae: 0.0031\n",
      "Epoch 407: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5710e-05 - mae: 0.0031 - val_loss: 3.7831e-05 - val_mae: 0.0049\n",
      "Epoch 408/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5646e-05 - mae: 0.0031\n",
      "Epoch 408: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6053e-05 - mae: 0.0032 - val_loss: 3.9687e-05 - val_mae: 0.0051\n",
      "Epoch 409/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0505e-05 - mae: 0.0036\n",
      "Epoch 409: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8585e-05 - mae: 0.0034 - val_loss: 4.2115e-05 - val_mae: 0.0050\n",
      "Epoch 410/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5242e-05 - mae: 0.0032\n",
      "Epoch 410: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.6093e-05 - mae: 0.0032 - val_loss: 3.7861e-05 - val_mae: 0.0049\n",
      "Epoch 411/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6753e-05 - mae: 0.0033\n",
      "Epoch 411: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5599e-05 - mae: 0.0031 - val_loss: 3.7679e-05 - val_mae: 0.0049\n",
      "Epoch 412/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5678e-05 - mae: 0.0031\n",
      "Epoch 412: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.5636e-05 - mae: 0.0031 - val_loss: 3.7781e-05 - val_mae: 0.0049\n",
      "Epoch 413/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5787e-05 - mae: 0.0031\n",
      "Epoch 413: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7337e-05 - mae: 0.0033 - val_loss: 3.7668e-05 - val_mae: 0.0048\n",
      "Epoch 414/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9374e-05 - mae: 0.0035\n",
      "Epoch 414: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7894e-05 - mae: 0.0033 - val_loss: 3.7923e-05 - val_mae: 0.0049\n",
      "Epoch 415/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4126e-05 - mae: 0.0030\n",
      "Epoch 415: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.6087e-05 - mae: 0.0032 - val_loss: 4.0533e-05 - val_mae: 0.0049\n",
      "Epoch 416/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5163e-05 - mae: 0.0031\n",
      "Epoch 416: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.6720e-05 - mae: 0.0032 - val_loss: 3.9151e-05 - val_mae: 0.0050\n",
      "Epoch 417/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9310e-05 - mae: 0.0035\n",
      "Epoch 417: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9320e-05 - mae: 0.0035 - val_loss: 3.7869e-05 - val_mae: 0.0049\n",
      "Epoch 418/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4598e-05 - mae: 0.0030\n",
      "Epoch 418: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6212e-05 - mae: 0.0032 - val_loss: 3.9616e-05 - val_mae: 0.0049\n",
      "Epoch 419/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3791e-05 - mae: 0.0029\n",
      "Epoch 419: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6316e-05 - mae: 0.0032 - val_loss: 3.8671e-05 - val_mae: 0.0050\n",
      "Epoch 420/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6661e-05 - mae: 0.0032\n",
      "Epoch 420: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6739e-05 - mae: 0.0032 - val_loss: 3.7400e-05 - val_mae: 0.0048\n",
      "Epoch 421/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.4843e-05 - mae: 0.0030\n",
      "Epoch 421: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.5063e-05 - mae: 0.0030 - val_loss: 3.8297e-05 - val_mae: 0.0048\n",
      "Epoch 422/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.5812e-05 - mae: 0.0032\n",
      "Epoch 422: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.5619e-05 - mae: 0.0032 - val_loss: 3.7755e-05 - val_mae: 0.0049\n",
      "Epoch 423/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4877e-05 - mae: 0.0030\n",
      "Epoch 423: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.6150e-05 - mae: 0.0032 - val_loss: 4.1641e-05 - val_mae: 0.0052\n",
      "Epoch 424/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6941e-05 - mae: 0.0032\n",
      "Epoch 424: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7664e-05 - mae: 0.0033 - val_loss: 3.9873e-05 - val_mae: 0.0049\n",
      "Epoch 425/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5427e-05 - mae: 0.0031\n",
      "Epoch 425: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.5609e-05 - mae: 0.0031 - val_loss: 3.8328e-05 - val_mae: 0.0049\n",
      "Epoch 426/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.6197e-05 - mae: 0.0032\n",
      "Epoch 426: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.6247e-05 - mae: 0.0032 - val_loss: 4.0238e-05 - val_mae: 0.0051\n",
      "Epoch 427/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8629e-05 - mae: 0.0034\n",
      "Epoch 427: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.8527e-05 - mae: 0.0034 - val_loss: 3.7479e-05 - val_mae: 0.0048\n",
      "Epoch 428/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0621e-05 - mae: 0.0037\n",
      "Epoch 428: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9138e-05 - mae: 0.0035 - val_loss: 3.8220e-05 - val_mae: 0.0049\n",
      "Epoch 429/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.5260e-05 - mae: 0.0031\n",
      "Epoch 429: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.5639e-05 - mae: 0.0031 - val_loss: 3.9584e-05 - val_mae: 0.0050\n",
      "Epoch 430/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.6556e-05 - mae: 0.0032\n",
      "Epoch 430: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.7166e-05 - mae: 0.0033 - val_loss: 4.1747e-05 - val_mae: 0.0052\n",
      "Epoch 431/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6738e-05 - mae: 0.0032\n",
      "Epoch 431: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.6636e-05 - mae: 0.0032 - val_loss: 3.9439e-05 - val_mae: 0.0050\n",
      "Epoch 432/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.4787e-05 - mae: 0.0030\n",
      "Epoch 432: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.5111e-05 - mae: 0.0031 - val_loss: 3.8300e-05 - val_mae: 0.0049\n",
      "Epoch 433/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.6749e-05 - mae: 0.0032\n",
      "Epoch 433: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.6520e-05 - mae: 0.0032 - val_loss: 3.8023e-05 - val_mae: 0.0049\n",
      "Epoch 434/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5871e-05 - mae: 0.0032\n",
      "Epoch 434: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.6529e-05 - mae: 0.0032 - val_loss: 5.4404e-05 - val_mae: 0.0060\n",
      "Epoch 435/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8530e-05 - mae: 0.0033\n",
      "Epoch 435: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7272e-05 - mae: 0.0032 - val_loss: 4.7610e-05 - val_mae: 0.0056\n",
      "Epoch 436/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5918e-05 - mae: 0.0031\n",
      "Epoch 436: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7775e-05 - mae: 0.0034 - val_loss: 4.6492e-05 - val_mae: 0.0053\n",
      "Epoch 437/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.5925e-05 - mae: 0.0031\n",
      "Epoch 437: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.5699e-05 - mae: 0.0031 - val_loss: 3.9159e-05 - val_mae: 0.0049\n",
      "Epoch 438/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9497e-05 - mae: 0.0035\n",
      "Epoch 438: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9065e-05 - mae: 0.0034 - val_loss: 3.8284e-05 - val_mae: 0.0049\n",
      "Epoch 439/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5717e-05 - mae: 0.0031\n",
      "Epoch 439: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.7763e-05 - mae: 0.0033 - val_loss: 4.5609e-05 - val_mae: 0.0055\n",
      "Epoch 440/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5882e-05 - mae: 0.0031\n",
      "Epoch 440: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5746e-05 - mae: 0.0031 - val_loss: 4.0811e-05 - val_mae: 0.0051\n",
      "Epoch 441/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.5514e-05 - mae: 0.0031\n",
      "Epoch 441: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.5825e-05 - mae: 0.0031 - val_loss: 4.2733e-05 - val_mae: 0.0053\n",
      "Epoch 442/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.5928e-05 - mae: 0.0031\n",
      "Epoch 442: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.6338e-05 - mae: 0.0032 - val_loss: 4.3553e-05 - val_mae: 0.0053\n",
      "Epoch 443/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7689e-05 - mae: 0.0033\n",
      "Epoch 443: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7406e-05 - mae: 0.0032 - val_loss: 4.1255e-05 - val_mae: 0.0050\n",
      "Epoch 444/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7256e-05 - mae: 0.0033\n",
      "Epoch 444: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.7257e-05 - mae: 0.0033 - val_loss: 3.9922e-05 - val_mae: 0.0051\n",
      "Epoch 445/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5898e-05 - mae: 0.0032\n",
      "Epoch 445: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.5946e-05 - mae: 0.0032 - val_loss: 4.0127e-05 - val_mae: 0.0049\n",
      "Epoch 446/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5962e-05 - mae: 0.0032\n",
      "Epoch 446: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.6214e-05 - mae: 0.0032 - val_loss: 3.8305e-05 - val_mae: 0.0049\n",
      "Epoch 447/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6004e-05 - mae: 0.0032\n",
      "Epoch 447: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5514e-05 - mae: 0.0031 - val_loss: 4.2895e-05 - val_mae: 0.0053\n",
      "Epoch 448/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8366e-05 - mae: 0.0034\n",
      "Epoch 448: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6777e-05 - mae: 0.0032 - val_loss: 4.9560e-05 - val_mae: 0.0057\n",
      "Epoch 449/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7365e-05 - mae: 0.0033\n",
      "Epoch 449: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7330e-05 - mae: 0.0033 - val_loss: 3.8898e-05 - val_mae: 0.0050\n",
      "Epoch 450/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7340e-05 - mae: 0.0033\n",
      "Epoch 450: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7148e-05 - mae: 0.0033 - val_loss: 3.8856e-05 - val_mae: 0.0050\n",
      "Epoch 451/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0225e-05 - mae: 0.0035\n",
      "Epoch 451: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9633e-05 - mae: 0.0035 - val_loss: 3.8821e-05 - val_mae: 0.0049\n",
      "Epoch 452/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5894e-05 - mae: 0.0031\n",
      "Epoch 452: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.6024e-05 - mae: 0.0031 - val_loss: 3.8326e-05 - val_mae: 0.0049\n",
      "Epoch 453/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6429e-05 - mae: 0.0032\n",
      "Epoch 453: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.6041e-05 - mae: 0.0032 - val_loss: 3.8999e-05 - val_mae: 0.0050\n",
      "Epoch 454/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0710e-05 - mae: 0.0037\n",
      "Epoch 454: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 2.1054e-05 - mae: 0.0037 - val_loss: 3.9266e-05 - val_mae: 0.0050\n",
      "Epoch 455/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6599e-05 - mae: 0.0032\n",
      "Epoch 455: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.6714e-05 - mae: 0.0032 - val_loss: 4.4142e-05 - val_mae: 0.0051\n",
      "Epoch 456/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8200e-05 - mae: 0.0034\n",
      "Epoch 456: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.8054e-05 - mae: 0.0034 - val_loss: 3.9420e-05 - val_mae: 0.0049\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/800 [========================>.....] - ETA: 0s - loss: 1.4632e-05 - mae: 0.0030\n",
      "Epoch 457: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.5256e-05 - mae: 0.0031 - val_loss: 3.9286e-05 - val_mae: 0.0050\n",
      "Epoch 458/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9810e-05 - mae: 0.0035\n",
      "Epoch 458: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9780e-05 - mae: 0.0035 - val_loss: 4.0805e-05 - val_mae: 0.0051\n",
      "Epoch 459/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5157e-05 - mae: 0.0031\n",
      "Epoch 459: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.6516e-05 - mae: 0.0032 - val_loss: 3.8933e-05 - val_mae: 0.0049\n",
      "Epoch 460/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6179e-05 - mae: 0.0032\n",
      "Epoch 460: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6580e-05 - mae: 0.0032 - val_loss: 3.9562e-05 - val_mae: 0.0050\n",
      "Epoch 461/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.8512e-05 - mae: 0.0034\n",
      "Epoch 461: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.8676e-05 - mae: 0.0035 - val_loss: 4.3431e-05 - val_mae: 0.0053\n",
      "Epoch 462/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5339e-05 - mae: 0.0030\n",
      "Epoch 462: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.5294e-05 - mae: 0.0030 - val_loss: 3.9556e-05 - val_mae: 0.0049\n",
      "Epoch 463/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6646e-05 - mae: 0.0032\n",
      "Epoch 463: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7325e-05 - mae: 0.0033 - val_loss: 3.9174e-05 - val_mae: 0.0050\n",
      "Epoch 464/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6250e-05 - mae: 0.0032\n",
      "Epoch 464: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.6250e-05 - mae: 0.0032 - val_loss: 4.1263e-05 - val_mae: 0.0050\n",
      "Epoch 465/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7442e-05 - mae: 0.0033\n",
      "Epoch 465: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8430e-05 - mae: 0.0034 - val_loss: 4.4734e-05 - val_mae: 0.0054\n",
      "Epoch 466/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5414e-05 - mae: 0.0031\n",
      "Epoch 466: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.6758e-05 - mae: 0.0033 - val_loss: 3.9651e-05 - val_mae: 0.0050\n",
      "Epoch 467/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6345e-05 - mae: 0.0032\n",
      "Epoch 467: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.6134e-05 - mae: 0.0032 - val_loss: 4.4681e-05 - val_mae: 0.0052\n",
      "Epoch 468/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.4917e-05 - mae: 0.0030\n",
      "Epoch 468: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.5707e-05 - mae: 0.0031 - val_loss: 3.8673e-05 - val_mae: 0.0049\n",
      "Epoch 469/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5572e-05 - mae: 0.0031\n",
      "Epoch 469: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6088e-05 - mae: 0.0031 - val_loss: 3.8730e-05 - val_mae: 0.0049\n",
      "Epoch 470/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8160e-05 - mae: 0.0034\n",
      "Epoch 470: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7249e-05 - mae: 0.0033 - val_loss: 4.1164e-05 - val_mae: 0.0051\n",
      "Epoch 471/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5616e-05 - mae: 0.0031\n",
      "Epoch 471: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5582e-05 - mae: 0.0031 - val_loss: 4.5522e-05 - val_mae: 0.0055\n",
      "Epoch 472/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4895e-05 - mae: 0.0031\n",
      "Epoch 472: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5169e-05 - mae: 0.0031 - val_loss: 4.3741e-05 - val_mae: 0.0053\n",
      "Epoch 473/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7032e-05 - mae: 0.0033\n",
      "Epoch 473: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5453e-05 - mae: 0.0031 - val_loss: 3.8592e-05 - val_mae: 0.0049\n",
      "Epoch 474/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7610e-05 - mae: 0.0033\n",
      "Epoch 474: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.6263e-05 - mae: 0.0032 - val_loss: 3.9842e-05 - val_mae: 0.0049\n",
      "Epoch 475/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5085e-05 - mae: 0.0031\n",
      "Epoch 475: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.5178e-05 - mae: 0.0031 - val_loss: 4.2140e-05 - val_mae: 0.0052\n",
      "Epoch 476/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7085e-05 - mae: 0.0032\n",
      "Epoch 476: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5423e-05 - mae: 0.0031 - val_loss: 4.0731e-05 - val_mae: 0.0051\n",
      "Epoch 477/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7496e-05 - mae: 0.0034\n",
      "Epoch 477: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7453e-05 - mae: 0.0033 - val_loss: 3.8667e-05 - val_mae: 0.0049\n",
      "Epoch 478/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5147e-05 - mae: 0.0031\n",
      "Epoch 478: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.5225e-05 - mae: 0.0031 - val_loss: 3.9157e-05 - val_mae: 0.0049\n",
      "Epoch 479/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4336e-05 - mae: 0.0030\n",
      "Epoch 479: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.4919e-05 - mae: 0.0030 - val_loss: 3.9206e-05 - val_mae: 0.0050\n",
      "Epoch 480/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6984e-05 - mae: 0.0033\n",
      "Epoch 480: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7607e-05 - mae: 0.0033 - val_loss: 4.4720e-05 - val_mae: 0.0054\n",
      "Epoch 481/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9353e-05 - mae: 0.0035\n",
      "Epoch 481: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9255e-05 - mae: 0.0035 - val_loss: 4.2323e-05 - val_mae: 0.0051\n",
      "Epoch 482/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5091e-05 - mae: 0.0031\n",
      "Epoch 482: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5378e-05 - mae: 0.0031 - val_loss: 3.9788e-05 - val_mae: 0.0050\n",
      "Epoch 483/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3544e-05 - mae: 0.0029\n",
      "Epoch 483: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.6253e-05 - mae: 0.0032 - val_loss: 3.9453e-05 - val_mae: 0.0050\n",
      "Epoch 484/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5784e-05 - mae: 0.0031\n",
      "Epoch 484: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.6147e-05 - mae: 0.0032 - val_loss: 4.0243e-05 - val_mae: 0.0050\n",
      "Epoch 485/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9271e-05 - mae: 0.0035\n",
      "Epoch 485: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9284e-05 - mae: 0.0035 - val_loss: 3.8821e-05 - val_mae: 0.0049\n",
      "Epoch 486/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6240e-05 - mae: 0.0032\n",
      "Epoch 486: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.6274e-05 - mae: 0.0032 - val_loss: 3.8882e-05 - val_mae: 0.0049\n",
      "Epoch 487/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6306e-05 - mae: 0.0032\n",
      "Epoch 487: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.6266e-05 - mae: 0.0032 - val_loss: 3.9048e-05 - val_mae: 0.0050\n",
      "Epoch 488/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.5703e-05 - mae: 0.0031\n",
      "Epoch 488: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.5271e-05 - mae: 0.0031 - val_loss: 4.2593e-05 - val_mae: 0.0052\n",
      "Epoch 489/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8547e-05 - mae: 0.0034\n",
      "Epoch 489: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8530e-05 - mae: 0.0034 - val_loss: 4.1272e-05 - val_mae: 0.0052\n",
      "Epoch 490/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.5468e-05 - mae: 0.0031\n",
      "Epoch 490: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.6149e-05 - mae: 0.0032 - val_loss: 4.0026e-05 - val_mae: 0.0050\n",
      "Epoch 491/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.4975e-05 - mae: 0.0031\n",
      "Epoch 491: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.5078e-05 - mae: 0.0031 - val_loss: 4.4819e-05 - val_mae: 0.0054\n",
      "Epoch 492/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7334e-05 - mae: 0.0033\n",
      "Epoch 492: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.6854e-05 - mae: 0.0032 - val_loss: 4.1186e-05 - val_mae: 0.0050\n",
      "Epoch 493/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.4984e-05 - mae: 0.0031\n",
      "Epoch 493: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.5178e-05 - mae: 0.0031 - val_loss: 4.0083e-05 - val_mae: 0.0051\n",
      "Epoch 494/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0739e-05 - mae: 0.0036\n",
      "Epoch 494: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0570e-05 - mae: 0.0036 - val_loss: 4.0782e-05 - val_mae: 0.0050\n",
      "Epoch 495/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6767e-05 - mae: 0.0033\n",
      "Epoch 495: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6185e-05 - mae: 0.0032 - val_loss: 3.9308e-05 - val_mae: 0.0050\n",
      "Epoch 496/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8225e-05 - mae: 0.0034\n",
      "Epoch 496: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7460e-05 - mae: 0.0033 - val_loss: 4.5488e-05 - val_mae: 0.0054\n",
      "Epoch 497/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7861e-05 - mae: 0.0034\n",
      "Epoch 497: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7472e-05 - mae: 0.0034 - val_loss: 3.9838e-05 - val_mae: 0.0050\n",
      "Epoch 498/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3932e-05 - mae: 0.0029\n",
      "Epoch 498: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5812e-05 - mae: 0.0031 - val_loss: 4.0463e-05 - val_mae: 0.0051\n",
      "Epoch 499/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.3893e-05 - mae: 0.0030\n",
      "Epoch 499: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5243e-05 - mae: 0.0031 - val_loss: 4.4529e-05 - val_mae: 0.0054\n",
      "Epoch 500/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9590e-05 - mae: 0.0036\n",
      "Epoch 500: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8819e-05 - mae: 0.0035 - val_loss: 3.9019e-05 - val_mae: 0.0050\n",
      "Epoch 501/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4369e-05 - mae: 0.0030\n",
      "Epoch 501: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.5357e-05 - mae: 0.0031 - val_loss: 3.9200e-05 - val_mae: 0.0050\n",
      "Epoch 502/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8714e-05 - mae: 0.0035\n",
      "Epoch 502: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6566e-05 - mae: 0.0032 - val_loss: 3.9472e-05 - val_mae: 0.0050\n",
      "Epoch 503/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5023e-05 - mae: 0.0031\n",
      "Epoch 503: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5576e-05 - mae: 0.0031 - val_loss: 4.5625e-05 - val_mae: 0.0053\n",
      "Epoch 504/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5803e-05 - mae: 0.0031\n",
      "Epoch 504: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.5976e-05 - mae: 0.0032 - val_loss: 3.9381e-05 - val_mae: 0.0050\n",
      "Epoch 505/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4372e-05 - mae: 0.0030\n",
      "Epoch 505: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.5513e-05 - mae: 0.0031 - val_loss: 3.9862e-05 - val_mae: 0.0050\n",
      "Epoch 506/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4170e-05 - mae: 0.0029\n",
      "Epoch 506: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.6147e-05 - mae: 0.0032 - val_loss: 4.1741e-05 - val_mae: 0.0051\n",
      "Epoch 507/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6354e-05 - mae: 0.0032\n",
      "Epoch 507: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.6815e-05 - mae: 0.0032 - val_loss: 4.1022e-05 - val_mae: 0.0052\n",
      "Epoch 508/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9508e-05 - mae: 0.0036\n",
      "Epoch 508: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9155e-05 - mae: 0.0035 - val_loss: 4.2086e-05 - val_mae: 0.0052\n",
      "Epoch 509/1000\n",
      "370/800 [============>.................] - ETA: 0s - loss: 1.3344e-05 - mae: 0.0029\n",
      "Epoch 509: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.6352e-05 - mae: 0.0032 - val_loss: 3.9413e-05 - val_mae: 0.0050\n",
      "Epoch 510/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.5551e-05 - mae: 0.0031\n",
      "Epoch 510: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6179e-05 - mae: 0.0032 - val_loss: 4.0666e-05 - val_mae: 0.0051\n",
      "Epoch 511/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4828e-05 - mae: 0.0030\n",
      "Epoch 511: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4709e-05 - mae: 0.0030 - val_loss: 3.9862e-05 - val_mae: 0.0050\n",
      "Epoch 512/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5445e-05 - mae: 0.0031\n",
      "Epoch 512: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7646e-05 - mae: 0.0034 - val_loss: 3.9260e-05 - val_mae: 0.0050\n",
      "Epoch 513/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8784e-05 - mae: 0.0034\n",
      "Epoch 513: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7329e-05 - mae: 0.0033 - val_loss: 3.9816e-05 - val_mae: 0.0050\n",
      "Epoch 514/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5249e-05 - mae: 0.0030\n",
      "Epoch 514: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5397e-05 - mae: 0.0031 - val_loss: 3.9290e-05 - val_mae: 0.0050\n",
      "Epoch 515/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.5929e-05 - mae: 0.0031\n",
      "Epoch 515: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.5663e-05 - mae: 0.0031 - val_loss: 3.9355e-05 - val_mae: 0.0050\n",
      "Epoch 516/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5099e-05 - mae: 0.0030\n",
      "Epoch 516: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.6792e-05 - mae: 0.0032 - val_loss: 4.0375e-05 - val_mae: 0.0051\n",
      "Epoch 517/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5873e-05 - mae: 0.0031\n",
      "Epoch 517: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.6313e-05 - mae: 0.0032 - val_loss: 4.0926e-05 - val_mae: 0.0051\n",
      "Epoch 518/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6450e-05 - mae: 0.0032\n",
      "Epoch 518: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5859e-05 - mae: 0.0031 - val_loss: 4.8862e-05 - val_mae: 0.0057\n",
      "Epoch 519/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9712e-05 - mae: 0.0035\n",
      "Epoch 519: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9794e-05 - mae: 0.0035 - val_loss: 4.0721e-05 - val_mae: 0.0050\n",
      "Epoch 520/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7478e-05 - mae: 0.0033\n",
      "Epoch 520: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7997e-05 - mae: 0.0033 - val_loss: 5.2946e-05 - val_mae: 0.0059\n",
      "Epoch 521/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6147e-05 - mae: 0.0032\n",
      "Epoch 521: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5858e-05 - mae: 0.0032 - val_loss: 4.1454e-05 - val_mae: 0.0052\n",
      "Epoch 522/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5833e-05 - mae: 0.0032\n",
      "Epoch 522: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.6374e-05 - mae: 0.0032 - val_loss: 3.9186e-05 - val_mae: 0.0050\n",
      "Epoch 523/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7590e-05 - mae: 0.0033\n",
      "Epoch 523: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6329e-05 - mae: 0.0032 - val_loss: 3.9065e-05 - val_mae: 0.0049\n",
      "Epoch 524/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.4732e-05 - mae: 0.0030\n",
      "Epoch 524: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.4857e-05 - mae: 0.0030 - val_loss: 3.9705e-05 - val_mae: 0.0050\n",
      "Epoch 525/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4989e-05 - mae: 0.0030\n",
      "Epoch 525: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5127e-05 - mae: 0.0031 - val_loss: 4.0037e-05 - val_mae: 0.0050\n",
      "Epoch 526/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6671e-05 - mae: 0.0032\n",
      "Epoch 526: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6053e-05 - mae: 0.0031 - val_loss: 3.9316e-05 - val_mae: 0.0049\n",
      "Epoch 527/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3346e-05 - mae: 0.0029\n",
      "Epoch 527: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5427e-05 - mae: 0.0031 - val_loss: 3.8872e-05 - val_mae: 0.0049\n",
      "Epoch 528/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6697e-05 - mae: 0.0032\n",
      "Epoch 528: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6069e-05 - mae: 0.0032 - val_loss: 3.9254e-05 - val_mae: 0.0049\n",
      "Epoch 529/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4927e-05 - mae: 0.0031\n",
      "Epoch 529: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.6069e-05 - mae: 0.0031 - val_loss: 4.0124e-05 - val_mae: 0.0051\n",
      "Epoch 530/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5219e-05 - mae: 0.0031\n",
      "Epoch 530: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5477e-05 - mae: 0.0031 - val_loss: 4.1167e-05 - val_mae: 0.0051\n",
      "Epoch 531/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6363e-05 - mae: 0.0031\n",
      "Epoch 531: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7971e-05 - mae: 0.0033 - val_loss: 3.9180e-05 - val_mae: 0.0050\n",
      "Epoch 532/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3629e-05 - mae: 0.0030\n",
      "Epoch 532: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5041e-05 - mae: 0.0031 - val_loss: 3.9219e-05 - val_mae: 0.0049\n",
      "Epoch 533/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8420e-05 - mae: 0.0033\n",
      "Epoch 533: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0383e-05 - mae: 0.0036 - val_loss: 4.1490e-05 - val_mae: 0.0052\n",
      "Epoch 534/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5163e-05 - mae: 0.0030\n",
      "Epoch 534: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.5644e-05 - mae: 0.0031 - val_loss: 5.6463e-05 - val_mae: 0.0062\n",
      "Epoch 535/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9377e-05 - mae: 0.0035\n",
      "Epoch 535: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7532e-05 - mae: 0.0033 - val_loss: 3.9715e-05 - val_mae: 0.0050\n",
      "Epoch 536/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4631e-05 - mae: 0.0031\n",
      "Epoch 536: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.5433e-05 - mae: 0.0031 - val_loss: 4.0097e-05 - val_mae: 0.0050\n",
      "Epoch 537/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5931e-05 - mae: 0.0031\n",
      "Epoch 537: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.6930e-05 - mae: 0.0032 - val_loss: 4.5168e-05 - val_mae: 0.0053\n",
      "Epoch 538/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5219e-05 - mae: 0.0031\n",
      "Epoch 538: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5171e-05 - mae: 0.0030 - val_loss: 3.9518e-05 - val_mae: 0.0050\n",
      "Epoch 539/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4834e-05 - mae: 0.0031\n",
      "Epoch 539: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5989e-05 - mae: 0.0032 - val_loss: 3.9995e-05 - val_mae: 0.0050\n",
      "Epoch 540/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7809e-05 - mae: 0.0034\n",
      "Epoch 540: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7190e-05 - mae: 0.0033 - val_loss: 3.9506e-05 - val_mae: 0.0050\n",
      "Epoch 541/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9692e-05 - mae: 0.0036\n",
      "Epoch 541: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.0579e-05 - mae: 0.0037 - val_loss: 4.2592e-05 - val_mae: 0.0053\n",
      "Epoch 542/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8704e-05 - mae: 0.0034\n",
      "Epoch 542: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8088e-05 - mae: 0.0034 - val_loss: 4.1835e-05 - val_mae: 0.0052\n",
      "Epoch 543/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5474e-05 - mae: 0.0031\n",
      "Epoch 543: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5640e-05 - mae: 0.0031 - val_loss: 3.9220e-05 - val_mae: 0.0050\n",
      "Epoch 544/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/800 [=================>............] - ETA: 0s - loss: 1.6830e-05 - mae: 0.0032\n",
      "Epoch 544: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.6234e-05 - mae: 0.0032 - val_loss: 4.0880e-05 - val_mae: 0.0051\n",
      "Epoch 545/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.7211e-05 - mae: 0.0033\n",
      "Epoch 545: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7707e-05 - mae: 0.0033 - val_loss: 4.0238e-05 - val_mae: 0.0051\n",
      "Epoch 546/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.5272e-05 - mae: 0.0031\n",
      "Epoch 546: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.5272e-05 - mae: 0.0031 - val_loss: 4.7802e-05 - val_mae: 0.0056\n",
      "Epoch 547/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6527e-05 - mae: 0.0033\n",
      "Epoch 547: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6346e-05 - mae: 0.0032 - val_loss: 4.0387e-05 - val_mae: 0.0051\n",
      "Epoch 548/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.3922e-05 - mae: 0.0029\n",
      "Epoch 548: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4658e-05 - mae: 0.0030 - val_loss: 3.9311e-05 - val_mae: 0.0050\n",
      "Epoch 549/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6628e-05 - mae: 0.0033\n",
      "Epoch 549: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6444e-05 - mae: 0.0032 - val_loss: 3.9512e-05 - val_mae: 0.0050\n",
      "Epoch 550/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8206e-05 - mae: 0.0033\n",
      "Epoch 550: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.6690e-05 - mae: 0.0032 - val_loss: 3.9092e-05 - val_mae: 0.0050\n",
      "Epoch 551/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6298e-05 - mae: 0.0032\n",
      "Epoch 551: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5926e-05 - mae: 0.0032 - val_loss: 3.9662e-05 - val_mae: 0.0050\n",
      "Epoch 552/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5121e-05 - mae: 0.0030\n",
      "Epoch 552: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4428e-05 - mae: 0.0030 - val_loss: 3.9449e-05 - val_mae: 0.0050\n",
      "Epoch 553/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7085e-05 - mae: 0.0033\n",
      "Epoch 553: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.5429e-05 - mae: 0.0031 - val_loss: 3.9971e-05 - val_mae: 0.0050\n",
      "Epoch 554/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0036e-05 - mae: 0.0035\n",
      "Epoch 554: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8188e-05 - mae: 0.0033 - val_loss: 4.0572e-05 - val_mae: 0.0050\n",
      "Epoch 555/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9236e-05 - mae: 0.0035\n",
      "Epoch 555: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7911e-05 - mae: 0.0033 - val_loss: 4.3029e-05 - val_mae: 0.0051\n",
      "Epoch 556/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6359e-05 - mae: 0.0032\n",
      "Epoch 556: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.6487e-05 - mae: 0.0032 - val_loss: 4.0319e-05 - val_mae: 0.0051\n",
      "Epoch 557/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7165e-05 - mae: 0.0033\n",
      "Epoch 557: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.6760e-05 - mae: 0.0033 - val_loss: 5.6445e-05 - val_mae: 0.0061\n",
      "Epoch 558/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6916e-05 - mae: 0.0033\n",
      "Epoch 558: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.6691e-05 - mae: 0.0032 - val_loss: 4.1688e-05 - val_mae: 0.0052\n",
      "Epoch 559/1000\n",
      "360/800 [============>.................] - ETA: 0s - loss: 1.5097e-05 - mae: 0.0030\n",
      "Epoch 559: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.6639e-05 - mae: 0.0032 - val_loss: 4.0440e-05 - val_mae: 0.0051\n",
      "Epoch 560/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4423e-05 - mae: 0.0030\n",
      "Epoch 560: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.4681e-05 - mae: 0.0030 - val_loss: 3.9503e-05 - val_mae: 0.0050\n",
      "Epoch 561/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5186e-05 - mae: 0.0031\n",
      "Epoch 561: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.5165e-05 - mae: 0.0031 - val_loss: 3.9112e-05 - val_mae: 0.0050\n",
      "Epoch 562/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5220e-05 - mae: 0.0031\n",
      "Epoch 562: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9623e-05 - mae: 0.0035 - val_loss: 5.3401e-05 - val_mae: 0.0060\n",
      "Epoch 563/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5403e-05 - mae: 0.0031\n",
      "Epoch 563: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.5490e-05 - mae: 0.0031 - val_loss: 3.9156e-05 - val_mae: 0.0049\n",
      "Epoch 564/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4682e-05 - mae: 0.0030\n",
      "Epoch 564: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.4744e-05 - mae: 0.0030 - val_loss: 4.3885e-05 - val_mae: 0.0053\n",
      "Epoch 565/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.7167e-05 - mae: 0.0033\n",
      "Epoch 565: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.8235e-05 - mae: 0.0034 - val_loss: 5.9446e-05 - val_mae: 0.0061\n",
      "Epoch 566/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6133e-05 - mae: 0.0032\n",
      "Epoch 566: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.6056e-05 - mae: 0.0032 - val_loss: 4.1533e-05 - val_mae: 0.0052\n",
      "Epoch 567/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.5899e-05 - mae: 0.0031\n",
      "Epoch 567: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.5480e-05 - mae: 0.0031 - val_loss: 3.9171e-05 - val_mae: 0.0050\n",
      "Epoch 568/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.6619e-05 - mae: 0.0032\n",
      "Epoch 568: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7025e-05 - mae: 0.0033 - val_loss: 4.5880e-05 - val_mae: 0.0053\n",
      "Epoch 569/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0592e-05 - mae: 0.0036\n",
      "Epoch 569: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7648e-05 - mae: 0.0033 - val_loss: 3.9867e-05 - val_mae: 0.0051\n",
      "Epoch 570/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5991e-05 - mae: 0.0031\n",
      "Epoch 570: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.5668e-05 - mae: 0.0031 - val_loss: 4.5275e-05 - val_mae: 0.0054\n",
      "Epoch 571/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6822e-05 - mae: 0.0033\n",
      "Epoch 571: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5616e-05 - mae: 0.0031 - val_loss: 4.4598e-05 - val_mae: 0.0054\n",
      "Epoch 572/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.4048e-05 - mae: 0.0029\n",
      "Epoch 572: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5820e-05 - mae: 0.0031 - val_loss: 4.7999e-05 - val_mae: 0.0056\n",
      "Epoch 573/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 0s - loss: 1.5985e-05 - mae: 0.0031\n",
      "Epoch 573: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.5985e-05 - mae: 0.0031 - val_loss: 3.9642e-05 - val_mae: 0.0050\n",
      "Epoch 574/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5121e-05 - mae: 0.0031\n",
      "Epoch 574: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5169e-05 - mae: 0.0031 - val_loss: 3.9553e-05 - val_mae: 0.0050\n",
      "Epoch 575/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4808e-05 - mae: 0.0031\n",
      "Epoch 575: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6113e-05 - mae: 0.0032 - val_loss: 4.3861e-05 - val_mae: 0.0052\n",
      "Epoch 576/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.4763e-05 - mae: 0.0030\n",
      "Epoch 576: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.4776e-05 - mae: 0.0030 - val_loss: 3.9606e-05 - val_mae: 0.0050\n",
      "Epoch 577/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.7081e-05 - mae: 0.0032\n",
      "Epoch 577: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5638e-05 - mae: 0.0031 - val_loss: 3.9754e-05 - val_mae: 0.0050\n",
      "Epoch 578/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5664e-05 - mae: 0.0031\n",
      "Epoch 578: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5162e-05 - mae: 0.0030 - val_loss: 4.0035e-05 - val_mae: 0.0051\n",
      "Epoch 579/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4761e-05 - mae: 0.0030\n",
      "Epoch 579: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6255e-05 - mae: 0.0032 - val_loss: 4.0964e-05 - val_mae: 0.0050\n",
      "Epoch 580/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1591e-05 - mae: 0.0038\n",
      "Epoch 580: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9470e-05 - mae: 0.0036 - val_loss: 4.1550e-05 - val_mae: 0.0052\n",
      "Epoch 581/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.4383e-05 - mae: 0.0030\n",
      "Epoch 581: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.4812e-05 - mae: 0.0031 - val_loss: 4.0641e-05 - val_mae: 0.0050\n",
      "Epoch 582/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6494e-05 - mae: 0.0032\n",
      "Epoch 582: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6260e-05 - mae: 0.0032 - val_loss: 3.9678e-05 - val_mae: 0.0050\n",
      "Epoch 583/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7429e-05 - mae: 0.0033\n",
      "Epoch 583: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7269e-05 - mae: 0.0033 - val_loss: 4.3670e-05 - val_mae: 0.0051\n",
      "Epoch 584/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7529e-05 - mae: 0.0033\n",
      "Epoch 584: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7709e-05 - mae: 0.0033 - val_loss: 4.0348e-05 - val_mae: 0.0050\n",
      "Epoch 585/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.5155e-05 - mae: 0.0030\n",
      "Epoch 585: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.5344e-05 - mae: 0.0031 - val_loss: 3.9549e-05 - val_mae: 0.0050\n",
      "Epoch 586/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6056e-05 - mae: 0.0031\n",
      "Epoch 586: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5513e-05 - mae: 0.0031 - val_loss: 3.9606e-05 - val_mae: 0.0050\n",
      "Epoch 587/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7180e-05 - mae: 0.0033\n",
      "Epoch 587: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6991e-05 - mae: 0.0033 - val_loss: 3.9582e-05 - val_mae: 0.0050\n",
      "Epoch 588/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4903e-05 - mae: 0.0031\n",
      "Epoch 588: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7983e-05 - mae: 0.0034 - val_loss: 4.4779e-05 - val_mae: 0.0054\n",
      "Epoch 589/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.3552e-05 - mae: 0.0028\n",
      "Epoch 589: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4288e-05 - mae: 0.0029 - val_loss: 4.6191e-05 - val_mae: 0.0053\n",
      "Epoch 590/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4671e-05 - mae: 0.0030\n",
      "Epoch 590: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5121e-05 - mae: 0.0031 - val_loss: 4.3000e-05 - val_mae: 0.0051\n",
      "Epoch 591/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4974e-05 - mae: 0.0030\n",
      "Epoch 591: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.6513e-05 - mae: 0.0032 - val_loss: 4.3948e-05 - val_mae: 0.0053\n",
      "Epoch 592/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7511e-05 - mae: 0.0032\n",
      "Epoch 592: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5958e-05 - mae: 0.0031 - val_loss: 3.9851e-05 - val_mae: 0.0050\n",
      "Epoch 593/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4992e-05 - mae: 0.0031\n",
      "Epoch 593: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5360e-05 - mae: 0.0031 - val_loss: 4.9666e-05 - val_mae: 0.0057\n",
      "Epoch 594/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.6050e-05 - mae: 0.0032\n",
      "Epoch 594: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.6039e-05 - mae: 0.0032 - val_loss: 4.3643e-05 - val_mae: 0.0053\n",
      "Epoch 595/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.4661e-05 - mae: 0.0030\n",
      "Epoch 595: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.4803e-05 - mae: 0.0030 - val_loss: 4.5604e-05 - val_mae: 0.0053\n",
      "Epoch 596/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7649e-05 - mae: 0.0033\n",
      "Epoch 596: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7103e-05 - mae: 0.0033 - val_loss: 4.0407e-05 - val_mae: 0.0050\n",
      "Epoch 597/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.4369e-05 - mae: 0.0030\n",
      "Epoch 597: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.4356e-05 - mae: 0.0030 - val_loss: 4.4176e-05 - val_mae: 0.0054\n",
      "Epoch 598/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.4203e-05 - mae: 0.0029\n",
      "Epoch 598: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.4203e-05 - mae: 0.0029 - val_loss: 4.0149e-05 - val_mae: 0.0051\n",
      "Epoch 599/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4049e-05 - mae: 0.0029\n",
      "Epoch 599: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.6567e-05 - mae: 0.0032 - val_loss: 3.9795e-05 - val_mae: 0.0050\n",
      "Epoch 600/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.6100e-05 - mae: 0.0032\n",
      "Epoch 600: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.6045e-05 - mae: 0.0032 - val_loss: 3.9771e-05 - val_mae: 0.0050\n",
      "Epoch 601/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6443e-05 - mae: 0.0032\n",
      "Epoch 601: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.6813e-05 - mae: 0.0032 - val_loss: 3.9795e-05 - val_mae: 0.0050\n",
      "Epoch 602/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.7357e-05 - mae: 0.0032   \n",
      "Epoch 602: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7373e-05 - mae: 0.0032 - val_loss: 3.9850e-05 - val_mae: 0.0050\n",
      "Epoch 603/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.2483e-05 - mae: 0.0028\n",
      "Epoch 603: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.4051e-05 - mae: 0.0029 - val_loss: 4.1084e-05 - val_mae: 0.0052\n",
      "Epoch 604/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.3941e-05 - mae: 0.0029\n",
      "Epoch 604: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.5008e-05 - mae: 0.0030 - val_loss: 4.1667e-05 - val_mae: 0.0052\n",
      "Epoch 605/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5123e-05 - mae: 0.0031\n",
      "Epoch 605: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6456e-05 - mae: 0.0031 - val_loss: 4.0019e-05 - val_mae: 0.0051\n",
      "Epoch 606/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.5372e-05 - mae: 0.0031\n",
      "Epoch 606: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.5321e-05 - mae: 0.0031 - val_loss: 3.9946e-05 - val_mae: 0.0051\n",
      "Epoch 607/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6267e-05 - mae: 0.0031\n",
      "Epoch 607: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5765e-05 - mae: 0.0031 - val_loss: 3.9936e-05 - val_mae: 0.0050\n",
      "Epoch 608/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3965e-05 - mae: 0.0029\n",
      "Epoch 608: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.4390e-05 - mae: 0.0030 - val_loss: 3.9973e-05 - val_mae: 0.0050\n",
      "Epoch 609/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6026e-05 - mae: 0.0031\n",
      "Epoch 609: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.6217e-05 - mae: 0.0031 - val_loss: 4.5098e-05 - val_mae: 0.0054\n",
      "Epoch 610/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5187e-05 - mae: 0.0030\n",
      "Epoch 610: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5239e-05 - mae: 0.0031 - val_loss: 4.0005e-05 - val_mae: 0.0050\n",
      "Epoch 611/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4103e-05 - mae: 0.0029\n",
      "Epoch 611: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5488e-05 - mae: 0.0031 - val_loss: 4.2981e-05 - val_mae: 0.0053\n",
      "Epoch 612/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8769e-05 - mae: 0.0034\n",
      "Epoch 612: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7684e-05 - mae: 0.0033 - val_loss: 4.3739e-05 - val_mae: 0.0053\n",
      "Epoch 613/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4815e-05 - mae: 0.0031\n",
      "Epoch 613: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5062e-05 - mae: 0.0030 - val_loss: 4.2239e-05 - val_mae: 0.0051\n",
      "Epoch 614/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5352e-05 - mae: 0.0031\n",
      "Epoch 614: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.5952e-05 - mae: 0.0032 - val_loss: 4.5241e-05 - val_mae: 0.0054\n",
      "Epoch 615/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7678e-05 - mae: 0.0033\n",
      "Epoch 615: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7678e-05 - mae: 0.0033 - val_loss: 4.0161e-05 - val_mae: 0.0051\n",
      "Epoch 616/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5694e-05 - mae: 0.0031\n",
      "Epoch 616: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5622e-05 - mae: 0.0031 - val_loss: 4.0478e-05 - val_mae: 0.0050\n",
      "Epoch 617/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8595e-05 - mae: 0.0034\n",
      "Epoch 617: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7095e-05 - mae: 0.0033 - val_loss: 4.0288e-05 - val_mae: 0.0050\n",
      "Epoch 618/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.3327e-05 - mae: 0.0029\n",
      "Epoch 618: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.5182e-05 - mae: 0.0031 - val_loss: 4.3523e-05 - val_mae: 0.0053\n",
      "Epoch 619/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7359e-05 - mae: 0.0033\n",
      "Epoch 619: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7415e-05 - mae: 0.0033 - val_loss: 5.0985e-05 - val_mae: 0.0058\n",
      "Epoch 620/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.4764e-05 - mae: 0.0030\n",
      "Epoch 620: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.4819e-05 - mae: 0.0031 - val_loss: 4.2550e-05 - val_mae: 0.0052\n",
      "Epoch 621/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.5778e-05 - mae: 0.0031\n",
      "Epoch 621: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.5267e-05 - mae: 0.0031 - val_loss: 4.3904e-05 - val_mae: 0.0053\n",
      "Epoch 622/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5935e-05 - mae: 0.0031\n",
      "Epoch 622: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6330e-05 - mae: 0.0032 - val_loss: 4.0298e-05 - val_mae: 0.0051\n",
      "Epoch 623/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.6729e-05 - mae: 0.0032\n",
      "Epoch 623: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.6643e-05 - mae: 0.0032 - val_loss: 4.2053e-05 - val_mae: 0.0052\n",
      "Epoch 624/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.4452e-05 - mae: 0.0030\n",
      "Epoch 624: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.4629e-05 - mae: 0.0030 - val_loss: 4.3628e-05 - val_mae: 0.0051\n",
      "Epoch 625/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.5553e-05 - mae: 0.0031\n",
      "Epoch 625: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.5956e-05 - mae: 0.0032 - val_loss: 4.3838e-05 - val_mae: 0.0052\n",
      "Epoch 626/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9394e-05 - mae: 0.0035\n",
      "Epoch 626: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.9467e-05 - mae: 0.0035 - val_loss: 4.6566e-05 - val_mae: 0.0055\n",
      "Epoch 627/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7236e-05 - mae: 0.0033\n",
      "Epoch 627: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.7265e-05 - mae: 0.0033 - val_loss: 4.1728e-05 - val_mae: 0.0052\n",
      "Epoch 628/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6741e-05 - mae: 0.0032\n",
      "Epoch 628: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6715e-05 - mae: 0.0032 - val_loss: 4.9628e-05 - val_mae: 0.0057\n",
      "Epoch 629/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7111e-05 - mae: 0.0033\n",
      "Epoch 629: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7195e-05 - mae: 0.0033 - val_loss: 4.5176e-05 - val_mae: 0.0052\n",
      "Epoch 630/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.5707e-05 - mae: 0.0031\n",
      "Epoch 630: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.5952e-05 - mae: 0.0032 - val_loss: 4.7934e-05 - val_mae: 0.0056\n",
      "Epoch 631/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/800 [========================>.....] - ETA: 0s - loss: 1.4413e-05 - mae: 0.0029\n",
      "Epoch 631: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.4432e-05 - mae: 0.0030 - val_loss: 4.2271e-05 - val_mae: 0.0052\n",
      "Epoch 632/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.4573e-05 - mae: 0.0030\n",
      "Epoch 632: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.4687e-05 - mae: 0.0030 - val_loss: 4.0459e-05 - val_mae: 0.0051\n",
      "Epoch 633/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.4795e-05 - mae: 0.0030\n",
      "Epoch 633: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.4899e-05 - mae: 0.0031 - val_loss: 4.1241e-05 - val_mae: 0.0050\n",
      "Epoch 634/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.4854e-05 - mae: 0.0030\n",
      "Epoch 634: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.5005e-05 - mae: 0.0031 - val_loss: 4.0359e-05 - val_mae: 0.0050\n",
      "Epoch 635/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7479e-05 - mae: 0.0033\n",
      "Epoch 635: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.6765e-05 - mae: 0.0032 - val_loss: 4.0303e-05 - val_mae: 0.0050\n",
      "Epoch 636/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5209e-05 - mae: 0.0031\n",
      "Epoch 636: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.6201e-05 - mae: 0.0032 - val_loss: 4.0950e-05 - val_mae: 0.0051\n",
      "Epoch 637/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.4677e-05 - mae: 0.0030\n",
      "Epoch 637: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.4696e-05 - mae: 0.0030 - val_loss: 4.2295e-05 - val_mae: 0.0051\n",
      "Epoch 638/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5893e-05 - mae: 0.0031\n",
      "Epoch 638: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.6754e-05 - mae: 0.0032 - val_loss: 3.9946e-05 - val_mae: 0.0050\n",
      "Epoch 639/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5422e-05 - mae: 0.0031\n",
      "Epoch 639: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.4916e-05 - mae: 0.0030 - val_loss: 4.1002e-05 - val_mae: 0.0051\n",
      "Epoch 640/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7940e-05 - mae: 0.0034\n",
      "Epoch 640: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6583e-05 - mae: 0.0032 - val_loss: 4.2532e-05 - val_mae: 0.0052\n",
      "Epoch 641/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.4536e-05 - mae: 0.0030\n",
      "Epoch 641: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.4979e-05 - mae: 0.0030 - val_loss: 4.0122e-05 - val_mae: 0.0050\n",
      "Epoch 642/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4502e-05 - mae: 0.0030\n",
      "Epoch 642: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.4390e-05 - mae: 0.0030 - val_loss: 4.4274e-05 - val_mae: 0.0054\n",
      "Epoch 643/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5229e-05 - mae: 0.0031\n",
      "Epoch 643: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.5335e-05 - mae: 0.0031 - val_loss: 4.5558e-05 - val_mae: 0.0053\n",
      "Epoch 644/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.6108e-05 - mae: 0.0031\n",
      "Epoch 644: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.5603e-05 - mae: 0.0031 - val_loss: 4.0861e-05 - val_mae: 0.0051\n",
      "Epoch 645/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.5156e-05 - mae: 0.0030\n",
      "Epoch 645: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.5242e-05 - mae: 0.0031 - val_loss: 4.0442e-05 - val_mae: 0.0051\n",
      "Epoch 646/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4954e-05 - mae: 0.0030\n",
      "Epoch 646: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6245e-05 - mae: 0.0031 - val_loss: 5.1545e-05 - val_mae: 0.0058\n",
      "Epoch 647/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.6050e-05 - mae: 0.0031\n",
      "Epoch 647: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.6050e-05 - mae: 0.0031 - val_loss: 4.5832e-05 - val_mae: 0.0053\n",
      "Epoch 648/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.6376e-05 - mae: 0.0032\n",
      "Epoch 648: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.7249e-05 - mae: 0.0033 - val_loss: 4.4283e-05 - val_mae: 0.0052\n",
      "Epoch 649/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5742e-05 - mae: 0.0031\n",
      "Epoch 649: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.5823e-05 - mae: 0.0031 - val_loss: 4.4740e-05 - val_mae: 0.0052\n",
      "Epoch 650/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6521e-05 - mae: 0.0032\n",
      "Epoch 650: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.6706e-05 - mae: 0.0032 - val_loss: 5.0044e-05 - val_mae: 0.0057\n",
      "Epoch 651/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.4773e-05 - mae: 0.0041\n",
      "Epoch 651: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.3186e-05 - mae: 0.0039 - val_loss: 4.6962e-05 - val_mae: 0.0053\n",
      "Epoch 652/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4528e-05 - mae: 0.0030\n",
      "Epoch 652: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5808e-05 - mae: 0.0031 - val_loss: 4.0521e-05 - val_mae: 0.0050\n",
      "Epoch 653/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5364e-05 - mae: 0.0031\n",
      "Epoch 653: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.5235e-05 - mae: 0.0031 - val_loss: 4.0483e-05 - val_mae: 0.0051\n",
      "Epoch 654/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5476e-05 - mae: 0.0031\n",
      "Epoch 654: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.5592e-05 - mae: 0.0031 - val_loss: 4.1851e-05 - val_mae: 0.0052\n",
      "Epoch 655/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5265e-05 - mae: 0.0031\n",
      "Epoch 655: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.5141e-05 - mae: 0.0030 - val_loss: 4.4816e-05 - val_mae: 0.0052\n",
      "Epoch 656/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7350e-05 - mae: 0.0033\n",
      "Epoch 656: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7290e-05 - mae: 0.0033 - val_loss: 4.1268e-05 - val_mae: 0.0052\n",
      "Epoch 657/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7379e-05 - mae: 0.0032\n",
      "Epoch 657: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7395e-05 - mae: 0.0032 - val_loss: 4.4778e-05 - val_mae: 0.0054\n",
      "Epoch 658/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6062e-05 - mae: 0.0032\n",
      "Epoch 658: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.6431e-05 - mae: 0.0032 - val_loss: 5.2570e-05 - val_mae: 0.0057\n",
      "Epoch 659/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.6950e-05 - mae: 0.0032\n",
      "Epoch 659: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 1.6886e-05 - mae: 0.0032 - val_loss: 4.0911e-05 - val_mae: 0.0051\n",
      "Epoch 660/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/800 [=======================>......] - ETA: 0s - loss: 1.5460e-05 - mae: 0.0031\n",
      "Epoch 660: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.5553e-05 - mae: 0.0031 - val_loss: 4.2865e-05 - val_mae: 0.0053\n",
      "Epoch 661/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.4701e-05 - mae: 0.0030\n",
      "Epoch 661: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.4955e-05 - mae: 0.0030 - val_loss: 4.1663e-05 - val_mae: 0.0051\n",
      "Epoch 662/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.5761e-05 - mae: 0.0032\n",
      "Epoch 662: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.5739e-05 - mae: 0.0032 - val_loss: 4.1850e-05 - val_mae: 0.0051\n",
      "Epoch 663/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5624e-05 - mae: 0.0031\n",
      "Epoch 663: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.5459e-05 - mae: 0.0031 - val_loss: 4.0631e-05 - val_mae: 0.0051\n",
      "Epoch 664/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.5532e-05 - mae: 0.0030\n",
      "Epoch 664: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.6120e-05 - mae: 0.0031 - val_loss: 4.2618e-05 - val_mae: 0.0051\n",
      "Epoch 665/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5627e-05 - mae: 0.0031\n",
      "Epoch 665: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.6113e-05 - mae: 0.0032 - val_loss: 4.1629e-05 - val_mae: 0.0052\n",
      "Epoch 666/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6091e-05 - mae: 0.0032\n",
      "Epoch 666: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6050e-05 - mae: 0.0032 - val_loss: 4.7381e-05 - val_mae: 0.0056\n",
      "Epoch 667/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.4983e-05 - mae: 0.0031\n",
      "Epoch 667: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.5282e-05 - mae: 0.0031 - val_loss: 4.4848e-05 - val_mae: 0.0054\n",
      "Epoch 668/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4869e-05 - mae: 0.0031\n",
      "Epoch 668: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5253e-05 - mae: 0.0031 - val_loss: 4.0112e-05 - val_mae: 0.0051\n",
      "Epoch 669/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5595e-05 - mae: 0.0031\n",
      "Epoch 669: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4985e-05 - mae: 0.0030 - val_loss: 4.0052e-05 - val_mae: 0.0050\n",
      "Epoch 670/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3416e-05 - mae: 0.0029\n",
      "Epoch 670: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4435e-05 - mae: 0.0030 - val_loss: 4.0288e-05 - val_mae: 0.0051\n",
      "Epoch 671/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4939e-05 - mae: 0.0030\n",
      "Epoch 671: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.4854e-05 - mae: 0.0030 - val_loss: 4.0705e-05 - val_mae: 0.0050\n",
      "Epoch 672/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6176e-05 - mae: 0.0032\n",
      "Epoch 672: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.6116e-05 - mae: 0.0032 - val_loss: 5.5798e-05 - val_mae: 0.0061\n",
      "Epoch 673/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4270e-05 - mae: 0.0029\n",
      "Epoch 673: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5413e-05 - mae: 0.0031 - val_loss: 4.3045e-05 - val_mae: 0.0051\n",
      "Epoch 674/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.3707e-05 - mae: 0.0029\n",
      "Epoch 674: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5137e-05 - mae: 0.0030 - val_loss: 4.0925e-05 - val_mae: 0.0050\n",
      "Epoch 675/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.4448e-05 - mae: 0.0030\n",
      "Epoch 675: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.6388e-05 - mae: 0.0032 - val_loss: 4.1238e-05 - val_mae: 0.0051\n",
      "Epoch 676/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3318e-05 - mae: 0.0028\n",
      "Epoch 676: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.4175e-05 - mae: 0.0029 - val_loss: 4.2396e-05 - val_mae: 0.0051\n",
      "Epoch 677/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6080e-05 - mae: 0.0032\n",
      "Epoch 677: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.5930e-05 - mae: 0.0031 - val_loss: 4.2588e-05 - val_mae: 0.0051\n",
      "Epoch 678/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5249e-05 - mae: 0.0031\n",
      "Epoch 678: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5263e-05 - mae: 0.0031 - val_loss: 5.0874e-05 - val_mae: 0.0058\n",
      "Epoch 679/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5584e-05 - mae: 0.0032\n",
      "Epoch 679: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7096e-05 - mae: 0.0033 - val_loss: 4.1860e-05 - val_mae: 0.0052\n",
      "Epoch 680/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6375e-05 - mae: 0.0032\n",
      "Epoch 680: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.5903e-05 - mae: 0.0031 - val_loss: 4.2230e-05 - val_mae: 0.0051\n",
      "Epoch 681/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7284e-05 - mae: 0.0032\n",
      "Epoch 681: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6373e-05 - mae: 0.0032 - val_loss: 4.3831e-05 - val_mae: 0.0053\n",
      "Epoch 682/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5379e-05 - mae: 0.0031\n",
      "Epoch 682: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.4997e-05 - mae: 0.0030 - val_loss: 4.0541e-05 - val_mae: 0.0051\n",
      "Epoch 683/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3959e-05 - mae: 0.0029\n",
      "Epoch 683: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4395e-05 - mae: 0.0030 - val_loss: 4.0932e-05 - val_mae: 0.0050\n",
      "Epoch 684/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.2699e-05 - mae: 0.0028\n",
      "Epoch 684: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.4231e-05 - mae: 0.0030 - val_loss: 4.0805e-05 - val_mae: 0.0051\n",
      "Epoch 685/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6387e-05 - mae: 0.0031\n",
      "Epoch 685: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.6164e-05 - mae: 0.0031 - val_loss: 4.8781e-05 - val_mae: 0.0056\n",
      "Epoch 686/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8061e-05 - mae: 0.0033\n",
      "Epoch 686: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7788e-05 - mae: 0.0033 - val_loss: 4.2200e-05 - val_mae: 0.0052\n",
      "Epoch 687/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6662e-05 - mae: 0.0032\n",
      "Epoch 687: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7066e-05 - mae: 0.0032 - val_loss: 4.0788e-05 - val_mae: 0.0051\n",
      "Epoch 688/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4680e-05 - mae: 0.0030\n",
      "Epoch 688: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.5077e-05 - mae: 0.0031 - val_loss: 4.0184e-05 - val_mae: 0.0051\n",
      "Epoch 689/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 0s - loss: 1.5072e-05 - mae: 0.0030\n",
      "Epoch 689: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.5072e-05 - mae: 0.0030 - val_loss: 4.5977e-05 - val_mae: 0.0053\n",
      "Epoch 690/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5529e-05 - mae: 0.0031\n",
      "Epoch 690: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7398e-05 - mae: 0.0033 - val_loss: 4.3312e-05 - val_mae: 0.0051\n",
      "Epoch 691/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6212e-05 - mae: 0.0032\n",
      "Epoch 691: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.6433e-05 - mae: 0.0032 - val_loss: 6.1186e-05 - val_mae: 0.0064\n",
      "Epoch 692/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8041e-05 - mae: 0.0034\n",
      "Epoch 692: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8849e-05 - mae: 0.0034 - val_loss: 4.0077e-05 - val_mae: 0.0051\n",
      "Epoch 693/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5493e-05 - mae: 0.0031\n",
      "Epoch 693: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5153e-05 - mae: 0.0031 - val_loss: 4.0804e-05 - val_mae: 0.0050\n",
      "Epoch 694/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5727e-05 - mae: 0.0031\n",
      "Epoch 694: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.5239e-05 - mae: 0.0031 - val_loss: 4.0230e-05 - val_mae: 0.0051\n",
      "Epoch 695/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3004e-05 - mae: 0.0028\n",
      "Epoch 695: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4892e-05 - mae: 0.0030 - val_loss: 4.1893e-05 - val_mae: 0.0052\n",
      "Epoch 696/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4865e-05 - mae: 0.0030\n",
      "Epoch 696: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6342e-05 - mae: 0.0032 - val_loss: 4.4731e-05 - val_mae: 0.0054\n",
      "Epoch 697/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7970e-05 - mae: 0.0032\n",
      "Epoch 697: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7306e-05 - mae: 0.0032 - val_loss: 4.1243e-05 - val_mae: 0.0050\n",
      "Epoch 698/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4345e-05 - mae: 0.0030\n",
      "Epoch 698: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5952e-05 - mae: 0.0032 - val_loss: 4.0037e-05 - val_mae: 0.0051\n",
      "Epoch 699/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4407e-05 - mae: 0.0030\n",
      "Epoch 699: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.4232e-05 - mae: 0.0030 - val_loss: 4.5379e-05 - val_mae: 0.0054\n",
      "Epoch 700/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.4217e-05 - mae: 0.0030\n",
      "Epoch 700: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.5998e-05 - mae: 0.0031 - val_loss: 5.7254e-05 - val_mae: 0.0060\n",
      "Epoch 701/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7855e-05 - mae: 0.0034\n",
      "Epoch 701: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6711e-05 - mae: 0.0033 - val_loss: 4.2281e-05 - val_mae: 0.0052\n",
      "Epoch 702/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5280e-05 - mae: 0.0031\n",
      "Epoch 702: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.5834e-05 - mae: 0.0031 - val_loss: 3.9877e-05 - val_mae: 0.0051\n",
      "Epoch 703/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4923e-05 - mae: 0.0030\n",
      "Epoch 703: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5010e-05 - mae: 0.0030 - val_loss: 4.0028e-05 - val_mae: 0.0050\n",
      "Epoch 704/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.4447e-05 - mae: 0.0030\n",
      "Epoch 704: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 1.5155e-05 - mae: 0.0030 - val_loss: 4.0904e-05 - val_mae: 0.0050\n",
      "Epoch 705/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3901e-05 - mae: 0.0029\n",
      "Epoch 705: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5004e-05 - mae: 0.0030 - val_loss: 4.0686e-05 - val_mae: 0.0050\n",
      "Epoch 706/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4778e-05 - mae: 0.0030\n",
      "Epoch 706: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.4822e-05 - mae: 0.0030 - val_loss: 4.0623e-05 - val_mae: 0.0051\n",
      "Epoch 707/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3343e-05 - mae: 0.0028\n",
      "Epoch 707: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.4444e-05 - mae: 0.0030 - val_loss: 4.1539e-05 - val_mae: 0.0052\n",
      "Epoch 708/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4779e-05 - mae: 0.0030\n",
      "Epoch 708: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.6796e-05 - mae: 0.0032 - val_loss: 4.3931e-05 - val_mae: 0.0053\n",
      "Epoch 709/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5609e-05 - mae: 0.0031\n",
      "Epoch 709: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5705e-05 - mae: 0.0031 - val_loss: 4.2399e-05 - val_mae: 0.0052\n",
      "Epoch 710/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4654e-05 - mae: 0.0030\n",
      "Epoch 710: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.5222e-05 - mae: 0.0031 - val_loss: 5.0285e-05 - val_mae: 0.0057\n",
      "Epoch 711/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5082e-05 - mae: 0.0030\n",
      "Epoch 711: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5209e-05 - mae: 0.0031 - val_loss: 4.0847e-05 - val_mae: 0.0051\n",
      "Epoch 712/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.3067e-05 - mae: 0.0028\n",
      "Epoch 712: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4160e-05 - mae: 0.0029 - val_loss: 4.4692e-05 - val_mae: 0.0054\n",
      "Epoch 713/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4407e-05 - mae: 0.0030\n",
      "Epoch 713: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4869e-05 - mae: 0.0030 - val_loss: 4.0606e-05 - val_mae: 0.0050\n",
      "Epoch 714/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5113e-05 - mae: 0.0031\n",
      "Epoch 714: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.5743e-05 - mae: 0.0031 - val_loss: 4.2166e-05 - val_mae: 0.0051\n",
      "Epoch 715/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4351e-05 - mae: 0.0030\n",
      "Epoch 715: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5768e-05 - mae: 0.0031 - val_loss: 4.0514e-05 - val_mae: 0.0051\n",
      "Epoch 716/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6677e-05 - mae: 0.0032\n",
      "Epoch 716: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5791e-05 - mae: 0.0031 - val_loss: 4.2113e-05 - val_mae: 0.0052\n",
      "Epoch 717/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3786e-05 - mae: 0.0029\n",
      "Epoch 717: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5688e-05 - mae: 0.0031 - val_loss: 4.0466e-05 - val_mae: 0.0050\n",
      "Epoch 718/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4953e-05 - mae: 0.0030\n",
      "Epoch 718: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.4874e-05 - mae: 0.0030 - val_loss: 4.0313e-05 - val_mae: 0.0050\n",
      "Epoch 719/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8035e-05 - mae: 0.0034\n",
      "Epoch 719: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9157e-05 - mae: 0.0035 - val_loss: 4.0609e-05 - val_mae: 0.0051\n",
      "Epoch 720/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7655e-05 - mae: 0.0034\n",
      "Epoch 720: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7762e-05 - mae: 0.0033 - val_loss: 4.0392e-05 - val_mae: 0.0051\n",
      "Epoch 721/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7917e-05 - mae: 0.0033\n",
      "Epoch 721: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6383e-05 - mae: 0.0032 - val_loss: 4.0341e-05 - val_mae: 0.0051\n",
      "Epoch 722/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4526e-05 - mae: 0.0030\n",
      "Epoch 722: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.4873e-05 - mae: 0.0030 - val_loss: 4.0917e-05 - val_mae: 0.0051\n",
      "Epoch 723/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4544e-05 - mae: 0.0029\n",
      "Epoch 723: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.4934e-05 - mae: 0.0030 - val_loss: 4.1724e-05 - val_mae: 0.0052\n",
      "Epoch 724/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4966e-05 - mae: 0.0030\n",
      "Epoch 724: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5795e-05 - mae: 0.0031 - val_loss: 4.1556e-05 - val_mae: 0.0051\n",
      "Epoch 725/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.3813e-05 - mae: 0.0029\n",
      "Epoch 725: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4118e-05 - mae: 0.0029 - val_loss: 4.0788e-05 - val_mae: 0.0051\n",
      "Epoch 726/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.3227e-05 - mae: 0.0029\n",
      "Epoch 726: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4656e-05 - mae: 0.0030 - val_loss: 4.0750e-05 - val_mae: 0.0051\n",
      "Epoch 727/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6187e-05 - mae: 0.0031\n",
      "Epoch 727: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7268e-05 - mae: 0.0033 - val_loss: 4.2209e-05 - val_mae: 0.0051\n",
      "Epoch 728/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.4894e-05 - mae: 0.0030\n",
      "Epoch 728: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.5161e-05 - mae: 0.0030 - val_loss: 4.5919e-05 - val_mae: 0.0053\n",
      "Epoch 729/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7025e-05 - mae: 0.0032\n",
      "Epoch 729: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7074e-05 - mae: 0.0033 - val_loss: 4.8540e-05 - val_mae: 0.0056\n",
      "Epoch 730/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9841e-05 - mae: 0.0036\n",
      "Epoch 730: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7677e-05 - mae: 0.0034 - val_loss: 4.6281e-05 - val_mae: 0.0055\n",
      "Epoch 731/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6767e-05 - mae: 0.0032\n",
      "Epoch 731: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.6557e-05 - mae: 0.0032 - val_loss: 4.0441e-05 - val_mae: 0.0051\n",
      "Epoch 732/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6886e-05 - mae: 0.0032\n",
      "Epoch 732: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5952e-05 - mae: 0.0031 - val_loss: 4.7911e-05 - val_mae: 0.0056\n",
      "Epoch 733/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8591e-05 - mae: 0.0034\n",
      "Epoch 733: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7896e-05 - mae: 0.0033 - val_loss: 4.3878e-05 - val_mae: 0.0053\n",
      "Epoch 734/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6477e-05 - mae: 0.0033\n",
      "Epoch 734: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6260e-05 - mae: 0.0032 - val_loss: 4.9806e-05 - val_mae: 0.0057\n",
      "Epoch 735/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.5984e-05 - mae: 0.0032\n",
      "Epoch 735: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6808e-05 - mae: 0.0033 - val_loss: 5.5769e-05 - val_mae: 0.0061\n",
      "Epoch 736/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7014e-05 - mae: 0.0033\n",
      "Epoch 736: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.6650e-05 - mae: 0.0033 - val_loss: 4.0343e-05 - val_mae: 0.0050\n",
      "Epoch 737/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4982e-05 - mae: 0.0030\n",
      "Epoch 737: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6306e-05 - mae: 0.0032 - val_loss: 4.0158e-05 - val_mae: 0.0051\n",
      "Epoch 738/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6620e-05 - mae: 0.0032\n",
      "Epoch 738: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5893e-05 - mae: 0.0032 - val_loss: 4.0387e-05 - val_mae: 0.0051\n",
      "Epoch 739/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3862e-05 - mae: 0.0029\n",
      "Epoch 739: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5867e-05 - mae: 0.0031 - val_loss: 4.0432e-05 - val_mae: 0.0051\n",
      "Epoch 740/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4709e-05 - mae: 0.0030\n",
      "Epoch 740: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4689e-05 - mae: 0.0030 - val_loss: 4.0536e-05 - val_mae: 0.0050\n",
      "Epoch 741/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4241e-05 - mae: 0.0030\n",
      "Epoch 741: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4823e-05 - mae: 0.0030 - val_loss: 4.1517e-05 - val_mae: 0.0050\n",
      "Epoch 742/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3735e-05 - mae: 0.0029\n",
      "Epoch 742: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4498e-05 - mae: 0.0030 - val_loss: 4.0402e-05 - val_mae: 0.0050\n",
      "Epoch 743/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4253e-05 - mae: 0.0029\n",
      "Epoch 743: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.4885e-05 - mae: 0.0030 - val_loss: 4.0596e-05 - val_mae: 0.0050\n",
      "Epoch 744/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3803e-05 - mae: 0.0029\n",
      "Epoch 744: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4616e-05 - mae: 0.0030 - val_loss: 4.0828e-05 - val_mae: 0.0051\n",
      "Epoch 745/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7629e-05 - mae: 0.0033\n",
      "Epoch 745: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7697e-05 - mae: 0.0033 - val_loss: 4.0602e-05 - val_mae: 0.0050\n",
      "Epoch 746/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6607e-05 - mae: 0.0032\n",
      "Epoch 746: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5732e-05 - mae: 0.0031 - val_loss: 4.0285e-05 - val_mae: 0.0050\n",
      "Epoch 747/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4453e-05 - mae: 0.0030\n",
      "Epoch 747: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5424e-05 - mae: 0.0031 - val_loss: 4.0082e-05 - val_mae: 0.0050\n",
      "Epoch 748/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6103e-05 - mae: 0.0031\n",
      "Epoch 748: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.4864e-05 - mae: 0.0030 - val_loss: 4.1294e-05 - val_mae: 0.0050\n",
      "Epoch 749/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3572e-05 - mae: 0.0029\n",
      "Epoch 749: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.4935e-05 - mae: 0.0031 - val_loss: 5.4388e-05 - val_mae: 0.0060\n",
      "Epoch 750/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1725e-05 - mae: 0.0038\n",
      "Epoch 750: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8571e-05 - mae: 0.0034 - val_loss: 4.0479e-05 - val_mae: 0.0051\n",
      "Epoch 751/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5701e-05 - mae: 0.0031\n",
      "Epoch 751: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5354e-05 - mae: 0.0031 - val_loss: 4.1432e-05 - val_mae: 0.0051\n",
      "Epoch 752/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.2899e-05 - mae: 0.0028\n",
      "Epoch 752: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.4302e-05 - mae: 0.0029 - val_loss: 4.0683e-05 - val_mae: 0.0051\n",
      "Epoch 753/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.3543e-05 - mae: 0.0029\n",
      "Epoch 753: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8284e-05 - mae: 0.0033 - val_loss: 5.4214e-05 - val_mae: 0.0060\n",
      "Epoch 754/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0932e-05 - mae: 0.0036\n",
      "Epoch 754: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.0474e-05 - mae: 0.0035 - val_loss: 4.8876e-05 - val_mae: 0.0057\n",
      "Epoch 755/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4826e-05 - mae: 0.0031\n",
      "Epoch 755: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5442e-05 - mae: 0.0031 - val_loss: 4.9178e-05 - val_mae: 0.0055\n",
      "Epoch 756/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5629e-05 - mae: 0.0031\n",
      "Epoch 756: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6789e-05 - mae: 0.0032 - val_loss: 4.0529e-05 - val_mae: 0.0051\n",
      "Epoch 757/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.5193e-05 - mae: 0.0030\n",
      "Epoch 757: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6593e-05 - mae: 0.0032 - val_loss: 4.0765e-05 - val_mae: 0.0051\n",
      "Epoch 758/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6129e-05 - mae: 0.0031\n",
      "Epoch 758: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.6591e-05 - mae: 0.0032 - val_loss: 4.1274e-05 - val_mae: 0.0052\n",
      "Epoch 759/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4998e-05 - mae: 0.0031\n",
      "Epoch 759: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.5938e-05 - mae: 0.0032 - val_loss: 4.1201e-05 - val_mae: 0.0050\n",
      "Epoch 760/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5753e-05 - mae: 0.0031\n",
      "Epoch 760: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5642e-05 - mae: 0.0031 - val_loss: 4.4088e-05 - val_mae: 0.0054\n",
      "Epoch 761/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5266e-05 - mae: 0.0030\n",
      "Epoch 761: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5623e-05 - mae: 0.0031 - val_loss: 4.1894e-05 - val_mae: 0.0051\n",
      "Epoch 762/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4964e-05 - mae: 0.0030\n",
      "Epoch 762: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4579e-05 - mae: 0.0030 - val_loss: 4.0844e-05 - val_mae: 0.0051\n",
      "Epoch 763/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4478e-05 - mae: 0.0030\n",
      "Epoch 763: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4749e-05 - mae: 0.0030 - val_loss: 4.0553e-05 - val_mae: 0.0051\n",
      "Epoch 764/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4838e-05 - mae: 0.0030\n",
      "Epoch 764: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.4683e-05 - mae: 0.0030 - val_loss: 4.0487e-05 - val_mae: 0.0051\n",
      "Epoch 765/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.3928e-05 - mae: 0.0029\n",
      "Epoch 765: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.4541e-05 - mae: 0.0030 - val_loss: 4.0469e-05 - val_mae: 0.0050\n",
      "Epoch 766/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5711e-05 - mae: 0.0030\n",
      "Epoch 766: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.4906e-05 - mae: 0.0030 - val_loss: 4.0369e-05 - val_mae: 0.0051\n",
      "Epoch 767/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3634e-05 - mae: 0.0029\n",
      "Epoch 767: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5356e-05 - mae: 0.0031 - val_loss: 4.0118e-05 - val_mae: 0.0051\n",
      "Epoch 768/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.3938e-05 - mae: 0.0029\n",
      "Epoch 768: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.4200e-05 - mae: 0.0029 - val_loss: 4.0401e-05 - val_mae: 0.0051\n",
      "Epoch 769/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5329e-05 - mae: 0.0031\n",
      "Epoch 769: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5585e-05 - mae: 0.0031 - val_loss: 4.1840e-05 - val_mae: 0.0051\n",
      "Epoch 770/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.4821e-05 - mae: 0.0030\n",
      "Epoch 770: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.4731e-05 - mae: 0.0030 - val_loss: 4.2441e-05 - val_mae: 0.0052\n",
      "Epoch 771/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6559e-05 - mae: 0.0032\n",
      "Epoch 771: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.6321e-05 - mae: 0.0032 - val_loss: 4.5522e-05 - val_mae: 0.0053\n",
      "Epoch 772/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8249e-05 - mae: 0.0033\n",
      "Epoch 772: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7169e-05 - mae: 0.0032 - val_loss: 4.0504e-05 - val_mae: 0.0050\n",
      "Epoch 773/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3934e-05 - mae: 0.0029\n",
      "Epoch 773: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4519e-05 - mae: 0.0029 - val_loss: 4.0308e-05 - val_mae: 0.0050\n",
      "Epoch 774/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3613e-05 - mae: 0.0029\n",
      "Epoch 774: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.4185e-05 - mae: 0.0029 - val_loss: 4.2086e-05 - val_mae: 0.0052\n",
      "Epoch 775/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4556e-05 - mae: 0.0030\n",
      "Epoch 775: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.4500e-05 - mae: 0.0030 - val_loss: 4.3925e-05 - val_mae: 0.0053\n",
      "Epoch 776/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4862e-05 - mae: 0.0030\n",
      "Epoch 776: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.6123e-05 - mae: 0.0031 - val_loss: 4.0866e-05 - val_mae: 0.0051\n",
      "Epoch 777/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5374e-05 - mae: 0.0030\n",
      "Epoch 777: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6182e-05 - mae: 0.0031 - val_loss: 4.1095e-05 - val_mae: 0.0051\n",
      "Epoch 778/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3939e-05 - mae: 0.0029\n",
      "Epoch 778: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.4985e-05 - mae: 0.0030 - val_loss: 4.2925e-05 - val_mae: 0.0053\n",
      "Epoch 779/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4209e-05 - mae: 0.0030\n",
      "Epoch 779: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.4882e-05 - mae: 0.0030 - val_loss: 4.1999e-05 - val_mae: 0.0052\n",
      "Epoch 780/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3121e-05 - mae: 0.0028\n",
      "Epoch 780: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4125e-05 - mae: 0.0029 - val_loss: 4.1935e-05 - val_mae: 0.0051\n",
      "Epoch 781/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4874e-05 - mae: 0.0030\n",
      "Epoch 781: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6648e-05 - mae: 0.0032 - val_loss: 4.1524e-05 - val_mae: 0.0051\n",
      "Epoch 782/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5654e-05 - mae: 0.0031\n",
      "Epoch 782: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5428e-05 - mae: 0.0031 - val_loss: 4.5312e-05 - val_mae: 0.0052\n",
      "Epoch 783/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6191e-05 - mae: 0.0032\n",
      "Epoch 783: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6240e-05 - mae: 0.0032 - val_loss: 4.6165e-05 - val_mae: 0.0055\n",
      "Epoch 784/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4292e-05 - mae: 0.0029\n",
      "Epoch 784: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.5659e-05 - mae: 0.0031 - val_loss: 4.1143e-05 - val_mae: 0.0051\n",
      "Epoch 785/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5059e-05 - mae: 0.0031\n",
      "Epoch 785: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.4583e-05 - mae: 0.0030 - val_loss: 4.3496e-05 - val_mae: 0.0053\n",
      "Epoch 786/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3772e-05 - mae: 0.0029\n",
      "Epoch 786: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.5050e-05 - mae: 0.0030 - val_loss: 4.3201e-05 - val_mae: 0.0053\n",
      "Epoch 787/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5902e-05 - mae: 0.0031\n",
      "Epoch 787: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.5739e-05 - mae: 0.0031 - val_loss: 4.5502e-05 - val_mae: 0.0054\n",
      "Epoch 788/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4230e-05 - mae: 0.0029\n",
      "Epoch 788: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6093e-05 - mae: 0.0031 - val_loss: 4.0739e-05 - val_mae: 0.0051\n",
      "Epoch 789/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6048e-05 - mae: 0.0031\n",
      "Epoch 789: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5442e-05 - mae: 0.0031 - val_loss: 4.1465e-05 - val_mae: 0.0051\n",
      "Epoch 790/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4558e-05 - mae: 0.0029\n",
      "Epoch 790: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5111e-05 - mae: 0.0030 - val_loss: 4.1243e-05 - val_mae: 0.0051\n",
      "Epoch 791/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5285e-05 - mae: 0.0031\n",
      "Epoch 791: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4636e-05 - mae: 0.0030 - val_loss: 4.0946e-05 - val_mae: 0.0051\n",
      "Epoch 792/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4255e-05 - mae: 0.0029\n",
      "Epoch 792: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5460e-05 - mae: 0.0031 - val_loss: 4.3387e-05 - val_mae: 0.0051\n",
      "Epoch 793/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5070e-05 - mae: 0.0031\n",
      "Epoch 793: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6086e-05 - mae: 0.0032 - val_loss: 4.0910e-05 - val_mae: 0.0051\n",
      "Epoch 794/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7154e-05 - mae: 0.0032\n",
      "Epoch 794: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5844e-05 - mae: 0.0031 - val_loss: 4.2682e-05 - val_mae: 0.0053\n",
      "Epoch 795/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3251e-05 - mae: 0.0028\n",
      "Epoch 795: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.4536e-05 - mae: 0.0030 - val_loss: 4.7683e-05 - val_mae: 0.0056\n",
      "Epoch 796/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4772e-05 - mae: 0.0030\n",
      "Epoch 796: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.5017e-05 - mae: 0.0031 - val_loss: 4.3307e-05 - val_mae: 0.0053\n",
      "Epoch 797/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5797e-05 - mae: 0.0031\n",
      "Epoch 797: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.5473e-05 - mae: 0.0031 - val_loss: 4.0886e-05 - val_mae: 0.0051\n",
      "Epoch 798/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.4759e-05 - mae: 0.0030\n",
      "Epoch 798: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.5006e-05 - mae: 0.0031 - val_loss: 4.3743e-05 - val_mae: 0.0052\n",
      "Epoch 799/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5387e-05 - mae: 0.0031\n",
      "Epoch 799: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.5560e-05 - mae: 0.0031 - val_loss: 4.1436e-05 - val_mae: 0.0052\n",
      "Epoch 800/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.6930e-05 - mae: 0.0032\n",
      "Epoch 800: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.6430e-05 - mae: 0.0032 - val_loss: 4.1157e-05 - val_mae: 0.0051\n",
      "Epoch 801/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6434e-05 - mae: 0.0033\n",
      "Epoch 801: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5720e-05 - mae: 0.0031 - val_loss: 4.1372e-05 - val_mae: 0.0052\n",
      "Epoch 802/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4281e-05 - mae: 0.0029\n",
      "Epoch 802: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.5627e-05 - mae: 0.0031 - val_loss: 4.1165e-05 - val_mae: 0.0051\n",
      "Epoch 803/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5616e-05 - mae: 0.0031\n",
      "Epoch 803: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5389e-05 - mae: 0.0031 - val_loss: 4.0975e-05 - val_mae: 0.0051\n",
      "Epoch 804/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5771e-05 - mae: 0.0031\n",
      "Epoch 804: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6590e-05 - mae: 0.0032 - val_loss: 4.5297e-05 - val_mae: 0.0052\n",
      "Epoch 805/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6025e-05 - mae: 0.0031\n",
      "Epoch 805: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.6077e-05 - mae: 0.0031 - val_loss: 4.1195e-05 - val_mae: 0.0051\n",
      "Epoch 806/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.4857e-05 - mae: 0.0030\n",
      "Epoch 806: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5179e-05 - mae: 0.0030 - val_loss: 4.7833e-05 - val_mae: 0.0056\n",
      "Epoch 807/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.1911e-05 - mae: 0.0027\n",
      "Epoch 807: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4023e-05 - mae: 0.0029 - val_loss: 4.5158e-05 - val_mae: 0.0054\n",
      "Epoch 808/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.5067e-05 - mae: 0.0030\n",
      "Epoch 808: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5496e-05 - mae: 0.0031 - val_loss: 4.2977e-05 - val_mae: 0.0051\n",
      "Epoch 809/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.5119e-05 - mae: 0.0031\n",
      "Epoch 809: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.5148e-05 - mae: 0.0031 - val_loss: 4.0717e-05 - val_mae: 0.0051\n",
      "Epoch 810/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4026e-05 - mae: 0.0029\n",
      "Epoch 810: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4955e-05 - mae: 0.0030 - val_loss: 4.1144e-05 - val_mae: 0.0052\n",
      "Epoch 811/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6150e-05 - mae: 0.0031\n",
      "Epoch 811: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6780e-05 - mae: 0.0032 - val_loss: 4.1264e-05 - val_mae: 0.0052\n",
      "Epoch 812/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6020e-05 - mae: 0.0032\n",
      "Epoch 812: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5223e-05 - mae: 0.0031 - val_loss: 4.1750e-05 - val_mae: 0.0052\n",
      "Epoch 813/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5893e-05 - mae: 0.0031\n",
      "Epoch 813: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9479e-05 - mae: 0.0035 - val_loss: 4.6332e-05 - val_mae: 0.0055\n",
      "Epoch 814/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6929e-05 - mae: 0.0033\n",
      "Epoch 814: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.6266e-05 - mae: 0.0032 - val_loss: 4.1514e-05 - val_mae: 0.0051\n",
      "Epoch 815/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3844e-05 - mae: 0.0029\n",
      "Epoch 815: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.3848e-05 - mae: 0.0029 - val_loss: 4.1372e-05 - val_mae: 0.0051\n",
      "Epoch 816/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3438e-05 - mae: 0.0029\n",
      "Epoch 816: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6176e-05 - mae: 0.0031 - val_loss: 4.7116e-05 - val_mae: 0.0055\n",
      "Epoch 817/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8491e-05 - mae: 0.0034\n",
      "Epoch 817: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8569e-05 - mae: 0.0033 - val_loss: 4.5924e-05 - val_mae: 0.0053\n",
      "Epoch 818/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7517e-05 - mae: 0.0034\n",
      "Epoch 818: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6613e-05 - mae: 0.0032 - val_loss: 4.2251e-05 - val_mae: 0.0052\n",
      "Epoch 819/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5847e-05 - mae: 0.0031\n",
      "Epoch 819: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5207e-05 - mae: 0.0030 - val_loss: 4.3772e-05 - val_mae: 0.0053\n",
      "Epoch 820/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7069e-05 - mae: 0.0032\n",
      "Epoch 820: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.6439e-05 - mae: 0.0032 - val_loss: 4.4553e-05 - val_mae: 0.0052\n",
      "Epoch 821/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4638e-05 - mae: 0.0030\n",
      "Epoch 821: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4496e-05 - mae: 0.0030 - val_loss: 4.3712e-05 - val_mae: 0.0053\n",
      "Epoch 822/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4982e-05 - mae: 0.0031\n",
      "Epoch 822: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5919e-05 - mae: 0.0032 - val_loss: 4.0581e-05 - val_mae: 0.0051\n",
      "Epoch 823/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.8719e-05 - mae: 0.0034\n",
      "Epoch 823: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6942e-05 - mae: 0.0033 - val_loss: 4.0174e-05 - val_mae: 0.0050\n",
      "Epoch 824/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4255e-05 - mae: 0.0029\n",
      "Epoch 824: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5614e-05 - mae: 0.0031 - val_loss: 5.2673e-05 - val_mae: 0.0057\n",
      "Epoch 825/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4428e-05 - mae: 0.0030\n",
      "Epoch 825: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.5889e-05 - mae: 0.0031 - val_loss: 4.8926e-05 - val_mae: 0.0057\n",
      "Epoch 826/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4529e-05 - mae: 0.0030\n",
      "Epoch 826: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.4973e-05 - mae: 0.0030 - val_loss: 4.2191e-05 - val_mae: 0.0051\n",
      "Epoch 827/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.5845e-05 - mae: 0.0031\n",
      "Epoch 827: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.4820e-05 - mae: 0.0030 - val_loss: 4.0207e-05 - val_mae: 0.0050\n",
      "Epoch 828/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.3272e-05 - mae: 0.0029\n",
      "Epoch 828: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.4750e-05 - mae: 0.0030 - val_loss: 4.0929e-05 - val_mae: 0.0050\n",
      "Epoch 829/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4393e-05 - mae: 0.0030\n",
      "Epoch 829: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.4319e-05 - mae: 0.0030 - val_loss: 4.1019e-05 - val_mae: 0.0051\n",
      "Epoch 830/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.4868e-05 - mae: 0.0030\n",
      "Epoch 830: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.5715e-05 - mae: 0.0031 - val_loss: 4.0418e-05 - val_mae: 0.0051\n",
      "Epoch 831/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3998e-05 - mae: 0.0029\n",
      "Epoch 831: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4919e-05 - mae: 0.0031 - val_loss: 4.4714e-05 - val_mae: 0.0054\n",
      "Epoch 832/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5376e-05 - mae: 0.0031\n",
      "Epoch 832: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5764e-05 - mae: 0.0031 - val_loss: 4.1538e-05 - val_mae: 0.0052\n",
      "Epoch 833/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6071e-05 - mae: 0.0031\n",
      "Epoch 833: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5653e-05 - mae: 0.0031 - val_loss: 5.4838e-05 - val_mae: 0.0060\n",
      "Epoch 834/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/800 [================>.............] - ETA: 0s - loss: 1.6562e-05 - mae: 0.0032\n",
      "Epoch 834: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5930e-05 - mae: 0.0031 - val_loss: 5.2078e-05 - val_mae: 0.0058\n",
      "Epoch 835/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.3998e-05 - mae: 0.0029\n",
      "Epoch 835: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.5556e-05 - mae: 0.0031 - val_loss: 4.0683e-05 - val_mae: 0.0051\n",
      "Epoch 836/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4259e-05 - mae: 0.0030\n",
      "Epoch 836: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.4864e-05 - mae: 0.0030 - val_loss: 4.0521e-05 - val_mae: 0.0051\n",
      "Epoch 837/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.4588e-05 - mae: 0.0030\n",
      "Epoch 837: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.4698e-05 - mae: 0.0030 - val_loss: 4.2255e-05 - val_mae: 0.0052\n",
      "Epoch 838/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7141e-05 - mae: 0.0032\n",
      "Epoch 838: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.7141e-05 - mae: 0.0032 - val_loss: 3.9892e-05 - val_mae: 0.0050\n",
      "Epoch 839/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4636e-05 - mae: 0.0030\n",
      "Epoch 839: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4510e-05 - mae: 0.0030 - val_loss: 4.4688e-05 - val_mae: 0.0054\n",
      "Epoch 840/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4699e-05 - mae: 0.0030\n",
      "Epoch 840: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.5568e-05 - mae: 0.0031 - val_loss: 5.5850e-05 - val_mae: 0.0061\n",
      "Epoch 841/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8943e-05 - mae: 0.0034\n",
      "Epoch 841: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7343e-05 - mae: 0.0032 - val_loss: 4.6734e-05 - val_mae: 0.0055\n",
      "Epoch 842/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7239e-05 - mae: 0.0033\n",
      "Epoch 842: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7167e-05 - mae: 0.0032 - val_loss: 4.2467e-05 - val_mae: 0.0052\n",
      "Epoch 843/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4441e-05 - mae: 0.0029\n",
      "Epoch 843: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.5519e-05 - mae: 0.0031 - val_loss: 4.2243e-05 - val_mae: 0.0052\n",
      "Epoch 844/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4026e-05 - mae: 0.0029\n",
      "Epoch 844: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.5296e-05 - mae: 0.0031 - val_loss: 4.1569e-05 - val_mae: 0.0051\n",
      "Epoch 845/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6043e-05 - mae: 0.0031\n",
      "Epoch 845: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.6059e-05 - mae: 0.0031 - val_loss: 4.0751e-05 - val_mae: 0.0051\n",
      "Epoch 846/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4851e-05 - mae: 0.0031\n",
      "Epoch 846: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.4756e-05 - mae: 0.0030 - val_loss: 4.2134e-05 - val_mae: 0.0051\n",
      "Epoch 847/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3658e-05 - mae: 0.0029\n",
      "Epoch 847: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4718e-05 - mae: 0.0030 - val_loss: 4.2227e-05 - val_mae: 0.0051\n",
      "Epoch 848/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.4742e-05 - mae: 0.0030\n",
      "Epoch 848: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5370e-05 - mae: 0.0031 - val_loss: 4.1046e-05 - val_mae: 0.0051\n",
      "Epoch 849/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.4715e-05 - mae: 0.0030\n",
      "Epoch 849: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.5116e-05 - mae: 0.0030 - val_loss: 4.0835e-05 - val_mae: 0.0051\n",
      "Epoch 850/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4026e-05 - mae: 0.0030\n",
      "Epoch 850: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5220e-05 - mae: 0.0031 - val_loss: 4.3599e-05 - val_mae: 0.0053\n",
      "Epoch 851/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.4666e-05 - mae: 0.0030\n",
      "Epoch 851: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.4366e-05 - mae: 0.0030 - val_loss: 4.2619e-05 - val_mae: 0.0051\n",
      "Epoch 852/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.3864e-05 - mae: 0.0029\n",
      "Epoch 852: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.4216e-05 - mae: 0.0029 - val_loss: 4.7182e-05 - val_mae: 0.0055\n",
      "Epoch 853/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.6427e-05 - mae: 0.0033\n",
      "Epoch 853: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.6612e-05 - mae: 0.0033 - val_loss: 4.4807e-05 - val_mae: 0.0054\n",
      "Epoch 854/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.5391e-05 - mae: 0.0030\n",
      "Epoch 854: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.6689e-05 - mae: 0.0032 - val_loss: 4.2897e-05 - val_mae: 0.0053\n",
      "Epoch 855/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.4679e-05 - mae: 0.0030\n",
      "Epoch 855: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.5012e-05 - mae: 0.0030 - val_loss: 4.3428e-05 - val_mae: 0.0053\n",
      "Epoch 856/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5066e-05 - mae: 0.0031\n",
      "Epoch 856: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5162e-05 - mae: 0.0031 - val_loss: 4.5558e-05 - val_mae: 0.0054\n",
      "Epoch 857/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5008e-05 - mae: 0.0030\n",
      "Epoch 857: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.5797e-05 - mae: 0.0031 - val_loss: 4.4076e-05 - val_mae: 0.0053\n",
      "Epoch 858/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6093e-05 - mae: 0.0032\n",
      "Epoch 858: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5431e-05 - mae: 0.0031 - val_loss: 4.0853e-05 - val_mae: 0.0051\n",
      "Epoch 859/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5650e-05 - mae: 0.0031    \n",
      "Epoch 859: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4718e-05 - mae: 0.0030 - val_loss: 4.2647e-05 - val_mae: 0.0051\n",
      "Epoch 860/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6218e-05 - mae: 0.0032\n",
      "Epoch 860: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.4951e-05 - mae: 0.0030 - val_loss: 4.1188e-05 - val_mae: 0.0051\n",
      "Epoch 861/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.4352e-05 - mae: 0.0029\n",
      "Epoch 861: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.4297e-05 - mae: 0.0029 - val_loss: 4.1305e-05 - val_mae: 0.0051\n",
      "Epoch 862/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7950e-05 - mae: 0.0033\n",
      "Epoch 862: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.6299e-05 - mae: 0.0032 - val_loss: 4.2556e-05 - val_mae: 0.0051\n",
      "Epoch 863/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4416e-05 - mae: 0.0030\n",
      "Epoch 863: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5157e-05 - mae: 0.0030 - val_loss: 4.1989e-05 - val_mae: 0.0051\n",
      "Epoch 864/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4038e-05 - mae: 0.0029\n",
      "Epoch 864: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.4237e-05 - mae: 0.0030 - val_loss: 4.2158e-05 - val_mae: 0.0051\n",
      "Epoch 865/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.4670e-05 - mae: 0.0030\n",
      "Epoch 865: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.4670e-05 - mae: 0.0030 - val_loss: 4.0946e-05 - val_mae: 0.0051\n",
      "Epoch 866/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.5077e-05 - mae: 0.0031\n",
      "Epoch 866: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.6238e-05 - mae: 0.0032 - val_loss: 4.3019e-05 - val_mae: 0.0053\n",
      "Epoch 867/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5263e-05 - mae: 0.0030\n",
      "Epoch 867: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4706e-05 - mae: 0.0030 - val_loss: 4.2792e-05 - val_mae: 0.0051\n",
      "Epoch 868/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3081e-05 - mae: 0.0028\n",
      "Epoch 868: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4543e-05 - mae: 0.0030 - val_loss: 4.1599e-05 - val_mae: 0.0052\n",
      "Epoch 869/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.5472e-05 - mae: 0.0031\n",
      "Epoch 869: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.5472e-05 - mae: 0.0031 - val_loss: 4.1245e-05 - val_mae: 0.0051\n",
      "Epoch 870/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.4256e-05 - mae: 0.0030\n",
      "Epoch 870: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5137e-05 - mae: 0.0030 - val_loss: 4.1114e-05 - val_mae: 0.0051\n",
      "Epoch 871/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4345e-05 - mae: 0.0030\n",
      "Epoch 871: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.4991e-05 - mae: 0.0030 - val_loss: 4.2532e-05 - val_mae: 0.0053\n",
      "Epoch 872/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6152e-05 - mae: 0.0032\n",
      "Epoch 872: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.6172e-05 - mae: 0.0032 - val_loss: 4.7161e-05 - val_mae: 0.0055\n",
      "Epoch 873/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.5363e-05 - mae: 0.0031\n",
      "Epoch 873: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.5057e-05 - mae: 0.0030 - val_loss: 4.5384e-05 - val_mae: 0.0054\n",
      "Epoch 874/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4935e-05 - mae: 0.0030\n",
      "Epoch 874: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.4866e-05 - mae: 0.0030 - val_loss: 4.1114e-05 - val_mae: 0.0051\n",
      "Epoch 875/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5357e-05 - mae: 0.0031\n",
      "Epoch 875: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4364e-05 - mae: 0.0030 - val_loss: 4.1404e-05 - val_mae: 0.0052\n",
      "Epoch 876/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4415e-05 - mae: 0.0030\n",
      "Epoch 876: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5457e-05 - mae: 0.0031 - val_loss: 4.4910e-05 - val_mae: 0.0052\n",
      "Epoch 877/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7367e-05 - mae: 0.0033\n",
      "Epoch 877: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5934e-05 - mae: 0.0032 - val_loss: 4.2711e-05 - val_mae: 0.0053\n",
      "Epoch 878/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5399e-05 - mae: 0.0031\n",
      "Epoch 878: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6488e-05 - mae: 0.0032 - val_loss: 4.4754e-05 - val_mae: 0.0054\n",
      "Epoch 879/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4484e-05 - mae: 0.0031\n",
      "Epoch 879: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.5614e-05 - mae: 0.0031 - val_loss: 4.6688e-05 - val_mae: 0.0055\n",
      "Epoch 880/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6757e-05 - mae: 0.0033\n",
      "Epoch 880: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7013e-05 - mae: 0.0033 - val_loss: 4.0992e-05 - val_mae: 0.0051\n",
      "Epoch 881/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3968e-05 - mae: 0.0030\n",
      "Epoch 881: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.4707e-05 - mae: 0.0030 - val_loss: 4.1353e-05 - val_mae: 0.0051\n",
      "Epoch 882/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4346e-05 - mae: 0.0030\n",
      "Epoch 882: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.4760e-05 - mae: 0.0030 - val_loss: 4.1225e-05 - val_mae: 0.0051\n",
      "Epoch 883/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5544e-05 - mae: 0.0031\n",
      "Epoch 883: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.4919e-05 - mae: 0.0030 - val_loss: 4.8445e-05 - val_mae: 0.0054\n",
      "Epoch 884/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9715e-05 - mae: 0.0035\n",
      "Epoch 884: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7925e-05 - mae: 0.0033 - val_loss: 4.1281e-05 - val_mae: 0.0051\n",
      "Epoch 885/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6290e-05 - mae: 0.0032\n",
      "Epoch 885: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.6533e-05 - mae: 0.0032 - val_loss: 4.5112e-05 - val_mae: 0.0054\n",
      "Epoch 886/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.2813e-05 - mae: 0.0028\n",
      "Epoch 886: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.4748e-05 - mae: 0.0030 - val_loss: 4.2080e-05 - val_mae: 0.0052\n",
      "Epoch 887/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5651e-05 - mae: 0.0031\n",
      "Epoch 887: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.4553e-05 - mae: 0.0030 - val_loss: 4.0987e-05 - val_mae: 0.0051\n",
      "Epoch 888/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4767e-05 - mae: 0.0030\n",
      "Epoch 888: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.4761e-05 - mae: 0.0030 - val_loss: 4.1555e-05 - val_mae: 0.0052\n",
      "Epoch 889/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3663e-05 - mae: 0.0029\n",
      "Epoch 889: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4349e-05 - mae: 0.0029 - val_loss: 4.1291e-05 - val_mae: 0.0051\n",
      "Epoch 890/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4869e-05 - mae: 0.0030\n",
      "Epoch 890: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4859e-05 - mae: 0.0030 - val_loss: 4.2825e-05 - val_mae: 0.0053\n",
      "Epoch 891/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5396e-05 - mae: 0.0031\n",
      "Epoch 891: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.4419e-05 - mae: 0.0030 - val_loss: 4.5830e-05 - val_mae: 0.0055\n",
      "Epoch 892/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/800 [================>.............] - ETA: 0s - loss: 1.6131e-05 - mae: 0.0032\n",
      "Epoch 892: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7837e-05 - mae: 0.0034 - val_loss: 4.9385e-05 - val_mae: 0.0055\n",
      "Epoch 893/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5734e-05 - mae: 0.0031\n",
      "Epoch 893: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.5610e-05 - mae: 0.0031 - val_loss: 4.0749e-05 - val_mae: 0.0051\n",
      "Epoch 894/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4560e-05 - mae: 0.0030\n",
      "Epoch 894: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.3966e-05 - mae: 0.0029 - val_loss: 4.6361e-05 - val_mae: 0.0053\n",
      "Epoch 895/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6259e-05 - mae: 0.0032\n",
      "Epoch 895: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6055e-05 - mae: 0.0031 - val_loss: 4.3908e-05 - val_mae: 0.0052\n",
      "Epoch 896/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5447e-05 - mae: 0.0030\n",
      "Epoch 896: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5109e-05 - mae: 0.0030 - val_loss: 4.2830e-05 - val_mae: 0.0051\n",
      "Epoch 897/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4060e-05 - mae: 0.0029\n",
      "Epoch 897: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4457e-05 - mae: 0.0029 - val_loss: 4.0847e-05 - val_mae: 0.0051\n",
      "Epoch 898/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3281e-05 - mae: 0.0028\n",
      "Epoch 898: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4332e-05 - mae: 0.0029 - val_loss: 4.1189e-05 - val_mae: 0.0051\n",
      "Epoch 899/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3168e-05 - mae: 0.0029\n",
      "Epoch 899: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4147e-05 - mae: 0.0030 - val_loss: 4.1134e-05 - val_mae: 0.0051\n",
      "Epoch 900/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7081e-05 - mae: 0.0032\n",
      "Epoch 900: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5795e-05 - mae: 0.0031 - val_loss: 4.2303e-05 - val_mae: 0.0052\n",
      "Epoch 901/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4279e-05 - mae: 0.0029\n",
      "Epoch 901: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.5838e-05 - mae: 0.0031 - val_loss: 4.3294e-05 - val_mae: 0.0053\n",
      "Epoch 902/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7075e-05 - mae: 0.0033\n",
      "Epoch 902: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7014e-05 - mae: 0.0032 - val_loss: 4.1657e-05 - val_mae: 0.0051\n",
      "Epoch 903/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4507e-05 - mae: 0.0030\n",
      "Epoch 903: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4855e-05 - mae: 0.0030 - val_loss: 4.2092e-05 - val_mae: 0.0052\n",
      "Epoch 904/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.5630e-05 - mae: 0.0031\n",
      "Epoch 904: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.5630e-05 - mae: 0.0031 - val_loss: 4.5555e-05 - val_mae: 0.0054\n",
      "Epoch 905/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8061e-05 - mae: 0.0034\n",
      "Epoch 905: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8806e-05 - mae: 0.0034 - val_loss: 4.3303e-05 - val_mae: 0.0051\n",
      "Epoch 906/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7588e-05 - mae: 0.0033\n",
      "Epoch 906: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6480e-05 - mae: 0.0032 - val_loss: 4.2539e-05 - val_mae: 0.0051\n",
      "Epoch 907/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5897e-05 - mae: 0.0032\n",
      "Epoch 907: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.5926e-05 - mae: 0.0031 - val_loss: 4.1630e-05 - val_mae: 0.0052\n",
      "Epoch 908/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.4542e-05 - mae: 0.0030\n",
      "Epoch 908: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.4759e-05 - mae: 0.0030 - val_loss: 4.1391e-05 - val_mae: 0.0051\n",
      "Epoch 909/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.4560e-05 - mae: 0.0029\n",
      "Epoch 909: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.4753e-05 - mae: 0.0030 - val_loss: 4.1103e-05 - val_mae: 0.0051\n",
      "Epoch 910/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4480e-05 - mae: 0.0030\n",
      "Epoch 910: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.4840e-05 - mae: 0.0030 - val_loss: 4.1194e-05 - val_mae: 0.0051\n",
      "Epoch 911/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.4384e-05 - mae: 0.0030\n",
      "Epoch 911: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.4313e-05 - mae: 0.0030 - val_loss: 4.2977e-05 - val_mae: 0.0053\n",
      "Epoch 912/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.5184e-05 - mae: 0.0030\n",
      "Epoch 912: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.5184e-05 - mae: 0.0030 - val_loss: 4.2901e-05 - val_mae: 0.0051\n",
      "Epoch 913/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.5480e-05 - mae: 0.0031\n",
      "Epoch 913: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.5416e-05 - mae: 0.0031 - val_loss: 4.0891e-05 - val_mae: 0.0051\n",
      "Epoch 914/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4201e-05 - mae: 0.0030\n",
      "Epoch 914: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.5738e-05 - mae: 0.0031 - val_loss: 4.7809e-05 - val_mae: 0.0056\n",
      "Epoch 915/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4410e-05 - mae: 0.0029\n",
      "Epoch 915: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.4894e-05 - mae: 0.0030 - val_loss: 4.0797e-05 - val_mae: 0.0051\n",
      "Epoch 916/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3634e-05 - mae: 0.0028\n",
      "Epoch 916: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4502e-05 - mae: 0.0029 - val_loss: 4.6226e-05 - val_mae: 0.0055\n",
      "Epoch 917/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3770e-05 - mae: 0.0029\n",
      "Epoch 917: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4412e-05 - mae: 0.0030 - val_loss: 4.5802e-05 - val_mae: 0.0055\n",
      "Epoch 918/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.5858e-05 - mae: 0.0031\n",
      "Epoch 918: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6021e-05 - mae: 0.0031 - val_loss: 4.3837e-05 - val_mae: 0.0052\n",
      "Epoch 919/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6462e-05 - mae: 0.0033\n",
      "Epoch 919: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.5903e-05 - mae: 0.0032 - val_loss: 4.1463e-05 - val_mae: 0.0052\n",
      "Epoch 920/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.5148e-05 - mae: 0.0030\n",
      "Epoch 920: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.5204e-05 - mae: 0.0030 - val_loss: 4.1811e-05 - val_mae: 0.0051\n",
      "Epoch 921/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4597e-05 - mae: 0.0029\n",
      "Epoch 921: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5052e-05 - mae: 0.0030 - val_loss: 4.1165e-05 - val_mae: 0.0051\n",
      "Epoch 922/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.4827e-05 - mae: 0.0030\n",
      "Epoch 922: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.4711e-05 - mae: 0.0030 - val_loss: 4.1950e-05 - val_mae: 0.0051\n",
      "Epoch 923/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4834e-05 - mae: 0.0030\n",
      "Epoch 923: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.4462e-05 - mae: 0.0029 - val_loss: 4.4747e-05 - val_mae: 0.0054\n",
      "Epoch 924/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3492e-05 - mae: 0.0028\n",
      "Epoch 924: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4567e-05 - mae: 0.0029 - val_loss: 4.3198e-05 - val_mae: 0.0053\n",
      "Epoch 925/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6115e-05 - mae: 0.0031\n",
      "Epoch 925: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5079e-05 - mae: 0.0030 - val_loss: 4.4318e-05 - val_mae: 0.0054\n",
      "Epoch 926/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5838e-05 - mae: 0.0031\n",
      "Epoch 926: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5316e-05 - mae: 0.0031 - val_loss: 4.1229e-05 - val_mae: 0.0051\n",
      "Epoch 927/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.3642e-05 - mae: 0.0029\n",
      "Epoch 927: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4052e-05 - mae: 0.0029 - val_loss: 4.1108e-05 - val_mae: 0.0051\n",
      "Epoch 928/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5055e-05 - mae: 0.0030\n",
      "Epoch 928: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.4267e-05 - mae: 0.0029 - val_loss: 4.1936e-05 - val_mae: 0.0052\n",
      "Epoch 929/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6469e-05 - mae: 0.0032\n",
      "Epoch 929: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.5865e-05 - mae: 0.0031 - val_loss: 4.1071e-05 - val_mae: 0.0051\n",
      "Epoch 930/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3012e-05 - mae: 0.0029\n",
      "Epoch 930: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.4729e-05 - mae: 0.0030 - val_loss: 4.1170e-05 - val_mae: 0.0051\n",
      "Epoch 931/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4557e-05 - mae: 0.0029\n",
      "Epoch 931: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.4534e-05 - mae: 0.0029 - val_loss: 4.4257e-05 - val_mae: 0.0054\n",
      "Epoch 932/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8004e-05 - mae: 0.0034\n",
      "Epoch 932: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6745e-05 - mae: 0.0032 - val_loss: 4.2076e-05 - val_mae: 0.0052\n",
      "Epoch 933/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4578e-05 - mae: 0.0030\n",
      "Epoch 933: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4647e-05 - mae: 0.0030 - val_loss: 4.5435e-05 - val_mae: 0.0052\n",
      "Epoch 934/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3189e-05 - mae: 0.0029\n",
      "Epoch 934: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.4388e-05 - mae: 0.0030 - val_loss: 4.2027e-05 - val_mae: 0.0052\n",
      "Epoch 935/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.1977e-05 - mae: 0.0026\n",
      "Epoch 935: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.3949e-05 - mae: 0.0029 - val_loss: 4.4799e-05 - val_mae: 0.0054\n",
      "Epoch 936/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5412e-05 - mae: 0.0031\n",
      "Epoch 936: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5338e-05 - mae: 0.0031 - val_loss: 4.3107e-05 - val_mae: 0.0051\n",
      "Epoch 937/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.6202e-05 - mae: 0.0032\n",
      "Epoch 937: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.6315e-05 - mae: 0.0032 - val_loss: 4.5005e-05 - val_mae: 0.0052\n",
      "Epoch 938/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4784e-05 - mae: 0.0031\n",
      "Epoch 938: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.5451e-05 - mae: 0.0032 - val_loss: 4.1046e-05 - val_mae: 0.0051\n",
      "Epoch 939/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.4730e-05 - mae: 0.0030\n",
      "Epoch 939: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.4969e-05 - mae: 0.0031 - val_loss: 4.6363e-05 - val_mae: 0.0053\n",
      "Epoch 940/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.4943e-05 - mae: 0.0030\n",
      "Epoch 940: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.4765e-05 - mae: 0.0030 - val_loss: 4.1254e-05 - val_mae: 0.0051\n",
      "Epoch 941/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.3409e-05 - mae: 0.0029\n",
      "Epoch 941: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.3446e-05 - mae: 0.0028 - val_loss: 4.1138e-05 - val_mae: 0.0051\n",
      "Epoch 942/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6412e-05 - mae: 0.0031\n",
      "Epoch 942: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.6329e-05 - mae: 0.0031 - val_loss: 4.5071e-05 - val_mae: 0.0052\n",
      "Epoch 943/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.4832e-05 - mae: 0.0030\n",
      "Epoch 943: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.4832e-05 - mae: 0.0030 - val_loss: 4.1420e-05 - val_mae: 0.0051\n",
      "Epoch 944/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.5428e-05 - mae: 0.0031\n",
      "Epoch 944: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.5077e-05 - mae: 0.0030 - val_loss: 4.1425e-05 - val_mae: 0.0052\n",
      "Epoch 945/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4519e-05 - mae: 0.0030\n",
      "Epoch 945: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.5106e-05 - mae: 0.0030 - val_loss: 4.1725e-05 - val_mae: 0.0052\n",
      "Epoch 946/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.5822e-05 - mae: 0.0031\n",
      "Epoch 946: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.5822e-05 - mae: 0.0031 - val_loss: 4.2691e-05 - val_mae: 0.0053\n",
      "Epoch 947/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.4323e-05 - mae: 0.0029\n",
      "Epoch 947: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.4094e-05 - mae: 0.0029 - val_loss: 4.1588e-05 - val_mae: 0.0052\n",
      "Epoch 948/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.4742e-05 - mae: 0.0030\n",
      "Epoch 948: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.4909e-05 - mae: 0.0030 - val_loss: 4.1846e-05 - val_mae: 0.0051\n",
      "Epoch 949/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.4974e-05 - mae: 0.0030\n",
      "Epoch 949: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.4160e-05 - mae: 0.0029 - val_loss: 4.1214e-05 - val_mae: 0.0051\n",
      "Epoch 950/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6341e-05 - mae: 0.0032\n",
      "Epoch 950: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.5940e-05 - mae: 0.0031 - val_loss: 4.2505e-05 - val_mae: 0.0051\n",
      "Epoch 951/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5176e-05 - mae: 0.0030\n",
      "Epoch 951: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4667e-05 - mae: 0.0030 - val_loss: 4.5073e-05 - val_mae: 0.0054\n",
      "Epoch 952/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4311e-05 - mae: 0.0030\n",
      "Epoch 952: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.5673e-05 - mae: 0.0031 - val_loss: 4.3083e-05 - val_mae: 0.0052\n",
      "Epoch 953/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5679e-05 - mae: 0.0032\n",
      "Epoch 953: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6904e-05 - mae: 0.0033 - val_loss: 4.4100e-05 - val_mae: 0.0052\n",
      "Epoch 954/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5056e-05 - mae: 0.0030\n",
      "Epoch 954: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.6428e-05 - mae: 0.0032 - val_loss: 4.7329e-05 - val_mae: 0.0055\n",
      "Epoch 955/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4265e-05 - mae: 0.0030\n",
      "Epoch 955: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.4749e-05 - mae: 0.0030 - val_loss: 5.2929e-05 - val_mae: 0.0059\n",
      "Epoch 956/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.3851e-05 - mae: 0.0029\n",
      "Epoch 956: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.5528e-05 - mae: 0.0030 - val_loss: 4.1422e-05 - val_mae: 0.0051\n",
      "Epoch 957/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4077e-05 - mae: 0.0029\n",
      "Epoch 957: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4722e-05 - mae: 0.0030 - val_loss: 4.2080e-05 - val_mae: 0.0051\n",
      "Epoch 958/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3984e-05 - mae: 0.0029\n",
      "Epoch 958: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4488e-05 - mae: 0.0030 - val_loss: 4.1516e-05 - val_mae: 0.0052\n",
      "Epoch 959/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.3249e-05 - mae: 0.0029\n",
      "Epoch 959: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4942e-05 - mae: 0.0030 - val_loss: 4.1655e-05 - val_mae: 0.0051\n",
      "Epoch 960/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3640e-05 - mae: 0.0029\n",
      "Epoch 960: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4992e-05 - mae: 0.0030 - val_loss: 4.0867e-05 - val_mae: 0.0051\n",
      "Epoch 961/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4721e-05 - mae: 0.0030\n",
      "Epoch 961: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.4171e-05 - mae: 0.0029 - val_loss: 4.2539e-05 - val_mae: 0.0051\n",
      "Epoch 962/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5598e-05 - mae: 0.0031\n",
      "Epoch 962: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4526e-05 - mae: 0.0029 - val_loss: 4.0908e-05 - val_mae: 0.0051\n",
      "Epoch 963/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.4223e-05 - mae: 0.0030\n",
      "Epoch 963: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4744e-05 - mae: 0.0030 - val_loss: 4.0849e-05 - val_mae: 0.0051\n",
      "Epoch 964/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5137e-05 - mae: 0.0030\n",
      "Epoch 964: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.4959e-05 - mae: 0.0030 - val_loss: 4.0740e-05 - val_mae: 0.0051\n",
      "Epoch 965/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.4010e-05 - mae: 0.0029\n",
      "Epoch 965: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.4460e-05 - mae: 0.0030 - val_loss: 4.1807e-05 - val_mae: 0.0052\n",
      "Epoch 966/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.4074e-05 - mae: 0.0030\n",
      "Epoch 966: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.4633e-05 - mae: 0.0030 - val_loss: 4.4188e-05 - val_mae: 0.0054\n",
      "Epoch 967/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.3611e-05 - mae: 0.0028\n",
      "Epoch 967: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.4886e-05 - mae: 0.0030 - val_loss: 4.1066e-05 - val_mae: 0.0051\n",
      "Epoch 968/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.2893e-05 - mae: 0.0028\n",
      "Epoch 968: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4907e-05 - mae: 0.0030 - val_loss: 4.6768e-05 - val_mae: 0.0055\n",
      "Epoch 969/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5132e-05 - mae: 0.0030\n",
      "Epoch 969: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.4534e-05 - mae: 0.0030 - val_loss: 4.7053e-05 - val_mae: 0.0055\n",
      "Epoch 970/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4435e-05 - mae: 0.0029\n",
      "Epoch 970: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.5096e-05 - mae: 0.0030 - val_loss: 4.1076e-05 - val_mae: 0.0051\n",
      "Epoch 971/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.4089e-05 - mae: 0.0029\n",
      "Epoch 971: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.5029e-05 - mae: 0.0030 - val_loss: 4.7186e-05 - val_mae: 0.0056\n",
      "Epoch 972/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6179e-05 - mae: 0.0032\n",
      "Epoch 972: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.5828e-05 - mae: 0.0031 - val_loss: 4.1626e-05 - val_mae: 0.0052\n",
      "Epoch 973/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4221e-05 - mae: 0.0030\n",
      "Epoch 973: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6767e-05 - mae: 0.0032 - val_loss: 5.0258e-05 - val_mae: 0.0057\n",
      "Epoch 974/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 1.4221e-05 - mae: 0.0029\n",
      "Epoch 974: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 1.4579e-05 - mae: 0.0030 - val_loss: 4.0970e-05 - val_mae: 0.0051\n",
      "Epoch 975/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.6146e-05 - mae: 0.0031\n",
      "Epoch 975: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.5932e-05 - mae: 0.0031 - val_loss: 4.0873e-05 - val_mae: 0.0051\n",
      "Epoch 976/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.4751e-05 - mae: 0.0030\n",
      "Epoch 976: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.5871e-05 - mae: 0.0031 - val_loss: 4.3673e-05 - val_mae: 0.0051\n",
      "Epoch 977/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 1.5089e-05 - mae: 0.0030\n",
      "Epoch 977: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 195us/sample - loss: 1.4586e-05 - mae: 0.0030 - val_loss: 4.1048e-05 - val_mae: 0.0051\n",
      "Epoch 978/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.4545e-05 - mae: 0.0030\n",
      "Epoch 978: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.5122e-05 - mae: 0.0030 - val_loss: 4.1495e-05 - val_mae: 0.0051\n",
      "Epoch 979/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710/800 [=========================>....] - ETA: 0s - loss: 1.5282e-05 - mae: 0.0030\n",
      "Epoch 979: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.5171e-05 - mae: 0.0030 - val_loss: 5.2469e-05 - val_mae: 0.0059\n",
      "Epoch 980/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6426e-05 - mae: 0.0032\n",
      "Epoch 980: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.6694e-05 - mae: 0.0032 - val_loss: 4.2497e-05 - val_mae: 0.0052\n",
      "Epoch 981/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.4865e-05 - mae: 0.0030\n",
      "Epoch 981: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.4848e-05 - mae: 0.0030 - val_loss: 4.1137e-05 - val_mae: 0.0051\n",
      "Epoch 982/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.5191e-05 - mae: 0.0030\n",
      "Epoch 982: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.5303e-05 - mae: 0.0030 - val_loss: 4.1814e-05 - val_mae: 0.0052\n",
      "Epoch 983/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.5184e-05 - mae: 0.0031\n",
      "Epoch 983: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.5175e-05 - mae: 0.0031 - val_loss: 4.6193e-05 - val_mae: 0.0055\n",
      "Epoch 984/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.3183e-05 - mae: 0.0028\n",
      "Epoch 984: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.4237e-05 - mae: 0.0029 - val_loss: 4.1159e-05 - val_mae: 0.0050\n",
      "Epoch 985/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.6254e-05 - mae: 0.0032\n",
      "Epoch 985: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.6221e-05 - mae: 0.0031 - val_loss: 4.6438e-05 - val_mae: 0.0055\n",
      "Epoch 986/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.2875e-05 - mae: 0.0028\n",
      "Epoch 986: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.3843e-05 - mae: 0.0029 - val_loss: 4.0993e-05 - val_mae: 0.0050\n",
      "Epoch 987/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3332e-05 - mae: 0.0029\n",
      "Epoch 987: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.5744e-05 - mae: 0.0031 - val_loss: 4.2837e-05 - val_mae: 0.0053\n",
      "Epoch 988/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.2992e-05 - mae: 0.0028\n",
      "Epoch 988: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4358e-05 - mae: 0.0029 - val_loss: 4.0610e-05 - val_mae: 0.0051\n",
      "Epoch 989/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.3835e-05 - mae: 0.0029\n",
      "Epoch 989: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.4228e-05 - mae: 0.0029 - val_loss: 4.0946e-05 - val_mae: 0.0050\n",
      "Epoch 990/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4106e-05 - mae: 0.0029\n",
      "Epoch 990: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6066e-05 - mae: 0.0031 - val_loss: 4.4421e-05 - val_mae: 0.0054\n",
      "Epoch 991/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3802e-05 - mae: 0.0030\n",
      "Epoch 991: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.4482e-05 - mae: 0.0030 - val_loss: 4.1870e-05 - val_mae: 0.0052\n",
      "Epoch 992/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.2765e-05 - mae: 0.0028\n",
      "Epoch 992: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.3877e-05 - mae: 0.0029 - val_loss: 4.1306e-05 - val_mae: 0.0052\n",
      "Epoch 993/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.4732e-05 - mae: 0.0030\n",
      "Epoch 993: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.5028e-05 - mae: 0.0030 - val_loss: 4.1910e-05 - val_mae: 0.0051\n",
      "Epoch 994/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3813e-05 - mae: 0.0029\n",
      "Epoch 994: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.3730e-05 - mae: 0.0029 - val_loss: 4.0887e-05 - val_mae: 0.0051\n",
      "Epoch 995/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.4283e-05 - mae: 0.0029\n",
      "Epoch 995: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.3647e-05 - mae: 0.0028 - val_loss: 4.1384e-05 - val_mae: 0.0052\n",
      "Epoch 996/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.5265e-05 - mae: 0.0031\n",
      "Epoch 996: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4408e-05 - mae: 0.0029 - val_loss: 4.0740e-05 - val_mae: 0.0051\n",
      "Epoch 997/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5463e-05 - mae: 0.0031\n",
      "Epoch 997: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5908e-05 - mae: 0.0032 - val_loss: 4.2716e-05 - val_mae: 0.0051\n",
      "Epoch 998/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.4014e-05 - mae: 0.0030\n",
      "Epoch 998: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.5566e-05 - mae: 0.0031 - val_loss: 4.5916e-05 - val_mae: 0.0055\n",
      "Epoch 999/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.3986e-05 - mae: 0.0029\n",
      "Epoch 999: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.3951e-05 - mae: 0.0029 - val_loss: 4.3632e-05 - val_mae: 0.0053\n",
      "Epoch 1000/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.4259e-05 - mae: 0.0029\n",
      "Epoch 1000: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.4042e-05 - mae: 0.0029 - val_loss: 4.0541e-05 - val_mae: 0.0051\n"
     ]
    }
   ],
   "source": [
    "Layers = [{'size': nx+1, 'activation': None    , 'use_bias': None},\n",
    "          {'size': 10 , 'activation': 'relu'  , 'use_bias': True},\n",
    "          {'size': 1  , 'activation': 'linear', 'use_bias': False}]\n",
    "Losses = [{'kind': 'mse', 'weight': 1.0}]\n",
    "\n",
    "K = TrainFullyConnected(M_samples, H_samples, \n",
    "                    Layers, Losses,\n",
    "                    'adam', ['mae'], \n",
    "                    10, 1000, 0.2, \n",
    "                    'model', os.path.abspath(''))\n",
    "\n",
    "best_model = K.quickTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ccd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.061140154095025656\r",
      "iteration 2 - y: 0.06093163126859248\r",
      "iteration 3 - y: 0.06072310844215929\r",
      "iteration 4 - y: 0.06051458561572612\r",
      "iteration 5 - y: 0.060306062789292916\r",
      "iteration 6 - y: 0.06009753996285974\r",
      "iteration 7 - y: 0.05988901713642656\r",
      "iteration 8 - y: 0.05968049430999338\r",
      "iteration 9 - y: 0.059471971483560204\r",
      "iteration 10 - y: 0.059263448657127016\r",
      "iteration 11 - y: 0.05905492583069383\r",
      "iteration 12 - y: 0.058846403004260646\r",
      "iteration 13 - y: 0.05863788017782746\r",
      "iteration 14 - y: 0.058429357351394276\r",
      "iteration 15 - y: 0.0582208345249611\r",
      "iteration 16 - y: 0.05801231169852791\r",
      "iteration 17 - y: 0.05780378887209473\r",
      "iteration 18 - y: 0.057595266045661536\r",
      "iteration 19 - y: 0.05738674321922837\r",
      "iteration 20 - y: 0.05717822039279518\r",
      "iteration 21 - y: 0.056969697566361985\r",
      "iteration 22 - y: 0.05676117473992881\r",
      "iteration 23 - y: 0.05655265191349562\r",
      "iteration 24 - y: 0.05634412908706244\r",
      "iteration 25 - y: 0.05613560626062926\r",
      "iteration 26 - y: 0.055927083434196084\r",
      "iteration 27 - y: 0.055718560607762896\r",
      "iteration 28 - y: 0.055510037781329714\r",
      "iteration 29 - y: 0.05530151495489653\r",
      "iteration 30 - y: 0.055092992128463344\r",
      "iteration 31 - y: 0.054884469302030156\r",
      "iteration 32 - y: 0.05467594647559698\r",
      "iteration 33 - y: 0.05446742364916379\r",
      "iteration 34 - y: 0.054258900822730605\r",
      "iteration 35 - y: 0.05405037799629743\r",
      "iteration 36 - y: 0.05384185516986424\r",
      "iteration 37 - y: 0.05363333234343105\r",
      "iteration 38 - y: 0.05342480951699788\r",
      "iteration 39 - y: 0.0532162866905647\r",
      "iteration 40 - y: 0.053007763864131516\r",
      "iteration 41 - y: 0.05279924103769833\r",
      "iteration 42 - y: 0.05259071821126515\r",
      "iteration 43 - y: 0.05238219538483196\r",
      "iteration 44 - y: 0.052173672558398776\r",
      "iteration 45 - y: 0.051965149731965594\r",
      "iteration 46 - y: 0.05175662690553241\r",
      "iteration 47 - y: 0.051548104079099225\r",
      "iteration 48 - y: 0.05133958125266604\r",
      "iteration 49 - y: 0.05113105842623287\r",
      "iteration 50 - y: 0.05092253559979967\r",
      "iteration 51 - y: 0.0507140127733665\r",
      "iteration 52 - y: 0.05050548994693332\r",
      "iteration 53 - y: 0.05029696712050013\r",
      "iteration 54 - y: 0.05008844429406694\r",
      "iteration 55 - y: 0.04987992146763376\r",
      "iteration 56 - y: 0.049671398641200584\r",
      "iteration 57 - y: 0.049462875814767396\r",
      "iteration 58 - y: 0.04925435298833421\r",
      "iteration 59 - y: 0.04904583016190103\r",
      "iteration 60 - y: 0.048837307335467844\r",
      "iteration 61 - y: 0.048628784509034656\r",
      "iteration 62 - y: 0.04842026168260148\r",
      "iteration 63 - y: 0.04821173885616829\r",
      "iteration 64 - y: 0.04800321602973511\r",
      "iteration 65 - y: 0.04779469320330192\r",
      "iteration 66 - y: 0.04758617037686874\r",
      "iteration 67 - y: 0.04737764755043557\r",
      "iteration 68 - y: 0.04716912472400237\r",
      "iteration 69 - y: 0.0469606018975692\r",
      "iteration 70 - y: 0.04675207907113601\r",
      "iteration 71 - y: 0.04654355624470283\r",
      "iteration 72 - y: 0.046335033418269646\r",
      "iteration 73 - y: 0.04612651059183646\r",
      "iteration 74 - y: 0.045917987765403276\r",
      "iteration 75 - y: 0.045709464938970094\r",
      "iteration 76 - y: 0.04550094211253691\r",
      "iteration 77 - y: 0.045292419286103724\r",
      "iteration 78 - y: 0.04508389645967055\r",
      "iteration 79 - y: 0.04487537363323737\r",
      "iteration 80 - y: 0.04466685080680418\r",
      "iteration 81 - y: 0.04445832798037099\r",
      "iteration 82 - y: 0.04424980515393782\r",
      "iteration 83 - y: 0.04404128232750463\r",
      "iteration 84 - y: 0.04383275950107144\r",
      "iteration 85 - y: 0.04362423667463826\r",
      "iteration 86 - y: 0.04341571384820508\r",
      "iteration 87 - y: 0.043207191021771896\r",
      "iteration 88 - y: 0.04299866819533871\r",
      "iteration 89 - y: 0.04279014536890553\r",
      "iteration 90 - y: 0.042581622542472344\r",
      "iteration 91 - y: 0.042373099716039156\r",
      "iteration 92 - y: 0.042164576889605974\r",
      "iteration 93 - y: 0.04195605406317279\r",
      "iteration 94 - y: 0.041747531236739605\r",
      "iteration 95 - y: 0.04153900841030642\r",
      "iteration 96 - y: 0.04133048558387324\r",
      "iteration 97 - y: 0.04112196275744005\r",
      "iteration 98 - y: 0.04091343993100688\r",
      "iteration 99 - y: 0.0407049171045737\r",
      "iteration 100 - y: 0.04049639427814051\r",
      "iteration 101 - y: 0.04028787145170732\r",
      "iteration 102 - y: 0.04007934862527414\r",
      "iteration 103 - y: 0.03987082579884096\r",
      "iteration 104 - y: 0.03966230297240777\r",
      "iteration 105 - y: 0.03945378014597459\r",
      "iteration 106 - y: 0.039245257319541406\r",
      "iteration 107 - y: 0.03903673449310822\r",
      "iteration 108 - y: 0.038828211666675036\r",
      "iteration 109 - y: 0.03861968884024186\r",
      "iteration 110 - y: 0.03841116601380868\r",
      "iteration 111 - y: 0.0382026431873755\r",
      "iteration 112 - y: 0.0379941203609423\r",
      "iteration 113 - y: 0.03778559753450913\r",
      "iteration 114 - y: 0.03757707470807594\r",
      "iteration 115 - y: 0.03736855188164276\r",
      "iteration 116 - y: 0.03716002905520957\r",
      "iteration 117 - y: 0.03695150622877639\r",
      "iteration 118 - y: 0.03674298340234321\r",
      "iteration 119 - y: 0.03653446057591002\r",
      "iteration 120 - y: 0.036325937749476844\r",
      "iteration 121 - y: 0.036117414923043656\r",
      "iteration 122 - y: 0.035908892096610474\r",
      "iteration 123 - y: 0.03570036927017729\r",
      "iteration 124 - y: 0.035491846443744104\r",
      "iteration 125 - y: 0.03528332361731093\r",
      "iteration 126 - y: 0.03507480079087775\r",
      "iteration 127 - y: 0.03486627796444456\r",
      "iteration 128 - y: 0.03465775513801138\r",
      "iteration 129 - y: 0.03444923231157819\r",
      "iteration 130 - y: 0.034240709485145016\r",
      "iteration 131 - y: 0.03403218665871183\r",
      "iteration 132 - y: 0.03382366383227865\r",
      "iteration 133 - y: 0.03361514100584547\r",
      "iteration 134 - y: 0.03340661817941229\r",
      "iteration 135 - y: 0.03319809535297911\r",
      "iteration 136 - y: 0.03298957252654593\r",
      "iteration 137 - y: 0.032781049700112745\r",
      "iteration 138 - y: 0.032572526873679564\r",
      "iteration 139 - y: 0.03236400404724639\r",
      "iteration 140 - y: 0.0321554812208132\r",
      "iteration 141 - y: 0.031946958394380026\r",
      "iteration 142 - y: 0.031738435567946845\r",
      "iteration 143 - y: 0.03152991274151366\r",
      "iteration 144 - y: 0.031321389915080475\r",
      "iteration 145 - y: 0.031112867088647297\r",
      "iteration 146 - y: 0.03090434426221412\r",
      "iteration 147 - y: 0.030695821435780937\r",
      "iteration 148 - y: 0.03048729860934776\r",
      "iteration 149 - y: 0.030278775782914574\r",
      "iteration 150 - y: 0.030070252956481393\r",
      "iteration 151 - y: 0.02986173013004821\r",
      "iteration 152 - y: 0.02965320730361503\r",
      "iteration 153 - y: 0.02944468447718185\r",
      "iteration 154 - y: 0.029236161650748674\r",
      "iteration 155 - y: 0.029027638824315485\r",
      "iteration 156 - y: 0.028819115997882304\r",
      "iteration 157 - y: 0.028610593171449122\r",
      "iteration 158 - y: 0.028402070345015944\r",
      "iteration 159 - y: 0.028193547518582766\r",
      "iteration 160 - y: 0.02798502469214958\r",
      "iteration 161 - y: 0.0277765018657164\r",
      "iteration 162 - y: 0.027567979039283222\r",
      "iteration 163 - y: 0.02735945621285004\r",
      "iteration 164 - y: 0.0271560690051037\r",
      "iteration 165 - y: 0.02697951346096575\r",
      "iteration 166 - y: 0.026802957916827793\r",
      "iteration 167 - y: 0.026626402372689838\r",
      "iteration 168 - y: 0.026449846828551886\r",
      "iteration 169 - y: 0.026273291284413934\r",
      "iteration 170 - y: 0.026096735740275975\r",
      "iteration 171 - y: 0.025920180196138023\r",
      "iteration 172 - y: 0.025743624652000065\r",
      "iteration 173 - y: 0.025567069107862113\r",
      "iteration 174 - y: 0.02539051356372416\r",
      "iteration 175 - y: 0.02524306356926449\r",
      "iteration 176 - y: 0.02510785305221565\r",
      "iteration 177 - y: 0.024972642535166817\r",
      "iteration 178 - y: 0.02483743201811798\r",
      "iteration 179 - y: 0.024702221501069144\r",
      "iteration 180 - y: 0.024567010984020313\r",
      "iteration 181 - y: 0.024431800466971475\r",
      "iteration 182 - y: 0.02429658994992264\r",
      "iteration 183 - y: 0.02416137943287381\r",
      "iteration 184 - y: 0.024026168915824972\r",
      "iteration 185 - y: 0.023890958398776134\r",
      "iteration 186 - y: 0.023755747881727303\r",
      "iteration 187 - y: 0.023620537364678465\r",
      "iteration 188 - y: 0.023485326847629634\r",
      "iteration 189 - y: 0.0233501163305808\r",
      "iteration 190 - y: 0.02321490581353196\r",
      "iteration 191 - y: 0.023079695296483124\r",
      "iteration 192 - y: 0.02294448477943429\r",
      "iteration 193 - y: 0.022809274262385455\r",
      "iteration 194 - y: 0.02268518578711406\r",
      "iteration 195 - y: 0.02257348012331382\r",
      "iteration 196 - y: 0.022461774459513584\r",
      "iteration 197 - y: 0.022350068795713343\r",
      "iteration 198 - y: 0.022238363131913103\r",
      "iteration 199 - y: 0.02212665746811287\r",
      "iteration 200 - y: 0.02201495180431263\r",
      "iteration 201 - y: 0.021903246140512388\r",
      "iteration 202 - y: 0.02179154047671215\r",
      "iteration 203 - y: 0.02167983481291191\r",
      "iteration 204 - y: 0.021568129149111673\r",
      "iteration 205 - y: 0.021456423485311436\r",
      "iteration 206 - y: 0.021344717821511195\r",
      "iteration 207 - y: 0.021233012157710954\r",
      "iteration 208 - y: 0.021121306493910714\r",
      "iteration 209 - y: 0.021009600830110477\r",
      "iteration 210 - y: 0.02089789516631024\r",
      "iteration 211 - y: 0.02078618950251\r",
      "iteration 212 - y: 0.02067448383870976\r",
      "iteration 213 - y: 0.02056277817490952\r",
      "iteration 214 - y: 0.02045107251110928\r",
      "iteration 215 - y: 0.020339366847309047\r",
      "iteration 216 - y: 0.020227661183508806\r",
      "iteration 217 - y: 0.020115955519708566\r",
      "iteration 218 - y: 0.020004249855908325\r",
      "iteration 219 - y: 0.019892544192108084\r",
      "iteration 220 - y: 0.01978083852830785\r",
      "iteration 221 - y: 0.019669132864507614\r",
      "iteration 222 - y: 0.019557427200707373\r",
      "iteration 223 - y: 0.019445721536907132\r",
      "iteration 224 - y: 0.019334015873106892\r",
      "iteration 225 - y: 0.019222310209306655\r",
      "iteration 226 - y: 0.019110604545506418\r",
      "iteration 227 - y: 0.018998898881706177\r",
      "iteration 228 - y: 0.01888719321790594\r",
      "iteration 229 - y: 0.0187754875541057\r",
      "iteration 230 - y: 0.01866378189030546\r",
      "iteration 231 - y: 0.018552076226505218\r",
      "iteration 232 - y: 0.018440370562704977\r",
      "iteration 233 - y: 0.018328664898904744\r",
      "iteration 234 - y: 0.018216959235104507\r",
      "iteration 235 - y: 0.018105253571304263\r",
      "iteration 236 - y: 0.017993547907504025\r",
      "iteration 237 - y: 0.017881842243703785\r",
      "iteration 238 - y: 0.017770136579903544\r",
      "iteration 239 - y: 0.017658430916103307\r",
      "iteration 240 - y: 0.017546725252303066\r",
      "iteration 241 - y: 0.017435019588502826\r",
      "iteration 242 - y: 0.017323313924702592\r",
      "iteration 243 - y: 0.01721160826090235\r",
      "iteration 244 - y: 0.01709990259710211\r",
      "iteration 245 - y: 0.016988196933301874\r",
      "iteration 246 - y: 0.016876491269501633\r",
      "iteration 247 - y: 0.016764785605701393\r",
      "iteration 248 - y: 0.016653079941901152\r",
      "iteration 249 - y: 0.016541374278100915\r",
      "iteration 250 - y: 0.016429668614300674\r",
      "iteration 251 - y: 0.016317962950500437\r",
      "iteration 252 - y: 0.0162062572867002\r",
      "iteration 253 - y: 0.01609455162289996\r",
      "iteration 254 - y: 0.01598284595909972\r",
      "iteration 255 - y: 0.01587114029529948\r",
      "iteration 256 - y: 0.01575943463149924\r",
      "iteration 257 - y: 0.015647728967699\r",
      "iteration 258 - y: 0.015536023303898763\r",
      "iteration 259 - y: 0.015424317640098523\r",
      "iteration 260 - y: 0.015312611976298284\r",
      "iteration 261 - y: 0.015200906312498045\r",
      "iteration 262 - y: 0.015089200648697806\r",
      "iteration 263 - y: 0.014977494984897567\r",
      "iteration 264 - y: 0.014865789321097327\r",
      "iteration 265 - y: 0.01475408365729709\r",
      "iteration 266 - y: 0.01464237799349685\r",
      "iteration 267 - y: 0.01453067232969661\r",
      "iteration 268 - y: 0.014418966665896373\r",
      "iteration 269 - y: 0.014307261002096134\r",
      "iteration 270 - y: 0.014195555338295893\r",
      "iteration 271 - y: 0.014083849674495656\r",
      "iteration 272 - y: 0.013972144010695416\r",
      "iteration 273 - y: 0.013860438346895177\r",
      "iteration 274 - y: 0.013748732683094938\r",
      "iteration 275 - y: 0.013637027019294699\r",
      "iteration 276 - y: 0.01352532135549446\r",
      "iteration 277 - y: 0.013413615691694221\r",
      "iteration 278 - y: 0.013301910027893982\r",
      "iteration 279 - y: 0.013190204364093744\r",
      "iteration 280 - y: 0.013078498700293503\r",
      "iteration 281 - y: 0.012966793036493264\r",
      "iteration 282 - y: 0.012855087372693027\r",
      "iteration 283 - y: 0.012743381708892786\r",
      "iteration 284 - y: 0.012631676045092546\r",
      "iteration 285 - y: 0.01251997038129231\r",
      "iteration 286 - y: 0.01240826471749207\r",
      "iteration 287 - y: 0.012296559053691829\r",
      "iteration 288 - y: 0.01218485338989159\r",
      "iteration 289 - y: 0.012073147726091353\r",
      "iteration 290 - y: 0.011961442062291112\r",
      "iteration 291 - y: 0.011849736398490874\r",
      "iteration 292 - y: 0.011738030734690636\r",
      "iteration 293 - y: 0.011626325070890396\r",
      "iteration 294 - y: 0.011514619407090155\r",
      "iteration 295 - y: 0.01140291374328992\r",
      "iteration 296 - y: 0.01129120807948968\r",
      "iteration 297 - y: 0.011179502415689439\r",
      "iteration 298 - y: 0.0110677967518892\r",
      "iteration 299 - y: 0.010956091088088963\r",
      "iteration 300 - y: 0.010844385424288722\r",
      "iteration 301 - y: 0.010732679760488483\r",
      "iteration 302 - y: 0.010620974096688243\r",
      "iteration 303 - y: 0.010529177327546095\r",
      "iteration 304 - y: 0.010447887171896911\r",
      "iteration 305 - y: 0.010366597016247725\r",
      "iteration 306 - y: 0.010285306860598541\r",
      "iteration 307 - y: 0.010204016704949356\r",
      "iteration 308 - y: 0.010122726549300172\r",
      "iteration 309 - y: 0.010041436393650988\r",
      "iteration 310 - y: 0.009960146238001802\r",
      "iteration 311 - y: 0.009878856082352618\r",
      "iteration 312 - y: 0.009797565926703434\r",
      "iteration 313 - y: 0.009716275771054248\r",
      "iteration 314 - y: 0.009634985615405062\r",
      "iteration 315 - y: 0.00955369545975588\r",
      "iteration 316 - y: 0.009472405304106694\r",
      "iteration 317 - y: 0.009391115148457509\r",
      "iteration 318 - y: 0.009309824992808325\r",
      "iteration 319 - y: 0.00922853483715914\r",
      "iteration 320 - y: 0.009147244681509955\r",
      "iteration 321 - y: 0.009065954525860771\r",
      "iteration 322 - y: 0.008984664370211585\r",
      "iteration 323 - y: 0.008903374214562401\r",
      "iteration 324 - y: 0.008822084058913217\r",
      "iteration 325 - y: 0.008740793903264031\r",
      "iteration 326 - y: 0.008659503747614847\r",
      "iteration 327 - y: 0.008578213591965663\r",
      "iteration 328 - y: 0.008496923436316478\r",
      "iteration 329 - y: 0.008415633280667294\r",
      "iteration 330 - y: 0.008334343125018108\r",
      "iteration 331 - y: 0.008253052969368924\r",
      "iteration 332 - y: 0.00817176281371974\r",
      "iteration 333 - y: 0.008090472658070554\r",
      "iteration 334 - y: 0.00800918250242137\r",
      "iteration 335 - y: 0.007927892346772186\r",
      "iteration 336 - y: 0.007846602191123\r",
      "iteration 337 - y: 0.007765312035473816\r",
      "iteration 338 - y: 0.007684021879824632\r",
      "iteration 339 - y: 0.007602731724175447\r",
      "iteration 340 - y: 0.007521441568526261\r",
      "iteration 341 - y: 0.007440151412877076\r",
      "iteration 342 - y: 0.007358861257227891\r",
      "iteration 343 - y: 0.007277571101578706\r",
      "iteration 344 - y: 0.0071962809459295216\r",
      "iteration 345 - y: 0.007114990790280336\r",
      "iteration 346 - y: 0.007033700634631152\r",
      "iteration 347 - y: 0.006952410478981966\r",
      "iteration 348 - y: 0.006871120323332782\r",
      "iteration 349 - y: 0.006789830167683596\r",
      "iteration 350 - y: 0.006708540012034412\r",
      "iteration 351 - y: 0.006627249856385227\r",
      "iteration 352 - y: 0.006545959700736042\r",
      "iteration 353 - y: 0.006464669545086857\r",
      "iteration 354 - y: 0.006383379389437672\r",
      "iteration 355 - y: 0.006302089233788487\r",
      "iteration 356 - y: 0.006220799078139302\r",
      "iteration 357 - y: 0.0061395089224901165\r",
      "iteration 358 - y: 0.0060582187668409325\r",
      "iteration 359 - y: 0.005976928611191748\r",
      "iteration 360 - y: 0.005895638455542562\r",
      "iteration 361 - y: 0.005814348299893377\r",
      "iteration 362 - y: 0.005733058144244192\r",
      "iteration 363 - y: 0.005651767988595007\r",
      "iteration 364 - y: 0.0055704778329458225\r",
      "iteration 365 - y: 0.005489187677296637\r",
      "iteration 366 - y: 0.005407897521647451\r",
      "iteration 367 - y: 0.005326607365998266\r",
      "iteration 368 - y: 0.00524531721034908\r",
      "iteration 369 - y: 0.005164027054699895\r",
      "iteration 370 - y: 0.00508273689905071\r",
      "iteration 371 - y: 0.005001446743401524\r",
      "iteration 372 - y: 0.004920156587752339\r",
      "iteration 373 - y: 0.004838866432103153\r",
      "iteration 374 - y: 0.0047575762764539686\r",
      "iteration 375 - y: 0.004676286120804783\r",
      "iteration 376 - y: 0.004594995965155597\r",
      "iteration 377 - y: 0.004513705809506411\r",
      "iteration 378 - y: 0.0044324156538572265\r",
      "iteration 379 - y: 0.004351125498208042\r",
      "iteration 380 - y: 0.004269835342558856\r",
      "iteration 381 - y: 0.004188545186909671\r",
      "iteration 382 - y: 0.004107255031260486\r",
      "iteration 383 - y: 0.0040259648756113\r",
      "iteration 384 - y: 0.0039446747199621155\r",
      "iteration 385 - y: 0.00386338456431293\r",
      "iteration 386 - y: 0.003782094408663745\r",
      "iteration 387 - y: 0.003700804253014559\r",
      "iteration 388 - y: 0.003619514097365374\r",
      "iteration 389 - y: 0.0035382239417161886\r",
      "iteration 390 - y: 0.0034569337860670033\r",
      "iteration 391 - y: 0.003375643630417818\r",
      "iteration 392 - y: 0.0032943534747686327\r",
      "iteration 393 - y: 0.003213063319119448\r",
      "iteration 394 - y: 0.003131773163470262\r",
      "iteration 395 - y: 0.0030504830078210772\r",
      "iteration 396 - y: 0.002969192852171892\r",
      "iteration 397 - y: 0.0028879026965227066\r",
      "iteration 398 - y: 0.0028066125408735213\r",
      "iteration 399 - y: 0.002725322385224336\r",
      "iteration 400 - y: 0.0026440322295751507\r",
      "iteration 401 - y: 0.0025627420739259654\r",
      "iteration 402 - y: 0.0024814519182767797\r",
      "iteration 403 - y: 0.002400161762627595\r",
      "iteration 404 - y: 0.0023188716069784095\r",
      "iteration 405 - y: 0.0022375814513292246\r",
      "iteration 406 - y: 0.0021562912956800393\r",
      "iteration 407 - y: 0.0020750011400308545\r",
      "iteration 408 - y: 0.001993710984381669\r",
      "iteration 409 - y: 0.001912420828732484\r",
      "iteration 410 - y: 0.001831130673083299\r",
      "iteration 411 - y: 0.001749840517434114\r",
      "iteration 412 - y: 0.0016685503617849288\r",
      "iteration 413 - y: 0.0015872602061357437\r",
      "iteration 414 - y: 0.0015059700504865586\r",
      "iteration 415 - y: 0.0014246798948373736\r",
      "iteration 416 - y: 0.0013433897391881885\r",
      "iteration 417 - y: 0.001276115077254026\r",
      "iteration 418 - y: 0.0012246331925360849\r",
      "iteration 419 - y: 0.0011731513078181437\r",
      "iteration 420 - y: 0.0011216694231002025\r",
      "iteration 421 - y: 0.0010701875383822613\r",
      "iteration 422 - y: 0.0010187056536643201\r",
      "iteration 423 - y: 0.0009672237689463787\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0803168945623185\r",
      "iteration 2 - y: 0.08010837173588532\r",
      "iteration 3 - y: 0.07989984890945212\r",
      "iteration 4 - y: 0.07969132608301895\r",
      "iteration 5 - y: 0.07948280325658576\r",
      "iteration 6 - y: 0.07927428043015257\r",
      "iteration 7 - y: 0.0790657576037194\r",
      "iteration 8 - y: 0.0788572347772862\r",
      "iteration 9 - y: 0.07864871195085303\r",
      "iteration 10 - y: 0.07844018912441984\r",
      "iteration 11 - y: 0.07823166629798664\r",
      "iteration 12 - y: 0.07802314347155347\r",
      "iteration 13 - y: 0.07781462064512028\r",
      "iteration 14 - y: 0.07760609781868709\r",
      "iteration 15 - y: 0.07739757499225391\r",
      "iteration 16 - y: 0.07718905216582073\r",
      "iteration 17 - y: 0.07698052933938754\r",
      "iteration 18 - y: 0.07677200651295435\r",
      "iteration 19 - y: 0.07656348368652116\r",
      "iteration 20 - y: 0.07635496086008797\r",
      "iteration 21 - y: 0.07614643803365478\r",
      "iteration 22 - y: 0.07593791520722162\r",
      "iteration 23 - y: 0.07572939238078842\r",
      "iteration 24 - y: 0.07552086955435525\r",
      "iteration 25 - y: 0.07531234672792204\r",
      "iteration 26 - y: 0.07510382390148887\r",
      "iteration 27 - y: 0.07489530107505568\r",
      "iteration 28 - y: 0.07468677824862249\r",
      "iteration 29 - y: 0.07447825542218932\r",
      "iteration 30 - y: 0.07426973259575613\r",
      "iteration 31 - y: 0.07406120976932296\r",
      "iteration 32 - y: 0.07385268694288977\r",
      "iteration 33 - y: 0.07364416411645658\r",
      "iteration 34 - y: 0.07343564129002339\r",
      "iteration 35 - y: 0.07322711846359022\r",
      "iteration 36 - y: 0.07301859563715704\r",
      "iteration 37 - y: 0.07281007281072385\r",
      "iteration 38 - y: 0.07260154998429066\r",
      "iteration 39 - y: 0.07239302715785748\r",
      "iteration 40 - y: 0.07218450433142429\r",
      "iteration 41 - y: 0.07197598150499111\r",
      "iteration 42 - y: 0.07176745867855792\r",
      "iteration 43 - y: 0.07155893585212474\r",
      "iteration 44 - y: 0.07135041302569156\r",
      "iteration 45 - y: 0.07114189019925837\r",
      "iteration 46 - y: 0.0709333673728252\r",
      "iteration 47 - y: 0.07072484454639201\r",
      "iteration 48 - y: 0.07051632171995884\r",
      "iteration 49 - y: 0.07030779889352565\r",
      "iteration 50 - y: 0.07009927606709246\r",
      "iteration 51 - y: 0.06989075324065928\r",
      "iteration 52 - y: 0.0696822304142261\r",
      "iteration 53 - y: 0.06947370758779292\r",
      "iteration 54 - y: 0.06926518476135973\r",
      "iteration 55 - y: 0.06905666193492654\r",
      "iteration 56 - y: 0.06884813910849336\r",
      "iteration 57 - y: 0.06863961628206018\r",
      "iteration 58 - y: 0.06843109345562699\r",
      "iteration 59 - y: 0.06822257062919382\r",
      "iteration 60 - y: 0.06801404780276063\r",
      "iteration 61 - y: 0.06780552497632744\r",
      "iteration 62 - y: 0.06759700214989427\r",
      "iteration 63 - y: 0.06738847932346108\r",
      "iteration 64 - y: 0.06717995649702789\r",
      "iteration 65 - y: 0.0669714336705947\r",
      "iteration 66 - y: 0.06676291084416153\r",
      "iteration 67 - y: 0.06655438801772834\r",
      "iteration 68 - y: 0.06634586519129515\r",
      "iteration 69 - y: 0.06613734236486198\r",
      "iteration 70 - y: 0.0659288195384288\r",
      "iteration 71 - y: 0.06572029671199561\r",
      "iteration 72 - y: 0.06551177388556242\r",
      "iteration 73 - y: 0.06530325105912925\r",
      "iteration 74 - y: 0.06509472823269605\r",
      "iteration 75 - y: 0.06488620540626287\r",
      "iteration 76 - y: 0.0646776825798297\r",
      "iteration 77 - y: 0.0644691597533965\r",
      "iteration 78 - y: 0.06426063692696334\r",
      "iteration 79 - y: 0.06405211410053015\r",
      "iteration 80 - y: 0.06384359127409696\r",
      "iteration 81 - y: 0.06363506844766378\r",
      "iteration 82 - y: 0.0634265456212306\r",
      "iteration 83 - y: 0.06321802279479741\r",
      "iteration 84 - y: 0.06300949996836422\r",
      "iteration 85 - y: 0.06280097714193104\r",
      "iteration 86 - y: 0.06259245431549787\r",
      "iteration 87 - y: 0.062383931489064674\r",
      "iteration 88 - y: 0.062175408662631486\r",
      "iteration 89 - y: 0.06196688583619831\r",
      "iteration 90 - y: 0.061758363009765116\r",
      "iteration 91 - y: 0.06154984018333195\r",
      "iteration 92 - y: 0.06134131735689876\r",
      "iteration 93 - y: 0.06113279453046557\r",
      "iteration 94 - y: 0.06092427170403239\r",
      "iteration 95 - y: 0.06071574887759921\r",
      "iteration 96 - y: 0.06050722605116603\r",
      "iteration 97 - y: 0.06029870322473284\r",
      "iteration 98 - y: 0.06009018039829965\r",
      "iteration 99 - y: 0.059881657571866476\r",
      "iteration 100 - y: 0.05967313474543329\r",
      "iteration 101 - y: 0.05946461191900011\r",
      "iteration 102 - y: 0.059256089092566924\r",
      "iteration 103 - y: 0.05904756626613374\r",
      "iteration 104 - y: 0.058839043439700554\r",
      "iteration 105 - y: 0.05863052061326738\r",
      "iteration 106 - y: 0.05842199778683419\r",
      "iteration 107 - y: 0.05821347496040101\r",
      "iteration 108 - y: 0.058004952133967835\r",
      "iteration 109 - y: 0.05779642930753465\r",
      "iteration 110 - y: 0.057587906481101465\r",
      "iteration 111 - y: 0.057379383654668284\r",
      "iteration 112 - y: 0.0571708608282351\r",
      "iteration 113 - y: 0.05696233800180192\r",
      "iteration 114 - y: 0.05675381517536873\r",
      "iteration 115 - y: 0.05654529234893556\r",
      "iteration 116 - y: 0.05633676952250237\r",
      "iteration 117 - y: 0.05612824669606918\r",
      "iteration 118 - y: 0.05591972386963601\r",
      "iteration 119 - y: 0.05571120104320282\r",
      "iteration 120 - y: 0.055502678216769644\r",
      "iteration 121 - y: 0.05529415539033645\r",
      "iteration 122 - y: 0.055085632563903274\r",
      "iteration 123 - y: 0.05487710973747009\r",
      "iteration 124 - y: 0.054668586911036904\r",
      "iteration 125 - y: 0.05446006408460372\r",
      "iteration 126 - y: 0.05425154125817054\r",
      "iteration 127 - y: 0.05404301843173736\r",
      "iteration 128 - y: 0.053834495605304185\r",
      "iteration 129 - y: 0.053625972778870996\r",
      "iteration 130 - y: 0.053417449952437815\r",
      "iteration 131 - y: 0.05320892712600463\r",
      "iteration 132 - y: 0.05300040429957145\r",
      "iteration 133 - y: 0.05279188147313827\r",
      "iteration 134 - y: 0.052583358646705075\r",
      "iteration 135 - y: 0.0523748358202719\r",
      "iteration 136 - y: 0.05216631299383871\r",
      "iteration 137 - y: 0.05195779016740554\r",
      "iteration 138 - y: 0.05174926734097236\r",
      "iteration 139 - y: 0.05154074451453917\r",
      "iteration 140 - y: 0.051332221688105986\r",
      "iteration 141 - y: 0.0511236988616728\r",
      "iteration 142 - y: 0.05091517603523962\r",
      "iteration 143 - y: 0.05070665320880644\r",
      "iteration 144 - y: 0.05049813038237326\r",
      "iteration 145 - y: 0.05028960755594007\r",
      "iteration 146 - y: 0.0500810847295069\r",
      "iteration 147 - y: 0.04987256190307371\r",
      "iteration 148 - y: 0.04966403907664053\r",
      "iteration 149 - y: 0.049455516250207346\r",
      "iteration 150 - y: 0.049246993423774164\r",
      "iteration 151 - y: 0.04903847059734098\r",
      "iteration 152 - y: 0.048829947770907794\r",
      "iteration 153 - y: 0.048621424944474606\r",
      "iteration 154 - y: 0.04841290211804143\r",
      "iteration 155 - y: 0.04820437929160824\r",
      "iteration 156 - y: 0.04799585646517507\r",
      "iteration 157 - y: 0.04778733363874188\r",
      "iteration 158 - y: 0.0475788108123087\r",
      "iteration 159 - y: 0.04737028798587552\r",
      "iteration 160 - y: 0.047161765159442336\r",
      "iteration 161 - y: 0.046953242333009154\r",
      "iteration 162 - y: 0.04674471950657597\r",
      "iteration 163 - y: 0.04653619668014279\r",
      "iteration 164 - y: 0.04632767385370961\r",
      "iteration 165 - y: 0.04611915102727642\r",
      "iteration 166 - y: 0.04591062820084324\r",
      "iteration 167 - y: 0.045702105374410065\r",
      "iteration 168 - y: 0.04549358254797688\r",
      "iteration 169 - y: 0.045285059721543695\r",
      "iteration 170 - y: 0.045076536895110514\r",
      "iteration 171 - y: 0.044868014068677325\r",
      "iteration 172 - y: 0.04465949124224415\r",
      "iteration 173 - y: 0.04445096841581096\r",
      "iteration 174 - y: 0.044242445589377774\r",
      "iteration 175 - y: 0.04403392276294459\r",
      "iteration 176 - y: 0.04382539993651141\r",
      "iteration 177 - y: 0.04361687711007823\r",
      "iteration 178 - y: 0.043408354283645055\r",
      "iteration 179 - y: 0.04319983145721187\r",
      "iteration 180 - y: 0.042991308630778685\r",
      "iteration 181 - y: 0.042782785804345504\r",
      "iteration 182 - y: 0.042574262977912315\r",
      "iteration 183 - y: 0.04236574015147914\r",
      "iteration 184 - y: 0.04215721732504595\r",
      "iteration 185 - y: 0.04194869449861277\r",
      "iteration 186 - y: 0.04174017167217959\r",
      "iteration 187 - y: 0.04153164884574641\r",
      "iteration 188 - y: 0.041323126019313226\r",
      "iteration 189 - y: 0.041114603192880045\r",
      "iteration 190 - y: 0.04090608036644686\r",
      "iteration 191 - y: 0.04069755754001368\r",
      "iteration 192 - y: 0.0404890347135805\r",
      "iteration 193 - y: 0.04028051188714732\r",
      "iteration 194 - y: 0.04007198906071414\r",
      "iteration 195 - y: 0.039863466234280956\r",
      "iteration 196 - y: 0.03965494340784777\r",
      "iteration 197 - y: 0.03944642058141459\r",
      "iteration 198 - y: 0.03923789775498141\r",
      "iteration 199 - y: 0.03902937492854823\r",
      "iteration 200 - y: 0.038820852102115055\r",
      "iteration 201 - y: 0.03861232927568187\r",
      "iteration 202 - y: 0.038403806449248686\r",
      "iteration 203 - y: 0.038195283622815504\r",
      "iteration 204 - y: 0.03798676079638232\r",
      "iteration 205 - y: 0.03777823796994914\r",
      "iteration 206 - y: 0.03756971514351596\r",
      "iteration 207 - y: 0.03736119231708278\r",
      "iteration 208 - y: 0.03715266949064959\r",
      "iteration 209 - y: 0.036944146664216415\r",
      "iteration 210 - y: 0.03673562383778324\r",
      "iteration 211 - y: 0.036527101011350045\r",
      "iteration 212 - y: 0.03631857818491687\r",
      "iteration 213 - y: 0.03611005535848369\r",
      "iteration 214 - y: 0.03590153253205051\r",
      "iteration 215 - y: 0.035693009705617326\r",
      "iteration 216 - y: 0.035484486879184145\r",
      "iteration 217 - y: 0.035275964052750956\r",
      "iteration 218 - y: 0.035067441226317775\r",
      "iteration 219 - y: 0.03485891839988459\r",
      "iteration 220 - y: 0.03465039557345142\r",
      "iteration 221 - y: 0.03444187274701823\r",
      "iteration 222 - y: 0.03423334992058505\r",
      "iteration 223 - y: 0.03402482709415187\r",
      "iteration 224 - y: 0.033816304267718686\r",
      "iteration 225 - y: 0.033607781441285504\r",
      "iteration 226 - y: 0.03339925861485232\r",
      "iteration 227 - y: 0.033190735788419135\r",
      "iteration 228 - y: 0.03298221296198596\r",
      "iteration 229 - y: 0.03277369013555278\r",
      "iteration 230 - y: 0.0325651673091196\r",
      "iteration 231 - y: 0.03235664448268641\r",
      "iteration 232 - y: 0.032148121656253234\r",
      "iteration 233 - y: 0.031939598829820046\r",
      "iteration 234 - y: 0.03173107600338687\r",
      "iteration 235 - y: 0.03152255317695368\r",
      "iteration 236 - y: 0.03131403035052051\r",
      "iteration 237 - y: 0.031105507524087327\r",
      "iteration 238 - y: 0.030896984697654138\r",
      "iteration 239 - y: 0.03068846187122096\r",
      "iteration 240 - y: 0.030479939044787782\r",
      "iteration 241 - y: 0.030271416218354594\r",
      "iteration 242 - y: 0.03006289339192142\r",
      "iteration 243 - y: 0.029854370565488238\r",
      "iteration 244 - y: 0.029645847739055056\r",
      "iteration 245 - y: 0.02943732491262187\r",
      "iteration 246 - y: 0.029228802086188693\r",
      "iteration 247 - y: 0.029020279259755512\r",
      "iteration 248 - y: 0.02881175643332233\r",
      "iteration 249 - y: 0.02860323360688915\r",
      "iteration 250 - y: 0.02839471078045597\r",
      "iteration 251 - y: 0.028186187954022786\r",
      "iteration 252 - y: 0.02797766512758961\r",
      "iteration 253 - y: 0.027769142301156423\r",
      "iteration 254 - y: 0.02756061947472324\r",
      "iteration 255 - y: 0.027352096648290063\r",
      "iteration 256 - y: 0.027143573821856882\r",
      "iteration 257 - y: 0.026935050995423704\r",
      "iteration 258 - y: 0.026726528168990522\r",
      "iteration 259 - y: 0.02651800534255734\r",
      "iteration 260 - y: 0.02630948251612416\r",
      "iteration 261 - y: 0.026100959689690978\r",
      "iteration 262 - y: 0.025892436863257796\r",
      "iteration 263 - y: 0.025683914036824615\r",
      "iteration 264 - y: 0.025475391210391433\r",
      "iteration 265 - y: 0.025266868383958252\r",
      "iteration 266 - y: 0.025058345557525074\r",
      "iteration 267 - y: 0.02484982273109189\r",
      "iteration 268 - y: 0.024641299904658708\r",
      "iteration 269 - y: 0.024432777078225526\r",
      "iteration 270 - y: 0.024224254251792348\r",
      "iteration 271 - y: 0.024015731425359163\r",
      "iteration 272 - y: 0.023807208598925985\r",
      "iteration 273 - y: 0.023598685772492804\r",
      "iteration 274 - y: 0.023390162946059626\r",
      "iteration 275 - y: 0.023181640119626437\r",
      "iteration 276 - y: 0.02297311729319326\r",
      "iteration 277 - y: 0.022764594466760078\r",
      "iteration 278 - y: 0.0225560716403269\r",
      "iteration 279 - y: 0.022347548813893715\r",
      "iteration 280 - y: 0.022139025987460533\r",
      "iteration 281 - y: 0.021930503161027355\r",
      "iteration 282 - y: 0.02172198033459417\r",
      "iteration 283 - y: 0.021513457508160992\r",
      "iteration 284 - y: 0.021304934681727807\r",
      "iteration 285 - y: 0.02109641185529463\r",
      "iteration 286 - y: 0.020887889028861448\r",
      "iteration 287 - y: 0.020679366202428266\r",
      "iteration 288 - y: 0.020470843375995085\r",
      "iteration 289 - y: 0.020262320549561903\r",
      "iteration 290 - y: 0.020075796292896086\r",
      "iteration 291 - y: 0.019908618493552025\r",
      "iteration 292 - y: 0.01974144069420796\r",
      "iteration 293 - y: 0.0195742628948639\r",
      "iteration 294 - y: 0.019407085095519836\r",
      "iteration 295 - y: 0.019239907296175772\r",
      "iteration 296 - y: 0.01907272949683171\r",
      "iteration 297 - y: 0.01890555169748765\r",
      "iteration 298 - y: 0.018738373898143586\r",
      "iteration 299 - y: 0.018571196098799522\r",
      "iteration 300 - y: 0.01840401829945546\r",
      "iteration 301 - y: 0.0182368405001114\r",
      "iteration 302 - y: 0.018069662700767337\r",
      "iteration 303 - y: 0.017902484901423276\r",
      "iteration 304 - y: 0.017735307102079212\r",
      "iteration 305 - y: 0.01756812930273515\r",
      "iteration 306 - y: 0.01740309204426817\r",
      "iteration 307 - y: 0.017267881527219336\r",
      "iteration 308 - y: 0.017132671010170502\r",
      "iteration 309 - y: 0.016997460493121667\r",
      "iteration 310 - y: 0.01686224997607283\r",
      "iteration 311 - y: 0.016727039459023995\r",
      "iteration 312 - y: 0.01659182894197516\r",
      "iteration 313 - y: 0.016456618424926323\r",
      "iteration 314 - y: 0.01632140790787749\r",
      "iteration 315 - y: 0.016186197390828654\r",
      "iteration 316 - y: 0.01605098687377982\r",
      "iteration 317 - y: 0.015915776356730985\r",
      "iteration 318 - y: 0.01578056583968215\r",
      "iteration 319 - y: 0.015645355322633312\r",
      "iteration 320 - y: 0.01551014480558448\r",
      "iteration 321 - y: 0.015374934288535643\r",
      "iteration 322 - y: 0.015239723771486809\r",
      "iteration 323 - y: 0.015104513254437974\r",
      "iteration 324 - y: 0.01496930273738914\r",
      "iteration 325 - y: 0.014834092220340303\r",
      "iteration 326 - y: 0.014698881703291467\r",
      "iteration 327 - y: 0.014563671186242633\r",
      "iteration 328 - y: 0.014428460669193798\r",
      "iteration 329 - y: 0.014293250152144964\r",
      "iteration 330 - y: 0.014158039635096127\r",
      "iteration 331 - y: 0.014022829118047291\r",
      "iteration 332 - y: 0.013887618600998457\r",
      "iteration 333 - y: 0.01375240808394962\r",
      "iteration 334 - y: 0.013617197566900786\r",
      "iteration 335 - y: 0.01348198704985195\r",
      "iteration 336 - y: 0.013346776532803115\r",
      "iteration 337 - y: 0.01321156601575428\r",
      "iteration 338 - y: 0.013076355498705443\r",
      "iteration 339 - y: 0.012941144981656608\r",
      "iteration 340 - y: 0.012805934464607772\r",
      "iteration 341 - y: 0.012670723947558938\r",
      "iteration 342 - y: 0.012535513430510101\r",
      "iteration 343 - y: 0.012401024294965455\r",
      "iteration 344 - y: 0.012317295662634559\r",
      "iteration 345 - y: 0.012233567030303667\r",
      "iteration 346 - y: 0.012149838397972772\r",
      "iteration 347 - y: 0.012066109765641878\r",
      "iteration 348 - y: 0.011982381133310984\r",
      "iteration 349 - y: 0.01189865250098009\r",
      "iteration 350 - y: 0.011814923868649196\r",
      "iteration 351 - y: 0.011731195236318302\r",
      "iteration 352 - y: 0.011647466603987407\r",
      "iteration 353 - y: 0.011563737971656513\r",
      "iteration 354 - y: 0.011480009339325619\r",
      "iteration 355 - y: 0.011396280706994723\r",
      "iteration 356 - y: 0.01131255207466383\r",
      "iteration 357 - y: 0.011228823442332937\r",
      "iteration 358 - y: 0.01114509481000204\r",
      "iteration 359 - y: 0.011061366177671146\r",
      "iteration 360 - y: 0.010977637545340254\r",
      "iteration 361 - y: 0.01089390891300936\r",
      "iteration 362 - y: 0.010810180280678464\r",
      "iteration 363 - y: 0.010726451648347572\r",
      "iteration 364 - y: 0.010642723016016677\r",
      "iteration 365 - y: 0.010558994383685783\r",
      "iteration 366 - y: 0.010475265751354889\r",
      "iteration 367 - y: 0.010391537119023995\r",
      "iteration 368 - y: 0.0103078084866931\r",
      "iteration 369 - y: 0.010224079854362206\r",
      "iteration 370 - y: 0.010140351222031312\r",
      "iteration 371 - y: 0.010056622589700418\r",
      "iteration 372 - y: 0.009972893957369522\r",
      "iteration 373 - y: 0.00988916532503863\r",
      "iteration 374 - y: 0.009805436692707736\r",
      "iteration 375 - y: 0.009721708060376841\r",
      "iteration 376 - y: 0.009637979428045947\r",
      "iteration 377 - y: 0.009554250795715053\r",
      "iteration 378 - y: 0.009470522163384159\r",
      "iteration 379 - y: 0.009386793531053265\r",
      "iteration 380 - y: 0.00930306489872237\r",
      "iteration 381 - y: 0.009219336266391476\r",
      "iteration 382 - y: 0.009135607634060582\r",
      "iteration 383 - y: 0.009051879001729688\r",
      "iteration 384 - y: 0.008968150369398794\r",
      "iteration 385 - y: 0.0088844217370679\r",
      "iteration 386 - y: 0.008800693104737006\r",
      "iteration 387 - y: 0.008716964472406111\r",
      "iteration 388 - y: 0.008633235840075217\r",
      "iteration 389 - y: 0.008549507207744323\r",
      "iteration 390 - y: 0.008465778575413429\r",
      "iteration 391 - y: 0.008382049943082536\r",
      "iteration 392 - y: 0.008298321310751642\r",
      "iteration 393 - y: 0.008214592678420746\r",
      "iteration 394 - y: 0.008130864046089852\r",
      "iteration 395 - y: 0.008047135413758958\r",
      "iteration 396 - y: 0.007963406781428066\r",
      "iteration 397 - y: 0.007879678149097171\r",
      "iteration 398 - y: 0.007795949516766276\r",
      "iteration 399 - y: 0.007712220884435383\r",
      "iteration 400 - y: 0.007628492252104489\r",
      "iteration 401 - y: 0.007544763619773595\r",
      "iteration 402 - y: 0.007461034987442701\r",
      "iteration 403 - y: 0.007377306355111806\r",
      "iteration 404 - y: 0.007293577722780912\r",
      "iteration 405 - y: 0.007209849090450018\r",
      "iteration 406 - y: 0.007126120458119124\r",
      "iteration 407 - y: 0.00704239182578823\r",
      "iteration 408 - y: 0.006958663193457336\r",
      "iteration 409 - y: 0.006874934561126441\r",
      "iteration 410 - y: 0.006791205928795548\r",
      "iteration 411 - y: 0.006707477296464653\r",
      "iteration 412 - y: 0.006623748664133759\r",
      "iteration 413 - y: 0.006541526396521083\r",
      "iteration 414 - y: 0.006481302617438785\r",
      "iteration 415 - y: 0.006421078838356488\r",
      "iteration 416 - y: 0.006360855059274189\r",
      "iteration 417 - y: 0.0063006312801918915\r",
      "iteration 418 - y: 0.006240407501109593\r",
      "iteration 419 - y: 0.006180183722027295\r",
      "iteration 420 - y: 0.006119959942944998\r",
      "iteration 421 - y: 0.006059736163862699\r",
      "iteration 422 - y: 0.0059995123847804005\r",
      "iteration 423 - y: 0.005939288605698103\r",
      "iteration 424 - y: 0.005879064826615805\r",
      "iteration 425 - y: 0.005818841047533507\r",
      "iteration 426 - y: 0.005758617268451208\r",
      "iteration 427 - y: 0.00569839348936891\r",
      "iteration 428 - y: 0.005638169710286612\r",
      "iteration 429 - y: 0.005577945931204314\r",
      "iteration 430 - y: 0.005517722152122016\r",
      "iteration 431 - y: 0.005457498373039717\r",
      "iteration 432 - y: 0.0053972745939574185\r",
      "iteration 433 - y: 0.005337050814875121\r",
      "iteration 434 - y: 0.005276827035792823\r",
      "iteration 435 - y: 0.005216603256710525\r",
      "iteration 436 - y: 0.005156379477628227\r",
      "iteration 437 - y: 0.005096155698545928\r",
      "iteration 438 - y: 0.005035931919463631\r",
      "iteration 439 - y: 0.004975708140381332\r",
      "iteration 440 - y: 0.004915484361299034\r",
      "iteration 441 - y: 0.004855260582216735\r",
      "iteration 442 - y: 0.004795036803134437\r",
      "iteration 443 - y: 0.00473481302405214\r",
      "iteration 444 - y: 0.004674589244969841\r",
      "iteration 445 - y: 0.0046143654658875434\r",
      "iteration 446 - y: 0.004554141686805245\r",
      "iteration 447 - y: 0.004493917907722947\r",
      "iteration 448 - y: 0.004433694128640649\r",
      "iteration 449 - y: 0.00437347034955835\r",
      "iteration 450 - y: 0.0043132465704760516\r",
      "iteration 451 - y: 0.004253022791393754\r",
      "iteration 452 - y: 0.004192799012311455\r",
      "iteration 453 - y: 0.004132575233229158\r",
      "iteration 454 - y: 0.004072351454146859\r",
      "iteration 455 - y: 0.004012127675064561\r",
      "iteration 456 - y: 0.003951903895982264\r",
      "iteration 457 - y: 0.0038916801168999648\r",
      "iteration 458 - y: 0.0038314563378176666\r",
      "iteration 459 - y: 0.0037712325587353685\r",
      "iteration 460 - y: 0.0037110087796530704\r",
      "iteration 461 - y: 0.0036507850005707727\r",
      "iteration 462 - y: 0.0035905612214884746\r",
      "iteration 463 - y: 0.0035303374424061765\r",
      "iteration 464 - y: 0.0034701136633238784\r",
      "iteration 465 - y: 0.0034098898842415807\r",
      "iteration 466 - y: 0.0033496661051592826\r",
      "iteration 467 - y: 0.0032894423260769845\r",
      "iteration 468 - y: 0.0032292185469946864\r",
      "iteration 469 - y: 0.003168994767912388\r",
      "iteration 470 - y: 0.0031087709888300897\r",
      "iteration 471 - y: 0.0030485472097477907\r",
      "iteration 472 - y: 0.0029883234306654926\r",
      "iteration 473 - y: 0.0029280996515831945\r",
      "iteration 474 - y: 0.002867875872500896\r",
      "iteration 475 - y: 0.002807652093418598\r",
      "iteration 476 - y: 0.0027474283143362997\r",
      "iteration 477 - y: 0.002687204535254001\r",
      "iteration 478 - y: 0.0026269807561717026\r",
      "iteration 479 - y: 0.0025667569770894045\r",
      "iteration 480 - y: 0.0025065331980071064\r",
      "iteration 481 - y: 0.002446309418924808\r",
      "iteration 482 - y: 0.0023860856398425093\r",
      "iteration 483 - y: 0.002325861860760211\r",
      "iteration 484 - y: 0.0022656380816779126\r",
      "iteration 485 - y: 0.002205414302595614\r",
      "iteration 486 - y: 0.002145190523513316\r",
      "iteration 487 - y: 0.0020849667444310174\r",
      "iteration 488 - y: 0.0020247429653487193\r",
      "iteration 489 - y: 0.0019645191862664208\r",
      "iteration 490 - y: 0.0019042954071841224\r",
      "iteration 491 - y: 0.001844071628101824\r",
      "iteration 492 - y: 0.0017838478490195258\r",
      "iteration 493 - y: 0.0017236240699372272\r",
      "iteration 494 - y: 0.001663400290854929\r",
      "iteration 495 - y: 0.0016031765117726306\r",
      "iteration 496 - y: 0.0015429527326903322\r",
      "iteration 497 - y: 0.001482728953608034\r",
      "iteration 498 - y: 0.0014365330981137834\r",
      "iteration 499 - y: 0.001406117589962729\r",
      "iteration 500 - y: 0.0013757020818116745\r",
      "iteration 501 - y: 0.0013452865736606199\r",
      "iteration 502 - y: 0.0013148710655095654\r",
      "iteration 503 - y: 0.001284455557358511\r",
      "iteration 504 - y: 0.0012540400492074566\r",
      "iteration 505 - y: 0.0012236245410564021\r",
      "iteration 506 - y: 0.0011932090329053477\r",
      "iteration 507 - y: 0.0011627935247542933\r",
      "iteration 508 - y: 0.001132378016603239\r",
      "iteration 509 - y: 0.0011019625084521846\r",
      "iteration 510 - y: 0.0010715470003011304\r",
      "iteration 511 - y: 0.001041131492150076\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 512 - y: 0.0010107159839990217\r",
      "iteration 513 - y: 0.0009803004758479673\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-3.2773125e-05]], dtype=float32),\n",
       " array([[-1.9697356e-05]], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.001}\n",
    "X = XAIR(best_model, 'lrp.alpha_1_beta_0', 'letzgus', M_samples[:2], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f591db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.061140154095025656\r",
      "iteration 2 - y: 0.06093163126859248\r",
      "iteration 3 - y: 0.06072310844215929\r",
      "iteration 4 - y: 0.06051458561572612\r",
      "iteration 5 - y: 0.060306062789292916\r",
      "iteration 6 - y: 0.06009753996285974\r",
      "iteration 7 - y: 0.05988901713642656\r",
      "iteration 8 - y: 0.05968049430999338\r",
      "iteration 9 - y: 0.059471971483560204\r",
      "iteration 10 - y: 0.059263448657127016\r",
      "iteration 11 - y: 0.05905492583069383\r",
      "iteration 12 - y: 0.058846403004260646\r",
      "iteration 13 - y: 0.05863788017782746\r",
      "iteration 14 - y: 0.058429357351394276\r",
      "iteration 15 - y: 0.0582208345249611\r",
      "iteration 16 - y: 0.05801231169852791\r",
      "iteration 17 - y: 0.05780378887209473\r",
      "iteration 18 - y: 0.057595266045661536\r",
      "iteration 19 - y: 0.05738674321922837\r",
      "iteration 20 - y: 0.05717822039279518\r",
      "iteration 21 - y: 0.056969697566361985\r",
      "iteration 22 - y: 0.05676117473992881\r",
      "iteration 23 - y: 0.05655265191349562\r",
      "iteration 24 - y: 0.05634412908706244\r",
      "iteration 25 - y: 0.05613560626062926\r",
      "iteration 26 - y: 0.055927083434196084\r",
      "iteration 27 - y: 0.055718560607762896\r",
      "iteration 28 - y: 0.055510037781329714\r",
      "iteration 29 - y: 0.05530151495489653\r",
      "iteration 30 - y: 0.055092992128463344\r",
      "iteration 31 - y: 0.054884469302030156\r",
      "iteration 32 - y: 0.05467594647559698\r",
      "iteration 33 - y: 0.05446742364916379\r",
      "iteration 34 - y: 0.054258900822730605\r",
      "iteration 35 - y: 0.05405037799629743\r",
      "iteration 36 - y: 0.05384185516986424\r",
      "iteration 37 - y: 0.05363333234343105\r",
      "iteration 38 - y: 0.05342480951699788\r",
      "iteration 39 - y: 0.0532162866905647\r",
      "iteration 40 - y: 0.053007763864131516\r",
      "iteration 41 - y: 0.05279924103769833\r",
      "iteration 42 - y: 0.05259071821126515\r",
      "iteration 43 - y: 0.05238219538483196\r",
      "iteration 44 - y: 0.052173672558398776\r",
      "iteration 45 - y: 0.051965149731965594\r",
      "iteration 46 - y: 0.05175662690553241\r",
      "iteration 47 - y: 0.051548104079099225\r",
      "iteration 48 - y: 0.05133958125266604\r",
      "iteration 49 - y: 0.05113105842623287\r",
      "iteration 50 - y: 0.05092253559979967\r",
      "iteration 51 - y: 0.0507140127733665\r",
      "iteration 52 - y: 0.05050548994693332\r",
      "iteration 53 - y: 0.05029696712050013\r",
      "iteration 54 - y: 0.05008844429406694\r",
      "iteration 55 - y: 0.04987992146763376\r",
      "iteration 56 - y: 0.049671398641200584\r",
      "iteration 57 - y: 0.049462875814767396\r",
      "iteration 58 - y: 0.04925435298833421\r",
      "iteration 59 - y: 0.04904583016190103\r",
      "iteration 60 - y: 0.048837307335467844\r",
      "iteration 61 - y: 0.048628784509034656\r",
      "iteration 62 - y: 0.04842026168260148\r",
      "iteration 63 - y: 0.04821173885616829\r",
      "iteration 64 - y: 0.04800321602973511\r",
      "iteration 65 - y: 0.04779469320330192\r",
      "iteration 66 - y: 0.04758617037686874\r",
      "iteration 67 - y: 0.04737764755043557\r",
      "iteration 68 - y: 0.04716912472400237\r",
      "iteration 69 - y: 0.0469606018975692\r",
      "iteration 70 - y: 0.04675207907113601\r",
      "iteration 71 - y: 0.04654355624470283\r",
      "iteration 72 - y: 0.046335033418269646\r",
      "iteration 73 - y: 0.04612651059183646\r",
      "iteration 74 - y: 0.045917987765403276\r",
      "iteration 75 - y: 0.045709464938970094\r",
      "iteration 76 - y: 0.04550094211253691\r",
      "iteration 77 - y: 0.045292419286103724\r",
      "iteration 78 - y: 0.04508389645967055\r",
      "iteration 79 - y: 0.04487537363323737\r",
      "iteration 80 - y: 0.04466685080680418\r",
      "iteration 81 - y: 0.04445832798037099\r",
      "iteration 82 - y: 0.04424980515393782\r",
      "iteration 83 - y: 0.04404128232750463\r",
      "iteration 84 - y: 0.04383275950107144\r",
      "iteration 85 - y: 0.04362423667463826\r",
      "iteration 86 - y: 0.04341571384820508\r",
      "iteration 87 - y: 0.043207191021771896\r",
      "iteration 88 - y: 0.04299866819533871\r",
      "iteration 89 - y: 0.04279014536890553\r",
      "iteration 90 - y: 0.042581622542472344\r",
      "iteration 91 - y: 0.042373099716039156\r",
      "iteration 92 - y: 0.042164576889605974\r",
      "iteration 93 - y: 0.04195605406317279\r",
      "iteration 94 - y: 0.041747531236739605\r",
      "iteration 95 - y: 0.04153900841030642\r",
      "iteration 96 - y: 0.04133048558387324\r",
      "iteration 97 - y: 0.04112196275744005\r",
      "iteration 98 - y: 0.04091343993100688\r",
      "iteration 99 - y: 0.0407049171045737\r",
      "iteration 100 - y: 0.04049639427814051\r",
      "iteration 101 - y: 0.04028787145170732\r",
      "iteration 102 - y: 0.04007934862527414\r",
      "iteration 103 - y: 0.03987082579884096\r",
      "iteration 104 - y: 0.03966230297240777\r",
      "iteration 105 - y: 0.03945378014597459\r",
      "iteration 106 - y: 0.039245257319541406\r",
      "iteration 107 - y: 0.03903673449310822\r",
      "iteration 108 - y: 0.038828211666675036\r",
      "iteration 109 - y: 0.03861968884024186\r",
      "iteration 110 - y: 0.03841116601380868\r",
      "iteration 111 - y: 0.0382026431873755\r",
      "iteration 112 - y: 0.0379941203609423\r",
      "iteration 113 - y: 0.03778559753450913\r",
      "iteration 114 - y: 0.03757707470807594\r",
      "iteration 115 - y: 0.03736855188164276\r",
      "iteration 116 - y: 0.03716002905520957\r",
      "iteration 117 - y: 0.03695150622877639\r",
      "iteration 118 - y: 0.03674298340234321\r",
      "iteration 119 - y: 0.03653446057591002\r",
      "iteration 120 - y: 0.036325937749476844\r",
      "iteration 121 - y: 0.036117414923043656\r",
      "iteration 122 - y: 0.035908892096610474\r",
      "iteration 123 - y: 0.03570036927017729\r",
      "iteration 124 - y: 0.035491846443744104\r",
      "iteration 125 - y: 0.03528332361731093\r",
      "iteration 126 - y: 0.03507480079087775\r",
      "iteration 127 - y: 0.03486627796444456\r",
      "iteration 128 - y: 0.03465775513801138\r",
      "iteration 129 - y: 0.03444923231157819\r",
      "iteration 130 - y: 0.034240709485145016\r",
      "iteration 131 - y: 0.03403218665871183\r",
      "iteration 132 - y: 0.03382366383227865\r",
      "iteration 133 - y: 0.03361514100584547\r",
      "iteration 134 - y: 0.03340661817941229\r",
      "iteration 135 - y: 0.03319809535297911\r",
      "iteration 136 - y: 0.03298957252654593\r",
      "iteration 137 - y: 0.032781049700112745\r",
      "iteration 138 - y: 0.032572526873679564\r",
      "iteration 139 - y: 0.03236400404724639\r",
      "iteration 140 - y: 0.0321554812208132\r",
      "iteration 141 - y: 0.031946958394380026\r",
      "iteration 142 - y: 0.031738435567946845\r",
      "iteration 143 - y: 0.03152991274151366\r",
      "iteration 144 - y: 0.031321389915080475\r",
      "iteration 145 - y: 0.031112867088647297\r",
      "iteration 146 - y: 0.03090434426221412\r",
      "iteration 147 - y: 0.030695821435780937\r",
      "iteration 148 - y: 0.03048729860934776\r",
      "iteration 149 - y: 0.030278775782914574\r",
      "iteration 150 - y: 0.030070252956481393\r",
      "iteration 151 - y: 0.02986173013004821\r",
      "iteration 152 - y: 0.02965320730361503\r",
      "iteration 153 - y: 0.02944468447718185\r",
      "iteration 154 - y: 0.029236161650748674\r",
      "iteration 155 - y: 0.029027638824315485\r",
      "iteration 156 - y: 0.028819115997882304\r",
      "iteration 157 - y: 0.028610593171449122\r",
      "iteration 158 - y: 0.028402070345015944\r",
      "iteration 159 - y: 0.028193547518582766\r",
      "iteration 160 - y: 0.02798502469214958\r",
      "iteration 161 - y: 0.0277765018657164\r",
      "iteration 162 - y: 0.027567979039283222\r",
      "iteration 163 - y: 0.02735945621285004\r",
      "iteration 164 - y: 0.0271560690051037\r",
      "iteration 165 - y: 0.02697951346096575\r",
      "iteration 166 - y: 0.026802957916827793\r",
      "iteration 167 - y: 0.026626402372689838\r",
      "iteration 168 - y: 0.026449846828551886\r",
      "iteration 169 - y: 0.026273291284413934\r",
      "iteration 170 - y: 0.026096735740275975\r",
      "iteration 171 - y: 0.025920180196138023\r",
      "iteration 172 - y: 0.025743624652000065\r",
      "iteration 173 - y: 0.025567069107862113\r",
      "iteration 174 - y: 0.02539051356372416\r",
      "iteration 175 - y: 0.02524306356926449\r",
      "iteration 176 - y: 0.02510785305221565\r",
      "iteration 177 - y: 0.024972642535166817\r",
      "iteration 178 - y: 0.02483743201811798\r",
      "iteration 179 - y: 0.024702221501069144\r",
      "iteration 180 - y: 0.024567010984020313\r",
      "iteration 181 - y: 0.024431800466971475\r",
      "iteration 182 - y: 0.02429658994992264\r",
      "iteration 183 - y: 0.02416137943287381\r",
      "iteration 184 - y: 0.024026168915824972\r",
      "iteration 185 - y: 0.023890958398776134\r",
      "iteration 186 - y: 0.023755747881727303\r",
      "iteration 187 - y: 0.023620537364678465\r",
      "iteration 188 - y: 0.023485326847629634\r",
      "iteration 189 - y: 0.0233501163305808\r",
      "iteration 190 - y: 0.02321490581353196\r",
      "iteration 191 - y: 0.023079695296483124\r",
      "iteration 192 - y: 0.02294448477943429\r",
      "iteration 193 - y: 0.022809274262385455\r",
      "iteration 194 - y: 0.02268518578711406\r",
      "iteration 195 - y: 0.02257348012331382\r",
      "iteration 196 - y: 0.022461774459513584\r",
      "iteration 197 - y: 0.022350068795713343\r",
      "iteration 198 - y: 0.022238363131913103\r",
      "iteration 199 - y: 0.02212665746811287\r",
      "iteration 200 - y: 0.02201495180431263\r",
      "iteration 201 - y: 0.021903246140512388\r",
      "iteration 202 - y: 0.02179154047671215\r",
      "iteration 203 - y: 0.02167983481291191\r",
      "iteration 204 - y: 0.021568129149111673\r",
      "iteration 205 - y: 0.021456423485311436\r",
      "iteration 206 - y: 0.021344717821511195\r",
      "iteration 207 - y: 0.021233012157710954\r",
      "iteration 208 - y: 0.021121306493910714\r",
      "iteration 209 - y: 0.021009600830110477\r",
      "iteration 210 - y: 0.02089789516631024\r",
      "iteration 211 - y: 0.02078618950251\r",
      "iteration 212 - y: 0.02067448383870976\r",
      "iteration 213 - y: 0.02056277817490952\r",
      "iteration 214 - y: 0.02045107251110928\r",
      "iteration 215 - y: 0.020339366847309047\r",
      "iteration 216 - y: 0.020227661183508806\r",
      "iteration 217 - y: 0.020115955519708566\r",
      "iteration 218 - y: 0.020004249855908325\r",
      "iteration 219 - y: 0.019892544192108084\r",
      "iteration 220 - y: 0.01978083852830785\r",
      "iteration 221 - y: 0.019669132864507614\r",
      "iteration 222 - y: 0.019557427200707373\r",
      "iteration 223 - y: 0.019445721536907132\r",
      "iteration 224 - y: 0.019334015873106892\r",
      "iteration 225 - y: 0.019222310209306655\r",
      "iteration 226 - y: 0.019110604545506418\r",
      "iteration 227 - y: 0.018998898881706177\r",
      "iteration 228 - y: 0.01888719321790594\r",
      "iteration 229 - y: 0.0187754875541057\r",
      "iteration 230 - y: 0.01866378189030546\r",
      "iteration 231 - y: 0.018552076226505218\r",
      "iteration 232 - y: 0.018440370562704977\r",
      "iteration 233 - y: 0.018328664898904744\r",
      "iteration 234 - y: 0.018216959235104507\r",
      "iteration 235 - y: 0.018105253571304263\r",
      "iteration 236 - y: 0.017993547907504025\r",
      "iteration 237 - y: 0.017881842243703785\r",
      "iteration 238 - y: 0.017770136579903544\r",
      "iteration 239 - y: 0.017658430916103307\r",
      "iteration 240 - y: 0.017546725252303066\r",
      "iteration 241 - y: 0.017435019588502826\r",
      "iteration 242 - y: 0.017323313924702592\r",
      "iteration 243 - y: 0.01721160826090235\r",
      "iteration 244 - y: 0.01709990259710211\r",
      "iteration 245 - y: 0.016988196933301874\r",
      "iteration 246 - y: 0.016876491269501633\r",
      "iteration 247 - y: 0.016764785605701393\r",
      "iteration 248 - y: 0.016653079941901152\r",
      "iteration 249 - y: 0.016541374278100915\r",
      "iteration 250 - y: 0.016429668614300674\r",
      "iteration 251 - y: 0.016317962950500437\r",
      "iteration 252 - y: 0.0162062572867002\r",
      "iteration 253 - y: 0.01609455162289996\r",
      "iteration 254 - y: 0.01598284595909972\r",
      "iteration 255 - y: 0.01587114029529948\r",
      "iteration 256 - y: 0.01575943463149924\r",
      "iteration 257 - y: 0.015647728967699\r",
      "iteration 258 - y: 0.015536023303898763\r",
      "iteration 259 - y: 0.015424317640098523\r",
      "iteration 260 - y: 0.015312611976298284\r",
      "iteration 261 - y: 0.015200906312498045\r",
      "iteration 262 - y: 0.015089200648697806\r",
      "iteration 263 - y: 0.014977494984897567\r",
      "iteration 264 - y: 0.014865789321097327\r",
      "iteration 265 - y: 0.01475408365729709\r",
      "iteration 266 - y: 0.01464237799349685\r",
      "iteration 267 - y: 0.01453067232969661\r",
      "iteration 268 - y: 0.014418966665896373\r",
      "iteration 269 - y: 0.014307261002096134\r",
      "iteration 270 - y: 0.014195555338295893\r",
      "iteration 271 - y: 0.014083849674495656\r",
      "iteration 272 - y: 0.013972144010695416\r",
      "iteration 273 - y: 0.013860438346895177\r",
      "iteration 274 - y: 0.013748732683094938\r",
      "iteration 275 - y: 0.013637027019294699\r",
      "iteration 276 - y: 0.01352532135549446\r",
      "iteration 277 - y: 0.013413615691694221\r",
      "iteration 278 - y: 0.013301910027893982\r",
      "iteration 279 - y: 0.013190204364093744\r",
      "iteration 280 - y: 0.013078498700293503\r",
      "iteration 281 - y: 0.012966793036493264\r",
      "iteration 282 - y: 0.012855087372693027\r",
      "iteration 283 - y: 0.012743381708892786\r",
      "iteration 284 - y: 0.012631676045092546\r",
      "iteration 285 - y: 0.01251997038129231\r",
      "iteration 286 - y: 0.01240826471749207\r",
      "iteration 287 - y: 0.012296559053691829\r",
      "iteration 288 - y: 0.01218485338989159\r",
      "iteration 289 - y: 0.012073147726091353\r",
      "iteration 290 - y: 0.011961442062291112\r",
      "iteration 291 - y: 0.011849736398490874\r",
      "iteration 292 - y: 0.011738030734690636\r",
      "iteration 293 - y: 0.011626325070890396\r",
      "iteration 294 - y: 0.011514619407090155\r",
      "iteration 295 - y: 0.01140291374328992\r",
      "iteration 296 - y: 0.01129120807948968\r",
      "iteration 297 - y: 0.011179502415689439\r",
      "iteration 298 - y: 0.0110677967518892\r",
      "iteration 299 - y: 0.010956091088088963\r",
      "iteration 300 - y: 0.010844385424288722\r",
      "iteration 301 - y: 0.010732679760488483\r",
      "iteration 302 - y: 0.010620974096688243\r",
      "iteration 303 - y: 0.010529177327546095\r",
      "iteration 304 - y: 0.010447887171896911\r",
      "iteration 305 - y: 0.010366597016247725\r",
      "iteration 306 - y: 0.010285306860598541\r",
      "iteration 307 - y: 0.010204016704949356\r",
      "iteration 308 - y: 0.010122726549300172\r",
      "iteration 309 - y: 0.010041436393650988\r",
      "iteration 310 - y: 0.009960146238001802\r",
      "iteration 311 - y: 0.009878856082352618\r",
      "iteration 312 - y: 0.009797565926703434\r",
      "iteration 313 - y: 0.009716275771054248\r",
      "iteration 314 - y: 0.009634985615405062\r",
      "iteration 315 - y: 0.00955369545975588\r",
      "iteration 316 - y: 0.009472405304106694\r",
      "iteration 317 - y: 0.009391115148457509\r",
      "iteration 318 - y: 0.009309824992808325\r",
      "iteration 319 - y: 0.00922853483715914\r",
      "iteration 320 - y: 0.009147244681509955\r",
      "iteration 321 - y: 0.009065954525860771\r",
      "iteration 322 - y: 0.008984664370211585\r",
      "iteration 323 - y: 0.008903374214562401\r",
      "iteration 324 - y: 0.008822084058913217\r",
      "iteration 325 - y: 0.008740793903264031\r",
      "iteration 326 - y: 0.008659503747614847\r",
      "iteration 327 - y: 0.008578213591965663\r",
      "iteration 328 - y: 0.008496923436316478\r",
      "iteration 329 - y: 0.008415633280667294\r",
      "iteration 330 - y: 0.008334343125018108\r",
      "iteration 331 - y: 0.008253052969368924\r",
      "iteration 332 - y: 0.00817176281371974\r",
      "iteration 333 - y: 0.008090472658070554\r",
      "iteration 334 - y: 0.00800918250242137\r",
      "iteration 335 - y: 0.007927892346772186\r",
      "iteration 336 - y: 0.007846602191123\r",
      "iteration 337 - y: 0.007765312035473816\r",
      "iteration 338 - y: 0.007684021879824632\r",
      "iteration 339 - y: 0.007602731724175447\r",
      "iteration 340 - y: 0.007521441568526261\r",
      "iteration 341 - y: 0.007440151412877076\r",
      "iteration 342 - y: 0.007358861257227891\r",
      "iteration 343 - y: 0.007277571101578706\r",
      "iteration 344 - y: 0.0071962809459295216\r",
      "iteration 345 - y: 0.007114990790280336\r",
      "iteration 346 - y: 0.007033700634631152\r",
      "iteration 347 - y: 0.006952410478981966\r",
      "iteration 348 - y: 0.006871120323332782\r",
      "iteration 349 - y: 0.006789830167683596\r",
      "iteration 350 - y: 0.006708540012034412\r",
      "iteration 351 - y: 0.006627249856385227\r",
      "iteration 352 - y: 0.006545959700736042\r",
      "iteration 353 - y: 0.006464669545086857\r",
      "iteration 354 - y: 0.006383379389437672\r",
      "iteration 355 - y: 0.006302089233788487\r",
      "iteration 356 - y: 0.006220799078139302\r",
      "iteration 357 - y: 0.0061395089224901165\r",
      "iteration 358 - y: 0.0060582187668409325\r",
      "iteration 359 - y: 0.005976928611191748\r",
      "iteration 360 - y: 0.005895638455542562\r",
      "iteration 361 - y: 0.005814348299893377\r",
      "iteration 362 - y: 0.005733058144244192\r",
      "iteration 363 - y: 0.005651767988595007\r",
      "iteration 364 - y: 0.0055704778329458225\r",
      "iteration 365 - y: 0.005489187677296637\r",
      "iteration 366 - y: 0.005407897521647451\r",
      "iteration 367 - y: 0.005326607365998266\r",
      "iteration 368 - y: 0.00524531721034908\r",
      "iteration 369 - y: 0.005164027054699895\r",
      "iteration 370 - y: 0.00508273689905071\r",
      "iteration 371 - y: 0.005001446743401524\r",
      "iteration 372 - y: 0.004920156587752339\r",
      "iteration 373 - y: 0.004838866432103153\r",
      "iteration 374 - y: 0.0047575762764539686\r",
      "iteration 375 - y: 0.004676286120804783\r",
      "iteration 376 - y: 0.004594995965155597\r",
      "iteration 377 - y: 0.004513705809506411\r",
      "iteration 378 - y: 0.0044324156538572265\r",
      "iteration 379 - y: 0.004351125498208042\r",
      "iteration 380 - y: 0.004269835342558856\r",
      "iteration 381 - y: 0.004188545186909671\r",
      "iteration 382 - y: 0.004107255031260486\r",
      "iteration 383 - y: 0.0040259648756113\r",
      "iteration 384 - y: 0.0039446747199621155\r",
      "iteration 385 - y: 0.00386338456431293\r",
      "iteration 386 - y: 0.003782094408663745\r",
      "iteration 387 - y: 0.003700804253014559\r",
      "iteration 388 - y: 0.003619514097365374\r",
      "iteration 389 - y: 0.0035382239417161886\r",
      "iteration 390 - y: 0.0034569337860670033\r",
      "iteration 391 - y: 0.003375643630417818\r",
      "iteration 392 - y: 0.0032943534747686327\r",
      "iteration 393 - y: 0.003213063319119448\r",
      "iteration 394 - y: 0.003131773163470262\r",
      "iteration 395 - y: 0.0030504830078210772\r",
      "iteration 396 - y: 0.002969192852171892\r",
      "iteration 397 - y: 0.0028879026965227066\r",
      "iteration 398 - y: 0.0028066125408735213\r",
      "iteration 399 - y: 0.002725322385224336\r",
      "iteration 400 - y: 0.0026440322295751507\r",
      "iteration 401 - y: 0.0025627420739259654\r",
      "iteration 402 - y: 0.0024814519182767797\r",
      "iteration 403 - y: 0.002400161762627595\r",
      "iteration 404 - y: 0.0023188716069784095\r",
      "iteration 405 - y: 0.0022375814513292246\r",
      "iteration 406 - y: 0.0021562912956800393\r",
      "iteration 407 - y: 0.0020750011400308545\r",
      "iteration 408 - y: 0.001993710984381669\r",
      "iteration 409 - y: 0.001912420828732484\r",
      "iteration 410 - y: 0.001831130673083299\r",
      "iteration 411 - y: 0.001749840517434114\r",
      "iteration 412 - y: 0.0016685503617849288\r",
      "iteration 413 - y: 0.0015872602061357437\r",
      "iteration 414 - y: 0.0015059700504865586\r",
      "iteration 415 - y: 0.0014246798948373736\r",
      "iteration 416 - y: 0.0013433897391881885\r",
      "iteration 417 - y: 0.001276115077254026\r",
      "iteration 418 - y: 0.0012246331925360849\r",
      "iteration 419 - y: 0.0011731513078181437\r",
      "iteration 420 - y: 0.0011216694231002025\r",
      "iteration 421 - y: 0.0010701875383822613\r",
      "iteration 422 - y: 0.0010187056536643201\r",
      "iteration 423 - y: 0.0009672237689463787\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.0803168945623185\r",
      "iteration 2 - y: 0.08010837173588532\r",
      "iteration 3 - y: 0.07989984890945212\r",
      "iteration 4 - y: 0.07969132608301895\r",
      "iteration 5 - y: 0.07948280325658576\r",
      "iteration 6 - y: 0.07927428043015257\r",
      "iteration 7 - y: 0.0790657576037194\r",
      "iteration 8 - y: 0.0788572347772862\r",
      "iteration 9 - y: 0.07864871195085303\r",
      "iteration 10 - y: 0.07844018912441984\r",
      "iteration 11 - y: 0.07823166629798664\r",
      "iteration 12 - y: 0.07802314347155347\r",
      "iteration 13 - y: 0.07781462064512028\r",
      "iteration 14 - y: 0.07760609781868709\r",
      "iteration 15 - y: 0.07739757499225391\r",
      "iteration 16 - y: 0.07718905216582073\r",
      "iteration 17 - y: 0.07698052933938754\r",
      "iteration 18 - y: 0.07677200651295435\r",
      "iteration 19 - y: 0.07656348368652116\r",
      "iteration 20 - y: 0.07635496086008797\r",
      "iteration 21 - y: 0.07614643803365478\r",
      "iteration 22 - y: 0.07593791520722162\r",
      "iteration 23 - y: 0.07572939238078842\r",
      "iteration 24 - y: 0.07552086955435525\r",
      "iteration 25 - y: 0.07531234672792204\r",
      "iteration 26 - y: 0.07510382390148887\r",
      "iteration 27 - y: 0.07489530107505568\r",
      "iteration 28 - y: 0.07468677824862249\r",
      "iteration 29 - y: 0.07447825542218932\r",
      "iteration 30 - y: 0.07426973259575613\r",
      "iteration 31 - y: 0.07406120976932296\r",
      "iteration 32 - y: 0.07385268694288977\r",
      "iteration 33 - y: 0.07364416411645658\r",
      "iteration 34 - y: 0.07343564129002339\r",
      "iteration 35 - y: 0.07322711846359022\r",
      "iteration 36 - y: 0.07301859563715704\r",
      "iteration 37 - y: 0.07281007281072385\r",
      "iteration 38 - y: 0.07260154998429066\r",
      "iteration 39 - y: 0.07239302715785748\r",
      "iteration 40 - y: 0.07218450433142429\r",
      "iteration 41 - y: 0.07197598150499111\r",
      "iteration 42 - y: 0.07176745867855792\r",
      "iteration 43 - y: 0.07155893585212474\r",
      "iteration 44 - y: 0.07135041302569156\r",
      "iteration 45 - y: 0.07114189019925837\r",
      "iteration 46 - y: 0.0709333673728252\r",
      "iteration 47 - y: 0.07072484454639201\r",
      "iteration 48 - y: 0.07051632171995884\r",
      "iteration 49 - y: 0.07030779889352565\r",
      "iteration 50 - y: 0.07009927606709246\r",
      "iteration 51 - y: 0.06989075324065928\r",
      "iteration 52 - y: 0.0696822304142261\r",
      "iteration 53 - y: 0.06947370758779292\r",
      "iteration 54 - y: 0.06926518476135973\r",
      "iteration 55 - y: 0.06905666193492654\r",
      "iteration 56 - y: 0.06884813910849336\r",
      "iteration 57 - y: 0.06863961628206018\r",
      "iteration 58 - y: 0.06843109345562699\r",
      "iteration 59 - y: 0.06822257062919382\r",
      "iteration 60 - y: 0.06801404780276063\r",
      "iteration 61 - y: 0.06780552497632744\r",
      "iteration 62 - y: 0.06759700214989427\r",
      "iteration 63 - y: 0.06738847932346108\r",
      "iteration 64 - y: 0.06717995649702789\r",
      "iteration 65 - y: 0.0669714336705947\r",
      "iteration 66 - y: 0.06676291084416153\r",
      "iteration 67 - y: 0.06655438801772834\r",
      "iteration 68 - y: 0.06634586519129515\r",
      "iteration 69 - y: 0.06613734236486198\r",
      "iteration 70 - y: 0.0659288195384288\r",
      "iteration 71 - y: 0.06572029671199561\r",
      "iteration 72 - y: 0.06551177388556242\r",
      "iteration 73 - y: 0.06530325105912925\r",
      "iteration 74 - y: 0.06509472823269605\r",
      "iteration 75 - y: 0.06488620540626287\r",
      "iteration 76 - y: 0.0646776825798297\r",
      "iteration 77 - y: 0.0644691597533965\r",
      "iteration 78 - y: 0.06426063692696334\r",
      "iteration 79 - y: 0.06405211410053015\r",
      "iteration 80 - y: 0.06384359127409696\r",
      "iteration 81 - y: 0.06363506844766378\r",
      "iteration 82 - y: 0.0634265456212306\r",
      "iteration 83 - y: 0.06321802279479741\r",
      "iteration 84 - y: 0.06300949996836422\r",
      "iteration 85 - y: 0.06280097714193104\r",
      "iteration 86 - y: 0.06259245431549787\r",
      "iteration 87 - y: 0.062383931489064674\r",
      "iteration 88 - y: 0.062175408662631486\r",
      "iteration 89 - y: 0.06196688583619831\r",
      "iteration 90 - y: 0.061758363009765116\r",
      "iteration 91 - y: 0.06154984018333195\r",
      "iteration 92 - y: 0.06134131735689876\r",
      "iteration 93 - y: 0.06113279453046557\r",
      "iteration 94 - y: 0.06092427170403239\r",
      "iteration 95 - y: 0.06071574887759921\r",
      "iteration 96 - y: 0.06050722605116603\r",
      "iteration 97 - y: 0.06029870322473284\r",
      "iteration 98 - y: 0.06009018039829965\r",
      "iteration 99 - y: 0.059881657571866476\r",
      "iteration 100 - y: 0.05967313474543329\r",
      "iteration 101 - y: 0.05946461191900011\r",
      "iteration 102 - y: 0.059256089092566924\r",
      "iteration 103 - y: 0.05904756626613374\r",
      "iteration 104 - y: 0.058839043439700554\r",
      "iteration 105 - y: 0.05863052061326738\r",
      "iteration 106 - y: 0.05842199778683419\r",
      "iteration 107 - y: 0.05821347496040101\r",
      "iteration 108 - y: 0.058004952133967835\r",
      "iteration 109 - y: 0.05779642930753465\r",
      "iteration 110 - y: 0.057587906481101465\r",
      "iteration 111 - y: 0.057379383654668284\r",
      "iteration 112 - y: 0.0571708608282351\r",
      "iteration 113 - y: 0.05696233800180192\r",
      "iteration 114 - y: 0.05675381517536873\r",
      "iteration 115 - y: 0.05654529234893556\r",
      "iteration 116 - y: 0.05633676952250237\r",
      "iteration 117 - y: 0.05612824669606918\r",
      "iteration 118 - y: 0.05591972386963601\r",
      "iteration 119 - y: 0.05571120104320282\r",
      "iteration 120 - y: 0.055502678216769644\r",
      "iteration 121 - y: 0.05529415539033645\r",
      "iteration 122 - y: 0.055085632563903274\r",
      "iteration 123 - y: 0.05487710973747009\r",
      "iteration 124 - y: 0.054668586911036904\r",
      "iteration 125 - y: 0.05446006408460372\r",
      "iteration 126 - y: 0.05425154125817054\r",
      "iteration 127 - y: 0.05404301843173736\r",
      "iteration 128 - y: 0.053834495605304185\r",
      "iteration 129 - y: 0.053625972778870996\r",
      "iteration 130 - y: 0.053417449952437815\r",
      "iteration 131 - y: 0.05320892712600463\r",
      "iteration 132 - y: 0.05300040429957145\r",
      "iteration 133 - y: 0.05279188147313827\r",
      "iteration 134 - y: 0.052583358646705075\r",
      "iteration 135 - y: 0.0523748358202719\r",
      "iteration 136 - y: 0.05216631299383871\r",
      "iteration 137 - y: 0.05195779016740554\r",
      "iteration 138 - y: 0.05174926734097236\r",
      "iteration 139 - y: 0.05154074451453917\r",
      "iteration 140 - y: 0.051332221688105986\r",
      "iteration 141 - y: 0.0511236988616728\r",
      "iteration 142 - y: 0.05091517603523962\r",
      "iteration 143 - y: 0.05070665320880644\r",
      "iteration 144 - y: 0.05049813038237326\r",
      "iteration 145 - y: 0.05028960755594007\r",
      "iteration 146 - y: 0.0500810847295069\r",
      "iteration 147 - y: 0.04987256190307371\r",
      "iteration 148 - y: 0.04966403907664053\r",
      "iteration 149 - y: 0.049455516250207346\r",
      "iteration 150 - y: 0.049246993423774164\r",
      "iteration 151 - y: 0.04903847059734098\r",
      "iteration 152 - y: 0.048829947770907794\r",
      "iteration 153 - y: 0.048621424944474606\r",
      "iteration 154 - y: 0.04841290211804143\r",
      "iteration 155 - y: 0.04820437929160824\r",
      "iteration 156 - y: 0.04799585646517507\r",
      "iteration 157 - y: 0.04778733363874188\r",
      "iteration 158 - y: 0.0475788108123087\r",
      "iteration 159 - y: 0.04737028798587552\r",
      "iteration 160 - y: 0.047161765159442336\r",
      "iteration 161 - y: 0.046953242333009154\r",
      "iteration 162 - y: 0.04674471950657597\r",
      "iteration 163 - y: 0.04653619668014279\r",
      "iteration 164 - y: 0.04632767385370961\r",
      "iteration 165 - y: 0.04611915102727642\r",
      "iteration 166 - y: 0.04591062820084324\r",
      "iteration 167 - y: 0.045702105374410065\r",
      "iteration 168 - y: 0.04549358254797688\r",
      "iteration 169 - y: 0.045285059721543695\r",
      "iteration 170 - y: 0.045076536895110514\r",
      "iteration 171 - y: 0.044868014068677325\r",
      "iteration 172 - y: 0.04465949124224415\r",
      "iteration 173 - y: 0.04445096841581096\r",
      "iteration 174 - y: 0.044242445589377774\r",
      "iteration 175 - y: 0.04403392276294459\r",
      "iteration 176 - y: 0.04382539993651141\r",
      "iteration 177 - y: 0.04361687711007823\r",
      "iteration 178 - y: 0.043408354283645055\r",
      "iteration 179 - y: 0.04319983145721187\r",
      "iteration 180 - y: 0.042991308630778685\r",
      "iteration 181 - y: 0.042782785804345504\r",
      "iteration 182 - y: 0.042574262977912315\r",
      "iteration 183 - y: 0.04236574015147914\r",
      "iteration 184 - y: 0.04215721732504595\r",
      "iteration 185 - y: 0.04194869449861277\r",
      "iteration 186 - y: 0.04174017167217959\r",
      "iteration 187 - y: 0.04153164884574641\r",
      "iteration 188 - y: 0.041323126019313226\r",
      "iteration 189 - y: 0.041114603192880045\r",
      "iteration 190 - y: 0.04090608036644686\r",
      "iteration 191 - y: 0.04069755754001368\r",
      "iteration 192 - y: 0.0404890347135805\r",
      "iteration 193 - y: 0.04028051188714732\r",
      "iteration 194 - y: 0.04007198906071414\r",
      "iteration 195 - y: 0.039863466234280956\r",
      "iteration 196 - y: 0.03965494340784777\r",
      "iteration 197 - y: 0.03944642058141459\r",
      "iteration 198 - y: 0.03923789775498141\r",
      "iteration 199 - y: 0.03902937492854823\r",
      "iteration 200 - y: 0.038820852102115055\r",
      "iteration 201 - y: 0.03861232927568187\r",
      "iteration 202 - y: 0.038403806449248686\r",
      "iteration 203 - y: 0.038195283622815504\r",
      "iteration 204 - y: 0.03798676079638232\r",
      "iteration 205 - y: 0.03777823796994914\r",
      "iteration 206 - y: 0.03756971514351596\r",
      "iteration 207 - y: 0.03736119231708278\r",
      "iteration 208 - y: 0.03715266949064959\r",
      "iteration 209 - y: 0.036944146664216415\r",
      "iteration 210 - y: 0.03673562383778324\r",
      "iteration 211 - y: 0.036527101011350045\r",
      "iteration 212 - y: 0.03631857818491687\r",
      "iteration 213 - y: 0.03611005535848369\r",
      "iteration 214 - y: 0.03590153253205051\r",
      "iteration 215 - y: 0.035693009705617326\r",
      "iteration 216 - y: 0.035484486879184145\r",
      "iteration 217 - y: 0.035275964052750956\r",
      "iteration 218 - y: 0.035067441226317775\r",
      "iteration 219 - y: 0.03485891839988459\r",
      "iteration 220 - y: 0.03465039557345142\r",
      "iteration 221 - y: 0.03444187274701823\r",
      "iteration 222 - y: 0.03423334992058505\r",
      "iteration 223 - y: 0.03402482709415187\r",
      "iteration 224 - y: 0.033816304267718686\r",
      "iteration 225 - y: 0.033607781441285504\r",
      "iteration 226 - y: 0.03339925861485232\r",
      "iteration 227 - y: 0.033190735788419135\r",
      "iteration 228 - y: 0.03298221296198596\r",
      "iteration 229 - y: 0.03277369013555278\r",
      "iteration 230 - y: 0.0325651673091196\r",
      "iteration 231 - y: 0.03235664448268641\r",
      "iteration 232 - y: 0.032148121656253234\r",
      "iteration 233 - y: 0.031939598829820046\r",
      "iteration 234 - y: 0.03173107600338687\r",
      "iteration 235 - y: 0.03152255317695368\r",
      "iteration 236 - y: 0.03131403035052051\r",
      "iteration 237 - y: 0.031105507524087327\r",
      "iteration 238 - y: 0.030896984697654138\r",
      "iteration 239 - y: 0.03068846187122096\r",
      "iteration 240 - y: 0.030479939044787782\r",
      "iteration 241 - y: 0.030271416218354594\r",
      "iteration 242 - y: 0.03006289339192142\r",
      "iteration 243 - y: 0.029854370565488238\r",
      "iteration 244 - y: 0.029645847739055056\r",
      "iteration 245 - y: 0.02943732491262187\r",
      "iteration 246 - y: 0.029228802086188693\r",
      "iteration 247 - y: 0.029020279259755512\r",
      "iteration 248 - y: 0.02881175643332233\r",
      "iteration 249 - y: 0.02860323360688915\r",
      "iteration 250 - y: 0.02839471078045597\r",
      "iteration 251 - y: 0.028186187954022786\r",
      "iteration 252 - y: 0.02797766512758961\r",
      "iteration 253 - y: 0.027769142301156423\r",
      "iteration 254 - y: 0.02756061947472324\r",
      "iteration 255 - y: 0.027352096648290063\r",
      "iteration 256 - y: 0.027143573821856882\r",
      "iteration 257 - y: 0.026935050995423704\r",
      "iteration 258 - y: 0.026726528168990522\r",
      "iteration 259 - y: 0.02651800534255734\r",
      "iteration 260 - y: 0.02630948251612416\r",
      "iteration 261 - y: 0.026100959689690978\r",
      "iteration 262 - y: 0.025892436863257796\r",
      "iteration 263 - y: 0.025683914036824615\r",
      "iteration 264 - y: 0.025475391210391433\r",
      "iteration 265 - y: 0.025266868383958252\r",
      "iteration 266 - y: 0.025058345557525074\r",
      "iteration 267 - y: 0.02484982273109189\r",
      "iteration 268 - y: 0.024641299904658708\r",
      "iteration 269 - y: 0.024432777078225526\r",
      "iteration 270 - y: 0.024224254251792348\r",
      "iteration 271 - y: 0.024015731425359163\r",
      "iteration 272 - y: 0.023807208598925985\r",
      "iteration 273 - y: 0.023598685772492804\r",
      "iteration 274 - y: 0.023390162946059626\r",
      "iteration 275 - y: 0.023181640119626437\r",
      "iteration 276 - y: 0.02297311729319326\r",
      "iteration 277 - y: 0.022764594466760078\r",
      "iteration 278 - y: 0.0225560716403269\r",
      "iteration 279 - y: 0.022347548813893715\r",
      "iteration 280 - y: 0.022139025987460533\r",
      "iteration 281 - y: 0.021930503161027355\r",
      "iteration 282 - y: 0.02172198033459417\r",
      "iteration 283 - y: 0.021513457508160992\r",
      "iteration 284 - y: 0.021304934681727807\r",
      "iteration 285 - y: 0.02109641185529463\r",
      "iteration 286 - y: 0.020887889028861448\r",
      "iteration 287 - y: 0.020679366202428266\r",
      "iteration 288 - y: 0.020470843375995085\r",
      "iteration 289 - y: 0.020262320549561903\r",
      "iteration 290 - y: 0.020075796292896086\r",
      "iteration 291 - y: 0.019908618493552025\r",
      "iteration 292 - y: 0.01974144069420796\r",
      "iteration 293 - y: 0.0195742628948639\r",
      "iteration 294 - y: 0.019407085095519836\r",
      "iteration 295 - y: 0.019239907296175772\r",
      "iteration 296 - y: 0.01907272949683171\r",
      "iteration 297 - y: 0.01890555169748765\r",
      "iteration 298 - y: 0.018738373898143586\r",
      "iteration 299 - y: 0.018571196098799522\r",
      "iteration 300 - y: 0.01840401829945546\r",
      "iteration 301 - y: 0.0182368405001114\r",
      "iteration 302 - y: 0.018069662700767337\r",
      "iteration 303 - y: 0.017902484901423276\r",
      "iteration 304 - y: 0.017735307102079212\r",
      "iteration 305 - y: 0.01756812930273515\r",
      "iteration 306 - y: 0.01740309204426817\r",
      "iteration 307 - y: 0.017267881527219336\r",
      "iteration 308 - y: 0.017132671010170502\r",
      "iteration 309 - y: 0.016997460493121667\r",
      "iteration 310 - y: 0.01686224997607283\r",
      "iteration 311 - y: 0.016727039459023995\r",
      "iteration 312 - y: 0.01659182894197516\r",
      "iteration 313 - y: 0.016456618424926323\r",
      "iteration 314 - y: 0.01632140790787749\r",
      "iteration 315 - y: 0.016186197390828654\r",
      "iteration 316 - y: 0.01605098687377982\r",
      "iteration 317 - y: 0.015915776356730985\r",
      "iteration 318 - y: 0.01578056583968215\r",
      "iteration 319 - y: 0.015645355322633312\r",
      "iteration 320 - y: 0.01551014480558448\r",
      "iteration 321 - y: 0.015374934288535643\r",
      "iteration 322 - y: 0.015239723771486809\r",
      "iteration 323 - y: 0.015104513254437974\r",
      "iteration 324 - y: 0.01496930273738914\r",
      "iteration 325 - y: 0.014834092220340303\r",
      "iteration 326 - y: 0.014698881703291467\r",
      "iteration 327 - y: 0.014563671186242633\r",
      "iteration 328 - y: 0.014428460669193798\r",
      "iteration 329 - y: 0.014293250152144964\r",
      "iteration 330 - y: 0.014158039635096127\r",
      "iteration 331 - y: 0.014022829118047291\r",
      "iteration 332 - y: 0.013887618600998457\r",
      "iteration 333 - y: 0.01375240808394962\r",
      "iteration 334 - y: 0.013617197566900786\r",
      "iteration 335 - y: 0.01348198704985195\r",
      "iteration 336 - y: 0.013346776532803115\r",
      "iteration 337 - y: 0.01321156601575428\r",
      "iteration 338 - y: 0.013076355498705443\r",
      "iteration 339 - y: 0.012941144981656608\r",
      "iteration 340 - y: 0.012805934464607772\r",
      "iteration 341 - y: 0.012670723947558938\r",
      "iteration 342 - y: 0.012535513430510101\r",
      "iteration 343 - y: 0.012401024294965455\r",
      "iteration 344 - y: 0.012317295662634559\r",
      "iteration 345 - y: 0.012233567030303667\r",
      "iteration 346 - y: 0.012149838397972772\r",
      "iteration 347 - y: 0.012066109765641878\r",
      "iteration 348 - y: 0.011982381133310984\r",
      "iteration 349 - y: 0.01189865250098009\r",
      "iteration 350 - y: 0.011814923868649196\r",
      "iteration 351 - y: 0.011731195236318302\r",
      "iteration 352 - y: 0.011647466603987407\r",
      "iteration 353 - y: 0.011563737971656513\r",
      "iteration 354 - y: 0.011480009339325619\r",
      "iteration 355 - y: 0.011396280706994723\r",
      "iteration 356 - y: 0.01131255207466383\r",
      "iteration 357 - y: 0.011228823442332937\r",
      "iteration 358 - y: 0.01114509481000204\r",
      "iteration 359 - y: 0.011061366177671146\r",
      "iteration 360 - y: 0.010977637545340254\r",
      "iteration 361 - y: 0.01089390891300936\r",
      "iteration 362 - y: 0.010810180280678464\r",
      "iteration 363 - y: 0.010726451648347572\r",
      "iteration 364 - y: 0.010642723016016677\r",
      "iteration 365 - y: 0.010558994383685783\r",
      "iteration 366 - y: 0.010475265751354889\r",
      "iteration 367 - y: 0.010391537119023995\r",
      "iteration 368 - y: 0.0103078084866931\r",
      "iteration 369 - y: 0.010224079854362206\r",
      "iteration 370 - y: 0.010140351222031312\r",
      "iteration 371 - y: 0.010056622589700418\r",
      "iteration 372 - y: 0.009972893957369522\r",
      "iteration 373 - y: 0.00988916532503863\r",
      "iteration 374 - y: 0.009805436692707736\r",
      "iteration 375 - y: 0.009721708060376841\r",
      "iteration 376 - y: 0.009637979428045947\r",
      "iteration 377 - y: 0.009554250795715053\r",
      "iteration 378 - y: 0.009470522163384159\r",
      "iteration 379 - y: 0.009386793531053265\r",
      "iteration 380 - y: 0.00930306489872237\r",
      "iteration 381 - y: 0.009219336266391476\r",
      "iteration 382 - y: 0.009135607634060582\r",
      "iteration 383 - y: 0.009051879001729688\r",
      "iteration 384 - y: 0.008968150369398794\r",
      "iteration 385 - y: 0.0088844217370679\r",
      "iteration 386 - y: 0.008800693104737006\r",
      "iteration 387 - y: 0.008716964472406111\r",
      "iteration 388 - y: 0.008633235840075217\r",
      "iteration 389 - y: 0.008549507207744323\r",
      "iteration 390 - y: 0.008465778575413429\r",
      "iteration 391 - y: 0.008382049943082536\r",
      "iteration 392 - y: 0.008298321310751642\r",
      "iteration 393 - y: 0.008214592678420746\r",
      "iteration 394 - y: 0.008130864046089852\r",
      "iteration 395 - y: 0.008047135413758958\r",
      "iteration 396 - y: 0.007963406781428066\r",
      "iteration 397 - y: 0.007879678149097171\r",
      "iteration 398 - y: 0.007795949516766276\r",
      "iteration 399 - y: 0.007712220884435383\r",
      "iteration 400 - y: 0.007628492252104489\r",
      "iteration 401 - y: 0.007544763619773595\r",
      "iteration 402 - y: 0.007461034987442701\r",
      "iteration 403 - y: 0.007377306355111806\r",
      "iteration 404 - y: 0.007293577722780912\r",
      "iteration 405 - y: 0.007209849090450018\r",
      "iteration 406 - y: 0.007126120458119124\r",
      "iteration 407 - y: 0.00704239182578823\r",
      "iteration 408 - y: 0.006958663193457336\r",
      "iteration 409 - y: 0.006874934561126441\r",
      "iteration 410 - y: 0.006791205928795548\r",
      "iteration 411 - y: 0.006707477296464653\r",
      "iteration 412 - y: 0.006623748664133759\r",
      "iteration 413 - y: 0.006541526396521083\r",
      "iteration 414 - y: 0.006481302617438785\r",
      "iteration 415 - y: 0.006421078838356488\r",
      "iteration 416 - y: 0.006360855059274189\r",
      "iteration 417 - y: 0.0063006312801918915\r",
      "iteration 418 - y: 0.006240407501109593\r",
      "iteration 419 - y: 0.006180183722027295\r",
      "iteration 420 - y: 0.006119959942944998\r",
      "iteration 421 - y: 0.006059736163862699\r",
      "iteration 422 - y: 0.0059995123847804005\r",
      "iteration 423 - y: 0.005939288605698103\r",
      "iteration 424 - y: 0.005879064826615805\r",
      "iteration 425 - y: 0.005818841047533507\r",
      "iteration 426 - y: 0.005758617268451208\r",
      "iteration 427 - y: 0.00569839348936891\r",
      "iteration 428 - y: 0.005638169710286612\r",
      "iteration 429 - y: 0.005577945931204314\r",
      "iteration 430 - y: 0.005517722152122016\r",
      "iteration 431 - y: 0.005457498373039717\r",
      "iteration 432 - y: 0.0053972745939574185\r",
      "iteration 433 - y: 0.005337050814875121\r",
      "iteration 434 - y: 0.005276827035792823\r",
      "iteration 435 - y: 0.005216603256710525\r",
      "iteration 436 - y: 0.005156379477628227\r",
      "iteration 437 - y: 0.005096155698545928\r",
      "iteration 438 - y: 0.005035931919463631\r",
      "iteration 439 - y: 0.004975708140381332\r",
      "iteration 440 - y: 0.004915484361299034\r",
      "iteration 441 - y: 0.004855260582216735\r",
      "iteration 442 - y: 0.004795036803134437\r",
      "iteration 443 - y: 0.00473481302405214\r",
      "iteration 444 - y: 0.004674589244969841\r",
      "iteration 445 - y: 0.0046143654658875434\r",
      "iteration 446 - y: 0.004554141686805245\r",
      "iteration 447 - y: 0.004493917907722947\r",
      "iteration 448 - y: 0.004433694128640649\r",
      "iteration 449 - y: 0.00437347034955835\r",
      "iteration 450 - y: 0.0043132465704760516\r",
      "iteration 451 - y: 0.004253022791393754\r",
      "iteration 452 - y: 0.004192799012311455\r",
      "iteration 453 - y: 0.004132575233229158\r",
      "iteration 454 - y: 0.004072351454146859\r",
      "iteration 455 - y: 0.004012127675064561\r",
      "iteration 456 - y: 0.003951903895982264\r",
      "iteration 457 - y: 0.0038916801168999648\r",
      "iteration 458 - y: 0.0038314563378176666\r",
      "iteration 459 - y: 0.0037712325587353685\r",
      "iteration 460 - y: 0.0037110087796530704\r",
      "iteration 461 - y: 0.0036507850005707727\r",
      "iteration 462 - y: 0.0035905612214884746\r",
      "iteration 463 - y: 0.0035303374424061765\r",
      "iteration 464 - y: 0.0034701136633238784\r",
      "iteration 465 - y: 0.0034098898842415807\r",
      "iteration 466 - y: 0.0033496661051592826\r",
      "iteration 467 - y: 0.0032894423260769845\r",
      "iteration 468 - y: 0.0032292185469946864\r",
      "iteration 469 - y: 0.003168994767912388\r",
      "iteration 470 - y: 0.0031087709888300897\r",
      "iteration 471 - y: 0.0030485472097477907\r",
      "iteration 472 - y: 0.0029883234306654926\r",
      "iteration 473 - y: 0.0029280996515831945\r",
      "iteration 474 - y: 0.002867875872500896\r",
      "iteration 475 - y: 0.002807652093418598\r",
      "iteration 476 - y: 0.0027474283143362997\r",
      "iteration 477 - y: 0.002687204535254001\r",
      "iteration 478 - y: 0.0026269807561717026\r",
      "iteration 479 - y: 0.0025667569770894045\r",
      "iteration 480 - y: 0.0025065331980071064\r",
      "iteration 481 - y: 0.002446309418924808\r",
      "iteration 482 - y: 0.0023860856398425093\r",
      "iteration 483 - y: 0.002325861860760211\r",
      "iteration 484 - y: 0.0022656380816779126\r",
      "iteration 485 - y: 0.002205414302595614\r",
      "iteration 486 - y: 0.002145190523513316\r",
      "iteration 487 - y: 0.0020849667444310174\r",
      "iteration 488 - y: 0.0020247429653487193\r",
      "iteration 489 - y: 0.0019645191862664208\r",
      "iteration 490 - y: 0.0019042954071841224\r",
      "iteration 491 - y: 0.001844071628101824\r",
      "iteration 492 - y: 0.0017838478490195258\r",
      "iteration 493 - y: 0.0017236240699372272\r",
      "iteration 494 - y: 0.001663400290854929\r",
      "iteration 495 - y: 0.0016031765117726306\r",
      "iteration 496 - y: 0.0015429527326903322\r",
      "iteration 497 - y: 0.001482728953608034\r",
      "iteration 498 - y: 0.0014365330981137834\r",
      "iteration 499 - y: 0.001406117589962729\r",
      "iteration 500 - y: 0.0013757020818116745\r",
      "iteration 501 - y: 0.0013452865736606199\r",
      "iteration 502 - y: 0.0013148710655095654\r",
      "iteration 503 - y: 0.001284455557358511\r",
      "iteration 504 - y: 0.0012540400492074566\r",
      "iteration 505 - y: 0.0012236245410564021\r",
      "iteration 506 - y: 0.0011932090329053477\r",
      "iteration 507 - y: 0.0011627935247542933\r",
      "iteration 508 - y: 0.001132378016603239\r",
      "iteration 509 - y: 0.0011019625084521846\r",
      "iteration 510 - y: 0.0010715470003011304\r",
      "iteration 511 - y: 0.001041131492150076\r",
      "iteration 512 - y: 0.0010107159839990217\r",
      "iteration 513 - y: 0.0009803004758479673\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad0e47e87f0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkVUlEQVR4nO3dd3wUdf7H8dfsbgotoYcWQhGUoqBgAcGCimI/vRP1TrGeWA9RT9H72e4U23HYsHdRsTdQjEpv0nsPkAAJaaQnm2T3+/tjNptdkmBAZMB9Px+PPAizs7uzk9mZ93y+3/mOZYwxiIiIiDjE5fQCiIiISGRTGBERERFHKYyIiIiIoxRGRERExFEKIyIiIuIohRERERFxlMKIiIiIOEphRERERBzlcXoB6sPv97Nz506aNGmCZVlOL46IiIjUgzGGwsJC2rVrh8tVd/3jsAgjO3fuJDEx0enFEBERkf2QlpZGhw4d6nz8sAgjTZo0AewPExcX5/DSiIiISH0UFBSQmJgYPI7X5bAII1VNM3FxcQojIiIih5lf62KhDqwiIiLiKIURERERcZTCiIiIiDhKYUREREQcpTAiIiIijlIYEREREUcpjIiIiIijFEZERETEUQojIiIi4qh9DiMzZ87kggsuoF27dliWxZdffvmrz5kxYwb9+vUjNjaWLl268PLLL+/PsoqIiMgf0D6HkeLiYvr06cMLL7xQr/m3bNnCueeey+DBg1m6dCn3338/d9xxB5999tk+L6yIiIj88ezzvWmGDRvGsGHD6j3/yy+/TMeOHRk/fjwAPXr0YNGiRTzzzDNceuml+/r2IiIi8gfzu/cZmTdvHkOHDg2bdvbZZ7No0SIqKipqfY7X66WgoCDsR0T+mBZ88gxr5n/v9GKIiIN+9zCSkZFBQkJC2LSEhAQqKyvJzs6u9Tljx44lPj4++JOYmPh7L6aIOCB1wzJOXP1vGv8w2ulFEREHHZSrafa8dbAxptbpVcaMGUN+fn7wJy0t7XdfRhE5+LzFdtUz1l/q8JKIiJP2uc/IvmrTpg0ZGRlh0zIzM/F4PLRo0aLW58TExBATE/N7L5qIOMwYPwAWxuElEREn/e6VkQEDBpCcnBw27YcffqB///5ERUX93m8vIoeyqiqpwohIRNvnMFJUVMSyZctYtmwZYF+6u2zZMlJTUwG7ieXqq68Ozj9y5Ei2bdvG6NGjWbt2LW+++SZvvPEGd99994H5BCJy2PKrMiIi7EczzaJFizj99NOD/x892u54NmLECN5++23S09ODwQSgc+fOTJkyhTvvvJMXX3yRdu3a8dxzz+myXhEBvyojIrIfYeS0004LdkCtzdtvv11j2qmnnsqSJUv29a1E5A9PlRER0b1pRMRJezmxEZHIoTAiIo7R1TQiAgojIuKkQGXEpTAiEtEURkTEMcH+Z2quEYloCiMi4hjjVzONiCiMiIijqsKIiEQyhRERcY5GYBURFEZExEkKIyKCwoiIOMgojIgICiMi4iCNMyIioDAiIk4KVkZEJJIpjIiIY1QZERFQGBERRymMiIjCiIg4KZBBFEZEIpvCiIg4R800IoLCiIg4qHo4eBGJZAojIuKgwF17LVN90zwRiTgKIyLimNAAYvwKIyKRSmFERJwT6DMC4A/5XUQii8KIiDgntDKiZhqRiKUwIiIOqq6GKIyIRC6FERFxTFifETXTiEQshRERcY6aaUQEhRERcVJINaRqzBERiTwKIyLiHDXTiAgKIyLiIIOaaUREYUREnBTSNONXM41IxFIYERHHhN4gz+hmeSIRS2FERByj4eBFBBRGRMRJoVfTqAOrSMRSGBERx4R1WlVlRCRiKYyIiIPUZ0REFEZExEka9ExEUBgRESeFNNP4Nc6ISMRSGBER54R1YPU5uCAi4iSFERFxTmg1RJURkYilMCIijgnrtKowIhKxFEZExDGW+oyICAojIuIgE9ZnRGFEJFIpjIjIIUEjsIpELoUREXFO6BU0GmdEJGIpjIiIc8L6r6qZRiRSKYyIiHNC79qrMCISsRRGRMQ56sAqIiiMiIijQisj6jMiEqkURkTEObpRnoigMCIiTgrtM4KaaUQilcKIiDhIw8GLiMKIiDhJzTQigsKIiDhKl/aKiMKIiDjJqJlGRBRGRMRJYR1Y1UwjEqkURkTEQdUBxO9XZUQkUimMiIhzQvOHBj0TiVgKIyLiGCtsOHgHF0REHKUwIiIOCkkgurRXJGIpjIiIc9SBVUTYzzAyYcIEOnfuTGxsLP369WPWrFl7nX/ixIn06dOHhg0b0rZtW6699lpycnL2a4FF5I9Ed+0Vkf0II5MmTWLUqFE88MADLF26lMGDBzNs2DBSU1NrnX/27NlcffXVXH/99axevZpPPvmEhQsXcsMNN/zmhReRw5vROCMiwn6EkXHjxnH99ddzww030KNHD8aPH09iYiIvvfRSrfPPnz+fTp06cccdd9C5c2cGDRrETTfdxKJFi37zwovI4S20A6tfYUQkYu1TGCkvL2fx4sUMHTo0bPrQoUOZO3durc8ZOHAg27dvZ8qUKRhj2LVrF59++innnXdene/j9XopKCgI+xGRPzhd2isSsfYpjGRnZ+Pz+UhISAibnpCQQEZGRq3PGThwIBMnTmT48OFER0fTpk0bmjZtyvPPP1/n+4wdO5b4+PjgT2Ji4r4spogcLkIDiMKISMTarw6slmWF/d8YU2NalTVr1nDHHXfw4IMPsnjxYr7//nu2bNnCyJEj63z9MWPGkJ+fH/xJS0vbn8UUkUOchfqMiAh49mXmli1b4na7a1RBMjMza1RLqowdO5aTTz6Ze+65B4BjjjmGRo0aMXjwYP7zn//Qtm3bGs+JiYkhJiZmXxZNRA5HIQFEfUZEItc+VUaio6Pp168fycnJYdOTk5MZOHBgrc8pKSnB5Qp/G7fbDehSPhFRZURE9qOZZvTo0bz++uu8+eabrF27ljvvvJPU1NRgs8uYMWO4+uqrg/NfcMEFfP7557z00kukpKQwZ84c7rjjDk444QTatWt34D6JiBx+woaDV58RkUi1T800AMOHDycnJ4dHH32U9PR0evfuzZQpU0hKSgIgPT09bMyRa665hsLCQl544QXuuusumjZtypAhQ3jyyScP3KcQkcNT2I3yVBkRiVSWOQzaSgoKCoiPjyc/P5+4uDinF0dEDpAlE67huMwvAFg5dBJHDzzH4SUSkQOpvsdv3ZtGRBxjhZ0LHfLnRSLyO1EYEREHhdwoz68wIhKpFEZExDGWOrCKCAojIuIgE9Y0ozAiEqkURkTEOUbNNCKiMCIiDrJQB1YRURgRESeFVkbUZ0QkYimMiIhjwm+U59xyiIizFEZExDm6mkZEUBgREUfpRnkiojAiIk5SnxERQWFERBwU2mdEhRGRyKUwIiLOCUkglgY9E4lYCiMi4qCQcohfYUQkUimMiIhjQqshGoBVJHIpjIiIc8ICiCojIpFKYUREHBN61171YBWJXAojIuKg0KtpFEZEIpXCiIg4KHTQMzXTiEQqhRERcYxldNdeEVEYERFHqZlGRBRGRMRBllEYERGFERFxUNioqwojIhFLYUREHKQOrCKiMCIiTjJ1/kdEIojCiIg4JryZRpURkUilMCIiDlIHVhFRGBERB1kKICKCwoiIOMgK68CqYCISqRRGRMQ5oeOM+NVnRCRSKYyIiGPCOrDqahqRiKUwIiIO0jgjIqIwIiIOssK6jKgyIhKpFEZExEEaDl5EFEZExEFhV9OgZhqRSKUwIiLOCa2GqDAiErEURkTEMVbYCKyqjIhEKoUREXFMaBixVBoRiVgKIyLioNDKiIOLISKOUhgREceE35tGzTQikUphREQcpHvTiIjCiIg4yKUwIiIojIiIo9RMIyIKIyLioNA+IyqMiEQuhRERcUzYXXs1zohIxFIYEZFDgqXSiEjEUhgREceENdNo0DORiKUwIiKOsXTXXhFBYUREDhUKIyIRS2FERBwTfj8ahRGRSKUwIiKO0dU0IgIKIyLiICv0P2qmEYlYCiMi4hyjZhoRURgREQfpahoRAYUREXFQWDONKiMiEUthREQcow6sIgL7GUYmTJhA586diY2NpV+/fsyaNWuv83u9Xh544AGSkpKIiYmha9euvPnmm/u1wCLyx6EOrCIC4NnXJ0yaNIlRo0YxYcIETj75ZF555RWGDRvGmjVr6NixY63Pueyyy9i1axdvvPEGRxxxBJmZmVRWVv7mhReRw5wCiIiwH2Fk3LhxXH/99dxwww0AjB8/nqlTp/LSSy8xduzYGvN///33zJgxg5SUFJo3bw5Ap06dfttSi8gfgkvNNCLCPjbTlJeXs3jxYoYOHRo2fejQocydO7fW53z99df079+fp556ivbt29O9e3fuvvtuSktL63wfr9dLQUFB2I+I/NGpSiISqfapMpKdnY3P5yMhISFsekJCAhkZGbU+JyUlhdmzZxMbG8sXX3xBdnY2t9xyC7m5uXX2Gxk7diyPPPLIviyaiByGwoaDVxYRiVj71YHVsva4IM+YGtOq+P1+LMti4sSJnHDCCZx77rmMGzeOt99+u87qyJgxY8jPzw/+pKWl7c9iisghLuxqGtRMIxKp9qky0rJlS9xud40qSGZmZo1qSZW2bdvSvn174uPjg9N69OiBMYbt27fTrVu3Gs+JiYkhJiZmXxZNRA5DuppGRGAfKyPR0dH069eP5OTksOnJyckMHDiw1uecfPLJ7Ny5k6KiouC0DRs24HK56NChw34ssoj8UYQ30yiMiESqfW6mGT16NK+//jpvvvkma9eu5c477yQ1NZWRI0cCdhPL1VdfHZz/yiuvpEWLFlx77bWsWbOGmTNncs8993DdddfRoEGDA/dJROSwY+neNCLCflzaO3z4cHJycnj00UdJT0+nd+/eTJkyhaSkJADS09NJTU0Nzt+4cWOSk5O5/fbb6d+/Py1atOCyyy7jP//5z4H7FCJyWAqrjCiMiEQsy5hDvzZaUFBAfHw8+fn5xMXFOb04InKAFD/chkbYHdlntbqcwbe+4vASiciBVN/jt+5NIyKOcakyIiIojIiIo6oDiKUsIhKxFEZExDHhlRGNMyISqRRGRMRBGoFVRBRGRMRB4eOMqDIiEqkURkTEMerAKiKgMCIijtIIrCKiMCIiDtrjlpsOLYWIOE1hREQc4w65gsZSZUQkYimMiMghQmFEJFIpjIiIM2pUQhRGRCKVwoiIOGPPS3nVTCMSsRRGRMQZe4QPS5URkYilMCIizlBlREQCFEZExCHqMyIiNoUREXHGns00Gg5eJGIpjIiIMxQ+RCRAYUREHLJHs4z6jIhELIUREXGGxhkRkQCFERFxRo1mGoURkUilMCIiDtmzA6vCiEikUhgREWfUCB/q0CoSqRRGRMQZezTTWA4thog4T2FERA4NaqYRiVgKIyLiDF1NIyIBCiMi4ow9m2k0CJpIxFIYERGHqDIiIjaFERFxhjqwikiAwoiIOGPPPiPqwCoSsRRGRMQhaqYREZvCiIg4Q8PBi0iAwoiIOMPsORy8rqYRiVQKIyLikD3CiENLISLOUxgREWeomUZEAhRGRMQZezbTKIyIRCyFERFxhNGlvSISoDAiIo4w/j0HPVMYEYlUCiMi4gijcUZEJEBhREQcUaMyomYakYilMCIijjC6mkZEAhRGRMQRCiMiUkVhREQcoWYaEamiMCIihwRdTSMSuRRGRMQRNcYZURgRiVgKIyLiCGN8Yf/XvWlEIpfCiIg4w68RWEXEpjAiIo7wG43AKiI2hRERccSel/Za7Hmpr4hECoUREXGGmmVEJEBhREScsUcWUTONSORSGBERR+x5NY0qJSKRS2FERBxh9riaRpURkcilMCIiztDVNCISoDAiIo7Qpb0iUkVhREScUSN7KIyIRCqFERFxRI3h4JVFRCLWfoWRCRMm0LlzZ2JjY+nXrx+zZs2q1/PmzJmDx+Ohb9+++/O2IvJHohvliUjAPoeRSZMmMWrUKB544AGWLl3K4MGDGTZsGKmpqXt9Xn5+PldffTVnnHHGfi+siPxx7HnXXvUZEYlc+xxGxo0bx/XXX88NN9xAjx49GD9+PImJibz00kt7fd5NN93ElVdeyYABA/Z7YUXkD0QdWEUkYJ/CSHl5OYsXL2bo0KFh04cOHcrcuXPrfN5bb73F5s2beeihh+r1Pl6vl4KCgrAfEflj8asyIiIB+xRGsrOz8fl8JCQkhE1PSEggIyOj1uds3LiR++67j4kTJ+LxeOr1PmPHjiU+Pj74k5iYuC+LKSKHA/UZEZGA/erAallW2P+NMTWmAfh8Pq688koeeeQRunfvXu/XHzNmDPn5+cGftLS0/VlMETmE1biaRmFEJGLVr1QR0LJlS9xud40qSGZmZo1qCUBhYSGLFi1i6dKl3HbbbQD4/X6MMXg8Hn744QeGDBlS43kxMTHExMTsy6KJyOFmj+HglUVEItc+VUaio6Pp168fycnJYdOTk5MZOHBgjfnj4uJYuXIly5YtC/6MHDmSI488kmXLlnHiiSf+tqUXkcNYePpw4a9jPhH5o9unygjA6NGjueqqq+jfvz8DBgzg1VdfJTU1lZEjRwJ2E8uOHTt49913cblc9O7dO+z5rVu3JjY2tsZ0EYksNW+UJyKRap/DyPDhw8nJyeHRRx8lPT2d3r17M2XKFJKSkgBIT0//1TFHRESM2bMSonYakUhlmT1HHjoEFRQUEB8fT35+PnFxcU4vjogcALsWfk7C5GuD/0+lDR0fXu/gEonIgVbf47fuTSMijtizmUaVEZHIpTAiIo4wgfDhN3ZvEZfCiEjEUhgREWf47T4j/kDXVY0zIhK5FEZExCF2GPFV7YaURUQilsKIiDiiqu+8UWVEJOIpjIiIMwIdWKsqIwojIpFLYUREHGH2aKZRGBGJXAojIuIMNdOISIDCiIg4omoEVn+wMiIikUphREScYcL7jOhyGpHIpTAiIo6ouprGrz4jIhFPYUREnGHCBz3TCKwikUthREScsUdlRM00IpFLYUREHGGMhoMXEZvCiIg4ZM8+IyISqRRGRMQR1R1Y3YAqIyKRTGFERJxRy117qwKKiEQWhRERcYgdRowVGkacXB4RcYrCiIg4oip4hPYZ8SuNiEQkhRERcUbgahoTMuiZoohIZFIYERFn1DICqwojIpFJYUREnFE1zoilZhqRSKcwIiKOqLpypqqZxhXo0CoikUdhREQcUT0Ca3VlRIURkcikMCIijrCqKiNW1dirRl1YRSKUwoiIOKTmCKx+ZRGRiKQwIiKOCA4Hb4U20yiNiEQihRERcUZwnBG7mcaFX400IhFKYUREnBGsgqgDq0ikUxgREUeYPcYZcVm6UZ5IpFIYERGHhI8zAqqMiEQqhRERcUbw0t7QMKI0IhKJFEZExBnBQc/c1ZOMRmEViUQKIyLijBqDnoHfrzAiEokURkTEGaa2PiMKIyKRSGFERBxhCL+axp6oPiMikUhhREScUTUCa8huyK/x4EUiksKIiDgj2ExT3WekqloiIpFFYUREnFE1HLxVfTUN6sAqEpEURkTEIbVVRtRMIxKJFEZExBnBS3urKyN+n8KISCRSGBERZ5harqZRZUQkIimMiIgjTK3jjCiMiEQihRERcUjg0t6we9P4nFoYEXGQwoiIOKPqaprQyojGGRGJSAojIuIICw0HLyI2hRERcUbwapqQMKIOrCIRSWFERBxhgoOehTbTOLU0IuIkhRERcUYtw8HjVwdWkUikMCIijlIzjYgojIiIM4JtMiHDwWucEZGIpDAiIs4IdmC18AV2RUY3yhOJSAojIuIIK1gZcQUbZ1QZEYlMCiMi4ojw/iFWLdNEJFIojIiIM0LGGQleUaNmGpGIpDAiIo6wqO7AqmYakci2X2FkwoQJdO7cmdjYWPr168esWbPqnPfzzz/nrLPOolWrVsTFxTFgwACmTp263wssIn8QVcHDsoJDwms4eJHItM9hZNKkSYwaNYoHHniApUuXMnjwYIYNG0Zqamqt88+cOZOzzjqLKVOmsHjxYk4//XQuuOACli5d+psXXkQOZ9WDnqkyIhLZ9jmMjBs3juuvv54bbriBHj16MH78eBITE3nppZdqnX/8+PH885//5Pjjj6dbt248/vjjdOvWjW+++eY3L7yIHMaClZGQPiPqwCoSkfYpjJSXl7N48WKGDh0aNn3o0KHMnTu3Xq/h9/spLCykefPmdc7j9XopKCgI+xGRP5iqJhnLCoYRvzqwikSkfQoj2dnZ+Hw+EhISwqYnJCSQkZFRr9f473//S3FxMZdddlmd84wdO5b4+PjgT2Ji4r4spogcVkLuTaNmGpGItF8dWC3LCvu/MabGtNp8+OGHPPzww0yaNInWrVvXOd+YMWPIz88P/qSlpe3PYorIoSykMuKv2hWpA6tIRPLsy8wtW7bE7XbXqIJkZmbWqJbsadKkSVx//fV88sknnHnmmXudNyYmhpiYmH1ZNBE57FR1YHVRVR3xqzIiEpH2qTISHR1Nv379SE5ODpuenJzMwIED63zehx9+yDXXXMMHH3zAeeedt39LKiJ/KMHh4K2QbqsKIyIRaZ8qIwCjR4/mqquuon///gwYMIBXX32V1NRURo4cCdhNLDt27ODdd98F7CBy9dVX8+yzz3LSSScFqyoNGjQgPj7+AH4UETmsBINH9dU0ulGeSGTa5zAyfPhwcnJyePTRR0lPT6d3795MmTKFpKQkANLT08PGHHnllVeorKzk1ltv5dZbbw1OHzFiBG+//fZv/wQicpiqHvQMq6o8osqISCTa5zACcMstt3DLLbfU+tieAWP69On78xYi8kcXrIxY+IN9RlQZEYlEujeNiDgkpDISCCOW+oyIRCSFERFxhqm+mibYZ0RZRCQiKYyIiCOq7tprQq6m0b1pRCKTwoiIOCMQPCyrepwRY3wOLpCIOEVhREQcUrMDq9ppRCKTwoiIOCLYWTWkA6uaaUQik8KIiDgjeBmvhQnc28ponBGRiKQwIiLOCqmM6EZ5IpFJYUREHOH2ewHwuaKq6yEaDl4kIimMiIgjPIEwUu5qELhzr/qvikQqhRERcYTbV1UZiQkZ9EyVEZFIpDAiIo6oqoxUumKC09SBVSQyKYyIiCM8/jLADiNVV9PgVxgRiUQKIyLiiPDKSCCMoGYakUikMCIijqgOI7EhfUZUGRGJRAojIuKIqmYan7s6jOhyGpHIpDAiIgefrxJ34KZ4le7qZhpdTSMSmRRGROTgqyyt/tUVW92BVYURkYikMCIiB19FdRjxuaKDv5tAtUREIovCiIgcfIEwUmaisCxXdZ8RlUZEIpLCiIgcfJV259UyorEsMFZgV6RxRkQiksKIiBx8VZURorFC7tqrEVhFIpPCiIgcfFWVEROojAQmG1VGRCKSwoiIHHyBykgp0VhUV0bUZ0QkMimMiMjBFwgj3mCfkapLezXOiEgkUhgRkYOvMqTPCFC1K9Jw8CKRSWFERA6+itr7jGg4eJHIpDAiIgdfWGXEgkAzjV/NNCIRSWFERA6+QGWklGhcFrgC44yUV2gEVpFIpDAiIgdfcATWaLAsLJddGSmvrHRyqUTEIQojInLw7dGBtaoyUqbKiEhEUhgRkYMv0ExTdWmv5VIzjUgkUxgRkYMvUBkpNXYHVlcgjHgrFUZEIpHCiIgcfBXhN8pzVfUZUWVEJCIpjIjIwVdRAthhxGWBu+pqGnVg3StvpU/VI/lDUhiRfTIueQMPfbVKI2XKb1MZWhmxcLncAJTrQFun8ko/5z47i3OfnUWlT+OxyB+LwojUW4XPz/M/b+SdedtIzy9zenHkcBa8tDcKAJfbbqbxqpmmTmvTC9icVczmrGK2ZBc7vTgiB5TCiNRbfmlFcLTu7CKvswsjh7dgZSTG7jMSbKZRGKnL0tTdwd/XZhQ6uCQiB57CiNRbXklF8HeFEflNKsKHg3e77V1RhcJInZam5QV/X5de4NyCiPwOFEak3vJLy4O/ZxeW72VOkV8RMgKrZYHbpcrIr1kSUhlZp8qI/MEojEi9hVVGilUZkd8g2EwTZY/A6qqujPj96hy9p6xCL2m5pcH/qzLy25RV+Hh9VgopWUVOL4oEKIxIvYWFEVVGDlm5xeUs2prr9GLsXUgzjcuygpURC0OpOrHWsCzQRHNpkzWc5lrGzvwy8kO+j7JvvluVzn8mr+WRb9b8bu8xPyWH/NJD92+0KbOQd+dtxec3vDxjM7d9sITF23b/+hN/Jx7H3lkOH+u/h4oS8kqPDU7as89Ien4pFZWGji0aHuylkz3c/clyfl6XyWc3D6RfUjOnF6d2lVV37Q10YA0JI8XllTSK0a4p1PK0PJpQwpOVT+CPhj5lr7Iuo4ATu7RwetEOS1uy7KuRlqbuxu83wUH39oUxhgqfIdpT85x+QUoOl786n5aNY/jl/jP26/V/b/d9tpJF23bjdll8vDCNlOxihhzV2rF9hiojh5Bp6zIZ8sz03+Ws9v3523hv/rZ9f6KvEj65Bj69joq89ODknJBmmvJKP396cS7nPT+LwrJD90wgUqzZaZfwV+/Md3hJbMaYmmeIoXftBSzL3llbQIn31ysj09dnctUbC9i+u6TOecoqfCwJHGwOd2m7SzjC2oHHVBJNJR2s7D9Uv5F/fbmSv7w896AN6LY9z97+Csoq2Zqzf5dJ3/XJcvr/J5nMwprDHFR1Ns4u8jJ5ZXqNx/fVtpxivl+VfsDGdyr2Vgarbc//tImU7GJiPC6G9mpzQF5/f0R0GHl66jrOGT+T71dlOL0oAHy5bAcp2cUHfHm2ZBfzry9X8X9friK3uP7NK0tSd3PZuK8C9xExxO5eH3wstJlmwZYcMgrKKCyrZGt2+MHh6+U7eXfe1t/6EQ4Kv98wZWU6oz5ayn2frcB3GB7EvJU+dgV2jqk5dR+oD6ax363j2Ed/qO6A6asAYx90yogKBJFAGLHsykjQpp/gg+FQsDPsNd+bt41ZG7P3+l15eup6Lpkwl29XpjN5RTpD/zeDtYdpX4udeaUc4doR/H8HK+sPE0YqfX4+/CWNhVt3s2L7wQnQO3ZX979Zvj1vv15j5oYsCsoqWZFWc5mLvdXb8DM/rKe88rcNUjfm85WMfH8JP63N/E2vU2VJ6m4qA/u3jAJ7f3FmjwQaO1iRjOgwkp5fxrqMQjYfIp2YtgYGMso6wJfNTglJ5qm59T9AvTojheLc6ufGFWzEhR8wYc00oV+QHXnVr19QVsGdk5bx4FerSduH93XKD2t2ccvEJXy5bCcfLUwLu3phbz5elMY9nyyn4hAYFTM9ryw4Fsy2Q2Sd/7R2F34DS6raoyuql8tLtB1DgpURQ0l5yNnxvBdhw/ew+suw16zagWYV1v1dWZdhB48VaXl8sjiNDbuK+Gb5zjrnP5TtzCvjCKs6jLS3stmceWjst36r9PyyYPCv6pj7e4/wvD0kjCxLzdvn53srfWQX2SdkVdtiqMyC6u1yW07Jb652bwr8raeurhm+MwvKGPHmL/t00vfLlprLc2Hfdvu9fAdCRIeRpOaNALsEdijYGjiT3dsOdn98u6I6UNQ3FHgrfczamEUrqzr1tyzZxOfRD/Jz9F0UlRTj8xuMMSSv2RWcZ0de9Rfzl5Tc4E5mz/fdsKvQ0c5StdkzlNZnlFmf3/Dvb9bwyeLtLEhxvtNo6E52vwJgWT6krzhgy1NW4QuOFhpcn4Gb5Pmx8BJl5xArpM9IyFklean2v/nbw153V2Bnn7mX70rVASE1t4Rtge/WgaomLNqae9CawXyzn+X8ok85wqoOUh2sLDYdAidRny7eziszNv9qeMgu8jLqo6Ws2lFznYWeIK3LKOTm9xdzytPT9l7FKi+B4pz9WuZKnz8sQCwLVGPKK/31bmYODRuZtYSRPU8od+SV1pinvip8/uDrTVufGdbsaIzh7k9XMGNDFk9+t44yrxd+fgy2zNzra1btq47vZPcPaRLr4bQjW+33Mh4IkR1GAp0tt+2lnL14227u+HApu2rZ4A6kvJLyYLv6gQwjm7OKwr7UVV/8Jam7GfbsLK56YwGfLd5e43kLUnIpLvfRysoLTjuuZDZ9XSl0cWXQhZ3kFpezfldh2BdtZ8jvczZnB3/fHjLd5zdc+dp8/vzy3GC75aFgz79xRn7tO5CcIm/w3iBrdhZQGDh47vyVHU5JeSUPfrWKeZv3bydaH6F9KFJzS4IHiaxCL2X1uUrl69vhlcGwbV7Y5E8Xb+fPL83d521zfUYhLlNJArlkVIWRSns9VVgxgIXLssBll4dbkl9dGTEG8tPs36v+xd45V/VZ2tvyVAWVlOziYDBbfwDCSEZ+GVe8Np+r3vjl92/KK8rE/eODjPFM5ETX2uDkRCuL3OLyX2129fnN79YPo7Tcx32frWDsd+tY8yvNX1/P/IWhq//J1998XuOx0DAye1M2363KIC23lOGvzGNFXU0o714Iz/aBnM1hkzfuKmTa+r03Zewq9Ib93dbuLMBb6ePm9xczcOzPpOWWUFbhIy87Az68EtZ/V+M1Qk9Uaq2MBJpKWzaOsef5DbfPyCz0hox8XR7WrPT+/G3M3JAFQHG5j5Uzv4SZT8GUe2p/seJsvJtmBve7j/3paK49uRNPXnoMMR73fi/jgRDRYSSxuR1G6mq6KCyr4NKX5vL18p28OWdL/V/YGJj1X1j8jv3/bfNg57Lgw95KH0tTd4edTYTea+JANtN8t0fnqaqD1Zuzt7A2vYBZG7O565PlzE8JP0D+vM7+Qrei+kymsalexs5WOilZRTz41WogWGUPa4sNPeiGnrFvzioiu6gcY2DslLWHzE33qnYYjaLdgf+H/x02ZxXxr5c/JPrpJGa9bn/ZF2wJ+Yy/Eka+W5nBu/O28fTUdfu1fPVZT6HruaTcLiWnZBUx8ImfuP3DpXt/cmU5bPjB/j11bthDr8zYzKJtu/lu1b51xlubXsBDnneZF3M7zbMWAJCXbx+4iv12ALEs4IgzAbjWM5Wy4sCBrSgzeNVNxe7qMJJdVL1zrq3zINgVmRZl27jH8xEZmZnB9vEdeaUU/MrZb1mFb69t/Au25FDhM+QWl9d6IDqgcjYFf21sVb9XZ4+93W3aS1ON328477lZDHlmRp1n5sYY7vtsBbd+sKRGR9935m7lhZ831vn6a9Lzg+v11wL2kRtf5Vz3L/w140lMynQ7SKz4GAjf/4aeGBaUVTLqo2U1/xZ5qbB9IZQXMuu1u/gxpDJ7w7uLuPathXtteq/aRyU2b0DThlGU+/wsT8un9aZJ/Ms3ge+Wp3HtWwt55bl/w/rJMOu/ZOSXcfmr8/hqmd1Ulh5yorIrpEpSUl6Jz2+CIfmYDvH2/HvZTiavSOeuj5fXeRnwnidFoc3iH/6SFvwsAGnrFtkPZG+0v89gN3FO+huUFcC3o4h5/wIGmiW0ahJDt9aNeeiCXpx7dNs6l+9giegwUlUZySgoq/WscVzyhuDvoTv5X7VrNfz0KHw7CnK32Cn+3YvsjnvAKzNS+NOEubw1Z2vwKaFfwrySigN2NlPVhDIgqTExlJOaW4Lfb5gb2Hn0CXxZHp+yNrgz8vmrm15aWrWXojtbGQx/dT6/bMmlSYyHW087AoCdgS9OdpE3rCQeGlJCO6kt2JIbDD618vtgyyzw/v6d9XYFdiB9EpsCkFFQvczGGG54ZxHdt39GE6uUnumfU1npY0FI22uNysiS9+CzG8Fr7xirKlQp+3GTs02ZRfR9NJnHJu99XITqyoj9t0zNLWHBllwqfPbfNL2Oag8A6cuDVQuyqjsrl1X4gjv3jbv2rWlgw45s/uSejcsyHFfws/1Z0uwdegmxQKDr6rF/IyuqLa2sfJI2T7SfHFINKc+t/j30LLOuykhWXhGvRP2PWz1fc6vnK8DQGHvdPPX9OgY/9TPLa6nKFZaW8/YTt/Hqs4/U+ZlC29vr28S7ZmcBmzLr2IbTl8MHl1OStpyS0M67EBZGQnWw7Krj3sLIrkK7T5w3L4MFL91EaXbNq+nS88v4aGEak1ekk5Jd/VoZ+WU89PVqnvlhQ53NfStDvsd7nsyE8VVwdN50AJJIh4nDYfdWmP8SULM58W7PJKYkvEybRi5SsotrnghumRX89eTS6Uydbr92YVlFcD8abA7yFsF398HO6iBe1a+tQ9OGXNQ6kwtdc/nil038n/tdhnums3LuFOal5HC0sff/JjeFd+ZtZX5KLq/NSgmunypVFdU1Owvo+0gy//52TbA/SVUY2Vtl5NYPlvDZku3846NaThaMIWrJWxxlpQZP+KoqP36/CZ7E3ndOD/vBrMCJjvFBbqBqNO0xWPsNrPkS0hYCMMS1lKPaNAlexUZFKWTu30nSgRLRYaRF+kz+E/Mu3UgLho3vVqbzy5Zctu8u4Z25W4Pz7qpnmW1HXinTf/jC/o/xw7KJ4CuHsjw7rQILA52ZPl5UvYPd8/KynKJy8Pvtg9nXtxM8FdyxGCZeFlZpqUtmQRnLt+fjws+rJXcyJXoM6TkFrN2eRWlxAY1jPLx6dX8ax3hYsT2fb1bYbdLvzN3KjrxS4mI9tI+qvfzaxWWfITeJ9TDppgEMO7oNYGiZuxQqymqcKYV2bK0qvVb13B773bq6b4m+/CN453yYMND+7FXKS2D2/yA35VfXQ30V5uXyiOctzm1q91MILcWu3JHPluwihrrtZUggl1WrlgT/llBLGPnpUVj5MbvnvkVZhY/1u+yDUV5JBbv34aomsMux+aUVfLM8vDKx59nsjrxSxngmsjhmJF2snaTmFoeNMjl5xV4qG9vmVP8eEkbWZxRS9TYbdu1bKPSkzg6e0R/nW47Pb6jcbu90N/g72DNZFrij+LH1tQD0TnkDCtIp2lVdgm9QnsOizRnc/uHSsJC7u6SCgrIKVu3ID6scmUWv0y1w9cmf3LO5z/Mhq2JvYJBrJe/PTyUtt5QXp9U80G9auYCRvg8YWfAcWbm19wEK/ZvvrYm3SmpOCRe/OIc/vzyv9pOMnx+DDd+R+uY1XPDszPCO0Hs0Q9C6JwBx/jwaUMbGugJOyLL93fMtl3i/YtMnD9aYJ7SZNDTY/Lh2Fx2sTLpaO+r8jCtC+n/sSFlDZUUdFaeUGcSZ6v2I5Qt8r3YuhdLdYWEkhnJGur+hZ/5MnjzB/j4999PG8AHettphpBI3LsswKNuusKRkVe9Dg6F5+Yew4CX48tbgPrTqxKhD02juyXmQ56JfoOvq52ho2cE2vmQbYDjOZe+vrZIcZq2wf9+cWYzfb8L2DVVh5PvVGZT7/Hy6eDs+v8GyoE8rNx2szDr7n4Xu96avzwr2Q0rLLbGD6aYfOWb5o/w36iVO6myPKbMmvYC8ggJKvxrFIN8C3C6Lob0S6NKqEV0IaXLPWgfeQkzguMP2hVBkd4Ad4FpDUrNYu1oCdrPOq6fByk9rXc6DIaLDiLXwdf5mfc/prmWk5hazKbOQmycuYcSbv/DRL2n4DTQMlOxDr0xYkJJTZ+eqJ79bR/HGkM5Dyz6s/n2X3aRR9aVZl1EY3AFs3eNsOavQC7tWwcqPYcm79kG3shw+/ztsnAofX1W9IYVKnQ/b7BJ7VcVhaNsSmhRupqsrneYFa2j/wRCmx4zm/MQyEuJiufm0rgA89f16UrKK+O8P9oHovmE9aO+xd3Y+V3TY23S27IPadSd3pme7ODo0bcgt7q940/8vKn9+nLmB/iJVVYbQMvHywBnVfcOOolnDKDZlFvFJLf1WANj8k/1vfiq8fT7kB64omPMs/PgwvHsxGZm7uOPDpbz72jiWvXcfxl93VSl5zS4mTN9Uo63f5zecXjqVEZ5kLtv4T1qQHxZAv1uVwdHWFtpY1Qei5TO/Jq+kgsGuFdzi/pJdu0POUotzoNhe/7umvcL9n68I66+wJTR8Vnoh+cEa/TSqVPj8watAMgrKyCuxg8yj36zh+Md+ZEnqbkrKK9mWU8z23BIucc+ihVXIte7vSc0pDdtJf7PXMBLSNJO9wQ7DENYfYG9n4lVSc0ooKKvAGMMRuTOC05OsXeTu2EDDTDuMLPPb1bSq4aDWtzqHZf6uxPiKYPJdpKVUn6m5MLz3w3y+Wb6TZ38Mbzq499MVnP/8bKauDpTrywpou2Q8AH5j0cbazUjPtwD82V29PD+ty6zRzFOwyV4HHstP+prwpiqA3cXlbAipDtVnjIrXZ6dQ7vPToXQ93rcvtZtv5z5vb88p02GzXTE6yqTQe3dyeMUmUBnxm8Ba6nA8xNhn2+2t7Bp/D2MMoz9exh0fLg3uU3pYdkWkUW7NqlronYA3ZeTb5fx3L2Leqo18Hf0vpkTfT27a+vAn7d4GWeuD1YfzXPP5zrqDrO8ep3zjdHyPtqJo7uvB2StX2Ae4z32D2G0aY7CgQTPA8MO3nwQ77h+Z0ITu1nY8lr3dDW6YSrdmFlHl+azYkcf23SWs2p4XrIx8Yg0FoFvlRoq9lWGVnWBoDuxzyVwNO5cA1fui413raVxhnzRda30bfG5nK4N25NDGql43LfNW8G7UWIb45rAjr9SuHHne4afou3CXZOOt9AWvFisK9CFr0Sia4xfdxc/Rd9Egb4/mLr8fvIU1QsrTU9ezLC2P056Zzj2frrCPAUAPK5Xj27ro0qoRxkDarA9otPxtnox6lS7Noohyu+jbPj6skzNZ68ncuBArUCWtWDM5+FA31w7+vuN+eKKjfen80vfA54VGLXFKRIcROp8KwEDXarbllDB7o30ALa3w8dIM+4zk1tPtHWZWoZdibyU780r56+sLuOyVeeG9/rF3BHM3ZXOCK6TcVVB9kE1b9wuFZRXsyCvFws/jntco+OqfAGzPzqeXtYVTXMtx47PDSGrIwWnLTNZ/9WR12TYvFV49FV4eZFcPAFOaR8VbF8BbwzAp0/kx0LZ4ftvqg8nfXFNpWpZGgpXHvbsfgbICrju5M23jY9mRV8q5z83CX15Mv6RmXH58YrCZZm2TgQDMMPYorJ0tO2FfM7ATAHFWMTd57I3dv25ysBnosv722W96nn35Xnmln7U7C2hFHqe1LuH2Id0Au0ms1g6WqQuqf68osYOZ3w/LPgish21seeM6pi7fymXbH6fv5pfYOu+Lmq+DXUUY/fEynvp+PVNXZ/DT2l28NjMFY+xLlY+17B1GdPluXooez00lr+DbuRxjDN+vymCo226PNYGvTYusBYBhfMyr/DPqY44r/Al/2mJ7GbOqOxse5UojdeWssCs/wsLnikkw51kqvr2LcckbuPzVeXyxtHq7mb0xm5yQSsq6jELKK/18tDCVnOJy/v7uYk57ejqnPj0df0E6rSz7732Rew4Z2Tlh7efL0/JIzSlhxoYs7v9iZfCM0/gqqdgacvCtKAk2k1QNogaQU1zOcz9t5LznZtXaYXdrVhFnjJvO8Ffmk5JVyGnY66w40CRTuu5nEgrtHexSEwgjgeNsg9gY/lnxdyqMG9ZPJm7dR2GvXbDLLtfv2U+jqklx3uZscovL+WXaF0RVFLDVn8AHviFh8zanMPiePr/hs8U7wh6PzlgS/L0sZX7w96oz2EV7XAG2LXvvlZHc4vJgBXSM50Pitk+Db+6AH/5ln+FPvAz8FfYBGrgn6mNmbQhptgxU/sZV/pmM5sfD8TdA046A3VSz5+W929J38fmSHXy9fCczAh0be3vsbal9xRaML7x6EVoZabnpY7ucnzKdq1L/j+ZWETFWBUlrX6p+QkUpvH4m5tXTyc20D3x/aWJffRWz/itSf3wZt7+clGnv2oG/ogxrvX2g/6ByCJeUP8KDrcZT1uNSADKXfx/sKzHs6Db0cFU3Jbl2LuI1/8PMjBlF6rat/PvViSS/PBoKtmNcUbxRdjoAXa0dbMnMC6+MVK2XkAofS94DIC59LiPcUzmusDqYuq3qk5POVjqnNdoatp5Gez7hFPdKbvd8waasIgrysvib+0e6utK5yD2H9LyysGAH0L6xRaMds4i2fPQsX1G9f9u91d53P30Eu7aE97mbvTGb9+Ztw+c3TF+XiT/LbipyWYbe/o0MCIy46988DYDmVhEXNLJfo098cVi/Im/6GiZ+8XXw/1Fl4dXqjjmzAWNfOg9w+v3Q5TScEtlhpIsdRo53rWd7Vj7zU3I53zWPAa7VwTLbX/p3oFnDKKCq/T2HSr+hsKyyxsh6mzKLiC/ZEjwY7Cll5Xwe/to+O+lrbeZKzzSO2zGR2T9+xbPZNzA55gHejX6Sez0f2Z1YQ8JI2Yovab/iBQDerTwLPy57R5WxEr64CX5+jNRVc4ky9kHL98kNrN9kb8jHN8oKvs4FrurXbFa8GeY8S4NoN3cPPRKAK/yTWRt7Ha93X4DLZRHvs79gr3uGc0X5AzwWe5f9XKuIO09uSbNGgRE0F75GvGXvDKJ3b6IsZztul8X5x7TD47Ko9Bsyd+1k47bt4PPyVeyDtP/wDP7WK4Z28bFkFXrDLhEG7Ms5C7aD5aby/GcBqFz0DiZlml0piW6Mz3IzwDub22OmEGsFDqyBjsOVPj9fLt0R7FewOauIwrJKPFTyv+QN3Pz+Eh6bspYZG7LYVVBGX1d12f4E13pGuKfim3wPMzdmsyW7iHPdvwDg7TsCsEudRzfMo4WxqyU3Wl/BexfZzWoLXgn7KFdZ4T3yw8LI1tkAuLLW8sZPK5ifksvYKevw+Q2vz9zMI9+sphkFvBs1luHuaaxLL2Dxtt3Bq06yi7zBoNPbVd2+HmeVckTqx+TutpevS0v7UvY5m7N5bPIaPliQyhPf28F57pxpRFUUUkwDTKuj7BcI7Mj3vIR1XPIG1u3czedL9qhmzXiaNq/24lj/GtamF/DM25NobeVRYjXk+4YXAtBowxckVNoHseX+rmFPbxTtZoNJ5H2f3Zm1vS98TJD48uqD9J9cs/gp+i6OsLYHO1GuyyjkP5PXsG7ONwBM9/fhE//pYa9xpCsNt8tizOCWPOh5F9/cCWHNO20KVwd/b5i5hLXpBYx48xd6PPg9L8/YHAwW7eLtcLVnZWTOpmwuf3UeR/7rO6avz+SDBdsoq/CTQC4DXIHKRGxTfA1bUeqOs89GgU+iL6bYxNDByiZtXaA50u8PhpGv/QNZe/YH0PaYkDCSxc78surqztzn6fTqkVzntre1aet30ZwCmpk8AGKoIGeb/fk+XpTGhGkbg1XKOIoYtuu14Oc4yVVdRemdNcU+gAJs/AGKM7EqijnW2kBCXAx93fY217x4M+2y7KpwYvlm3pyVApuScZcXssO0YF1UD7aYtnyZ2ZZZvt4AnOyyg2mzhlHcdEpXruwYsq1t+IFO3vXEWyUUrv6ex0oe5U7PZwDkNDuGzaYdRSaWGKuSXVvXsDmriIc9b/NF9IPk5mRSVl4ZdlLAqs8gdwujsh7ikah36LrNDrtewqu+PaMzub5TVti0YwLfq+7Wdrbt2MUReXOJsuzv31D3ImZuzKK4PPxk6vjoVCy/fcLa09pGVnoqJd/cS+kLgyBjBVSW4dowmZbkc3PHnXRrGUul3/BZ4HtVXO7Dm1Edpo7Kn8Gonfdwrfs7Ouz+JTj9rIrpAPSKCv++lOxcQ1J53R2QAfuEvElbOPoyGHTX3uf9nUV2GGndk9Lo5jS0vHjSF7E7ZREvRD/Pu1FP0NfaxImdm9O6SSwdW1SPRxI6NsYnIX0+AOal5ASrIummeY23O8qVypeB3tiXxlV/SY6cdRvtyaLE2JeBXef+Dl/GmrCyfWzqdBpbZazxJ/FQ5Qjuj3sMLpoAg+60Z5j5FEXLqisCntIsbjaf0KJRNK291QeoqjOAlKju9oRA++ufjm3PvUfu4v+i7M6DzeY/BbkpNPDZZ5LzMqOZ5++Fp2FTTJxd7bijj7E75U57HGY8BUCZsYPbANcajukQT3yDKAbFZZAcfQ9tX+lJx4kDucqdTDuysSqKid78PZccZ79e6MFtc1YRn3xhl3dNm6MZPi+JHNMET3E6hR/fAkBl78uY4jsJgJvdXwafm5Q7GwrS+WTxdkZNWsZlr8xjd3E5S9PyeCVqHBtiRvB63g2cYez1+8XSHezO2EYHKxsfLrjoRSa77IOYe8dCbn1zOgNca+hq7YSoRsQOfRCvFUsLq5DXey4Pvm931w5c5fb6MmvtA+Ji9zH4jcVF7rmc6aru8xLsxGoMvi12GHHj59zmO2kc4yGz0Mv4D75i+E8nMyL/JS7zzOIU90pGez5hfUYBMzfaO8uBXVtw8hEtuHFwZ9rExdLb2mq/rNvelq4veYOZUbczIHpzcKjnOZuyA2eOBrP4HTYum0P2fLs5caavN3mN7WoVWXYgquqj0aV5DMdYm/ki+kFWxtzA9nXVO0T8fljwMrEVeTwX/QLNKaBzfiC8JQ4mpbkd/FsErqjZ7G9LAVXfK7u60DDa7kP0le9kQmVif5faWdVndjd5vqWrK53L3dMY4Z7KspgbaZwxn7mbcoIHuDn+3lS26ctN5aO4ptyuQCZYebw/OI8bV17OdZ7vua38dbZMfRGAgvxckvzV22C7opVc+tIcmm36glmeW5gz9WOS12SQ6Mpl1BndONJK5e+5TzH6tW+5/UN7DI2r3ljA/JRcvJV+3pm7NTjGz70dVuGyDCtcPeHujYzu8CGPlV0afK83Ck9ksbG/k/GZC+zxLgp2QGUZFbjZYVrSLt6+YoLmnQG4K+ZLTnct5etlOyHtF0h+CIC7PR9zl+djFrqu427PpPB1uXEhuYUldP/6Is6adhFRlcW4XRb3eiYRbwowLbrhddnvU04US/xH4MZH5dwJduhZ+UnwtY51beKENm6allRXMxr67e26mVXEe8nzKFlsv/+3vpM4uVtrGka7KfRW8n/LmlJpXHR27aKDlcXukgoaRLvpGxWyT/VVVxJPzZlES6uAYhPDT75juXXnuRhcbDT2vqN0+yrSM+1qxbGuTVzsms22tFQo3Q1YlDTqAN4C/K+dQQPs8GZhILoxP7S4CoAtxr6ipI1/F12Lltlv3KpH2PpzWYbibYs4qbx633y8tZ5ZS9eypz5W9QUQPVypRH12DQ0Xv0wDXyGFxl7HcRnzeT36af65624m+u7haMsOnx2sLGLx4sqtDhMdUybRKmse/+d5nxYm195XAd3zZkNJ9ba72tjbR+OirfSx9uhzBMzwHQOAiYmDP78Fo9fCpa+By9k4ENlhxLIobGs3PzTcMYdTKuyDQpTl44Xo57iz3RrIXMsRTV2AIWr1p5y6+kHaksNJrjXctuNe0jYsC77cwo3pXO22L438zH8apYH7bmTTDL+xSLDy+KfrfYa7p3Gqq/ogVlVJ+aXnGDY2PxWP5WfY0puhKIMKPFRa1cl9Q7cbMLj4OCuJgh6XkTvgfkyH4wHotsMOI8m+4wC7TD8wMRora482X6DpOWPsX3YuhUovLp+Xm3OesEdYdcfYZfqvbgPAazzsqrDPBJs2jMJqYZ/RWjmbYdY4mPEk+MpZHX8K7/rsdtyBrtWc3NVuf7zKNTXYmbCJv5AHPBOrF2T9d/zpuPYAzNyYTd6qH8h+6wrMC8fzl632znWhrzuLd5Twqe8UAOLK7SaiBU2H8UXlAADcfrsiVGaicOOnfNZ4pq+zKy1bsou56f3FpGxcy9nuRbgsQ5Irk/uiPgIMU1dnULrVPnDujO4Ex/6N11rcw1Z/Am78nOhay/0tAz34+1wODZsTfaR99p6w1q7C+Pb4KlW1004sO5nXfOcC8ETUa5zYyj5Tqjqj9mZvwV1Y3VTw0HElwcGHEte9SROrlKuifuautnYpPMHKo2D7WmYFwshlfVsyseO3PNDoGyZf3owh8fbZkTXoTpZ6+pBnGtHUKuYpzysc187+G05dnYExcJZrMU9EvUarL4czoNjum/OFbxDLvQn2wmStZ2tOMSXlPm6N/pbk0uF8HfN/HOvaREPLy5EZ31K+/kf46lbY9COU2M2cbazdPB71BoOrznp7n0VZwrFM9fUPfs5lplvw96p+WY1i3IHHurLTVLddpzQ4GoB2gStIWrGbo1z2QWuAaw3Xur+nqVXM3b43cBek0dWVjs9YLPD34PhOzZnqP4G51nGYePvANWDZfViluRR47JDTcf5DsG0eaSvn4LIMu0wzvMZDcwpoXbGD+2I/o421m6eiXmVC9PPMir6NSzyzGRP1IZe4ZnHStlf5ZvlOrnl9Nn5TfYXarI3Z7M7Yxj+jPuYCr91UMcl7ErtK/Py4LocPfUOY4jqNjUmXs94kktrE/t4eb621A80u+2CW6m+Nsdy0a2r//TjpZmh5JM38u3klahxzFy7GfHYDGB+VuGloebnd8yVNrFKu9EwL2y69acvZOnsSfV2b6ebawY2eyVzRYjN/9dh//8xTxvKFZW/b29oM5cXKiwDYvehTznzsa7xrvw++1rHWJs5qVncfpD7+tXg2TwXga99AOrdszB1n2H/3DG80ywPNdCe7VjG0Z4IdaDPsbYaoRmGv1SPw957nOZ7rK+5hgbFDQnpMJ8CuKjbLXRbsbzLcPZ3slGUA+Jt2YmTB9fiNhavUDrQzou19Cb0vJeuYm/hPxV/5b4uH7fc1Pti10n78uKtqfK7o7fOC++8iVxwuy9BzxyckkEuXVtXL3b2iurm+l7WFNvnL8BuLO/x3cUX5AwB0yv+Fvi47gLQu3cyLUc8ywLWaGdGjeDf6CWIqalbZXYETyrm+nqz2J+E2FfDx1TTfbX/eab5jMJ4GRJkKugYuNNjobx98/lu+c3jadT3WXz+FRi2q24gctl9hZMKECXTu3JnY2Fj69evHrFmz9jr/jBkz6NevH7GxsXTp0oWXX355vxb29xDb3T4DPsW1gnNd9lmb8cTSwcrmxEWjYcJJ/HfjOayIuZEz1v6Lsyp+5uGod/hv7Juc6l7B7i/vxRhDUWEeg1L+Rw9XGhWxLdja5UrWB1L7Qt8R7LLsnetNnsk8GfUaiaWBJN0wsNONT+S0P9/Gyl7/pMTEBEv/q/1JLKy0v7Rp/lb0P/caklo0xG/gtg+Wcty/k/m6yO5hH43dTPFa5Xls8rejkeXlL1FzINDu6ItqDEB5THOaH3ux/d6+cvvKnPWToWgXxLWHq7+0lylwdUU28VR1M2zaMApa2MtD2gJYEPhbnvMk8/qNZ7bfLr8OdK9mYJfm4PfT32sf6MdV/Bmo/jIBkDKDrvEWfTvEc7P1OU0//Qstt00J64j1znb7jH5rj5G8wwW8Unke6wc9y0c7WjHLf4xd7gbA4pWYa+x1sfBlLkn5Fxe5ZnO6ZyXbtmzCt8ZuP93VpCelxJJk7eK6+CU8xbMMWmVfaZDRxD7wtYmLDX6WGxvN4ehCO6hywo32O/UOnNUGzt6+a3ktG/ztmVB5IaHW+xN51XMl60xHWloFPO55FTc+0rILKSyrYNKnH4bN3zhzGUN7tSGOIi5w22dfHlNBdNbK4DzNMuezaoe9kzpz9yS7M+T0x2nx7hD6lATO2LqcxrQTX+cU7//YZZqS6N/OyVueA+w7jQJcHWd3JG1KIa2tPApozHR/X6ZmNrVfY8diflydQQzl3Oz+GrepxGuiWOnvBMApLMX/9T9g6ftUfGyv9+X+LviMxTnuhZzgDuyMuw6hbdMG3FdxA1nYIz5ubtCLL24ZyDUDO3HtQPtMrqoyAhbrmlf39chtYfdTOsW1gj97ZnG6e1nwsV6ubXRy2aGzhyuNJ6NeBWCF6UoBjTj1yFb8pV8HRg/tjlV1luu1mwJSL/mWb30n4qES/6SriF5l/y1SG/dhHfYy3ev5iDZ++/XbWrmc67L7kXjmPcugQNga5v6F29xfMN//N56IeoMX/3IUXVo2otJv+HfUW9zi/pKowu2U42GK7wRenLaJ4nIfPtzcUvJ3rkq/DLCI6ToYgBNc6/hhVTo/zbH/lltNG+4++0iaxNpVR+I7wE0zqWx3PNGWj/vyHsLK20Ymzflb+ZhgZ9dgp1egONoOuLG5a2mx6s3g9L+7J/N/Zc8AdvPv5MIjeKj4Uu6tHEnLy55jtv9oikwsrUwOD3gmEkMFhZa9H+nj2syJHvvMO98VH3zNSuxQebvnC6JNOSn+Nqw2nUhq0ZCbTunC2b3ssLuz+Yn2MrTfxkMX9oK8bVBeCO5oOOo8AExUQ3whn6Pp0efw74t6Vf8/qQ8A0bnr6WuqqxM9XdtossEeYC0jJomZFUfyvO9iwD6Iz+3zJPxjBZz7DJee0IWso2/k8mFnQouQpsOOAyEpvEoHcGnlZBpZXjJpwcI2lwNwZ9RnzIr5B7f3qu7b1aG4uskvOtCks9I6gkfuvY9mXfqTaxrjwZ6e0/YUTGxTOrqyeDbqRdyW4QSXfRK50zQPVlJCKzVz/L25r+JG/NGNYess3Gu/Aux9TnaDzsH5TJO2LHL1Dv5/k2nHglZ/ho4n1vhsTtrnMDJp0iRGjRrFAw88wNKlSxk8eDDDhg0jNTW11vm3bNnCueeey+DBg1m6dCn3338/d9xxB5999tlvXvgDIa7X2RhcHOvaRGfXLipdMVg3ToP+10O744I91+OskmAzytnuRbT32wfLY0rmM/f564j6bzeGY58FWBe9wBn9e7PIb/fDWOg/iooWR9Z884Sj4VS7fMyQf4E7igatu3JV+X12Jz6gLHEwn/pOwW8sJsVdQ4cWcZzY2T6jqxp5761d1V8gn7FYaTrzge8MAE7e8oI9doQ7BneP8wGI7nqKXZJLDGyMaQuCnbvo+1dIGgg9qg+qWaZ6R9O0YTR0tneaLHkHSnPtNuzjb+BvAzox+IwLKSfKDnNL74bUucT7cikysbzku5CS7vaZFl1Og2ad7IP5xmSebPAOd0fZJeBJvtN5oOI6KqOaUGqime+3v4AXD+jFumP+ydjKv/JKTl9+XruLCjwUd7V3XLTtQ3aPETxacRU+XJzNfJ6NnsBbnrHMjBnFCJfdlh5z3BU06HMJAP/nHceF7nnBMSgKW9g7tzbxsczy28HkpIr59mXanU+B1oGdQfezIaphcL2kdriAoeVP81Tl5Wz1dA7+LTaZdhzVoSUzez9GOR667p7Nypgb+IWr+fzxq+iebjfn7G5tNzex4TvOznyTZ6Jfo4FV++W/A132Tm5gW4vGiwNhsM3RVI0tUvX/s3slUEBjHqi4HoBGy97g8YYfAoYYyjmx4pew163scTExsQ34oairXarPWkvegvc53bWMxqaIysbt6Fv5Nv9r9ww+XHR1pRNbbJeGoyrtSs/blWcz3WPvwN34IT4RmnehTXwsu4njCu8Ynq64jDWtzuPYjs14+MJexAf6ZFVVRgBKul9s/9K8K9EJdvNFR1cWz3he4lHP2zXWiTfQPDjIba+bOX77gNUmLpan/9KHkad2rf7bAXQcQK8ePXm12d2s8SfhKsmiW4bdAbuw/WBWN7YrbsPc9rgMwe+K5QLLDVnrggeSJlYpd0d9gsfyc7n7Jzq80YfX3WM52/ULZ7oCHWJPHsVriU+ymzgmLgjfV2YUlBEb5eK0Iefgc8fS0ipg3apFNEu1qxBJRx7LLYFxfIKiYvGcfCtgNw8CvFIxjPn+ntzk+j+uq7iXb/0nBWfP7Wp/77qWLCOpeAXlxk1hXHcaWl5iKvLYHt2FJyqv4J15W/ESzdbEi2nWvCVN45ow3d8XgOGe6QA8W34RBaYBDS0vrVPsptRVbf9CeWCftTz+zLDl+sx3CmDRsXlDLMti/PBjefxPRzPobPvkpGvRYtrHxdj7IYBWR0E3u8Jq9bqELZ4uwc/Rqu+5XDWgE9/ePog3r+lPp552ta2Lf1vw4F3utqsTx2QFmkpL7PDzv8o/c3X5vdxcMYqTu7WCZkngiSa+QRTPXn4sg7q1rD7RAuh9CTSvfm9/WzsUN7fszrHTGg1ja6c/M9vXixzThGjLx4XFn/J89AusjLmeRt5MsNxkNqx+zZy2p9GsUTR/HdCZ+f6ewekVx16LdezfAGgdMuo1wCZ/e37yDIboxvCnl6k8/iYyTVO+9Q9gpekCl38EMXHQqDWzGp/Dj/7jeLT8Cqb6+rM+5misMx6kKM5eBi/R7DQt6diiIYeafQ4j48aN4/rrr+eGG26gR48ejB8/nsTERF566aVa53/55Zfp2LEj48ePp0ePHtxwww1cd911PPPMM7954Q+IpolYQ/9d/f8jzoSEnnD+OPj7NBiTytIrlnGe93EGep9jsu+E4KzlbvsPenLu58RQznarDdtOfARPj3M5/ajWvOa5nJHlo/gi6lxaDb2LnGZ9ua78bpb7Axt496Fwwt/hvjS7/A+0ahLDYnMkg7zP8kHz2zjpb48Q1e+vdPe+Q8sBfwXghMD15mBfDlfUvDe7jX22khHbBaIa8ZlvMMUmBnfgIEHLbnDyP+wQMHi0Pa0qGa/6zL7EEKDvlfa/VX1RgGbRfqI9LvolNePKEzpCz4uh15+q19mJN4PbQ2yUmxvP6E30xc+By4N7zZcw8S8AbI47kY9vOYWGF4+HQaPhvHHQfZj9/E+u4ci0j/FjcX/F9dxbcSOr2v0Zz50reDjpbXKIJ6lFQ07o3JwLjrFv5vT50h0Ul/toFx9Li7PvgcST4NR/cl6fdrzpG8ZfvA/yVuXZbIjpjT++IzFWZXCgqPi+F0Gf4YDdnFJAQwpNAyqMm7JE+0Ba7vMzz9+z+qysUSu44NnqzxzdCLqfY//eOAF308TgQ0162DvjVNMaL9Ec1SaOv//lQqLPfhTAPgBYFYxwfcdJgSG+m505Ovj8mDlPM9RlHwTzj7ul+j2TBgF208StcbN5rcHz4C2wQ+2N0+3LPgGad4WYxvRsG0di8wb86O/H6j7/AuBK/7c84JnIYNdKon3F0KQdHDcCohrS/LRbGXlqV3KIZ7zXPnhdU/IWIzzJAHj6XMa0f57J89eeTm6zPsHFqgrOfmMx3d+HxR1GVC9zl1PBskhsZn9XNpkOvOi7mLYtmrKn2JDhqDsdMwj+9jlcPpH4xKPDzvKrOioXxlU39Xzd4jqm+vqzyt+J2e4TeL8y0IwWF1v9BqFhpNefsCyL64f04sby0Ww3LdluWvIE19D7/FvZ1edmfvQdWz3/RS/CVV/C9cnBAyVAjmkS/L2gWW9MfCKUF9Elfz6vRI/HZRlKO50FZz3Cqef8hSaxnuBl5X/p1yH43CtO6Eirpk1wdbT3L4/zPANYiZdougy9uca6AuCo86lsZFcNvZ44PgpcOdT4qDPY2WoQn/hODc7auN/llJgYorGbCSf7B8Clb0C3s2HY03w34ENKiA323xl0hF2x7di8Id/7jg++zg5PR97xnV19WXagY2uDI0/jHxW38a+Ka8ntVt0XptTViPd8Z+Gy4IjW9j6qQbSbK0/sSLPuJ9sH2JIcmDMevg3sczoNgqP/DCO+hXOfZnt8PwDWkUTHJHvf2bt9PEOOSqBVV7tpq4srI/hdqhj6RLA6AzA9167GtY1vwEx/H0rdcRzfqWafPqA6jFgu6HkRxMbZ333AdeJNwebYFNOOuDPvoV37JP5W8QDXlN8LgHvlJC5wzaWJFbjSrO0x7Io7Ovjy3QbZJ0GnH9WKpW57eo5pQvM+50L/66qXI7p6u9ps2vFNh3vgnynQri+e857ig8HJbDet6NSiIa4ug+HerXD3Bmb3fJhSYvkmvys3VYxm9uD3oO+V+NrY2/IqfxJ+XMH7sh1K9imMlJeXs3jxYoYOHRo2fejQocydW/OafIB58+bVmP/ss89m0aJFVNQxSI7X66WgoCDs53c14FY46Raw3HhOvLHGw12TOpAacwR5NOGFyj/Zl3bGxOO+wi7r+rGY3/0e4v65kqRhowCIjXJzVt+ufO8/gatOPoIGRw7Bd91U5rr7c3fUA/jPedI+4FuWvcEHtGpiV1920ZxWZ94BDZoy9pKjmXLnEK4e0AkgWBkBeOjCnky583QK2tnViiZdTqBXuzjyacyLcdWBgsat7ZB19VfQNnAgqTrbS18GGPvMP9A5jvbHBZ+alNiRtY+ew2c3D6R3+3h7mS94zj4INk2q2a7a90oY8Q3Exgfv0NpnyHCO7dgMGjaHMx+yy6GDR0PHAfZ7Y7HgmP8EKzp/6tsOGjZnxDmDOTKhCfecfSSWZXFilxZ0aNYg+FZ/G5Bk92G5fiocdR4ndWnBhX3ascR055HKEcwe/D6uWxeQ3cg+cKXGdMNqlgSdBkMz+7Mu7PMYg73jObP8aZq0satMlx7XngIas7Lp6XZz1l8/CTtLAqDfNYAFR53Huce0o0OzBvzznCNpMeBvGFc0P/rtneiRbQI7lpNugb99zvwzP+eVjs+wPWEIFT0vtYNZt6FwlF25ovMpcNwIzIkjiT/3YfuA4YqCs/+DP6oRzawi7imfQKOdc+33P/NhcHvsg+WgO+H8/wFgWRb/u6wvt51+BEdeeJf9NwNu9Ezhf1ET7PfqeaEdssZsh4Se3Hr6EXx80wCmNfszW/wJJFh5DAhUYjhmOG3iY2kU46Fhr3OCq+FRzx2URTVjqjmR3cTRstvx9jIDHHWB/dQO8ZzZo3XwOVXDV4dyuaoDx1FtmsARZ0DrHvTq2ZO74//LuO7vUz5gFADlVgzZx94anL/Z8ZdxU8Vozi9/nK1nvU4GLYhyW8Er4YCQMGLZBxrgwj7tOOHYvgz2jmeQ9zm6nHc3reMacOuQo+g08mM47mo4/QE7zHc9HTr0h75XBF9y16lP2ZfluqKI++s7WP9YDiNnY9pVf38anPoPwD6AfnzTANo3bUDv9nHcc/aRxHhcxEa5uOmUQD+sPvZrH+3aCsCSI27D1bp7jXUFgDsKz6A7AIg5bTSf/mMolxzXnltP70rv9vHM8fdmfaP+0Gkwzbocx4TEp3mi4nL+XfFXPm55C02SjoG/fgwn/p0rBx7BCSEH6JO7VYWRRkzz96UEe7+05cRHqcDDYn/IMsW1p0ufwXznP5H3fWfR7sjqvkENBt3KO7ecxXvXnxgeDAPLH2wG+ekRe1/RdYhdJbYsuwIb3ZCs7lew1p/ItJZ/rR4xNCAqLoHdnur+Rf7YZjQ64Sp2nPFicNpqfyf6JzULDkNwfOdmNIiu4z4sVfu9rmfY+0yAU++1t5deF+PtcDJ+K4q2f3uFYcd24oyjWvPIhb0YP/pau1knILvnNXDyKDjvvzTtYu8Hcl3NSOxhV6tiPG4qev2Zyb4TeD76RqJjYu194jHD7YD2l7eCr9WtZ1/+d8Wx4IkJTrt9SDce/9PRTPir/dq43GBZdG3VODhP6yYxXNDH7pTbuMsJXFV+H7eX3w5Ap5aHXmUEsw927NhhADNnzpyw6Y899pjp3r17rc/p1q2beeyxx8KmzZkzxwBm586dtT7noYceMthHqLCf/Pz8fVncfVfhrfOh3CKvWZ6226RkFRmTttCYXWvtB1JmGJO2qNbnlHgrzc9rd5lKnz84bfWOfLNxV2Gd71Ne6TNnjZtuLnh+lqmo9NU532szN5vXZm6unpC1wZhJVxmTvcmMnbLWJN37rXls8hpjZo0z5uFmxiz7qJY3KzXmyS7GPBRnzPhjjNm5LPzxqtdMX1H7QvgqjfHVvYxmxxJjxnY05rH2xhRl1T1fxmpjdi435ZU+M2z8TNPv3z+YnKK6/xY780rM7I1ZJj2vtNbHMwvKTJ9HpprO931rNmXa67oyZ4tJe/Vyk7vyh+oZ89KM2bHEVFT6zKUT5pjeD34f9r7peaX236Cyou5lz9tuTEVZzeneInPVa3NN9wemmG3ZxXU/P+w5xfbr7W369/cb85+2xrw5zJif/m3M9tq3vbqkTn7G/ns/FGfMI81r/s0DsgvLzI3jPjAz/nWyPe+bw8JnyFxn/I+0MP7XzjTG7zemstz8uDrd3DpxsckrLjemrNCY1F/CP0aFz9z+wRLT7YEpZkVaXo33rPT5zf2frzAfLthW9weoLLc/98pPTUlBrkl55Giz+L8Xm8yCMtPrwe/NOeNnmmJvhTln/Exz50dLw5/r8xkz+R5j5jwXNrmwrMJc/cYC889Plhu/329+VYXXmPf/bMwXN9v/3/ijMdvmhc9TkmvMxOHGfHmrvX5Cn17pC77PirQ8s3pH+H5t96pk8+ODQ8wnD15scgpK9r4sfr8x2ZtqvMfcTdnm9KenmRnrM4PTyioqzQ3vLDRJ935r/pe8vsZLFXsrzC0TF5ub3l0U3Ge9NnOzSbr3W/Pc2+8ZsyHZFHsrzDEPTzV9H/jUFEx9zJhVX9jbpzFm3A/rzb2fLjc+n9+Ydy609ynFOXtf/kVv2dvXoy2N+e6+WvfBRWUV5j/frjbrMwpqf42Umcb8t4f9Op/fFJy8afGP5tv3xpmxU9aajbsKjbfCZ16budneh9fF7zdm/ffGFGXX/nhZob3fqM2GH+xleOMce98Y/ABZpvilM0zZvNfDZl+5Pc90f2CKue+zkP2rz2fvl40x5r1LjXm4ad3731psziw0ne/71gx5ZppJy63e72zcVWi6jplsuo6ZbP7x4RJTWLaXfdoBlp+fX6/jt2VM/e9StnPnTtq3b8/cuXMZMGBAcPpjjz3Ge++9x7p1Nce27969O9deey1jxowJTpszZw6DBg0iPT2dNm3a1HiO1+vF662+rKugoIDExETy8/OJi4urMf8fjTEGvwG3y/r1mWtRWFbBl0t38KfjOthDrleUQVRs7TPv3goludDu2N+nV3Vxjt1nJb7Dr8+LfR8UY6j7zKWetmYXk13kpX9d5dg9lFf68RtDbNSBu3NlabmPIm9lsNp1wBjzm/5WSxb/QmIDL62Setq96etQXuknI7+MjqTbperYPb57u7fZI2nuOf1XlFfazX4HgjEmeLacnl9KrMcdHPvmcLZhV2GgaaPJr8+8D3x+w6od+fRoG1evv0FZhY+vl+3knKPbEBfoQLsps5DScj9Hd4jf+5P9PvuMfW+MsUdZbt0L4n7DzdoqymDbbLupMvZXluv3lLnW7gsXVbPyV5uCsgoaRXtq39d7C+2bRYZ2qq3PIhSW0axhNFHu8L9vWm4JDaLdwTsJHywFBQXEx8f/6vHbU+cjtWjZsiVut5uMjIyw6ZmZmSQkJNT6nDZt2tQ6v8fjoUWL2neEMTExxMQc3BV2KLEsC/dvyAVNYqO4KtCkA9QdRMD+4jTrVPfjv9VeDna1OVBhoFPLRnRqWf920QN1cAzVINr9m0NVrX5jaDyu3wm/PhP2OrE7utWxM2yWtF/vfyDXdWjZvm18/Q4Ah4PuCQc2hFRxu6zgLRrqIzbKzWXHJ4ZNq3dA+rUgAva2HLhj828SFXtgXue3Cu2XVA9VAa9WMU3sn31dhCa17++r7lJ/qNqnvUJ0dDT9+vUjOTk5bHpycjIDBw6s9TkDBgyoMf8PP/xA//79iYrayx9CREREIsI+n6KMHj2a119/nTfffJO1a9dy5513kpqaysiRIwEYM2YMV199dXD+kSNHsm3bNkaPHs3atWt58803eeONN7j77rsP3KcQERGRw9Y+NdMADB8+nJycHB599FHS09Pp3bs3U6ZMISnJLtmmp6eHjTnSuXNnpkyZwp133smLL75Iu3bteO6557j00kvregsRERGJIPvUgdUp9e0AIyIiIoeO+h6/I/veNCIiIuI4hRERERFxlMKIiIiIOEphRERERBylMCIiIiKOUhgRERERRymMiIiIiKMURkRERMRRCiMiIiLiqH0eDt4JVYPEFhQUOLwkIiIiUl9Vx+1fG+z9sAgjhYWFACQmJv7KnCIiInKoKSwsJD4+vs7HD4t70/j9fnbu3EmTJk2wLOuAvW5BQQGJiYmkpaXpnjf1oPVVf1pX9ad1VX9aV/WndVV/v+e6MsZQWFhIu3btcLnq7hlyWFRGXC4XHTp0+N1ePy4uThvrPtD6qj+tq/rTuqo/rav607qqv99rXe2tIlJFHVhFRETEUQojIiIi4qiIDiMxMTE89NBDxMTEOL0ohwWtr/rTuqo/rav607qqP62r+jsU1tVh0YFVRERE/rgiujIiIiIizlMYEREREUcpjIiIiIijFEZERETEUREdRiZMmEDnzp2JjY2lX79+zJo1y+lFctzDDz+MZVlhP23atAk+bozh4Ycfpl27djRo0IDTTjuN1atXO7jEB8/MmTO54IILaNeuHZZl8eWXX4Y9Xp914/V6uf3222nZsiWNGjXiwgsvZPv27QfxUxwcv7aurrnmmhrb2UknnRQ2T6Ssq7Fjx3L88cfTpEkTWrduzcUXX8z69evD5tG2ZavPutK2ZXvppZc45phjggOZDRgwgO+++y74+KG2TUVsGJk0aRKjRo3igQceYOnSpQwePJhhw4aRmprq9KI5rlevXqSnpwd/Vq5cGXzsqaeeYty4cbzwwgssXLiQNm3acNZZZwXvH/RHVlxcTJ8+fXjhhRdqfbw+62bUqFF88cUXfPTRR8yePZuioiLOP/98fD7fwfoYB8WvrSuAc845J2w7mzJlStjjkbKuZsyYwa233sr8+fNJTk6msrKSoUOHUlxcHJxH25atPusKtG0BdOjQgSeeeIJFixaxaNEihgwZwkUXXRQMHIfcNmUi1AknnGBGjhwZNu2oo44y9913n0NLdGh46KGHTJ8+fWp9zO/3mzZt2pgnnngiOK2srMzEx8ebl19++SAt4aEBMF988UXw//VZN3l5eSYqKsp89NFHwXl27NhhXC6X+f777w/ash9se64rY4wZMWKEueiii+p8TqSuK2OMyczMNICZMWOGMUbb1t7sua6M0ba1N82aNTOvv/76IblNRWRlpLy8nMWLFzN06NCw6UOHDmXu3LkOLdWhY+PGjbRr147OnTtz+eWXk5KSAsCWLVvIyMgIW28xMTGceuqpEb/e6rNuFi9eTEVFRdg87dq1o3fv3hG5/qZPn07r1q3p3r07N954I5mZmcHHInld5efnA9C8eXNA29be7LmuqmjbCufz+fjoo48oLi5mwIABh+Q2FZFhJDs7G5/PR0JCQtj0hIQEMjIyHFqqQ8OJJ57Iu+++y9SpU3nttdfIyMhg4MCB5OTkBNeN1ltN9Vk3GRkZREdH06xZszrniRTDhg1j4sSJ/Pzzz/z3v/9l4cKFDBkyBK/XC0TuujLGMHr0aAYNGkTv3r0BbVt1qW1dgbatUCtXrqRx48bExMQwcuRIvvjiC3r27HlIblOHxV17fy+WZYX93xhTY1qkGTZsWPD3o48+mgEDBtC1a1feeeedYCcwrbe67c+6icT1N3z48ODvvXv3pn///iQlJTF58mQuueSSOp/3R19Xt912GytWrGD27Nk1HtO2Fa6udaVtq9qRRx7JsmXLyMvL47PPPmPEiBHMmDEj+PihtE1FZGWkZcuWuN3uGukuMzOzRlKMdI0aNeLoo49m48aNwatqtN5qqs+6adOmDeXl5ezevbvOeSJV27ZtSUpKYuPGjUBkrqvbb7+dr7/+mmnTptGhQ4fgdG1bNdW1rmoTydtWdHQ0RxxxBP3792fs2LH06dOHZ5999pDcpiIyjERHR9OvXz+Sk5PDpicnJzNw4ECHlurQ5PV6Wbt2LW3btqVz5860adMmbL2Vl5czY8aMiF9v9Vk3/fr1IyoqKmye9PR0Vq1aFfHrLycnh7S0NNq2bQtE1royxnDbbbfx+eef8/PPP9O5c+ewx7VtVfu1dVWbSN629mSMwev1Hprb1AHvEnuY+Oijj0xUVJR54403zJo1a8yoUaNMo0aNzNatW51eNEfdddddZvr06SYlJcXMnz/fnH/++aZJkybB9fLEE0+Y+Ph48/nnn5uVK1eaK664wrRt29YUFBQ4vOS/v8LCQrN06VKzdOlSA5hx48aZpUuXmm3bthlj6rduRo4caTp06GB+/PFHs2TJEjNkyBDTp08fU1lZ6dTH+l3sbV0VFhaau+66y8ydO9ds2bLFTJs2zQwYMMC0b98+ItfVzTffbOLj48306dNNenp68KekpCQ4j7Yt26+tK21b1caMGWNmzpxptmzZYlasWGHuv/9+43K5zA8//GCMOfS2qYgNI8YY8+KLL5qkpCQTHR1tjjvuuLDLwyLV8OHDTdu2bU1UVJRp166dueSSS8zq1auDj/v9fvPQQw+ZNm3amJiYGHPKKaeYlStXOrjEB8+0adMMUONnxIgRxpj6rZvS0lJz2223mebNm5sGDRqY888/36SmpjrwaX5fe1tXJSUlZujQoaZVq1YmKirKdOzY0YwYMaLGeoiUdVXbegLMW2+9FZxH25bt19aVtq1q1113XfD41qpVK3PGGWcEg4gxh942ZRljzIGvt4iIiIjUT0T2GREREZFDh8KIiIiIOEphRERERBylMCIiIiKOUhgRERERRymMiIiIiKMURkRERMRRCiMiIiLiKIURERERcZTCiIiIiDhKYUREREQcpTAiIiIijvp/zcONENV3k7QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658003da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.061140154095025656\r",
      "iteration 2 - y: 0.06093163126859248\r",
      "iteration 3 - y: 0.06072310844215929\r",
      "iteration 4 - y: 0.06051458561572612\r",
      "iteration 5 - y: 0.060306062789292916\r",
      "iteration 6 - y: 0.06009753996285974\r",
      "iteration 7 - y: 0.05988901713642656\r",
      "iteration 8 - y: 0.05968049430999338\r",
      "iteration 9 - y: 0.059471971483560204\r",
      "iteration 10 - y: 0.059263448657127016\r",
      "iteration 11 - y: 0.05905492583069383\r",
      "iteration 12 - y: 0.058846403004260646\r",
      "iteration 13 - y: 0.05863788017782746\r",
      "iteration 14 - y: 0.058429357351394276\r",
      "iteration 15 - y: 0.0582208345249611\r",
      "iteration 16 - y: 0.05801231169852791\r",
      "iteration 17 - y: 0.05780378887209473\r",
      "iteration 18 - y: 0.057595266045661536\r",
      "iteration 19 - y: 0.05738674321922837\r",
      "iteration 20 - y: 0.05717822039279518\r",
      "iteration 21 - y: 0.056969697566361985\r",
      "iteration 22 - y: 0.05676117473992881\r",
      "iteration 23 - y: 0.05655265191349562\r",
      "iteration 24 - y: 0.05634412908706244\r",
      "iteration 25 - y: 0.05613560626062926\r",
      "iteration 26 - y: 0.055927083434196084\r",
      "iteration 27 - y: 0.055718560607762896\r",
      "iteration 28 - y: 0.055510037781329714\r",
      "iteration 29 - y: 0.05530151495489653\r",
      "iteration 30 - y: 0.055092992128463344\r",
      "iteration 31 - y: 0.054884469302030156\r",
      "iteration 32 - y: 0.05467594647559698\r",
      "iteration 33 - y: 0.05446742364916379\r",
      "iteration 34 - y: 0.054258900822730605\r",
      "iteration 35 - y: 0.05405037799629743\r",
      "iteration 36 - y: 0.05384185516986424\r",
      "iteration 37 - y: 0.05363333234343105\r",
      "iteration 38 - y: 0.05342480951699788\r",
      "iteration 39 - y: 0.0532162866905647\r",
      "iteration 40 - y: 0.053007763864131516\r",
      "iteration 41 - y: 0.05279924103769833\r",
      "iteration 42 - y: 0.05259071821126515\r",
      "iteration 43 - y: 0.05238219538483196\r",
      "iteration 44 - y: 0.052173672558398776\r",
      "iteration 45 - y: 0.051965149731965594\r",
      "iteration 46 - y: 0.05175662690553241\r",
      "iteration 47 - y: 0.051548104079099225\r",
      "iteration 48 - y: 0.05133958125266604\r",
      "iteration 49 - y: 0.05113105842623287\r",
      "iteration 50 - y: 0.05092253559979967\r",
      "iteration 51 - y: 0.0507140127733665\r",
      "iteration 52 - y: 0.05050548994693332\r",
      "iteration 53 - y: 0.05029696712050013\r",
      "iteration 54 - y: 0.05008844429406694\r",
      "iteration 55 - y: 0.04987992146763376\r",
      "iteration 56 - y: 0.049671398641200584\r",
      "iteration 57 - y: 0.049462875814767396\r",
      "iteration 58 - y: 0.04925435298833421\r",
      "iteration 59 - y: 0.04904583016190103\r",
      "iteration 60 - y: 0.048837307335467844\r",
      "iteration 61 - y: 0.048628784509034656\r",
      "iteration 62 - y: 0.04842026168260148\r",
      "iteration 63 - y: 0.04821173885616829\r",
      "iteration 64 - y: 0.04800321602973511\r",
      "iteration 65 - y: 0.04779469320330192\r",
      "iteration 66 - y: 0.04758617037686874\r",
      "iteration 67 - y: 0.04737764755043557\r",
      "iteration 68 - y: 0.04716912472400237\r",
      "iteration 69 - y: 0.0469606018975692\r",
      "iteration 70 - y: 0.04675207907113601\r",
      "iteration 71 - y: 0.04654355624470283\r",
      "iteration 72 - y: 0.046335033418269646\r",
      "iteration 73 - y: 0.04612651059183646\r",
      "iteration 74 - y: 0.045917987765403276\r",
      "iteration 75 - y: 0.045709464938970094\r",
      "iteration 76 - y: 0.04550094211253691\r",
      "iteration 77 - y: 0.045292419286103724\r",
      "iteration 78 - y: 0.04508389645967055\r",
      "iteration 79 - y: 0.04487537363323737\r",
      "iteration 80 - y: 0.04466685080680418\r",
      "iteration 81 - y: 0.04445832798037099\r",
      "iteration 82 - y: 0.04424980515393782\r",
      "iteration 83 - y: 0.04404128232750463\r",
      "iteration 84 - y: 0.04383275950107144\r",
      "iteration 85 - y: 0.04362423667463826\r",
      "iteration 86 - y: 0.04341571384820508\r",
      "iteration 87 - y: 0.043207191021771896\r",
      "iteration 88 - y: 0.04299866819533871\r",
      "iteration 89 - y: 0.04279014536890553\r",
      "iteration 90 - y: 0.042581622542472344\r",
      "iteration 91 - y: 0.042373099716039156\r",
      "iteration 92 - y: 0.042164576889605974\r",
      "iteration 93 - y: 0.04195605406317279\r",
      "iteration 94 - y: 0.041747531236739605\r",
      "iteration 95 - y: 0.04153900841030642\r",
      "iteration 96 - y: 0.04133048558387324\r",
      "iteration 97 - y: 0.04112196275744005\r",
      "iteration 98 - y: 0.04091343993100688\r",
      "iteration 99 - y: 0.0407049171045737\r",
      "iteration 100 - y: 0.04049639427814051\r",
      "iteration 101 - y: 0.04028787145170732\r",
      "iteration 102 - y: 0.04007934862527414\r",
      "iteration 103 - y: 0.03987082579884096\r",
      "iteration 104 - y: 0.03966230297240777\r",
      "iteration 105 - y: 0.03945378014597459\r",
      "iteration 106 - y: 0.039245257319541406\r",
      "iteration 107 - y: 0.03903673449310822\r",
      "iteration 108 - y: 0.038828211666675036\r",
      "iteration 109 - y: 0.03861968884024186\r",
      "iteration 110 - y: 0.03841116601380868\r",
      "iteration 111 - y: 0.0382026431873755\r",
      "iteration 112 - y: 0.0379941203609423\r",
      "iteration 113 - y: 0.03778559753450913\r",
      "iteration 114 - y: 0.03757707470807594\r",
      "iteration 115 - y: 0.03736855188164276\r",
      "iteration 116 - y: 0.03716002905520957\r",
      "iteration 117 - y: 0.03695150622877639\r",
      "iteration 118 - y: 0.03674298340234321\r",
      "iteration 119 - y: 0.03653446057591002\r",
      "iteration 120 - y: 0.036325937749476844\r",
      "iteration 121 - y: 0.036117414923043656\r",
      "iteration 122 - y: 0.035908892096610474\r",
      "iteration 123 - y: 0.03570036927017729\r",
      "iteration 124 - y: 0.035491846443744104\r",
      "iteration 125 - y: 0.03528332361731093\r",
      "iteration 126 - y: 0.03507480079087775\r",
      "iteration 127 - y: 0.03486627796444456\r",
      "iteration 128 - y: 0.03465775513801138\r",
      "iteration 129 - y: 0.03444923231157819\r",
      "iteration 130 - y: 0.034240709485145016\r",
      "iteration 131 - y: 0.03403218665871183\r",
      "iteration 132 - y: 0.03382366383227865\r",
      "iteration 133 - y: 0.03361514100584547\r",
      "iteration 134 - y: 0.03340661817941229\r",
      "iteration 135 - y: 0.03319809535297911\r",
      "iteration 136 - y: 0.03298957252654593\r",
      "iteration 137 - y: 0.032781049700112745\r",
      "iteration 138 - y: 0.032572526873679564\r",
      "iteration 139 - y: 0.03236400404724639\r",
      "iteration 140 - y: 0.0321554812208132\r",
      "iteration 141 - y: 0.031946958394380026\r",
      "iteration 142 - y: 0.031738435567946845\r",
      "iteration 143 - y: 0.03152991274151366\r",
      "iteration 144 - y: 0.031321389915080475\r",
      "iteration 145 - y: 0.031112867088647297\r",
      "iteration 146 - y: 0.03090434426221412\r",
      "iteration 147 - y: 0.030695821435780937\r",
      "iteration 148 - y: 0.03048729860934776\r",
      "iteration 149 - y: 0.030278775782914574\r",
      "iteration 150 - y: 0.030070252956481393\r",
      "iteration 151 - y: 0.02986173013004821\r",
      "iteration 152 - y: 0.02965320730361503\r",
      "iteration 153 - y: 0.02944468447718185\r",
      "iteration 154 - y: 0.029236161650748674\r",
      "iteration 155 - y: 0.029027638824315485\r",
      "iteration 156 - y: 0.028819115997882304\r",
      "iteration 157 - y: 0.028610593171449122\r",
      "iteration 158 - y: 0.028402070345015944\r",
      "iteration 159 - y: 0.028193547518582766\r",
      "iteration 160 - y: 0.02798502469214958\r",
      "iteration 161 - y: 0.0277765018657164\r",
      "iteration 162 - y: 0.027567979039283222\r",
      "iteration 163 - y: 0.02735945621285004\r",
      "iteration 164 - y: 0.0271560690051037\r",
      "iteration 165 - y: 0.02697951346096575\r",
      "iteration 166 - y: 0.026802957916827793\r",
      "iteration 167 - y: 0.026626402372689838\r",
      "iteration 168 - y: 0.026449846828551886\r",
      "iteration 169 - y: 0.026273291284413934\r",
      "iteration 170 - y: 0.026096735740275975\r",
      "iteration 171 - y: 0.025920180196138023\r",
      "iteration 172 - y: 0.025743624652000065\r",
      "iteration 173 - y: 0.025567069107862113\r",
      "iteration 174 - y: 0.02539051356372416\r",
      "iteration 175 - y: 0.02524306356926449\r",
      "iteration 176 - y: 0.02510785305221565\r",
      "iteration 177 - y: 0.024972642535166817\r",
      "iteration 178 - y: 0.02483743201811798\r",
      "iteration 179 - y: 0.024702221501069144\r",
      "iteration 180 - y: 0.024567010984020313\r",
      "iteration 181 - y: 0.024431800466971475\r",
      "iteration 182 - y: 0.02429658994992264\r",
      "iteration 183 - y: 0.02416137943287381\r",
      "iteration 184 - y: 0.024026168915824972\r",
      "iteration 185 - y: 0.023890958398776134\r",
      "iteration 186 - y: 0.023755747881727303\r",
      "iteration 187 - y: 0.023620537364678465\r",
      "iteration 188 - y: 0.023485326847629634\r",
      "iteration 189 - y: 0.0233501163305808\r",
      "iteration 190 - y: 0.02321490581353196\r",
      "iteration 191 - y: 0.023079695296483124\r",
      "iteration 192 - y: 0.02294448477943429\r",
      "iteration 193 - y: 0.022809274262385455\r",
      "iteration 194 - y: 0.02268518578711406\r",
      "iteration 195 - y: 0.02257348012331382\r",
      "iteration 196 - y: 0.022461774459513584\r",
      "iteration 197 - y: 0.022350068795713343\r",
      "iteration 198 - y: 0.022238363131913103\r",
      "iteration 199 - y: 0.02212665746811287\r",
      "iteration 200 - y: 0.02201495180431263\r",
      "iteration 201 - y: 0.021903246140512388\r",
      "iteration 202 - y: 0.02179154047671215\r",
      "iteration 203 - y: 0.02167983481291191\r",
      "iteration 204 - y: 0.021568129149111673\r",
      "iteration 205 - y: 0.021456423485311436\r",
      "iteration 206 - y: 0.021344717821511195\r",
      "iteration 207 - y: 0.021233012157710954\r",
      "iteration 208 - y: 0.021121306493910714\r",
      "iteration 209 - y: 0.021009600830110477\r",
      "iteration 210 - y: 0.02089789516631024\r",
      "iteration 211 - y: 0.02078618950251\r",
      "iteration 212 - y: 0.02067448383870976\r",
      "iteration 213 - y: 0.02056277817490952\r",
      "iteration 214 - y: 0.02045107251110928\r",
      "iteration 215 - y: 0.020339366847309047\r",
      "iteration 216 - y: 0.020227661183508806\r",
      "iteration 217 - y: 0.020115955519708566\r",
      "iteration 218 - y: 0.020004249855908325\r",
      "iteration 219 - y: 0.019892544192108084\r",
      "iteration 220 - y: 0.01978083852830785\r",
      "iteration 221 - y: 0.019669132864507614\r",
      "iteration 222 - y: 0.019557427200707373\r",
      "iteration 223 - y: 0.019445721536907132\r",
      "iteration 224 - y: 0.019334015873106892\r",
      "iteration 225 - y: 0.019222310209306655\r",
      "iteration 226 - y: 0.019110604545506418\r",
      "iteration 227 - y: 0.018998898881706177\r",
      "iteration 228 - y: 0.01888719321790594\r",
      "iteration 229 - y: 0.0187754875541057\r",
      "iteration 230 - y: 0.01866378189030546\r",
      "iteration 231 - y: 0.018552076226505218\r",
      "iteration 232 - y: 0.018440370562704977\r",
      "iteration 233 - y: 0.018328664898904744\r",
      "iteration 234 - y: 0.018216959235104507\r",
      "iteration 235 - y: 0.018105253571304263\r",
      "iteration 236 - y: 0.017993547907504025\r",
      "iteration 237 - y: 0.017881842243703785\r",
      "iteration 238 - y: 0.017770136579903544\r",
      "iteration 239 - y: 0.017658430916103307\r",
      "iteration 240 - y: 0.017546725252303066\r",
      "iteration 241 - y: 0.017435019588502826\r",
      "iteration 242 - y: 0.017323313924702592\r",
      "iteration 243 - y: 0.01721160826090235\r",
      "iteration 244 - y: 0.01709990259710211\r",
      "iteration 245 - y: 0.016988196933301874\r",
      "iteration 246 - y: 0.016876491269501633\r",
      "iteration 247 - y: 0.016764785605701393\r",
      "iteration 248 - y: 0.016653079941901152\r",
      "iteration 249 - y: 0.016541374278100915\r",
      "iteration 250 - y: 0.016429668614300674\r",
      "iteration 251 - y: 0.016317962950500437\r",
      "iteration 252 - y: 0.0162062572867002\r",
      "iteration 253 - y: 0.01609455162289996\r",
      "iteration 254 - y: 0.01598284595909972\r",
      "iteration 255 - y: 0.01587114029529948\r",
      "iteration 256 - y: 0.01575943463149924\r",
      "iteration 257 - y: 0.015647728967699\r",
      "iteration 258 - y: 0.015536023303898763\r",
      "iteration 259 - y: 0.015424317640098523\r",
      "iteration 260 - y: 0.015312611976298284\r",
      "iteration 261 - y: 0.015200906312498045\r",
      "iteration 262 - y: 0.015089200648697806\r",
      "iteration 263 - y: 0.014977494984897567\r",
      "iteration 264 - y: 0.014865789321097327\r",
      "iteration 265 - y: 0.01475408365729709\r",
      "iteration 266 - y: 0.01464237799349685\r",
      "iteration 267 - y: 0.01453067232969661\r",
      "iteration 268 - y: 0.014418966665896373\r",
      "iteration 269 - y: 0.014307261002096134\r",
      "iteration 270 - y: 0.014195555338295893\r",
      "iteration 271 - y: 0.014083849674495656\r",
      "iteration 272 - y: 0.013972144010695416\r",
      "iteration 273 - y: 0.013860438346895177\r",
      "iteration 274 - y: 0.013748732683094938\r",
      "iteration 275 - y: 0.013637027019294699\r",
      "iteration 276 - y: 0.01352532135549446\r",
      "iteration 277 - y: 0.013413615691694221\r",
      "iteration 278 - y: 0.013301910027893982\r",
      "iteration 279 - y: 0.013190204364093744\r",
      "iteration 280 - y: 0.013078498700293503\r",
      "iteration 281 - y: 0.012966793036493264\r",
      "iteration 282 - y: 0.012855087372693027\r",
      "iteration 283 - y: 0.012743381708892786\r",
      "iteration 284 - y: 0.012631676045092546\r",
      "iteration 285 - y: 0.01251997038129231\r",
      "iteration 286 - y: 0.01240826471749207\r",
      "iteration 287 - y: 0.012296559053691829\r",
      "iteration 288 - y: 0.01218485338989159\r",
      "iteration 289 - y: 0.012073147726091353\r",
      "iteration 290 - y: 0.011961442062291112\r",
      "iteration 291 - y: 0.011849736398490874\r",
      "iteration 292 - y: 0.011738030734690636\r",
      "iteration 293 - y: 0.011626325070890396\r",
      "iteration 294 - y: 0.011514619407090155\r",
      "iteration 295 - y: 0.01140291374328992\r",
      "iteration 296 - y: 0.01129120807948968\r",
      "iteration 297 - y: 0.011179502415689439\r",
      "iteration 298 - y: 0.0110677967518892\r",
      "iteration 299 - y: 0.010956091088088963\r",
      "iteration 300 - y: 0.010844385424288722\r",
      "iteration 301 - y: 0.010732679760488483\r",
      "iteration 302 - y: 0.010620974096688243\r",
      "iteration 303 - y: 0.010529177327546095\r",
      "iteration 304 - y: 0.010447887171896911\r",
      "iteration 305 - y: 0.010366597016247725\r",
      "iteration 306 - y: 0.010285306860598541\r",
      "iteration 307 - y: 0.010204016704949356\r",
      "iteration 308 - y: 0.010122726549300172\r",
      "iteration 309 - y: 0.010041436393650988\r",
      "iteration 310 - y: 0.009960146238001802\r",
      "iteration 311 - y: 0.009878856082352618\r",
      "iteration 312 - y: 0.009797565926703434\r",
      "iteration 313 - y: 0.009716275771054248\r",
      "iteration 314 - y: 0.009634985615405062\r",
      "iteration 315 - y: 0.00955369545975588\r",
      "iteration 316 - y: 0.009472405304106694\r",
      "iteration 317 - y: 0.009391115148457509\r",
      "iteration 318 - y: 0.009309824992808325\r",
      "iteration 319 - y: 0.00922853483715914\r",
      "iteration 320 - y: 0.009147244681509955\r",
      "iteration 321 - y: 0.009065954525860771\r",
      "iteration 322 - y: 0.008984664370211585\r",
      "iteration 323 - y: 0.008903374214562401\r",
      "iteration 324 - y: 0.008822084058913217\r",
      "iteration 325 - y: 0.008740793903264031\r",
      "iteration 326 - y: 0.008659503747614847\r",
      "iteration 327 - y: 0.008578213591965663\r",
      "iteration 328 - y: 0.008496923436316478\r",
      "iteration 329 - y: 0.008415633280667294\r",
      "iteration 330 - y: 0.008334343125018108\r",
      "iteration 331 - y: 0.008253052969368924\r",
      "iteration 332 - y: 0.00817176281371974\r",
      "iteration 333 - y: 0.008090472658070554\r",
      "iteration 334 - y: 0.00800918250242137\r",
      "iteration 335 - y: 0.007927892346772186\r",
      "iteration 336 - y: 0.007846602191123\r",
      "iteration 337 - y: 0.007765312035473816\r",
      "iteration 338 - y: 0.007684021879824632\r",
      "iteration 339 - y: 0.007602731724175447\r",
      "iteration 340 - y: 0.007521441568526261\r",
      "iteration 341 - y: 0.007440151412877076\r",
      "iteration 342 - y: 0.007358861257227891\r",
      "iteration 343 - y: 0.007277571101578706\r",
      "iteration 344 - y: 0.0071962809459295216\r",
      "iteration 345 - y: 0.007114990790280336\r",
      "iteration 346 - y: 0.007033700634631152\r",
      "iteration 347 - y: 0.006952410478981966\r",
      "iteration 348 - y: 0.006871120323332782\r",
      "iteration 349 - y: 0.006789830167683596\r",
      "iteration 350 - y: 0.006708540012034412\r",
      "iteration 351 - y: 0.006627249856385227\r",
      "iteration 352 - y: 0.006545959700736042\r",
      "iteration 353 - y: 0.006464669545086857\r",
      "iteration 354 - y: 0.006383379389437672\r",
      "iteration 355 - y: 0.006302089233788487\r",
      "iteration 356 - y: 0.006220799078139302\r",
      "iteration 357 - y: 0.0061395089224901165\r",
      "iteration 358 - y: 0.0060582187668409325\r",
      "iteration 359 - y: 0.005976928611191748\r",
      "iteration 360 - y: 0.005895638455542562\r",
      "iteration 361 - y: 0.005814348299893377\r",
      "iteration 362 - y: 0.005733058144244192\r",
      "iteration 363 - y: 0.005651767988595007\r",
      "iteration 364 - y: 0.0055704778329458225\r",
      "iteration 365 - y: 0.005489187677296637\r",
      "iteration 366 - y: 0.005407897521647451\r",
      "iteration 367 - y: 0.005326607365998266\r",
      "iteration 368 - y: 0.00524531721034908\r",
      "iteration 369 - y: 0.005164027054699895\r",
      "iteration 370 - y: 0.00508273689905071\r",
      "iteration 371 - y: 0.005001446743401524\r",
      "iteration 372 - y: 0.004920156587752339\r",
      "iteration 373 - y: 0.004838866432103153\r",
      "iteration 374 - y: 0.0047575762764539686\r",
      "iteration 375 - y: 0.004676286120804783\r",
      "iteration 376 - y: 0.004594995965155597\r",
      "iteration 377 - y: 0.004513705809506411\r",
      "iteration 378 - y: 0.0044324156538572265\r",
      "iteration 379 - y: 0.004351125498208042\r",
      "iteration 380 - y: 0.004269835342558856\r",
      "iteration 381 - y: 0.004188545186909671\r",
      "iteration 382 - y: 0.004107255031260486\r",
      "iteration 383 - y: 0.0040259648756113\r",
      "iteration 384 - y: 0.0039446747199621155\r",
      "iteration 385 - y: 0.00386338456431293\r",
      "iteration 386 - y: 0.003782094408663745\r",
      "iteration 387 - y: 0.003700804253014559\r",
      "iteration 388 - y: 0.003619514097365374\r",
      "iteration 389 - y: 0.0035382239417161886\r",
      "iteration 390 - y: 0.0034569337860670033\r",
      "iteration 391 - y: 0.003375643630417818\r",
      "iteration 392 - y: 0.0032943534747686327\r",
      "iteration 393 - y: 0.003213063319119448\r",
      "iteration 394 - y: 0.003131773163470262\r",
      "iteration 395 - y: 0.0030504830078210772\r",
      "iteration 396 - y: 0.002969192852171892\r",
      "iteration 397 - y: 0.0028879026965227066\r",
      "iteration 398 - y: 0.0028066125408735213\r",
      "iteration 399 - y: 0.002725322385224336\r",
      "iteration 400 - y: 0.0026440322295751507\r",
      "iteration 401 - y: 0.0025627420739259654\r",
      "iteration 402 - y: 0.0024814519182767797\r",
      "iteration 403 - y: 0.002400161762627595\r",
      "iteration 404 - y: 0.0023188716069784095\r",
      "iteration 405 - y: 0.0022375814513292246\r",
      "iteration 406 - y: 0.0021562912956800393\r",
      "iteration 407 - y: 0.0020750011400308545\r",
      "iteration 408 - y: 0.001993710984381669\r",
      "iteration 409 - y: 0.001912420828732484\r",
      "iteration 410 - y: 0.001831130673083299\r",
      "iteration 411 - y: 0.001749840517434114\r",
      "iteration 412 - y: 0.0016685503617849288\r",
      "iteration 413 - y: 0.0015872602061357437\r",
      "iteration 414 - y: 0.0015059700504865586\r",
      "iteration 415 - y: 0.0014246798948373736\r",
      "iteration 416 - y: 0.0013433897391881885\r",
      "iteration 417 - y: 0.001276115077254026\r",
      "iteration 418 - y: 0.0012246331925360849\r",
      "iteration 419 - y: 0.0011731513078181437\r",
      "iteration 420 - y: 0.0011216694231002025\r",
      "iteration 421 - y: 0.0010701875383822613\r",
      "iteration 422 - y: 0.0010187056536643201\r",
      "iteration 423 - y: 0.0009672237689463787\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.00852004512056601\r",
      "iteration 2 - y: 0.008311522294132831\r",
      "iteration 3 - y: 0.008102999467699648\r",
      "iteration 4 - y: 0.007894476641266466\r",
      "iteration 5 - y: 0.007685953814833285\r",
      "iteration 6 - y: 0.007480760215214157\r",
      "iteration 7 - y: 0.007313582415870093\r",
      "iteration 8 - y: 0.007146404616526031\r",
      "iteration 9 - y: 0.006979226817181968\r",
      "iteration 10 - y: 0.006812049017837905\r",
      "iteration 11 - y: 0.006644871218493842\r",
      "iteration 12 - y: 0.00647769341914978\r",
      "iteration 13 - y: 0.006310515619805716\r",
      "iteration 14 - y: 0.006143337820461654\r",
      "iteration 15 - y: 0.0059761600211175915\r",
      "iteration 16 - y: 0.005825157074335839\r",
      "iteration 17 - y: 0.00568778754592302\r",
      "iteration 18 - y: 0.005550418017510201\r",
      "iteration 19 - y: 0.005413048489097382\r",
      "iteration 20 - y: 0.005275678960684564\r",
      "iteration 21 - y: 0.005138309432271745\r",
      "iteration 22 - y: 0.0050009399038589255\r",
      "iteration 23 - y: 0.004863570375446107\r",
      "iteration 24 - y: 0.004726200847033288\r",
      "iteration 25 - y: 0.004588831318620469\r",
      "iteration 26 - y: 0.0044514617902076505\r",
      "iteration 27 - y: 0.004314092261794831\r",
      "iteration 28 - y: 0.004176722733382012\r",
      "iteration 29 - y: 0.004039353204969194\r",
      "iteration 30 - y: 0.003901983676556375\r",
      "iteration 31 - y: 0.003764614148143556\r",
      "iteration 32 - y: 0.0036416600053606834\r",
      "iteration 33 - y: 0.0035347059850989186\r",
      "iteration 34 - y: 0.003427751964837154\r",
      "iteration 35 - y: 0.0033207979445753896\r",
      "iteration 36 - y: 0.003213843924313625\r",
      "iteration 37 - y: 0.00310688990405186\r",
      "iteration 38 - y: 0.0029999358837900954\r",
      "iteration 39 - y: 0.002892981863528331\r",
      "iteration 40 - y: 0.0027860278432665663\r",
      "iteration 41 - y: 0.0026790738230048016\r",
      "iteration 42 - y: 0.0025721198027430373\r",
      "iteration 43 - y: 0.0024651657824812725\r",
      "iteration 44 - y: 0.002358211762219508\r",
      "iteration 45 - y: 0.002251257741957743\r",
      "iteration 46 - y: 0.0021443037216959783\r",
      "iteration 47 - y: 0.002037349701434214\r",
      "iteration 48 - y: 0.001930395681172449\r",
      "iteration 49 - y: 0.0018234416609106846\r",
      "iteration 50 - y: 0.00171648764064892\r",
      "iteration 51 - y: 0.0016095336203871553\r",
      "iteration 52 - y: 0.0015025796001253908\r",
      "iteration 53 - y: 0.0014018708977061088\r",
      "iteration 54 - y: 0.0013463987621622853\r",
      "iteration 55 - y: 0.0012909266266184622\r",
      "iteration 56 - y: 0.0012354544910746388\r",
      "iteration 57 - y: 0.0011943989351267561\r",
      "iteration 58 - y: 0.001162431652831529\r",
      "iteration 59 - y: 0.001130464370536302\r",
      "iteration 60 - y: 0.001098497088241075\r",
      "iteration 61 - y: 0.0010665298059458481\r",
      "iteration 62 - y: 0.001034562523650621\r",
      "iteration 63 - y: 0.001002595241355394\r",
      "iteration 64 - y: 0.0009706279590601671\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-3.2773125e-05]], dtype=float32),\n",
       " array([[-2.9371935e-05]], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = K.loadBestModel()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.001}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a05202",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad0ec4ebf70>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb211cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.061140154095025656\r",
      "iteration 2 - y: 0.06093163126859248\r",
      "iteration 3 - y: 0.06072310844215929\r",
      "iteration 4 - y: 0.06051458561572612\r",
      "iteration 5 - y: 0.060306062789292916\r",
      "iteration 6 - y: 0.06009753996285974\r",
      "iteration 7 - y: 0.05988901713642656\r",
      "iteration 8 - y: 0.05968049430999338\r",
      "iteration 9 - y: 0.059471971483560204\r",
      "iteration 10 - y: 0.059263448657127016\r",
      "iteration 11 - y: 0.05905492583069383\r",
      "iteration 12 - y: 0.058846403004260646\r",
      "iteration 13 - y: 0.05863788017782746\r",
      "iteration 14 - y: 0.058429357351394276\r",
      "iteration 15 - y: 0.0582208345249611\r",
      "iteration 16 - y: 0.05801231169852791\r",
      "iteration 17 - y: 0.05780378887209473\r",
      "iteration 18 - y: 0.057595266045661536\r",
      "iteration 19 - y: 0.05738674321922837\r",
      "iteration 20 - y: 0.05717822039279518\r",
      "iteration 21 - y: 0.056969697566361985\r",
      "iteration 22 - y: 0.05676117473992881\r",
      "iteration 23 - y: 0.05655265191349562\r",
      "iteration 24 - y: 0.05634412908706244\r",
      "iteration 25 - y: 0.05613560626062926\r",
      "iteration 26 - y: 0.055927083434196084\r",
      "iteration 27 - y: 0.055718560607762896\r",
      "iteration 28 - y: 0.055510037781329714\r",
      "iteration 29 - y: 0.05530151495489653\r",
      "iteration 30 - y: 0.055092992128463344\r",
      "iteration 31 - y: 0.054884469302030156\r",
      "iteration 32 - y: 0.05467594647559698\r",
      "iteration 33 - y: 0.05446742364916379\r",
      "iteration 34 - y: 0.054258900822730605\r",
      "iteration 35 - y: 0.05405037799629743\r",
      "iteration 36 - y: 0.05384185516986424\r",
      "iteration 37 - y: 0.05363333234343105\r",
      "iteration 38 - y: 0.05342480951699788\r",
      "iteration 39 - y: 0.0532162866905647\r",
      "iteration 40 - y: 0.053007763864131516\r",
      "iteration 41 - y: 0.05279924103769833\r",
      "iteration 42 - y: 0.05259071821126515\r",
      "iteration 43 - y: 0.05238219538483196\r",
      "iteration 44 - y: 0.052173672558398776\r",
      "iteration 45 - y: 0.051965149731965594\r",
      "iteration 46 - y: 0.05175662690553241\r",
      "iteration 47 - y: 0.051548104079099225\r",
      "iteration 48 - y: 0.05133958125266604\r",
      "iteration 49 - y: 0.05113105842623287\r",
      "iteration 50 - y: 0.05092253559979967\r",
      "iteration 51 - y: 0.0507140127733665\r",
      "iteration 52 - y: 0.05050548994693332\r",
      "iteration 53 - y: 0.05029696712050013\r",
      "iteration 54 - y: 0.05008844429406694\r",
      "iteration 55 - y: 0.04987992146763376\r",
      "iteration 56 - y: 0.049671398641200584\r",
      "iteration 57 - y: 0.049462875814767396\r",
      "iteration 58 - y: 0.04925435298833421\r",
      "iteration 59 - y: 0.04904583016190103\r",
      "iteration 60 - y: 0.048837307335467844\r",
      "iteration 61 - y: 0.048628784509034656\r",
      "iteration 62 - y: 0.04842026168260148\r",
      "iteration 63 - y: 0.04821173885616829\r",
      "iteration 64 - y: 0.04800321602973511\r",
      "iteration 65 - y: 0.04779469320330192\r",
      "iteration 66 - y: 0.04758617037686874\r",
      "iteration 67 - y: 0.04737764755043557\r",
      "iteration 68 - y: 0.04716912472400237\r",
      "iteration 69 - y: 0.0469606018975692\r",
      "iteration 70 - y: 0.04675207907113601\r",
      "iteration 71 - y: 0.04654355624470283\r",
      "iteration 72 - y: 0.046335033418269646\r",
      "iteration 73 - y: 0.04612651059183646\r",
      "iteration 74 - y: 0.045917987765403276\r",
      "iteration 75 - y: 0.045709464938970094\r",
      "iteration 76 - y: 0.04550094211253691\r",
      "iteration 77 - y: 0.045292419286103724\r",
      "iteration 78 - y: 0.04508389645967055\r",
      "iteration 79 - y: 0.04487537363323737\r",
      "iteration 80 - y: 0.04466685080680418\r",
      "iteration 81 - y: 0.04445832798037099\r",
      "iteration 82 - y: 0.04424980515393782\r",
      "iteration 83 - y: 0.04404128232750463\r",
      "iteration 84 - y: 0.04383275950107144\r",
      "iteration 85 - y: 0.04362423667463826\r",
      "iteration 86 - y: 0.04341571384820508\r",
      "iteration 87 - y: 0.043207191021771896\r",
      "iteration 88 - y: 0.04299866819533871\r",
      "iteration 89 - y: 0.04279014536890553\r",
      "iteration 90 - y: 0.042581622542472344\r",
      "iteration 91 - y: 0.042373099716039156\r",
      "iteration 92 - y: 0.042164576889605974\r",
      "iteration 93 - y: 0.04195605406317279\r",
      "iteration 94 - y: 0.041747531236739605\r",
      "iteration 95 - y: 0.04153900841030642\r",
      "iteration 96 - y: 0.04133048558387324\r",
      "iteration 97 - y: 0.04112196275744005\r",
      "iteration 98 - y: 0.04091343993100688\r",
      "iteration 99 - y: 0.0407049171045737\r",
      "iteration 100 - y: 0.04049639427814051\r",
      "iteration 101 - y: 0.04028787145170732\r",
      "iteration 102 - y: 0.04007934862527414\r",
      "iteration 103 - y: 0.03987082579884096\r",
      "iteration 104 - y: 0.03966230297240777\r",
      "iteration 105 - y: 0.03945378014597459\r",
      "iteration 106 - y: 0.039245257319541406\r",
      "iteration 107 - y: 0.03903673449310822\r",
      "iteration 108 - y: 0.038828211666675036\r",
      "iteration 109 - y: 0.03861968884024186\r",
      "iteration 110 - y: 0.03841116601380868\r",
      "iteration 111 - y: 0.0382026431873755\r",
      "iteration 112 - y: 0.0379941203609423\r",
      "iteration 113 - y: 0.03778559753450913\r",
      "iteration 114 - y: 0.03757707470807594\r",
      "iteration 115 - y: 0.03736855188164276\r",
      "iteration 116 - y: 0.03716002905520957\r",
      "iteration 117 - y: 0.03695150622877639\r",
      "iteration 118 - y: 0.03674298340234321\r",
      "iteration 119 - y: 0.03653446057591002\r",
      "iteration 120 - y: 0.036325937749476844\r",
      "iteration 121 - y: 0.036117414923043656\r",
      "iteration 122 - y: 0.035908892096610474\r",
      "iteration 123 - y: 0.03570036927017729\r",
      "iteration 124 - y: 0.035491846443744104\r",
      "iteration 125 - y: 0.03528332361731093\r",
      "iteration 126 - y: 0.03507480079087775\r",
      "iteration 127 - y: 0.03486627796444456\r",
      "iteration 128 - y: 0.03465775513801138\r",
      "iteration 129 - y: 0.03444923231157819\r",
      "iteration 130 - y: 0.034240709485145016\r",
      "iteration 131 - y: 0.03403218665871183\r",
      "iteration 132 - y: 0.03382366383227865\r",
      "iteration 133 - y: 0.03361514100584547\r",
      "iteration 134 - y: 0.03340661817941229\r",
      "iteration 135 - y: 0.03319809535297911\r",
      "iteration 136 - y: 0.03298957252654593\r",
      "iteration 137 - y: 0.032781049700112745\r",
      "iteration 138 - y: 0.032572526873679564\r",
      "iteration 139 - y: 0.03236400404724639\r",
      "iteration 140 - y: 0.0321554812208132\r",
      "iteration 141 - y: 0.031946958394380026\r",
      "iteration 142 - y: 0.031738435567946845\r",
      "iteration 143 - y: 0.03152991274151366\r",
      "iteration 144 - y: 0.031321389915080475\r",
      "iteration 145 - y: 0.031112867088647297\r",
      "iteration 146 - y: 0.03090434426221412\r",
      "iteration 147 - y: 0.030695821435780937\r",
      "iteration 148 - y: 0.03048729860934776\r",
      "iteration 149 - y: 0.030278775782914574\r",
      "iteration 150 - y: 0.030070252956481393\r",
      "iteration 151 - y: 0.02986173013004821\r",
      "iteration 152 - y: 0.02965320730361503\r",
      "iteration 153 - y: 0.02944468447718185\r",
      "iteration 154 - y: 0.029236161650748674\r",
      "iteration 155 - y: 0.029027638824315485\r",
      "iteration 156 - y: 0.028819115997882304\r",
      "iteration 157 - y: 0.028610593171449122\r",
      "iteration 158 - y: 0.028402070345015944\r",
      "iteration 159 - y: 0.028193547518582766\r",
      "iteration 160 - y: 0.02798502469214958\r",
      "iteration 161 - y: 0.0277765018657164\r",
      "iteration 162 - y: 0.027567979039283222\r",
      "iteration 163 - y: 0.02735945621285004\r",
      "iteration 164 - y: 0.0271560690051037\r",
      "iteration 165 - y: 0.02697951346096575\r",
      "iteration 166 - y: 0.026802957916827793\r",
      "iteration 167 - y: 0.026626402372689838\r",
      "iteration 168 - y: 0.026449846828551886\r",
      "iteration 169 - y: 0.026273291284413934\r",
      "iteration 170 - y: 0.026096735740275975\r",
      "iteration 171 - y: 0.025920180196138023\r",
      "iteration 172 - y: 0.025743624652000065\r",
      "iteration 173 - y: 0.025567069107862113\r",
      "iteration 174 - y: 0.02539051356372416\r",
      "iteration 175 - y: 0.02524306356926449\r",
      "iteration 176 - y: 0.02510785305221565\r",
      "iteration 177 - y: 0.024972642535166817\r",
      "iteration 178 - y: 0.02483743201811798\r",
      "iteration 179 - y: 0.024702221501069144\r",
      "iteration 180 - y: 0.024567010984020313\r",
      "iteration 181 - y: 0.024431800466971475\r",
      "iteration 182 - y: 0.02429658994992264\r",
      "iteration 183 - y: 0.02416137943287381\r",
      "iteration 184 - y: 0.024026168915824972\r",
      "iteration 185 - y: 0.023890958398776134\r",
      "iteration 186 - y: 0.023755747881727303\r",
      "iteration 187 - y: 0.023620537364678465\r",
      "iteration 188 - y: 0.023485326847629634\r",
      "iteration 189 - y: 0.0233501163305808\r",
      "iteration 190 - y: 0.02321490581353196\r",
      "iteration 191 - y: 0.023079695296483124\r",
      "iteration 192 - y: 0.02294448477943429\r",
      "iteration 193 - y: 0.022809274262385455\r",
      "iteration 194 - y: 0.02268518578711406\r",
      "iteration 195 - y: 0.02257348012331382\r",
      "iteration 196 - y: 0.022461774459513584\r",
      "iteration 197 - y: 0.022350068795713343\r",
      "iteration 198 - y: 0.022238363131913103\r",
      "iteration 199 - y: 0.02212665746811287\r",
      "iteration 200 - y: 0.02201495180431263\r",
      "iteration 201 - y: 0.021903246140512388\r",
      "iteration 202 - y: 0.02179154047671215\r",
      "iteration 203 - y: 0.02167983481291191\r",
      "iteration 204 - y: 0.021568129149111673\r",
      "iteration 205 - y: 0.021456423485311436\r",
      "iteration 206 - y: 0.021344717821511195\r",
      "iteration 207 - y: 0.021233012157710954\r",
      "iteration 208 - y: 0.021121306493910714\r",
      "iteration 209 - y: 0.021009600830110477\r",
      "iteration 210 - y: 0.02089789516631024\r",
      "iteration 211 - y: 0.02078618950251\r",
      "iteration 212 - y: 0.02067448383870976\r",
      "iteration 213 - y: 0.02056277817490952\r",
      "iteration 214 - y: 0.02045107251110928\r",
      "iteration 215 - y: 0.020339366847309047\r",
      "iteration 216 - y: 0.020227661183508806\r",
      "iteration 217 - y: 0.020115955519708566\r",
      "iteration 218 - y: 0.020004249855908325\r",
      "iteration 219 - y: 0.019892544192108084\r",
      "iteration 220 - y: 0.01978083852830785\r",
      "iteration 221 - y: 0.019669132864507614\r",
      "iteration 222 - y: 0.019557427200707373\r",
      "iteration 223 - y: 0.019445721536907132\r",
      "iteration 224 - y: 0.019334015873106892\r",
      "iteration 225 - y: 0.019222310209306655\r",
      "iteration 226 - y: 0.019110604545506418\r",
      "iteration 227 - y: 0.018998898881706177\r",
      "iteration 228 - y: 0.01888719321790594\r",
      "iteration 229 - y: 0.0187754875541057\r",
      "iteration 230 - y: 0.01866378189030546\r",
      "iteration 231 - y: 0.018552076226505218\r",
      "iteration 232 - y: 0.018440370562704977\r",
      "iteration 233 - y: 0.018328664898904744\r",
      "iteration 234 - y: 0.018216959235104507\r",
      "iteration 235 - y: 0.018105253571304263\r",
      "iteration 236 - y: 0.017993547907504025\r",
      "iteration 237 - y: 0.017881842243703785\r",
      "iteration 238 - y: 0.017770136579903544\r",
      "iteration 239 - y: 0.017658430916103307\r",
      "iteration 240 - y: 0.017546725252303066\r",
      "iteration 241 - y: 0.017435019588502826\r",
      "iteration 242 - y: 0.017323313924702592\r",
      "iteration 243 - y: 0.01721160826090235\r",
      "iteration 244 - y: 0.01709990259710211\r",
      "iteration 245 - y: 0.016988196933301874\r",
      "iteration 246 - y: 0.016876491269501633\r",
      "iteration 247 - y: 0.016764785605701393\r",
      "iteration 248 - y: 0.016653079941901152\r",
      "iteration 249 - y: 0.016541374278100915\r",
      "iteration 250 - y: 0.016429668614300674\r",
      "iteration 251 - y: 0.016317962950500437\r",
      "iteration 252 - y: 0.0162062572867002\r",
      "iteration 253 - y: 0.01609455162289996\r",
      "iteration 254 - y: 0.01598284595909972\r",
      "iteration 255 - y: 0.01587114029529948\r",
      "iteration 256 - y: 0.01575943463149924\r",
      "iteration 257 - y: 0.015647728967699\r",
      "iteration 258 - y: 0.015536023303898763\r",
      "iteration 259 - y: 0.015424317640098523\r",
      "iteration 260 - y: 0.015312611976298284\r",
      "iteration 261 - y: 0.015200906312498045\r",
      "iteration 262 - y: 0.015089200648697806\r",
      "iteration 263 - y: 0.014977494984897567\r",
      "iteration 264 - y: 0.014865789321097327\r",
      "iteration 265 - y: 0.01475408365729709\r",
      "iteration 266 - y: 0.01464237799349685\r",
      "iteration 267 - y: 0.01453067232969661\r",
      "iteration 268 - y: 0.014418966665896373\r",
      "iteration 269 - y: 0.014307261002096134\r",
      "iteration 270 - y: 0.014195555338295893\r",
      "iteration 271 - y: 0.014083849674495656\r",
      "iteration 272 - y: 0.013972144010695416\r",
      "iteration 273 - y: 0.013860438346895177\r",
      "iteration 274 - y: 0.013748732683094938\r",
      "iteration 275 - y: 0.013637027019294699\r",
      "iteration 276 - y: 0.01352532135549446\r",
      "iteration 277 - y: 0.013413615691694221\r",
      "iteration 278 - y: 0.013301910027893982\r",
      "iteration 279 - y: 0.013190204364093744\r",
      "iteration 280 - y: 0.013078498700293503\r",
      "iteration 281 - y: 0.012966793036493264\r",
      "iteration 282 - y: 0.012855087372693027\r",
      "iteration 283 - y: 0.012743381708892786\r",
      "iteration 284 - y: 0.012631676045092546\r",
      "iteration 285 - y: 0.01251997038129231\r",
      "iteration 286 - y: 0.01240826471749207\r",
      "iteration 287 - y: 0.012296559053691829\r",
      "iteration 288 - y: 0.01218485338989159\r",
      "iteration 289 - y: 0.012073147726091353\r",
      "iteration 290 - y: 0.011961442062291112\r",
      "iteration 291 - y: 0.011849736398490874\r",
      "iteration 292 - y: 0.011738030734690636\r",
      "iteration 293 - y: 0.011626325070890396\r",
      "iteration 294 - y: 0.011514619407090155\r",
      "iteration 295 - y: 0.01140291374328992\r",
      "iteration 296 - y: 0.01129120807948968\r",
      "iteration 297 - y: 0.011179502415689439\r",
      "iteration 298 - y: 0.0110677967518892\r",
      "iteration 299 - y: 0.010956091088088963\r",
      "iteration 300 - y: 0.010844385424288722\r",
      "iteration 301 - y: 0.010732679760488483\r",
      "iteration 302 - y: 0.010620974096688243\r",
      "iteration 303 - y: 0.010529177327546095\r",
      "iteration 304 - y: 0.010447887171896911\r",
      "iteration 305 - y: 0.010366597016247725\r",
      "iteration 306 - y: 0.010285306860598541\r",
      "iteration 307 - y: 0.010204016704949356\r",
      "iteration 308 - y: 0.010122726549300172\r",
      "iteration 309 - y: 0.010041436393650988\r",
      "iteration 310 - y: 0.009960146238001802\r",
      "iteration 311 - y: 0.009878856082352618\r",
      "iteration 312 - y: 0.009797565926703434\r",
      "iteration 313 - y: 0.009716275771054248\r",
      "iteration 314 - y: 0.009634985615405062\r",
      "iteration 315 - y: 0.00955369545975588\r",
      "iteration 316 - y: 0.009472405304106694\r",
      "iteration 317 - y: 0.009391115148457509\r",
      "iteration 318 - y: 0.009309824992808325\r",
      "iteration 319 - y: 0.00922853483715914\r",
      "iteration 320 - y: 0.009147244681509955\r",
      "iteration 321 - y: 0.009065954525860771\r",
      "iteration 322 - y: 0.008984664370211585\r",
      "iteration 323 - y: 0.008903374214562401\r",
      "iteration 324 - y: 0.008822084058913217\r",
      "iteration 325 - y: 0.008740793903264031\r",
      "iteration 326 - y: 0.008659503747614847\r",
      "iteration 327 - y: 0.008578213591965663\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 82 - y: 0.000395216877746080565\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-5.056031e-05]], dtype=float32),\n",
       " array([[-4.783942e-06]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = K.loadBestModel()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.0004}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1753dc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad0ecadb190>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa0ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "370/800 [============>.................] - ETA: 0s - loss: 9.3973e-04 - mae: 0.0242\n",
      "Epoch 1: val_loss improved from inf to 0.00052, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 6.9473e-04 - mae: 0.0213 - val_loss: 5.1573e-04 - val_mae: 0.0198\n",
      "Epoch 2/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 4.9739e-04 - mae: 0.0191\n",
      "Epoch 2: val_loss improved from 0.00052 to 0.00050, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 4.8557e-04 - mae: 0.0187 - val_loss: 4.9853e-04 - val_mae: 0.0194\n",
      "Epoch 3/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 4.4416e-04 - mae: 0.0179\n",
      "Epoch 3: val_loss improved from 0.00050 to 0.00048, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 4.5665e-04 - mae: 0.0182 - val_loss: 4.7989e-04 - val_mae: 0.0188\n",
      "Epoch 4/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 4.2301e-04 - mae: 0.0177\n",
      "Epoch 4: val_loss improved from 0.00048 to 0.00047, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 4.3341e-04 - mae: 0.0178 - val_loss: 4.6641e-04 - val_mae: 0.0186\n",
      "Epoch 5/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 4.3799e-04 - mae: 0.0178\n",
      "Epoch 5: val_loss did not improve from 0.00047\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 4.2471e-04 - mae: 0.0175 - val_loss: 4.7820e-04 - val_mae: 0.0191\n",
      "Epoch 6/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 4.1516e-04 - mae: 0.0174\n",
      "Epoch 6: val_loss improved from 0.00047 to 0.00046, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 4.1187e-04 - mae: 0.0173 - val_loss: 4.5709e-04 - val_mae: 0.0186\n",
      "Epoch 7/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 3.6915e-04 - mae: 0.0163\n",
      "Epoch 7: val_loss improved from 0.00046 to 0.00043, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 3.8145e-04 - mae: 0.0166 - val_loss: 4.2883e-04 - val_mae: 0.0177\n",
      "Epoch 8/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 3.7330e-04 - mae: 0.0164\n",
      "Epoch 8: val_loss improved from 0.00043 to 0.00041, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 3.7741e-04 - mae: 0.0164 - val_loss: 4.1456e-04 - val_mae: 0.0175\n",
      "Epoch 9/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 3.3611e-04 - mae: 0.0154\n",
      "Epoch 9: val_loss did not improve from 0.00041\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 3.5285e-04 - mae: 0.0158 - val_loss: 4.2545e-04 - val_mae: 0.0171\n",
      "Epoch 10/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 3.2637e-04 - mae: 0.0153\n",
      "Epoch 10: val_loss improved from 0.00041 to 0.00039, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 3.3183e-04 - mae: 0.0154 - val_loss: 3.8804e-04 - val_mae: 0.0168\n",
      "Epoch 11/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 3.1898e-04 - mae: 0.0150\n",
      "Epoch 11: val_loss improved from 0.00039 to 0.00038, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 203us/sample - loss: 3.2135e-04 - mae: 0.0151 - val_loss: 3.7594e-04 - val_mae: 0.0167\n",
      "Epoch 12/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.8922e-04 - mae: 0.0144\n",
      "Epoch 12: val_loss improved from 0.00038 to 0.00036, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 194us/sample - loss: 2.8649e-04 - mae: 0.0143 - val_loss: 3.6181e-04 - val_mae: 0.0164\n",
      "Epoch 13/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 2.7661e-04 - mae: 0.0140\n",
      "Epoch 13: val_loss improved from 0.00036 to 0.00034, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 208us/sample - loss: 2.7253e-04 - mae: 0.0139 - val_loss: 3.4426e-04 - val_mae: 0.0159\n",
      "Epoch 14/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.5566e-04 - mae: 0.0133\n",
      "Epoch 14: val_loss improved from 0.00034 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 2.5178e-04 - mae: 0.0132 - val_loss: 3.3211e-04 - val_mae: 0.0152\n",
      "Epoch 15/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.3315e-04 - mae: 0.0127\n",
      "Epoch 15: val_loss improved from 0.00033 to 0.00033, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 194us/sample - loss: 2.3256e-04 - mae: 0.0127 - val_loss: 3.3180e-04 - val_mae: 0.0159\n",
      "Epoch 16/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.1274e-04 - mae: 0.0121\n",
      "Epoch 16: val_loss improved from 0.00033 to 0.00030, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 2.1338e-04 - mae: 0.0121 - val_loss: 3.0209e-04 - val_mae: 0.0150\n",
      "Epoch 17/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9921e-04 - mae: 0.0116\n",
      "Epoch 17: val_loss improved from 0.00030 to 0.00028, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.9997e-04 - mae: 0.0117 - val_loss: 2.8057e-04 - val_mae: 0.0142\n",
      "Epoch 18/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7654e-04 - mae: 0.0110\n",
      "Epoch 18: val_loss improved from 0.00028 to 0.00027, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.7628e-04 - mae: 0.0110 - val_loss: 2.7200e-04 - val_mae: 0.0134\n",
      "Epoch 19/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7848e-04 - mae: 0.0109\n",
      "Epoch 19: val_loss improved from 0.00027 to 0.00026, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 207us/sample - loss: 1.7640e-04 - mae: 0.0109 - val_loss: 2.5634e-04 - val_mae: 0.0138\n",
      "Epoch 20/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.5074e-04 - mae: 0.0100\n",
      "Epoch 20: val_loss improved from 0.00026 to 0.00024, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 1.5364e-04 - mae: 0.0101 - val_loss: 2.3666e-04 - val_mae: 0.0126\n",
      "Epoch 21/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.3766e-04 - mae: 0.0095\n",
      "Epoch 21: val_loss improved from 0.00024 to 0.00022, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 198us/sample - loss: 1.3663e-04 - mae: 0.0095 - val_loss: 2.1915e-04 - val_mae: 0.0123\n",
      "Epoch 22/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.2341e-04 - mae: 0.0090\n",
      "Epoch 22: val_loss improved from 0.00022 to 0.00021, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 1.2285e-04 - mae: 0.0090 - val_loss: 2.1156e-04 - val_mae: 0.0116\n",
      "Epoch 23/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.1447e-04 - mae: 0.0087\n",
      "Epoch 23: val_loss improved from 0.00021 to 0.00019, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 1.1193e-04 - mae: 0.0086 - val_loss: 1.9247e-04 - val_mae: 0.0114\n",
      "Epoch 24/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.0317e-04 - mae: 0.0081\n",
      "Epoch 24: val_loss improved from 0.00019 to 0.00018, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 1.0314e-04 - mae: 0.0081 - val_loss: 1.7990e-04 - val_mae: 0.0110\n",
      "Epoch 25/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 9.5370e-05 - mae: 0.0078\n",
      "Epoch 25: val_loss improved from 0.00018 to 0.00017, saving model to /home/shreyas/XAIRT/examples/model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 188us/sample - loss: 9.3431e-05 - mae: 0.0077 - val_loss: 1.7154e-04 - val_mae: 0.0105\n",
      "Epoch 26/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 8.5872e-05 - mae: 0.0074\n",
      "Epoch 26: val_loss improved from 0.00017 to 0.00016, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 8.7360e-05 - mae: 0.0074 - val_loss: 1.6290e-04 - val_mae: 0.0101\n",
      "Epoch 27/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 7.6800e-05 - mae: 0.0070\n",
      "Epoch 27: val_loss improved from 0.00016 to 0.00016, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 7.6437e-05 - mae: 0.0070 - val_loss: 1.5761e-04 - val_mae: 0.0098\n",
      "Epoch 28/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 7.1954e-05 - mae: 0.0068\n",
      "Epoch 28: val_loss improved from 0.00016 to 0.00014, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 7.3597e-05 - mae: 0.0068 - val_loss: 1.3954e-04 - val_mae: 0.0098\n",
      "Epoch 29/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 6.6488e-05 - mae: 0.0065\n",
      "Epoch 29: val_loss improved from 0.00014 to 0.00013, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 207us/sample - loss: 6.5550e-05 - mae: 0.0065 - val_loss: 1.3025e-04 - val_mae: 0.0094\n",
      "Epoch 30/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 6.1004e-05 - mae: 0.0063\n",
      "Epoch 30: val_loss improved from 0.00013 to 0.00012, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 6.1340e-05 - mae: 0.0063 - val_loss: 1.2442e-04 - val_mae: 0.0088\n",
      "Epoch 31/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 5.7699e-05 - mae: 0.0060\n",
      "Epoch 31: val_loss improved from 0.00012 to 0.00012, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 5.9690e-05 - mae: 0.0061 - val_loss: 1.2390e-04 - val_mae: 0.0087\n",
      "Epoch 32/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 5.8705e-05 - mae: 0.0061\n",
      "Epoch 32: val_loss improved from 0.00012 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 210us/sample - loss: 5.8567e-05 - mae: 0.0061 - val_loss: 1.0429e-04 - val_mae: 0.0084\n",
      "Epoch 33/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 5.2048e-05 - mae: 0.0058\n",
      "Epoch 33: val_loss improved from 0.00010 to 0.00010, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 5.1558e-05 - mae: 0.0058 - val_loss: 9.8631e-05 - val_mae: 0.0081\n",
      "Epoch 34/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 4.7435e-05 - mae: 0.0056\n",
      "Epoch 34: val_loss improved from 0.00010 to 0.00009, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 202us/sample - loss: 4.7272e-05 - mae: 0.0056 - val_loss: 9.1493e-05 - val_mae: 0.0078\n",
      "Epoch 35/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 4.8057e-05 - mae: 0.0056\n",
      "Epoch 35: val_loss improved from 0.00009 to 0.00009, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 207us/sample - loss: 4.8340e-05 - mae: 0.0057 - val_loss: 8.5621e-05 - val_mae: 0.0075\n",
      "Epoch 36/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 4.0939e-05 - mae: 0.0051\n",
      "Epoch 36: val_loss did not improve from 0.00009\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 4.1145e-05 - mae: 0.0051 - val_loss: 1.1362e-04 - val_mae: 0.0084\n",
      "Epoch 37/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 3.7900e-05 - mae: 0.0049\n",
      "Epoch 37: val_loss improved from 0.00009 to 0.00008, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 4.2041e-05 - mae: 0.0052 - val_loss: 7.6066e-05 - val_mae: 0.0070\n",
      "Epoch 38/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 3.4236e-05 - mae: 0.0047\n",
      "Epoch 38: val_loss improved from 0.00008 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 3.4677e-05 - mae: 0.0047 - val_loss: 7.1481e-05 - val_mae: 0.0068\n",
      "Epoch 39/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 3.6233e-05 - mae: 0.0049\n",
      "Epoch 39: val_loss did not improve from 0.00007\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 3.6028e-05 - mae: 0.0048 - val_loss: 7.3724e-05 - val_mae: 0.0068\n",
      "Epoch 40/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 3.6615e-05 - mae: 0.0048\n",
      "Epoch 40: val_loss improved from 0.00007 to 0.00007, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 3.4473e-05 - mae: 0.0047 - val_loss: 6.5112e-05 - val_mae: 0.0065\n",
      "Epoch 41/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 3.0937e-05 - mae: 0.0045\n",
      "Epoch 41: val_loss improved from 0.00007 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 203us/sample - loss: 3.1105e-05 - mae: 0.0045 - val_loss: 6.0534e-05 - val_mae: 0.0062\n",
      "Epoch 42/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 3.0329e-05 - mae: 0.0044\n",
      "Epoch 42: val_loss improved from 0.00006 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 3.1073e-05 - mae: 0.0045 - val_loss: 5.8541e-05 - val_mae: 0.0061\n",
      "Epoch 43/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.8126e-05 - mae: 0.0043\n",
      "Epoch 43: val_loss improved from 0.00006 to 0.00006, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.8899e-05 - mae: 0.0043 - val_loss: 5.5123e-05 - val_mae: 0.0059\n",
      "Epoch 44/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.8467e-05 - mae: 0.0042\n",
      "Epoch 44: val_loss improved from 0.00006 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 2.8716e-05 - mae: 0.0042 - val_loss: 5.4896e-05 - val_mae: 0.0060\n",
      "Epoch 45/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.9638e-05 - mae: 0.0044\n",
      "Epoch 45: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 205us/sample - loss: 2.9440e-05 - mae: 0.0044 - val_loss: 5.0507e-05 - val_mae: 0.0056\n",
      "Epoch 46/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.9922e-05 - mae: 0.0045\n",
      "Epoch 46: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.9271e-05 - mae: 0.0044 - val_loss: 5.2396e-05 - val_mae: 0.0058\n",
      "Epoch 47/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.5877e-05 - mae: 0.0041\n",
      "Epoch 47: val_loss did not improve from 0.00005\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.7864e-05 - mae: 0.0042 - val_loss: 5.7596e-05 - val_mae: 0.0061\n",
      "Epoch 48/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.4483e-05 - mae: 0.0039\n",
      "Epoch 48: val_loss improved from 0.00005 to 0.00005, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.4136e-05 - mae: 0.0040 - val_loss: 4.6129e-05 - val_mae: 0.0054\n",
      "Epoch 49/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.3037e-05 - mae: 0.0039\n",
      "Epoch 49: val_loss improved from 0.00005 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 2.2808e-05 - mae: 0.0039 - val_loss: 4.4276e-05 - val_mae: 0.0053\n",
      "Epoch 50/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.2733e-05 - mae: 0.0039\n",
      "Epoch 50: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.2412e-05 - mae: 0.0038 - val_loss: 4.5003e-05 - val_mae: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.4449e-05 - mae: 0.0040\n",
      "Epoch 51: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.3543e-05 - mae: 0.0039 - val_loss: 4.5272e-05 - val_mae: 0.0053\n",
      "Epoch 52/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2461e-05 - mae: 0.0038\n",
      "Epoch 52: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 2.3074e-05 - mae: 0.0039 - val_loss: 4.3421e-05 - val_mae: 0.0053\n",
      "Epoch 53/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1839e-05 - mae: 0.0037\n",
      "Epoch 53: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 2.1951e-05 - mae: 0.0037 - val_loss: 4.3399e-05 - val_mae: 0.0053\n",
      "Epoch 54/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.5580e-05 - mae: 0.0042\n",
      "Epoch 54: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 2.4474e-05 - mae: 0.0040 - val_loss: 4.1313e-05 - val_mae: 0.0051\n",
      "Epoch 55/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0972e-05 - mae: 0.0037\n",
      "Epoch 55: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 2.0856e-05 - mae: 0.0037 - val_loss: 3.9564e-05 - val_mae: 0.0050\n",
      "Epoch 56/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.3592e-05 - mae: 0.0039\n",
      "Epoch 56: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.3231e-05 - mae: 0.0039 - val_loss: 4.0757e-05 - val_mae: 0.0051\n",
      "Epoch 57/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.5574e-05 - mae: 0.0041\n",
      "Epoch 57: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 2.4416e-05 - mae: 0.0040 - val_loss: 4.1498e-05 - val_mae: 0.0052\n",
      "Epoch 58/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 2.0139e-05 - mae: 0.0036\n",
      "Epoch 58: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9638e-05 - mae: 0.0035 - val_loss: 3.9821e-05 - val_mae: 0.0051\n",
      "Epoch 59/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8973e-05 - mae: 0.0035\n",
      "Epoch 59: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0096e-05 - mae: 0.0036 - val_loss: 4.3566e-05 - val_mae: 0.0053\n",
      "Epoch 60/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9155e-05 - mae: 0.0035\n",
      "Epoch 60: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.9730e-05 - mae: 0.0036 - val_loss: 3.7412e-05 - val_mae: 0.0049\n",
      "Epoch 61/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.0311e-05 - mae: 0.0036\n",
      "Epoch 61: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0138e-05 - mae: 0.0036 - val_loss: 4.2846e-05 - val_mae: 0.0053\n",
      "Epoch 62/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.4521e-05 - mae: 0.0040\n",
      "Epoch 62: val_loss improved from 0.00004 to 0.00004, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 2.3323e-05 - mae: 0.0038 - val_loss: 3.5416e-05 - val_mae: 0.0047\n",
      "Epoch 63/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 2.1290e-05 - mae: 0.0037\n",
      "Epoch 63: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 2.1276e-05 - mae: 0.0037 - val_loss: 4.0247e-05 - val_mae: 0.0050\n",
      "Epoch 64/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8591e-05 - mae: 0.0035\n",
      "Epoch 64: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8448e-05 - mae: 0.0035 - val_loss: 4.4771e-05 - val_mae: 0.0054\n",
      "Epoch 65/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0186e-05 - mae: 0.0035\n",
      "Epoch 65: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0623e-05 - mae: 0.0036 - val_loss: 3.7402e-05 - val_mae: 0.0049\n",
      "Epoch 66/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.3118e-05 - mae: 0.0039\n",
      "Epoch 66: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.3118e-05 - mae: 0.0039 - val_loss: 4.1406e-05 - val_mae: 0.0052\n",
      "Epoch 67/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1370e-05 - mae: 0.0037\n",
      "Epoch 67: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.1467e-05 - mae: 0.0037 - val_loss: 3.6594e-05 - val_mae: 0.0049\n",
      "Epoch 68/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.1351e-05 - mae: 0.0037\n",
      "Epoch 68: val_loss did not improve from 0.00004\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.1352e-05 - mae: 0.0037 - val_loss: 3.5473e-05 - val_mae: 0.0048\n",
      "Epoch 69/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9232e-05 - mae: 0.0036\n",
      "Epoch 69: val_loss improved from 0.00004 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 1.9232e-05 - mae: 0.0036 - val_loss: 3.4763e-05 - val_mae: 0.0047\n",
      "Epoch 70/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.0373e-05 - mae: 0.0036\n",
      "Epoch 70: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.0643e-05 - mae: 0.0036 - val_loss: 3.7124e-05 - val_mae: 0.0048\n",
      "Epoch 71/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.1662e-05 - mae: 0.0038\n",
      "Epoch 71: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1494e-05 - mae: 0.0037 - val_loss: 3.7113e-05 - val_mae: 0.0049\n",
      "Epoch 72/1000\n",
      "370/800 [============>.................] - ETA: 0s - loss: 1.9828e-05 - mae: 0.0036\n",
      "Epoch 72: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0651e-05 - mae: 0.0037 - val_loss: 3.9910e-05 - val_mae: 0.0051\n",
      "Epoch 73/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9051e-05 - mae: 0.0035\n",
      "Epoch 73: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8580e-05 - mae: 0.0035 - val_loss: 3.6259e-05 - val_mae: 0.0049\n",
      "Epoch 74/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6884e-05 - mae: 0.0033\n",
      "Epoch 74: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8451e-05 - mae: 0.0035 - val_loss: 4.0391e-05 - val_mae: 0.0051\n",
      "Epoch 75/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8522e-05 - mae: 0.0034\n",
      "Epoch 75: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.1228e-05 - mae: 0.0037 - val_loss: 3.4350e-05 - val_mae: 0.0046\n",
      "Epoch 76/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.2978e-05 - mae: 0.0038\n",
      "Epoch 76: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.3497e-05 - mae: 0.0038 - val_loss: 6.0909e-05 - val_mae: 0.0064\n",
      "Epoch 77/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0345e-05 - mae: 0.0036\n",
      "Epoch 77: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.0175e-05 - mae: 0.0036 - val_loss: 4.0421e-05 - val_mae: 0.0051\n",
      "Epoch 78/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0714e-05 - mae: 0.0037\n",
      "Epoch 78: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 2.0000e-05 - mae: 0.0036 - val_loss: 3.6526e-05 - val_mae: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0984e-05 - mae: 0.0037\n",
      "Epoch 79: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.9206e-05 - mae: 0.0035 - val_loss: 3.3863e-05 - val_mae: 0.0047\n",
      "Epoch 80/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0657e-05 - mae: 0.0036\n",
      "Epoch 80: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.0491e-05 - mae: 0.0036 - val_loss: 3.4399e-05 - val_mae: 0.0047\n",
      "Epoch 81/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8392e-05 - mae: 0.0034\n",
      "Epoch 81: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 1.8186e-05 - mae: 0.0034 - val_loss: 3.3447e-05 - val_mae: 0.0046\n",
      "Epoch 82/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8288e-05 - mae: 0.0035\n",
      "Epoch 82: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8006e-05 - mae: 0.0034 - val_loss: 4.3114e-05 - val_mae: 0.0054\n",
      "Epoch 83/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9910e-05 - mae: 0.0036\n",
      "Epoch 83: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9626e-05 - mae: 0.0036 - val_loss: 3.3781e-05 - val_mae: 0.0046\n",
      "Epoch 84/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6228e-05 - mae: 0.0032\n",
      "Epoch 84: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7563e-05 - mae: 0.0034 - val_loss: 3.7462e-05 - val_mae: 0.0048\n",
      "Epoch 85/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8796e-05 - mae: 0.0034\n",
      "Epoch 85: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7797e-05 - mae: 0.0034 - val_loss: 3.3746e-05 - val_mae: 0.0046\n",
      "Epoch 86/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6939e-05 - mae: 0.0032\n",
      "Epoch 86: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 2.0430e-05 - mae: 0.0036 - val_loss: 3.5414e-05 - val_mae: 0.0048\n",
      "Epoch 87/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0082e-05 - mae: 0.0036\n",
      "Epoch 87: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9367e-05 - mae: 0.0035 - val_loss: 3.3611e-05 - val_mae: 0.0046\n",
      "Epoch 88/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7581e-05 - mae: 0.0033\n",
      "Epoch 88: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.1599e-05 - mae: 0.0037 - val_loss: 3.7361e-05 - val_mae: 0.0049\n",
      "Epoch 89/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2754e-05 - mae: 0.0039\n",
      "Epoch 89: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 2.1132e-05 - mae: 0.0037 - val_loss: 3.3677e-05 - val_mae: 0.0046\n",
      "Epoch 90/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9151e-05 - mae: 0.0035\n",
      "Epoch 90: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.2501e-05 - mae: 0.0038 - val_loss: 4.7660e-05 - val_mae: 0.0057\n",
      "Epoch 91/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.4372e-05 - mae: 0.0040\n",
      "Epoch 91: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.2229e-05 - mae: 0.0037 - val_loss: 3.8360e-05 - val_mae: 0.0050\n",
      "Epoch 92/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.2227e-05 - mae: 0.0038\n",
      "Epoch 92: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.2153e-05 - mae: 0.0037 - val_loss: 3.4350e-05 - val_mae: 0.0047\n",
      "Epoch 93/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0906e-05 - mae: 0.0037\n",
      "Epoch 93: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9783e-05 - mae: 0.0036 - val_loss: 3.4629e-05 - val_mae: 0.0047\n",
      "Epoch 94/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0303e-05 - mae: 0.0036\n",
      "Epoch 94: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1858e-05 - mae: 0.0038 - val_loss: 3.6245e-05 - val_mae: 0.0048\n",
      "Epoch 95/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8668e-05 - mae: 0.0035\n",
      "Epoch 95: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0563e-05 - mae: 0.0036 - val_loss: 3.6949e-05 - val_mae: 0.0047\n",
      "Epoch 96/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.2156e-05 - mae: 0.0038\n",
      "Epoch 96: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 194us/sample - loss: 2.2100e-05 - mae: 0.0038 - val_loss: 3.3308e-05 - val_mae: 0.0045\n",
      "Epoch 97/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0167e-05 - mae: 0.0035\n",
      "Epoch 97: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.0205e-05 - mae: 0.0036 - val_loss: 3.9251e-05 - val_mae: 0.0049\n",
      "Epoch 98/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9077e-05 - mae: 0.0034\n",
      "Epoch 98: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9616e-05 - mae: 0.0036 - val_loss: 3.3642e-05 - val_mae: 0.0045\n",
      "Epoch 99/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8130e-05 - mae: 0.0034\n",
      "Epoch 99: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.7960e-05 - mae: 0.0034 - val_loss: 3.3918e-05 - val_mae: 0.0045\n",
      "Epoch 100/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9022e-05 - mae: 0.0035\n",
      "Epoch 100: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9720e-05 - mae: 0.0035 - val_loss: 3.4074e-05 - val_mae: 0.0047\n",
      "Epoch 101/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9399e-05 - mae: 0.0035\n",
      "Epoch 101: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9658e-05 - mae: 0.0035 - val_loss: 5.4085e-05 - val_mae: 0.0061\n",
      "Epoch 102/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1890e-05 - mae: 0.0038\n",
      "Epoch 102: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.1890e-05 - mae: 0.0038 - val_loss: 3.3426e-05 - val_mae: 0.0046\n",
      "Epoch 103/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7430e-05 - mae: 0.0034\n",
      "Epoch 103: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8635e-05 - mae: 0.0035 - val_loss: 3.7212e-05 - val_mae: 0.0049\n",
      "Epoch 104/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.3490e-05 - mae: 0.0039\n",
      "Epoch 104: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0646e-05 - mae: 0.0036 - val_loss: 4.3431e-05 - val_mae: 0.0052\n",
      "Epoch 105/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0873e-05 - mae: 0.0037\n",
      "Epoch 105: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0701e-05 - mae: 0.0036 - val_loss: 3.4342e-05 - val_mae: 0.0045\n",
      "Epoch 106/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9142e-05 - mae: 0.0034\n",
      "Epoch 106: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8649e-05 - mae: 0.0034 - val_loss: 3.7918e-05 - val_mae: 0.0050\n",
      "Epoch 107/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7864e-05 - mae: 0.0034\n",
      "Epoch 107: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8896e-05 - mae: 0.0035 - val_loss: 4.3131e-05 - val_mae: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9181e-05 - mae: 0.0036\n",
      "Epoch 108: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.8514e-05 - mae: 0.0035 - val_loss: 3.2918e-05 - val_mae: 0.0045\n",
      "Epoch 109/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2050e-05 - mae: 0.0038\n",
      "Epoch 109: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.3388e-05 - mae: 0.0039 - val_loss: 3.4111e-05 - val_mae: 0.0047\n",
      "Epoch 110/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0133e-05 - mae: 0.0036\n",
      "Epoch 110: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0133e-05 - mae: 0.0036 - val_loss: 3.4447e-05 - val_mae: 0.0046\n",
      "Epoch 111/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8800e-05 - mae: 0.0035\n",
      "Epoch 111: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8498e-05 - mae: 0.0035 - val_loss: 3.4525e-05 - val_mae: 0.0047\n",
      "Epoch 112/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8159e-05 - mae: 0.0034\n",
      "Epoch 112: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9116e-05 - mae: 0.0035 - val_loss: 4.4315e-05 - val_mae: 0.0055\n",
      "Epoch 113/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8329e-05 - mae: 0.0034\n",
      "Epoch 113: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.8262e-05 - mae: 0.0034 - val_loss: 3.2693e-05 - val_mae: 0.0045\n",
      "Epoch 114/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7826e-05 - mae: 0.0033\n",
      "Epoch 114: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.7729e-05 - mae: 0.0033 - val_loss: 3.6396e-05 - val_mae: 0.0047\n",
      "Epoch 115/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0537e-05 - mae: 0.0036\n",
      "Epoch 115: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0516e-05 - mae: 0.0036 - val_loss: 3.3575e-05 - val_mae: 0.0046\n",
      "Epoch 116/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8573e-05 - mae: 0.0035\n",
      "Epoch 116: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9312e-05 - mae: 0.0035 - val_loss: 3.4290e-05 - val_mae: 0.0045\n",
      "Epoch 117/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8014e-05 - mae: 0.0033\n",
      "Epoch 117: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0511e-05 - mae: 0.0036 - val_loss: 3.4524e-05 - val_mae: 0.0047\n",
      "Epoch 118/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7874e-05 - mae: 0.0034\n",
      "Epoch 118: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8435e-05 - mae: 0.0034 - val_loss: 3.3642e-05 - val_mae: 0.0046\n",
      "Epoch 119/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0969e-05 - mae: 0.0037\n",
      "Epoch 119: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0941e-05 - mae: 0.0037 - val_loss: 3.6219e-05 - val_mae: 0.0047\n",
      "Epoch 120/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0635e-05 - mae: 0.0037\n",
      "Epoch 120: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0138e-05 - mae: 0.0036 - val_loss: 3.4821e-05 - val_mae: 0.0047\n",
      "Epoch 121/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7925e-05 - mae: 0.0034\n",
      "Epoch 121: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9533e-05 - mae: 0.0035 - val_loss: 3.4230e-05 - val_mae: 0.0045\n",
      "Epoch 122/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2179e-05 - mae: 0.0039\n",
      "Epoch 122: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.1234e-05 - mae: 0.0037 - val_loss: 3.3055e-05 - val_mae: 0.0045\n",
      "Epoch 123/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0123e-05 - mae: 0.0036\n",
      "Epoch 123: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.0140e-05 - mae: 0.0036 - val_loss: 3.8505e-05 - val_mae: 0.0051\n",
      "Epoch 124/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9271e-05 - mae: 0.0035\n",
      "Epoch 124: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9769e-05 - mae: 0.0036 - val_loss: 3.3195e-05 - val_mae: 0.0045\n",
      "Epoch 125/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9498e-05 - mae: 0.0035\n",
      "Epoch 125: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.0637e-05 - mae: 0.0036 - val_loss: 6.3793e-05 - val_mae: 0.0067\n",
      "Epoch 126/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1137e-05 - mae: 0.0037\n",
      "Epoch 126: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.1137e-05 - mae: 0.0037 - val_loss: 3.7390e-05 - val_mae: 0.0050\n",
      "Epoch 127/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9739e-05 - mae: 0.0036\n",
      "Epoch 127: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9810e-05 - mae: 0.0036 - val_loss: 4.4530e-05 - val_mae: 0.0052\n",
      "Epoch 128/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1095e-05 - mae: 0.0037\n",
      "Epoch 128: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0538e-05 - mae: 0.0036 - val_loss: 4.4723e-05 - val_mae: 0.0055\n",
      "Epoch 129/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9702e-05 - mae: 0.0035\n",
      "Epoch 129: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9702e-05 - mae: 0.0035 - val_loss: 3.5594e-05 - val_mae: 0.0046\n",
      "Epoch 130/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9367e-05 - mae: 0.0035\n",
      "Epoch 130: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.9201e-05 - mae: 0.0035 - val_loss: 3.3434e-05 - val_mae: 0.0045\n",
      "Epoch 131/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8612e-05 - mae: 0.0035\n",
      "Epoch 131: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8612e-05 - mae: 0.0035 - val_loss: 4.3404e-05 - val_mae: 0.0054\n",
      "Epoch 132/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9207e-05 - mae: 0.0035\n",
      "Epoch 132: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9187e-05 - mae: 0.0035 - val_loss: 3.3664e-05 - val_mae: 0.0046\n",
      "Epoch 133/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 2.5746e-05 - mae: 0.0041\n",
      "Epoch 133: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.1776e-05 - mae: 0.0037 - val_loss: 3.4511e-05 - val_mae: 0.0047\n",
      "Epoch 134/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0206e-05 - mae: 0.0036\n",
      "Epoch 134: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0569e-05 - mae: 0.0036 - val_loss: 3.7810e-05 - val_mae: 0.0050\n",
      "Epoch 135/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8589e-05 - mae: 0.0035\n",
      "Epoch 135: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9398e-05 - mae: 0.0035 - val_loss: 3.3708e-05 - val_mae: 0.0045\n",
      "Epoch 136/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6843e-05 - mae: 0.0032\n",
      "Epoch 136: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7611e-05 - mae: 0.0034 - val_loss: 3.3519e-05 - val_mae: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9142e-05 - mae: 0.0035\n",
      "Epoch 137: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9109e-05 - mae: 0.0035 - val_loss: 4.7117e-05 - val_mae: 0.0057\n",
      "Epoch 138/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6723e-05 - mae: 0.0033\n",
      "Epoch 138: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9248e-05 - mae: 0.0035 - val_loss: 3.3026e-05 - val_mae: 0.0045\n",
      "Epoch 139/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.4737e-05 - mae: 0.0041\n",
      "Epoch 139: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1599e-05 - mae: 0.0037 - val_loss: 3.3230e-05 - val_mae: 0.0046\n",
      "Epoch 140/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.2779e-05 - mae: 0.0038\n",
      "Epoch 140: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.2746e-05 - mae: 0.0038 - val_loss: 3.6258e-05 - val_mae: 0.0049\n",
      "Epoch 141/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8990e-05 - mae: 0.0035\n",
      "Epoch 141: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9762e-05 - mae: 0.0036 - val_loss: 3.5640e-05 - val_mae: 0.0046\n",
      "Epoch 142/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2633e-05 - mae: 0.0038\n",
      "Epoch 142: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.1718e-05 - mae: 0.0037 - val_loss: 3.5119e-05 - val_mae: 0.0046\n",
      "Epoch 143/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7956e-05 - mae: 0.0034\n",
      "Epoch 143: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8156e-05 - mae: 0.0034 - val_loss: 3.3157e-05 - val_mae: 0.0046\n",
      "Epoch 144/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7853e-05 - mae: 0.0034\n",
      "Epoch 144: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8290e-05 - mae: 0.0034 - val_loss: 4.2694e-05 - val_mae: 0.0051\n",
      "Epoch 145/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.1559e-05 - mae: 0.0037\n",
      "Epoch 145: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.2437e-05 - mae: 0.0038 - val_loss: 4.6245e-05 - val_mae: 0.0056\n",
      "Epoch 146/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8182e-05 - mae: 0.0035\n",
      "Epoch 146: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0742e-05 - mae: 0.0037 - val_loss: 3.7314e-05 - val_mae: 0.0047\n",
      "Epoch 147/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7627e-05 - mae: 0.0033\n",
      "Epoch 147: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8261e-05 - mae: 0.0034 - val_loss: 3.3258e-05 - val_mae: 0.0046\n",
      "Epoch 148/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8545e-05 - mae: 0.0034\n",
      "Epoch 148: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7561e-05 - mae: 0.0033 - val_loss: 3.4708e-05 - val_mae: 0.0047\n",
      "Epoch 149/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9500e-05 - mae: 0.0036\n",
      "Epoch 149: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9102e-05 - mae: 0.0035 - val_loss: 3.4554e-05 - val_mae: 0.0047\n",
      "Epoch 150/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.1869e-05 - mae: 0.0038\n",
      "Epoch 150: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.0949e-05 - mae: 0.0036 - val_loss: 3.3686e-05 - val_mae: 0.0045\n",
      "Epoch 151/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0110e-05 - mae: 0.0036\n",
      "Epoch 151: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.2249e-05 - mae: 0.0038 - val_loss: 3.8530e-05 - val_mae: 0.0048\n",
      "Epoch 152/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.2165e-05 - mae: 0.0037\n",
      "Epoch 152: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.2112e-05 - mae: 0.0037 - val_loss: 3.6212e-05 - val_mae: 0.0046\n",
      "Epoch 153/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7729e-05 - mae: 0.0034\n",
      "Epoch 153: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8026e-05 - mae: 0.0035 - val_loss: 3.9300e-05 - val_mae: 0.0051\n",
      "Epoch 154/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5840e-05 - mae: 0.0032\n",
      "Epoch 154: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7634e-05 - mae: 0.0034 - val_loss: 3.7510e-05 - val_mae: 0.0050\n",
      "Epoch 155/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9296e-05 - mae: 0.0035\n",
      "Epoch 155: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9070e-05 - mae: 0.0035 - val_loss: 3.6726e-05 - val_mae: 0.0049\n",
      "Epoch 156/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8723e-05 - mae: 0.0034\n",
      "Epoch 156: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.1206e-05 - mae: 0.0037 - val_loss: 7.5800e-05 - val_mae: 0.0074\n",
      "Epoch 157/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.9969e-05 - mae: 0.0035\n",
      "Epoch 157: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.9255e-05 - mae: 0.0035 - val_loss: 3.4743e-05 - val_mae: 0.0046\n",
      "Epoch 158/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6708e-05 - mae: 0.0033\n",
      "Epoch 158: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.9185e-05 - mae: 0.0035 - val_loss: 4.8294e-05 - val_mae: 0.0057\n",
      "Epoch 159/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.8088e-05 - mae: 0.0034\n",
      "Epoch 159: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 2.0508e-05 - mae: 0.0036 - val_loss: 3.4963e-05 - val_mae: 0.0046\n",
      "Epoch 160/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.0683e-05 - mae: 0.0036\n",
      "Epoch 160: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.9942e-05 - mae: 0.0036 - val_loss: 3.4979e-05 - val_mae: 0.0047\n",
      "Epoch 161/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.8048e-05 - mae: 0.0034\n",
      "Epoch 161: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.8701e-05 - mae: 0.0035 - val_loss: 3.3949e-05 - val_mae: 0.0046\n",
      "Epoch 162/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7653e-05 - mae: 0.0033\n",
      "Epoch 162: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.9386e-05 - mae: 0.0035 - val_loss: 3.7383e-05 - val_mae: 0.0047\n",
      "Epoch 163/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.6293e-05 - mae: 0.0033\n",
      "Epoch 163: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.7086e-05 - mae: 0.0033 - val_loss: 3.5701e-05 - val_mae: 0.0046\n",
      "Epoch 164/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.1903e-05 - mae: 0.0037\n",
      "Epoch 164: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.0594e-05 - mae: 0.0036 - val_loss: 3.4427e-05 - val_mae: 0.0046\n",
      "Epoch 165/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9351e-05 - mae: 0.0035\n",
      "Epoch 165: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9191e-05 - mae: 0.0035 - val_loss: 3.8518e-05 - val_mae: 0.0050\n",
      "Epoch 166/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/800 [========================>.....] - ETA: 0s - loss: 2.1693e-05 - mae: 0.0037\n",
      "Epoch 166: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 2.1640e-05 - mae: 0.0037 - val_loss: 3.6563e-05 - val_mae: 0.0047\n",
      "Epoch 167/1000\n",
      "630/800 [======================>.......] - ETA: 0s - loss: 1.8249e-05 - mae: 0.0034\n",
      "Epoch 167: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 192us/sample - loss: 1.8135e-05 - mae: 0.0034 - val_loss: 3.3814e-05 - val_mae: 0.0045\n",
      "Epoch 168/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6833e-05 - mae: 0.0033\n",
      "Epoch 168: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.7347e-05 - mae: 0.0034 - val_loss: 3.4509e-05 - val_mae: 0.0046\n",
      "Epoch 169/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9008e-05 - mae: 0.0034\n",
      "Epoch 169: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8866e-05 - mae: 0.0035 - val_loss: 3.3746e-05 - val_mae: 0.0046\n",
      "Epoch 170/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7667e-05 - mae: 0.0033\n",
      "Epoch 170: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8087e-05 - mae: 0.0034 - val_loss: 4.5988e-05 - val_mae: 0.0056\n",
      "Epoch 171/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9722e-05 - mae: 0.0035\n",
      "Epoch 171: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.9455e-05 - mae: 0.0035 - val_loss: 3.6815e-05 - val_mae: 0.0049\n",
      "Epoch 172/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8206e-05 - mae: 0.0034\n",
      "Epoch 172: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8278e-05 - mae: 0.0034 - val_loss: 3.7488e-05 - val_mae: 0.0050\n",
      "Epoch 173/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9791e-05 - mae: 0.0036\n",
      "Epoch 173: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0061e-05 - mae: 0.0036 - val_loss: 3.3765e-05 - val_mae: 0.0045\n",
      "Epoch 174/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7159e-05 - mae: 0.0033\n",
      "Epoch 174: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7331e-05 - mae: 0.0033 - val_loss: 3.4554e-05 - val_mae: 0.0047\n",
      "Epoch 175/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9114e-05 - mae: 0.0035\n",
      "Epoch 175: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9294e-05 - mae: 0.0035 - val_loss: 3.3941e-05 - val_mae: 0.0046\n",
      "Epoch 176/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8413e-05 - mae: 0.0034\n",
      "Epoch 176: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9864e-05 - mae: 0.0035 - val_loss: 3.9754e-05 - val_mae: 0.0052\n",
      "Epoch 177/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.2042e-05 - mae: 0.0038\n",
      "Epoch 177: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.2362e-05 - mae: 0.0038 - val_loss: 3.3100e-05 - val_mae: 0.0045\n",
      "Epoch 178/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8279e-05 - mae: 0.0034\n",
      "Epoch 178: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8660e-05 - mae: 0.0034 - val_loss: 3.3478e-05 - val_mae: 0.0046\n",
      "Epoch 179/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0226e-05 - mae: 0.0035\n",
      "Epoch 179: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.0165e-05 - mae: 0.0035 - val_loss: 5.5501e-05 - val_mae: 0.0062\n",
      "Epoch 180/1000\n",
      "610/800 [=====================>........] - ETA: 0s - loss: 1.8658e-05 - mae: 0.0035\n",
      "Epoch 180: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 190us/sample - loss: 2.0200e-05 - mae: 0.0036 - val_loss: 3.8302e-05 - val_mae: 0.0048\n",
      "Epoch 181/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.5355e-05 - mae: 0.0041\n",
      "Epoch 181: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.5558e-05 - mae: 0.0041 - val_loss: 3.3182e-05 - val_mae: 0.0046\n",
      "Epoch 182/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9304e-05 - mae: 0.0036\n",
      "Epoch 182: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8832e-05 - mae: 0.0035 - val_loss: 3.4429e-05 - val_mae: 0.0045\n",
      "Epoch 183/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8976e-05 - mae: 0.0035\n",
      "Epoch 183: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 234us/sample - loss: 1.8838e-05 - mae: 0.0035 - val_loss: 3.3627e-05 - val_mae: 0.0046\n",
      "Epoch 184/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0922e-05 - mae: 0.0036\n",
      "Epoch 184: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 2.2326e-05 - mae: 0.0038 - val_loss: 5.7965e-05 - val_mae: 0.0063\n",
      "Epoch 185/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7488e-05 - mae: 0.0033\n",
      "Epoch 185: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.7777e-05 - mae: 0.0033 - val_loss: 4.2089e-05 - val_mae: 0.0053\n",
      "Epoch 186/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.0563e-05 - mae: 0.0036\n",
      "Epoch 186: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.0299e-05 - mae: 0.0036 - val_loss: 3.4537e-05 - val_mae: 0.0047\n",
      "Epoch 187/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9681e-05 - mae: 0.0035\n",
      "Epoch 187: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.9338e-05 - mae: 0.0035 - val_loss: 3.5017e-05 - val_mae: 0.0047\n",
      "Epoch 188/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5655e-05 - mae: 0.0032\n",
      "Epoch 188: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8924e-05 - mae: 0.0035 - val_loss: 3.4306e-05 - val_mae: 0.0046\n",
      "Epoch 189/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8158e-05 - mae: 0.0034\n",
      "Epoch 189: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8143e-05 - mae: 0.0034 - val_loss: 3.3482e-05 - val_mae: 0.0046\n",
      "Epoch 190/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7868e-05 - mae: 0.0034\n",
      "Epoch 190: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7964e-05 - mae: 0.0034 - val_loss: 3.3826e-05 - val_mae: 0.0046\n",
      "Epoch 191/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8325e-05 - mae: 0.0034\n",
      "Epoch 191: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8387e-05 - mae: 0.0034 - val_loss: 3.3582e-05 - val_mae: 0.0046\n",
      "Epoch 192/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9786e-05 - mae: 0.0036\n",
      "Epoch 192: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9667e-05 - mae: 0.0035 - val_loss: 3.5134e-05 - val_mae: 0.0048\n",
      "Epoch 193/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8012e-05 - mae: 0.0034\n",
      "Epoch 193: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.9365e-05 - mae: 0.0036 - val_loss: 3.6777e-05 - val_mae: 0.0047\n",
      "Epoch 194/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8490e-05 - mae: 0.0034\n",
      "Epoch 194: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8411e-05 - mae: 0.0034 - val_loss: 3.3928e-05 - val_mae: 0.0045\n",
      "Epoch 195/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 0s - loss: 1.9331e-05 - mae: 0.0035\n",
      "Epoch 195: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9331e-05 - mae: 0.0035 - val_loss: 3.3484e-05 - val_mae: 0.0046\n",
      "Epoch 196/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7457e-05 - mae: 0.0033\n",
      "Epoch 196: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7418e-05 - mae: 0.0033 - val_loss: 3.4247e-05 - val_mae: 0.0047\n",
      "Epoch 197/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8798e-05 - mae: 0.0034\n",
      "Epoch 197: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0078e-05 - mae: 0.0035 - val_loss: 3.4674e-05 - val_mae: 0.0046\n",
      "Epoch 198/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8144e-05 - mae: 0.0034\n",
      "Epoch 198: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8026e-05 - mae: 0.0033 - val_loss: 3.5596e-05 - val_mae: 0.0046\n",
      "Epoch 199/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7520e-05 - mae: 0.0034\n",
      "Epoch 199: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8441e-05 - mae: 0.0034 - val_loss: 3.3134e-05 - val_mae: 0.0046\n",
      "Epoch 200/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8510e-05 - mae: 0.0034\n",
      "Epoch 200: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8394e-05 - mae: 0.0034 - val_loss: 3.8283e-05 - val_mae: 0.0048\n",
      "Epoch 201/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0452e-05 - mae: 0.0036\n",
      "Epoch 201: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 2.0662e-05 - mae: 0.0037 - val_loss: 3.3052e-05 - val_mae: 0.0045\n",
      "Epoch 202/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7749e-05 - mae: 0.0034\n",
      "Epoch 202: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7749e-05 - mae: 0.0034 - val_loss: 3.3858e-05 - val_mae: 0.0046\n",
      "Epoch 203/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0954e-05 - mae: 0.0037\n",
      "Epoch 203: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0373e-05 - mae: 0.0036 - val_loss: 3.8070e-05 - val_mae: 0.0050\n",
      "Epoch 204/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6606e-05 - mae: 0.0032\n",
      "Epoch 204: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9002e-05 - mae: 0.0035 - val_loss: 4.0122e-05 - val_mae: 0.0052\n",
      "Epoch 205/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0429e-05 - mae: 0.0037\n",
      "Epoch 205: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9885e-05 - mae: 0.0036 - val_loss: 3.7373e-05 - val_mae: 0.0050\n",
      "Epoch 206/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6667e-05 - mae: 0.0033\n",
      "Epoch 206: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8909e-05 - mae: 0.0035 - val_loss: 3.4003e-05 - val_mae: 0.0047\n",
      "Epoch 207/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9547e-05 - mae: 0.0036\n",
      "Epoch 207: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 2.0020e-05 - mae: 0.0036 - val_loss: 3.7524e-05 - val_mae: 0.0050\n",
      "Epoch 208/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8602e-05 - mae: 0.0034\n",
      "Epoch 208: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0775e-05 - mae: 0.0036 - val_loss: 3.3409e-05 - val_mae: 0.0046\n",
      "Epoch 209/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7648e-05 - mae: 0.0033\n",
      "Epoch 209: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8140e-05 - mae: 0.0034 - val_loss: 3.5063e-05 - val_mae: 0.0047\n",
      "Epoch 210/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8574e-05 - mae: 0.0035\n",
      "Epoch 210: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.8406e-05 - mae: 0.0034 - val_loss: 3.3808e-05 - val_mae: 0.0045\n",
      "Epoch 211/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7370e-05 - mae: 0.0033\n",
      "Epoch 211: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7753e-05 - mae: 0.0034 - val_loss: 3.3545e-05 - val_mae: 0.0045\n",
      "Epoch 212/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0707e-05 - mae: 0.0037\n",
      "Epoch 212: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9463e-05 - mae: 0.0036 - val_loss: 3.4420e-05 - val_mae: 0.0046\n",
      "Epoch 213/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9809e-05 - mae: 0.0035\n",
      "Epoch 213: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 2.0859e-05 - mae: 0.0036 - val_loss: 4.0857e-05 - val_mae: 0.0052\n",
      "Epoch 214/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7833e-05 - mae: 0.0034\n",
      "Epoch 214: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7789e-05 - mae: 0.0034 - val_loss: 3.4827e-05 - val_mae: 0.0046\n",
      "Epoch 215/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8547e-05 - mae: 0.0035\n",
      "Epoch 215: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9324e-05 - mae: 0.0035 - val_loss: 3.3280e-05 - val_mae: 0.0046\n",
      "Epoch 216/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7576e-05 - mae: 0.0033\n",
      "Epoch 216: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9047e-05 - mae: 0.0035 - val_loss: 3.4393e-05 - val_mae: 0.0046\n",
      "Epoch 217/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8819e-05 - mae: 0.0034\n",
      "Epoch 217: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8513e-05 - mae: 0.0034 - val_loss: 3.3281e-05 - val_mae: 0.0045\n",
      "Epoch 218/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8112e-05 - mae: 0.0034\n",
      "Epoch 218: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8380e-05 - mae: 0.0034 - val_loss: 4.0963e-05 - val_mae: 0.0052\n",
      "Epoch 219/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0689e-05 - mae: 0.0036\n",
      "Epoch 219: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0309e-05 - mae: 0.0035 - val_loss: 3.6243e-05 - val_mae: 0.0047\n",
      "Epoch 220/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7207e-05 - mae: 0.0033\n",
      "Epoch 220: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8646e-05 - mae: 0.0034 - val_loss: 3.2897e-05 - val_mae: 0.0045\n",
      "Epoch 221/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9653e-05 - mae: 0.0036\n",
      "Epoch 221: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.2018e-05 - mae: 0.0037 - val_loss: 3.6624e-05 - val_mae: 0.0049\n",
      "Epoch 222/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7793e-05 - mae: 0.0034\n",
      "Epoch 222: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8413e-05 - mae: 0.0034 - val_loss: 3.8965e-05 - val_mae: 0.0051\n",
      "Epoch 223/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8387e-05 - mae: 0.0034\n",
      "Epoch 223: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8155e-05 - mae: 0.0034 - val_loss: 3.4834e-05 - val_mae: 0.0048\n",
      "Epoch 224/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6558e-05 - mae: 0.0033\n",
      "Epoch 224: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9279e-05 - mae: 0.0035 - val_loss: 4.5859e-05 - val_mae: 0.0056\n",
      "Epoch 225/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9189e-05 - mae: 0.0035\n",
      "Epoch 225: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9104e-05 - mae: 0.0035 - val_loss: 4.6454e-05 - val_mae: 0.0056\n",
      "Epoch 226/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8441e-05 - mae: 0.0034\n",
      "Epoch 226: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8836e-05 - mae: 0.0034 - val_loss: 3.3321e-05 - val_mae: 0.0045\n",
      "Epoch 227/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9397e-05 - mae: 0.0035\n",
      "Epoch 227: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.8986e-05 - mae: 0.0035 - val_loss: 4.3240e-05 - val_mae: 0.0054\n",
      "Epoch 228/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6973e-05 - mae: 0.0033\n",
      "Epoch 228: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.6842e-05 - mae: 0.0033 - val_loss: 3.3416e-05 - val_mae: 0.0046\n",
      "Epoch 229/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9456e-05 - mae: 0.0035\n",
      "Epoch 229: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.9603e-05 - mae: 0.0035 - val_loss: 3.3512e-05 - val_mae: 0.0046\n",
      "Epoch 230/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.9076e-05 - mae: 0.0035\n",
      "Epoch 230: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.8601e-05 - mae: 0.0034 - val_loss: 3.4241e-05 - val_mae: 0.0047\n",
      "Epoch 231/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7818e-05 - mae: 0.0034\n",
      "Epoch 231: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7973e-05 - mae: 0.0034 - val_loss: 4.3276e-05 - val_mae: 0.0052\n",
      "Epoch 232/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.3805e-05 - mae: 0.0039\n",
      "Epoch 232: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.2452e-05 - mae: 0.0038 - val_loss: 3.3218e-05 - val_mae: 0.0045\n",
      "Epoch 233/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1852e-05 - mae: 0.0037\n",
      "Epoch 233: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0912e-05 - mae: 0.0037 - val_loss: 3.2918e-05 - val_mae: 0.0045\n",
      "Epoch 234/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7599e-05 - mae: 0.0034\n",
      "Epoch 234: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7880e-05 - mae: 0.0034 - val_loss: 3.4736e-05 - val_mae: 0.0048\n",
      "Epoch 235/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8287e-05 - mae: 0.0034\n",
      "Epoch 235: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9722e-05 - mae: 0.0035 - val_loss: 3.5615e-05 - val_mae: 0.0048\n",
      "Epoch 236/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1461e-05 - mae: 0.0037\n",
      "Epoch 236: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9713e-05 - mae: 0.0036 - val_loss: 3.3650e-05 - val_mae: 0.0045\n",
      "Epoch 237/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0143e-05 - mae: 0.0035\n",
      "Epoch 237: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.0305e-05 - mae: 0.0035 - val_loss: 3.2974e-05 - val_mae: 0.0045\n",
      "Epoch 238/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0828e-05 - mae: 0.0036\n",
      "Epoch 238: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0760e-05 - mae: 0.0036 - val_loss: 3.3483e-05 - val_mae: 0.0046\n",
      "Epoch 239/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6724e-05 - mae: 0.0033\n",
      "Epoch 239: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7062e-05 - mae: 0.0033 - val_loss: 3.4083e-05 - val_mae: 0.0047\n",
      "Epoch 240/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9284e-05 - mae: 0.0035\n",
      "Epoch 240: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9870e-05 - mae: 0.0036 - val_loss: 3.3393e-05 - val_mae: 0.0045\n",
      "Epoch 241/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8132e-05 - mae: 0.0034\n",
      "Epoch 241: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8548e-05 - mae: 0.0034 - val_loss: 3.9727e-05 - val_mae: 0.0049\n",
      "Epoch 242/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.1866e-05 - mae: 0.0037\n",
      "Epoch 242: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 2.2108e-05 - mae: 0.0037 - val_loss: 4.2236e-05 - val_mae: 0.0054\n",
      "Epoch 243/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.0268e-05 - mae: 0.0036\n",
      "Epoch 243: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 2.0272e-05 - mae: 0.0036 - val_loss: 3.5194e-05 - val_mae: 0.0046\n",
      "Epoch 244/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1133e-05 - mae: 0.0037\n",
      "Epoch 244: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.1906e-05 - mae: 0.0037 - val_loss: 3.3424e-05 - val_mae: 0.0045\n",
      "Epoch 245/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0723e-05 - mae: 0.0037\n",
      "Epoch 245: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.0988e-05 - mae: 0.0037 - val_loss: 5.4200e-05 - val_mae: 0.0059\n",
      "Epoch 246/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.3038e-05 - mae: 0.0039\n",
      "Epoch 246: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 2.2406e-05 - mae: 0.0038 - val_loss: 3.4654e-05 - val_mae: 0.0046\n",
      "Epoch 247/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9853e-05 - mae: 0.0036\n",
      "Epoch 247: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 2.0706e-05 - mae: 0.0037 - val_loss: 3.6092e-05 - val_mae: 0.0047\n",
      "Epoch 248/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8744e-05 - mae: 0.0034\n",
      "Epoch 248: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8442e-05 - mae: 0.0034 - val_loss: 3.3187e-05 - val_mae: 0.0045\n",
      "Epoch 249/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7305e-05 - mae: 0.0033\n",
      "Epoch 249: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.7033e-05 - mae: 0.0033 - val_loss: 3.2838e-05 - val_mae: 0.0045\n",
      "Epoch 250/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8435e-05 - mae: 0.0035\n",
      "Epoch 250: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8377e-05 - mae: 0.0035 - val_loss: 3.3047e-05 - val_mae: 0.0045\n",
      "Epoch 251/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7729e-05 - mae: 0.0034\n",
      "Epoch 251: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8403e-05 - mae: 0.0035 - val_loss: 3.6240e-05 - val_mae: 0.0047\n",
      "Epoch 252/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.1248e-05 - mae: 0.0037\n",
      "Epoch 252: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.1167e-05 - mae: 0.0037 - val_loss: 3.3894e-05 - val_mae: 0.0047\n",
      "Epoch 253/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7501e-05 - mae: 0.0034\n",
      "Epoch 253: val_loss improved from 0.00003 to 0.00003, saving model to /home/shreyas/XAIRT/examples/model.h5\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 1.8133e-05 - mae: 0.0034 - val_loss: 3.2613e-05 - val_mae: 0.0045\n",
      "Epoch 254/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8853e-05 - mae: 0.0034\n",
      "Epoch 254: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8393e-05 - mae: 0.0034 - val_loss: 3.6363e-05 - val_mae: 0.0047\n",
      "Epoch 255/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7153e-05 - mae: 0.0033\n",
      "Epoch 255: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7183e-05 - mae: 0.0033 - val_loss: 3.2890e-05 - val_mae: 0.0045\n",
      "Epoch 256/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8306e-05 - mae: 0.0034\n",
      "Epoch 256: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8359e-05 - mae: 0.0034 - val_loss: 3.3118e-05 - val_mae: 0.0045\n",
      "Epoch 257/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7595e-05 - mae: 0.0033\n",
      "Epoch 257: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.7926e-05 - mae: 0.0033 - val_loss: 3.3534e-05 - val_mae: 0.0046\n",
      "Epoch 258/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9616e-05 - mae: 0.0035\n",
      "Epoch 258: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.9463e-05 - mae: 0.0035 - val_loss: 3.2968e-05 - val_mae: 0.0045\n",
      "Epoch 259/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.0551e-05 - mae: 0.0036\n",
      "Epoch 259: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 2.0890e-05 - mae: 0.0036 - val_loss: 3.4334e-05 - val_mae: 0.0047\n",
      "Epoch 260/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7971e-05 - mae: 0.0034\n",
      "Epoch 260: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.9257e-05 - mae: 0.0035 - val_loss: 3.3339e-05 - val_mae: 0.0046\n",
      "Epoch 261/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9885e-05 - mae: 0.0036\n",
      "Epoch 261: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9505e-05 - mae: 0.0036 - val_loss: 3.5739e-05 - val_mae: 0.0046\n",
      "Epoch 262/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7275e-05 - mae: 0.0033\n",
      "Epoch 262: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.7654e-05 - mae: 0.0033 - val_loss: 3.5913e-05 - val_mae: 0.0046\n",
      "Epoch 263/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9326e-05 - mae: 0.0035\n",
      "Epoch 263: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.9546e-05 - mae: 0.0035 - val_loss: 3.3559e-05 - val_mae: 0.0046\n",
      "Epoch 264/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7881e-05 - mae: 0.0034\n",
      "Epoch 264: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8131e-05 - mae: 0.0034 - val_loss: 3.3128e-05 - val_mae: 0.0045\n",
      "Epoch 265/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8921e-05 - mae: 0.0035\n",
      "Epoch 265: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9269e-05 - mae: 0.0035 - val_loss: 3.3176e-05 - val_mae: 0.0046\n",
      "Epoch 266/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9305e-05 - mae: 0.0035\n",
      "Epoch 266: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.0039e-05 - mae: 0.0036 - val_loss: 3.4289e-05 - val_mae: 0.0046\n",
      "Epoch 267/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0912e-05 - mae: 0.0036\n",
      "Epoch 267: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 2.2580e-05 - mae: 0.0038 - val_loss: 3.4347e-05 - val_mae: 0.0047\n",
      "Epoch 268/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.1584e-05 - mae: 0.0037\n",
      "Epoch 268: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 2.1710e-05 - mae: 0.0038 - val_loss: 3.3934e-05 - val_mae: 0.0046\n",
      "Epoch 269/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8796e-05 - mae: 0.0034\n",
      "Epoch 269: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.8499e-05 - mae: 0.0034 - val_loss: 3.4471e-05 - val_mae: 0.0047\n",
      "Epoch 270/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0944e-05 - mae: 0.0036\n",
      "Epoch 270: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 2.0779e-05 - mae: 0.0036 - val_loss: 3.3293e-05 - val_mae: 0.0045\n",
      "Epoch 271/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6553e-05 - mae: 0.0033\n",
      "Epoch 271: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.6700e-05 - mae: 0.0033 - val_loss: 3.6243e-05 - val_mae: 0.0047\n",
      "Epoch 272/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8284e-05 - mae: 0.0034\n",
      "Epoch 272: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7628e-05 - mae: 0.0034 - val_loss: 3.4350e-05 - val_mae: 0.0047\n",
      "Epoch 273/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7116e-05 - mae: 0.0033\n",
      "Epoch 273: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.7269e-05 - mae: 0.0033 - val_loss: 3.4467e-05 - val_mae: 0.0047\n",
      "Epoch 274/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9841e-05 - mae: 0.0035\n",
      "Epoch 274: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9675e-05 - mae: 0.0035 - val_loss: 3.3021e-05 - val_mae: 0.0045\n",
      "Epoch 275/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8653e-05 - mae: 0.0034\n",
      "Epoch 275: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8787e-05 - mae: 0.0034 - val_loss: 3.3519e-05 - val_mae: 0.0046\n",
      "Epoch 276/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6663e-05 - mae: 0.0033\n",
      "Epoch 276: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8109e-05 - mae: 0.0034 - val_loss: 3.3341e-05 - val_mae: 0.0045\n",
      "Epoch 277/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9287e-05 - mae: 0.0035\n",
      "Epoch 277: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9287e-05 - mae: 0.0035 - val_loss: 3.4978e-05 - val_mae: 0.0046\n",
      "Epoch 278/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7682e-05 - mae: 0.0033\n",
      "Epoch 278: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7682e-05 - mae: 0.0033 - val_loss: 3.4706e-05 - val_mae: 0.0046\n",
      "Epoch 279/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7360e-05 - mae: 0.0033\n",
      "Epoch 279: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7976e-05 - mae: 0.0034 - val_loss: 4.1839e-05 - val_mae: 0.0053\n",
      "Epoch 280/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6261e-05 - mae: 0.0032\n",
      "Epoch 280: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0196e-05 - mae: 0.0036 - val_loss: 3.3618e-05 - val_mae: 0.0046\n",
      "Epoch 281/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7262e-05 - mae: 0.0033\n",
      "Epoch 281: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8547e-05 - mae: 0.0034 - val_loss: 3.6003e-05 - val_mae: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9171e-05 - mae: 0.0035\n",
      "Epoch 282: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9606e-05 - mae: 0.0035 - val_loss: 4.1529e-05 - val_mae: 0.0053\n",
      "Epoch 283/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9153e-05 - mae: 0.0035\n",
      "Epoch 283: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8993e-05 - mae: 0.0035 - val_loss: 3.5497e-05 - val_mae: 0.0048\n",
      "Epoch 284/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8492e-05 - mae: 0.0034\n",
      "Epoch 284: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8324e-05 - mae: 0.0034 - val_loss: 3.5862e-05 - val_mae: 0.0048\n",
      "Epoch 285/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.8316e-05 - mae: 0.0034\n",
      "Epoch 285: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.7630e-05 - mae: 0.0033 - val_loss: 3.3805e-05 - val_mae: 0.0045\n",
      "Epoch 286/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.0454e-05 - mae: 0.0036\n",
      "Epoch 286: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.2352e-05 - mae: 0.0038 - val_loss: 3.7465e-05 - val_mae: 0.0047\n",
      "Epoch 287/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 2.0239e-05 - mae: 0.0036\n",
      "Epoch 287: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 187us/sample - loss: 2.0414e-05 - mae: 0.0036 - val_loss: 3.3816e-05 - val_mae: 0.0046\n",
      "Epoch 288/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7838e-05 - mae: 0.0034\n",
      "Epoch 288: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8490e-05 - mae: 0.0034 - val_loss: 4.0721e-05 - val_mae: 0.0052\n",
      "Epoch 289/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8763e-05 - mae: 0.0035\n",
      "Epoch 289: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8629e-05 - mae: 0.0035 - val_loss: 3.6306e-05 - val_mae: 0.0047\n",
      "Epoch 290/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7218e-05 - mae: 0.0033\n",
      "Epoch 290: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7356e-05 - mae: 0.0033 - val_loss: 4.0385e-05 - val_mae: 0.0052\n",
      "Epoch 291/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8825e-05 - mae: 0.0035\n",
      "Epoch 291: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8781e-05 - mae: 0.0035 - val_loss: 3.7310e-05 - val_mae: 0.0050\n",
      "Epoch 292/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7337e-05 - mae: 0.0033\n",
      "Epoch 292: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7337e-05 - mae: 0.0033 - val_loss: 3.3892e-05 - val_mae: 0.0046\n",
      "Epoch 293/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.7513e-05 - mae: 0.0033\n",
      "Epoch 293: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8696e-05 - mae: 0.0035 - val_loss: 3.6096e-05 - val_mae: 0.0047\n",
      "Epoch 294/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7918e-05 - mae: 0.0033\n",
      "Epoch 294: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8488e-05 - mae: 0.0034 - val_loss: 3.9864e-05 - val_mae: 0.0052\n",
      "Epoch 295/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7946e-05 - mae: 0.0034\n",
      "Epoch 295: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7941e-05 - mae: 0.0034 - val_loss: 4.6611e-05 - val_mae: 0.0056\n",
      "Epoch 296/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5880e-05 - mae: 0.0032\n",
      "Epoch 296: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9117e-05 - mae: 0.0035 - val_loss: 3.4489e-05 - val_mae: 0.0046\n",
      "Epoch 297/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8944e-05 - mae: 0.0035\n",
      "Epoch 297: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8908e-05 - mae: 0.0035 - val_loss: 3.3218e-05 - val_mae: 0.0045\n",
      "Epoch 298/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9305e-05 - mae: 0.0035\n",
      "Epoch 298: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9149e-05 - mae: 0.0035 - val_loss: 4.1018e-05 - val_mae: 0.0050\n",
      "Epoch 299/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1954e-05 - mae: 0.0037\n",
      "Epoch 299: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.1572e-05 - mae: 0.0037 - val_loss: 3.3795e-05 - val_mae: 0.0045\n",
      "Epoch 300/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8164e-05 - mae: 0.0034\n",
      "Epoch 300: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8164e-05 - mae: 0.0034 - val_loss: 3.4710e-05 - val_mae: 0.0047\n",
      "Epoch 301/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6308e-05 - mae: 0.0032\n",
      "Epoch 301: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8590e-05 - mae: 0.0035 - val_loss: 3.6310e-05 - val_mae: 0.0049\n",
      "Epoch 302/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9645e-05 - mae: 0.0036\n",
      "Epoch 302: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9558e-05 - mae: 0.0036 - val_loss: 3.3408e-05 - val_mae: 0.0046\n",
      "Epoch 303/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8161e-05 - mae: 0.0034\n",
      "Epoch 303: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8515e-05 - mae: 0.0034 - val_loss: 3.4333e-05 - val_mae: 0.0046\n",
      "Epoch 304/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.6219e-05 - mae: 0.0041\n",
      "Epoch 304: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.2775e-05 - mae: 0.0038 - val_loss: 3.4307e-05 - val_mae: 0.0047\n",
      "Epoch 305/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7498e-05 - mae: 0.0033\n",
      "Epoch 305: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8995e-05 - mae: 0.0035 - val_loss: 3.3174e-05 - val_mae: 0.0046\n",
      "Epoch 306/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.5558e-05 - mae: 0.0040\n",
      "Epoch 306: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.3589e-05 - mae: 0.0039 - val_loss: 3.3597e-05 - val_mae: 0.0046\n",
      "Epoch 307/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.3244e-05 - mae: 0.0038\n",
      "Epoch 307: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0601e-05 - mae: 0.0036 - val_loss: 3.5946e-05 - val_mae: 0.0049\n",
      "Epoch 308/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7308e-05 - mae: 0.0033\n",
      "Epoch 308: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7263e-05 - mae: 0.0033 - val_loss: 3.3209e-05 - val_mae: 0.0046\n",
      "Epoch 309/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8916e-05 - mae: 0.0035\n",
      "Epoch 309: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8677e-05 - mae: 0.0035 - val_loss: 3.5391e-05 - val_mae: 0.0048\n",
      "Epoch 310/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5797e-05 - mae: 0.0032\n",
      "Epoch 310: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7152e-05 - mae: 0.0033 - val_loss: 3.3536e-05 - val_mae: 0.0046\n",
      "Epoch 311/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6797e-05 - mae: 0.0033\n",
      "Epoch 311: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7547e-05 - mae: 0.0033 - val_loss: 3.8750e-05 - val_mae: 0.0051\n",
      "Epoch 312/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8256e-05 - mae: 0.0034\n",
      "Epoch 312: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7419e-05 - mae: 0.0033 - val_loss: 3.3823e-05 - val_mae: 0.0046\n",
      "Epoch 313/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1253e-05 - mae: 0.0037\n",
      "Epoch 313: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0284e-05 - mae: 0.0036 - val_loss: 4.2941e-05 - val_mae: 0.0051\n",
      "Epoch 314/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.2251e-05 - mae: 0.0038\n",
      "Epoch 314: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.0163e-05 - mae: 0.0036 - val_loss: 3.3851e-05 - val_mae: 0.0046\n",
      "Epoch 315/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6510e-05 - mae: 0.0032\n",
      "Epoch 315: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7211e-05 - mae: 0.0033 - val_loss: 3.4501e-05 - val_mae: 0.0047\n",
      "Epoch 316/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9242e-05 - mae: 0.0035\n",
      "Epoch 316: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8786e-05 - mae: 0.0034 - val_loss: 3.3800e-05 - val_mae: 0.0046\n",
      "Epoch 317/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.6657e-05 - mae: 0.0041\n",
      "Epoch 317: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.1140e-05 - mae: 0.0036 - val_loss: 3.3502e-05 - val_mae: 0.0045\n",
      "Epoch 318/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9054e-05 - mae: 0.0035\n",
      "Epoch 318: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9001e-05 - mae: 0.0035 - val_loss: 3.3437e-05 - val_mae: 0.0046\n",
      "Epoch 319/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0847e-05 - mae: 0.0036\n",
      "Epoch 319: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0252e-05 - mae: 0.0036 - val_loss: 3.3657e-05 - val_mae: 0.0046\n",
      "Epoch 320/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7380e-05 - mae: 0.0033\n",
      "Epoch 320: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.7477e-05 - mae: 0.0033 - val_loss: 3.3506e-05 - val_mae: 0.0046\n",
      "Epoch 321/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.9615e-05 - mae: 0.0035\n",
      "Epoch 321: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9593e-05 - mae: 0.0035 - val_loss: 3.3371e-05 - val_mae: 0.0045\n",
      "Epoch 322/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7801e-05 - mae: 0.0033\n",
      "Epoch 322: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9021e-05 - mae: 0.0034 - val_loss: 3.3452e-05 - val_mae: 0.0046\n",
      "Epoch 323/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0440e-05 - mae: 0.0036\n",
      "Epoch 323: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9600e-05 - mae: 0.0035 - val_loss: 3.8676e-05 - val_mae: 0.0051\n",
      "Epoch 324/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9827e-05 - mae: 0.0036\n",
      "Epoch 324: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9450e-05 - mae: 0.0035 - val_loss: 3.3278e-05 - val_mae: 0.0045\n",
      "Epoch 325/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7481e-05 - mae: 0.0033\n",
      "Epoch 325: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8212e-05 - mae: 0.0034 - val_loss: 3.3343e-05 - val_mae: 0.0045\n",
      "Epoch 326/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5874e-05 - mae: 0.0032\n",
      "Epoch 326: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7810e-05 - mae: 0.0034 - val_loss: 3.5722e-05 - val_mae: 0.0048\n",
      "Epoch 327/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5122e-05 - mae: 0.0031\n",
      "Epoch 327: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.6626e-05 - mae: 0.0033 - val_loss: 3.8178e-05 - val_mae: 0.0050\n",
      "Epoch 328/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0115e-05 - mae: 0.0036\n",
      "Epoch 328: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 2.0074e-05 - mae: 0.0036 - val_loss: 3.5414e-05 - val_mae: 0.0048\n",
      "Epoch 329/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9201e-05 - mae: 0.0035\n",
      "Epoch 329: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9669e-05 - mae: 0.0036 - val_loss: 3.7903e-05 - val_mae: 0.0050\n",
      "Epoch 330/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7453e-05 - mae: 0.0033\n",
      "Epoch 330: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9363e-05 - mae: 0.0035 - val_loss: 3.9509e-05 - val_mae: 0.0051\n",
      "Epoch 331/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9595e-05 - mae: 0.0036\n",
      "Epoch 331: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.1538e-05 - mae: 0.0037 - val_loss: 4.3807e-05 - val_mae: 0.0054\n",
      "Epoch 332/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7148e-05 - mae: 0.0033\n",
      "Epoch 332: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9791e-05 - mae: 0.0035 - val_loss: 3.6408e-05 - val_mae: 0.0047\n",
      "Epoch 333/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.3216e-05 - mae: 0.0038\n",
      "Epoch 333: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.1020e-05 - mae: 0.0037 - val_loss: 3.4780e-05 - val_mae: 0.0046\n",
      "Epoch 334/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8617e-05 - mae: 0.0034\n",
      "Epoch 334: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8484e-05 - mae: 0.0034 - val_loss: 4.0775e-05 - val_mae: 0.0052\n",
      "Epoch 335/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8852e-05 - mae: 0.0035\n",
      "Epoch 335: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8453e-05 - mae: 0.0034 - val_loss: 3.3344e-05 - val_mae: 0.0046\n",
      "Epoch 336/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7063e-05 - mae: 0.0033\n",
      "Epoch 336: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8127e-05 - mae: 0.0034 - val_loss: 3.3904e-05 - val_mae: 0.0046\n",
      "Epoch 337/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6139e-05 - mae: 0.0032\n",
      "Epoch 337: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8040e-05 - mae: 0.0034 - val_loss: 3.9421e-05 - val_mae: 0.0051\n",
      "Epoch 338/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9029e-05 - mae: 0.0035\n",
      "Epoch 338: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.9129e-05 - mae: 0.0035 - val_loss: 5.9653e-05 - val_mae: 0.0064\n",
      "Epoch 339/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.6453e-05 - mae: 0.0041\n",
      "Epoch 339: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.3249e-05 - mae: 0.0039 - val_loss: 4.2588e-05 - val_mae: 0.0054\n",
      "Epoch 340/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8452e-05 - mae: 0.0034\n",
      "Epoch 340: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8632e-05 - mae: 0.0034 - val_loss: 4.3961e-05 - val_mae: 0.0054\n",
      "Epoch 341/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9955e-05 - mae: 0.0036\n",
      "Epoch 341: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8905e-05 - mae: 0.0035 - val_loss: 3.7096e-05 - val_mae: 0.0049\n",
      "Epoch 342/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5843e-05 - mae: 0.0031\n",
      "Epoch 342: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7650e-05 - mae: 0.0033 - val_loss: 3.5263e-05 - val_mae: 0.0046\n",
      "Epoch 343/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6851e-05 - mae: 0.0033\n",
      "Epoch 343: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.6722e-05 - mae: 0.0033 - val_loss: 3.3325e-05 - val_mae: 0.0046\n",
      "Epoch 344/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8720e-05 - mae: 0.0035\n",
      "Epoch 344: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7723e-05 - mae: 0.0034 - val_loss: 3.4499e-05 - val_mae: 0.0047\n",
      "Epoch 345/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9603e-05 - mae: 0.0036\n",
      "Epoch 345: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9253e-05 - mae: 0.0035 - val_loss: 3.7351e-05 - val_mae: 0.0050\n",
      "Epoch 346/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8037e-05 - mae: 0.0034\n",
      "Epoch 346: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8230e-05 - mae: 0.0034 - val_loss: 3.8335e-05 - val_mae: 0.0048\n",
      "Epoch 347/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0006e-05 - mae: 0.0036\n",
      "Epoch 347: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9241e-05 - mae: 0.0035 - val_loss: 3.5334e-05 - val_mae: 0.0046\n",
      "Epoch 348/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8812e-05 - mae: 0.0035\n",
      "Epoch 348: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8797e-05 - mae: 0.0035 - val_loss: 4.2844e-05 - val_mae: 0.0054\n",
      "Epoch 349/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9597e-05 - mae: 0.0035\n",
      "Epoch 349: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8423e-05 - mae: 0.0034 - val_loss: 3.5727e-05 - val_mae: 0.0046\n",
      "Epoch 350/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8355e-05 - mae: 0.0034\n",
      "Epoch 350: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8485e-05 - mae: 0.0034 - val_loss: 5.3695e-05 - val_mae: 0.0058\n",
      "Epoch 351/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9554e-05 - mae: 0.0035\n",
      "Epoch 351: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9444e-05 - mae: 0.0035 - val_loss: 5.9262e-05 - val_mae: 0.0064\n",
      "Epoch 352/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9385e-05 - mae: 0.0035\n",
      "Epoch 352: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9444e-05 - mae: 0.0035 - val_loss: 3.5290e-05 - val_mae: 0.0048\n",
      "Epoch 353/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9260e-05 - mae: 0.0035\n",
      "Epoch 353: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9169e-05 - mae: 0.0035 - val_loss: 3.4803e-05 - val_mae: 0.0046\n",
      "Epoch 354/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6011e-05 - mae: 0.0032\n",
      "Epoch 354: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7272e-05 - mae: 0.0033 - val_loss: 3.7202e-05 - val_mae: 0.0049\n",
      "Epoch 355/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7255e-05 - mae: 0.0034\n",
      "Epoch 355: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7962e-05 - mae: 0.0034 - val_loss: 3.3418e-05 - val_mae: 0.0045\n",
      "Epoch 356/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7176e-05 - mae: 0.0033\n",
      "Epoch 356: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7854e-05 - mae: 0.0034 - val_loss: 3.7061e-05 - val_mae: 0.0049\n",
      "Epoch 357/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8105e-05 - mae: 0.0034\n",
      "Epoch 357: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7282e-05 - mae: 0.0033 - val_loss: 3.3363e-05 - val_mae: 0.0045\n",
      "Epoch 358/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6796e-05 - mae: 0.0033\n",
      "Epoch 358: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8607e-05 - mae: 0.0035 - val_loss: 5.3054e-05 - val_mae: 0.0060\n",
      "Epoch 359/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.1214e-05 - mae: 0.0037\n",
      "Epoch 359: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 2.1241e-05 - mae: 0.0037 - val_loss: 3.3656e-05 - val_mae: 0.0045\n",
      "Epoch 360/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6050e-05 - mae: 0.0032\n",
      "Epoch 360: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8603e-05 - mae: 0.0034 - val_loss: 3.4060e-05 - val_mae: 0.0046\n",
      "Epoch 361/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6375e-05 - mae: 0.0033\n",
      "Epoch 361: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9118e-05 - mae: 0.0035 - val_loss: 3.5618e-05 - val_mae: 0.0046\n",
      "Epoch 362/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6902e-05 - mae: 0.0033\n",
      "Epoch 362: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8717e-05 - mae: 0.0034 - val_loss: 3.3799e-05 - val_mae: 0.0046\n",
      "Epoch 363/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7073e-05 - mae: 0.0033\n",
      "Epoch 363: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7634e-05 - mae: 0.0033 - val_loss: 3.8109e-05 - val_mae: 0.0050\n",
      "Epoch 364/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.1464e-05 - mae: 0.0037\n",
      "Epoch 364: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.1216e-05 - mae: 0.0036 - val_loss: 3.7213e-05 - val_mae: 0.0049\n",
      "Epoch 365/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8642e-05 - mae: 0.0034\n",
      "Epoch 365: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9002e-05 - mae: 0.0035 - val_loss: 3.3736e-05 - val_mae: 0.0045\n",
      "Epoch 366/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8989e-05 - mae: 0.0035\n",
      "Epoch 366: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9103e-05 - mae: 0.0035 - val_loss: 3.4224e-05 - val_mae: 0.0045\n",
      "Epoch 367/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8145e-05 - mae: 0.0034\n",
      "Epoch 367: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7594e-05 - mae: 0.0033 - val_loss: 4.8048e-05 - val_mae: 0.0057\n",
      "Epoch 368/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1006e-05 - mae: 0.0038\n",
      "Epoch 368: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9388e-05 - mae: 0.0036 - val_loss: 3.6248e-05 - val_mae: 0.0047\n",
      "Epoch 369/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.8381e-05 - mae: 0.0034\n",
      "Epoch 369: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8200e-05 - mae: 0.0034 - val_loss: 3.4262e-05 - val_mae: 0.0047\n",
      "Epoch 370/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5440e-05 - mae: 0.0031\n",
      "Epoch 370: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.6734e-05 - mae: 0.0033 - val_loss: 3.3858e-05 - val_mae: 0.0045\n",
      "Epoch 371/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1039e-05 - mae: 0.0037\n",
      "Epoch 371: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0153e-05 - mae: 0.0036 - val_loss: 3.6834e-05 - val_mae: 0.0047\n",
      "Epoch 372/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7434e-05 - mae: 0.0033\n",
      "Epoch 372: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7781e-05 - mae: 0.0034 - val_loss: 3.6295e-05 - val_mae: 0.0049\n",
      "Epoch 373/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8097e-05 - mae: 0.0035\n",
      "Epoch 373: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8201e-05 - mae: 0.0034 - val_loss: 4.1747e-05 - val_mae: 0.0053\n",
      "Epoch 374/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9263e-05 - mae: 0.0035\n",
      "Epoch 374: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9263e-05 - mae: 0.0035 - val_loss: 3.5386e-05 - val_mae: 0.0046\n",
      "Epoch 375/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8192e-05 - mae: 0.0035\n",
      "Epoch 375: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0186e-05 - mae: 0.0036 - val_loss: 4.4273e-05 - val_mae: 0.0052\n",
      "Epoch 376/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 2.1492e-05 - mae: 0.0037\n",
      "Epoch 376: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.1501e-05 - mae: 0.0037 - val_loss: 3.3670e-05 - val_mae: 0.0045\n",
      "Epoch 377/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8680e-05 - mae: 0.0035\n",
      "Epoch 377: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8624e-05 - mae: 0.0035 - val_loss: 3.4060e-05 - val_mae: 0.0046\n",
      "Epoch 378/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5385e-05 - mae: 0.0031\n",
      "Epoch 378: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7428e-05 - mae: 0.0033 - val_loss: 3.3805e-05 - val_mae: 0.0046\n",
      "Epoch 379/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7978e-05 - mae: 0.0034\n",
      "Epoch 379: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8023e-05 - mae: 0.0034 - val_loss: 3.8120e-05 - val_mae: 0.0050\n",
      "Epoch 380/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.1316e-05 - mae: 0.0037\n",
      "Epoch 380: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.1465e-05 - mae: 0.0037 - val_loss: 3.4001e-05 - val_mae: 0.0046\n",
      "Epoch 381/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 2.0453e-05 - mae: 0.0036\n",
      "Epoch 381: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 2.0756e-05 - mae: 0.0037 - val_loss: 3.8246e-05 - val_mae: 0.0050\n",
      "Epoch 382/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8615e-05 - mae: 0.0035\n",
      "Epoch 382: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8671e-05 - mae: 0.0035 - val_loss: 3.3143e-05 - val_mae: 0.0045\n",
      "Epoch 383/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8162e-05 - mae: 0.0034\n",
      "Epoch 383: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.8183e-05 - mae: 0.0034 - val_loss: 3.4573e-05 - val_mae: 0.0047\n",
      "Epoch 384/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 2.0183e-05 - mae: 0.0036\n",
      "Epoch 384: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.9271e-05 - mae: 0.0035 - val_loss: 3.3686e-05 - val_mae: 0.0045\n",
      "Epoch 385/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.6944e-05 - mae: 0.0033\n",
      "Epoch 385: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 212us/sample - loss: 1.7332e-05 - mae: 0.0033 - val_loss: 3.3329e-05 - val_mae: 0.0046\n",
      "Epoch 386/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7483e-05 - mae: 0.0033\n",
      "Epoch 386: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.8062e-05 - mae: 0.0034 - val_loss: 3.3594e-05 - val_mae: 0.0045\n",
      "Epoch 387/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7232e-05 - mae: 0.0033\n",
      "Epoch 387: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.7772e-05 - mae: 0.0033 - val_loss: 3.4428e-05 - val_mae: 0.0046\n",
      "Epoch 388/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9321e-05 - mae: 0.0035\n",
      "Epoch 388: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 224us/sample - loss: 1.9106e-05 - mae: 0.0035 - val_loss: 3.2856e-05 - val_mae: 0.0045\n",
      "Epoch 389/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.9946e-05 - mae: 0.0036\n",
      "Epoch 389: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 198us/sample - loss: 2.0411e-05 - mae: 0.0036 - val_loss: 4.0400e-05 - val_mae: 0.0052\n",
      "Epoch 390/1000\n",
      "620/800 [======================>.......] - ETA: 0s - loss: 1.8566e-05 - mae: 0.0034\n",
      "Epoch 390: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 1.8036e-05 - mae: 0.0034 - val_loss: 3.3427e-05 - val_mae: 0.0046\n",
      "Epoch 391/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7691e-05 - mae: 0.0034\n",
      "Epoch 391: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7450e-05 - mae: 0.0033 - val_loss: 3.3897e-05 - val_mae: 0.0046\n",
      "Epoch 392/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8823e-05 - mae: 0.0035\n",
      "Epoch 392: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8781e-05 - mae: 0.0035 - val_loss: 3.3558e-05 - val_mae: 0.0046\n",
      "Epoch 393/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7980e-05 - mae: 0.0034\n",
      "Epoch 393: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.7427e-05 - mae: 0.0033 - val_loss: 3.7508e-05 - val_mae: 0.0047\n",
      "Epoch 394/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7049e-05 - mae: 0.0033\n",
      "Epoch 394: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.7486e-05 - mae: 0.0034 - val_loss: 3.4578e-05 - val_mae: 0.0046\n",
      "Epoch 395/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8705e-05 - mae: 0.0034\n",
      "Epoch 395: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8475e-05 - mae: 0.0034 - val_loss: 3.5160e-05 - val_mae: 0.0046\n",
      "Epoch 396/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 2.0145e-05 - mae: 0.0035\n",
      "Epoch 396: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.9339e-05 - mae: 0.0035 - val_loss: 3.3517e-05 - val_mae: 0.0046\n",
      "Epoch 397/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.8178e-05 - mae: 0.0034\n",
      "Epoch 397: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.8211e-05 - mae: 0.0034 - val_loss: 3.3379e-05 - val_mae: 0.0046\n",
      "Epoch 398/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 0s - loss: 2.0199e-05 - mae: 0.0036\n",
      "Epoch 398: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0199e-05 - mae: 0.0036 - val_loss: 3.4401e-05 - val_mae: 0.0046\n",
      "Epoch 399/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0379e-05 - mae: 0.0036\n",
      "Epoch 399: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.9849e-05 - mae: 0.0035 - val_loss: 3.4790e-05 - val_mae: 0.0047\n",
      "Epoch 400/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8700e-05 - mae: 0.0035\n",
      "Epoch 400: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8795e-05 - mae: 0.0035 - val_loss: 4.1107e-05 - val_mae: 0.0053\n",
      "Epoch 401/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.6895e-05 - mae: 0.0033\n",
      "Epoch 401: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.7253e-05 - mae: 0.0033 - val_loss: 3.7503e-05 - val_mae: 0.0048\n",
      "Epoch 402/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9510e-05 - mae: 0.0036\n",
      "Epoch 402: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9140e-05 - mae: 0.0035 - val_loss: 3.4465e-05 - val_mae: 0.0046\n",
      "Epoch 403/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5237e-05 - mae: 0.0031\n",
      "Epoch 403: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8758e-05 - mae: 0.0034 - val_loss: 3.3996e-05 - val_mae: 0.0045\n",
      "Epoch 404/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 2.0714e-05 - mae: 0.0037\n",
      "Epoch 404: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.0868e-05 - mae: 0.0037 - val_loss: 3.4708e-05 - val_mae: 0.0047\n",
      "Epoch 405/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8639e-05 - mae: 0.0035\n",
      "Epoch 405: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9090e-05 - mae: 0.0035 - val_loss: 3.8983e-05 - val_mae: 0.0049\n",
      "Epoch 406/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9875e-05 - mae: 0.0036\n",
      "Epoch 406: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.0444e-05 - mae: 0.0036 - val_loss: 3.5621e-05 - val_mae: 0.0046\n",
      "Epoch 407/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8742e-05 - mae: 0.0034\n",
      "Epoch 407: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7081e-05 - mae: 0.0033 - val_loss: 3.4671e-05 - val_mae: 0.0046\n",
      "Epoch 408/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9260e-05 - mae: 0.0035\n",
      "Epoch 408: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.9260e-05 - mae: 0.0035 - val_loss: 3.4699e-05 - val_mae: 0.0047\n",
      "Epoch 409/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8733e-05 - mae: 0.0034\n",
      "Epoch 409: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.8417e-05 - mae: 0.0034 - val_loss: 3.3584e-05 - val_mae: 0.0046\n",
      "Epoch 410/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.8667e-05 - mae: 0.0035\n",
      "Epoch 410: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 1.8250e-05 - mae: 0.0034 - val_loss: 3.4589e-05 - val_mae: 0.0047\n",
      "Epoch 411/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6336e-05 - mae: 0.0032\n",
      "Epoch 411: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.6767e-05 - mae: 0.0032 - val_loss: 3.5232e-05 - val_mae: 0.0046\n",
      "Epoch 412/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7830e-05 - mae: 0.0034\n",
      "Epoch 412: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.7793e-05 - mae: 0.0034 - val_loss: 3.4363e-05 - val_mae: 0.0047\n",
      "Epoch 413/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8455e-05 - mae: 0.0034\n",
      "Epoch 413: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8455e-05 - mae: 0.0034 - val_loss: 3.3669e-05 - val_mae: 0.0046\n",
      "Epoch 414/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7544e-05 - mae: 0.0033\n",
      "Epoch 414: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8093e-05 - mae: 0.0034 - val_loss: 3.7832e-05 - val_mae: 0.0050\n",
      "Epoch 415/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8265e-05 - mae: 0.0034\n",
      "Epoch 415: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8345e-05 - mae: 0.0034 - val_loss: 3.3727e-05 - val_mae: 0.0045\n",
      "Epoch 416/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.7007e-05 - mae: 0.0033\n",
      "Epoch 416: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 194us/sample - loss: 1.7306e-05 - mae: 0.0033 - val_loss: 3.3195e-05 - val_mae: 0.0045\n",
      "Epoch 417/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8965e-05 - mae: 0.0035\n",
      "Epoch 417: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.9249e-05 - mae: 0.0035 - val_loss: 3.5981e-05 - val_mae: 0.0047\n",
      "Epoch 418/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9486e-05 - mae: 0.0036\n",
      "Epoch 418: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0197e-05 - mae: 0.0036 - val_loss: 3.3649e-05 - val_mae: 0.0045\n",
      "Epoch 419/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7678e-05 - mae: 0.0033\n",
      "Epoch 419: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7869e-05 - mae: 0.0033 - val_loss: 5.5534e-05 - val_mae: 0.0062\n",
      "Epoch 420/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.3413e-05 - mae: 0.0039\n",
      "Epoch 420: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.2050e-05 - mae: 0.0038 - val_loss: 3.4845e-05 - val_mae: 0.0046\n",
      "Epoch 421/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5403e-05 - mae: 0.0031\n",
      "Epoch 421: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9255e-05 - mae: 0.0035 - val_loss: 3.5800e-05 - val_mae: 0.0048\n",
      "Epoch 422/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7727e-05 - mae: 0.0034\n",
      "Epoch 422: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8167e-05 - mae: 0.0034 - val_loss: 6.0589e-05 - val_mae: 0.0065\n",
      "Epoch 423/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7903e-05 - mae: 0.0034\n",
      "Epoch 423: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7800e-05 - mae: 0.0034 - val_loss: 3.3781e-05 - val_mae: 0.0045\n",
      "Epoch 424/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7048e-05 - mae: 0.0033\n",
      "Epoch 424: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7938e-05 - mae: 0.0034 - val_loss: 3.3393e-05 - val_mae: 0.0045\n",
      "Epoch 425/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5501e-05 - mae: 0.0031\n",
      "Epoch 425: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7063e-05 - mae: 0.0033 - val_loss: 3.3450e-05 - val_mae: 0.0045\n",
      "Epoch 426/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1143e-05 - mae: 0.0037\n",
      "Epoch 426: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0413e-05 - mae: 0.0036 - val_loss: 3.3420e-05 - val_mae: 0.0045\n",
      "Epoch 427/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.7989e-05 - mae: 0.0034\n",
      "Epoch 427: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8163e-05 - mae: 0.0034 - val_loss: 4.3650e-05 - val_mae: 0.0054\n",
      "Epoch 428/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.2749e-05 - mae: 0.0038\n",
      "Epoch 428: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 2.2124e-05 - mae: 0.0038 - val_loss: 3.3039e-05 - val_mae: 0.0045\n",
      "Epoch 429/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9572e-05 - mae: 0.0036\n",
      "Epoch 429: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8663e-05 - mae: 0.0035 - val_loss: 3.9544e-05 - val_mae: 0.0052\n",
      "Epoch 430/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7276e-05 - mae: 0.0033\n",
      "Epoch 430: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7168e-05 - mae: 0.0033 - val_loss: 3.3806e-05 - val_mae: 0.0045\n",
      "Epoch 431/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5862e-05 - mae: 0.0032\n",
      "Epoch 431: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7360e-05 - mae: 0.0033 - val_loss: 3.3243e-05 - val_mae: 0.0045\n",
      "Epoch 432/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8005e-05 - mae: 0.0034\n",
      "Epoch 432: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.7908e-05 - mae: 0.0034 - val_loss: 3.3329e-05 - val_mae: 0.0045\n",
      "Epoch 433/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7841e-05 - mae: 0.0034\n",
      "Epoch 433: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7647e-05 - mae: 0.0034 - val_loss: 3.6848e-05 - val_mae: 0.0049\n",
      "Epoch 434/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8582e-05 - mae: 0.0034\n",
      "Epoch 434: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8540e-05 - mae: 0.0034 - val_loss: 3.3161e-05 - val_mae: 0.0045\n",
      "Epoch 435/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6246e-05 - mae: 0.0032\n",
      "Epoch 435: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8411e-05 - mae: 0.0034 - val_loss: 3.6680e-05 - val_mae: 0.0047\n",
      "Epoch 436/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6529e-05 - mae: 0.0032\n",
      "Epoch 436: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8943e-05 - mae: 0.0034 - val_loss: 3.4997e-05 - val_mae: 0.0046\n",
      "Epoch 437/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7704e-05 - mae: 0.0034\n",
      "Epoch 437: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7587e-05 - mae: 0.0034 - val_loss: 3.3141e-05 - val_mae: 0.0046\n",
      "Epoch 438/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6955e-05 - mae: 0.0033\n",
      "Epoch 438: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7369e-05 - mae: 0.0033 - val_loss: 3.3607e-05 - val_mae: 0.0046\n",
      "Epoch 439/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7816e-05 - mae: 0.0034\n",
      "Epoch 439: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9199e-05 - mae: 0.0035 - val_loss: 3.8088e-05 - val_mae: 0.0050\n",
      "Epoch 440/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.5306e-05 - mae: 0.0041\n",
      "Epoch 440: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 2.3114e-05 - mae: 0.0039 - val_loss: 3.9944e-05 - val_mae: 0.0049\n",
      "Epoch 441/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8378e-05 - mae: 0.0034\n",
      "Epoch 441: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.7796e-05 - mae: 0.0033 - val_loss: 3.5424e-05 - val_mae: 0.0046\n",
      "Epoch 442/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.9770e-05 - mae: 0.0036\n",
      "Epoch 442: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.9435e-05 - mae: 0.0035 - val_loss: 3.8554e-05 - val_mae: 0.0048\n",
      "Epoch 443/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.1464e-05 - mae: 0.0038\n",
      "Epoch 443: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 2.1786e-05 - mae: 0.0038 - val_loss: 3.4253e-05 - val_mae: 0.0046\n",
      "Epoch 444/1000\n",
      "560/800 [====================>.........] - ETA: 0s - loss: 1.6710e-05 - mae: 0.0032\n",
      "Epoch 444: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.7413e-05 - mae: 0.0033 - val_loss: 3.4299e-05 - val_mae: 0.0047\n",
      "Epoch 445/1000\n",
      "590/800 [=====================>........] - ETA: 0s - loss: 1.7032e-05 - mae: 0.0033\n",
      "Epoch 445: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.7662e-05 - mae: 0.0034 - val_loss: 3.7033e-05 - val_mae: 0.0047\n",
      "Epoch 446/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.9467e-05 - mae: 0.0035\n",
      "Epoch 446: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.8477e-05 - mae: 0.0034 - val_loss: 3.6640e-05 - val_mae: 0.0049\n",
      "Epoch 447/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6316e-05 - mae: 0.0032\n",
      "Epoch 447: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7159e-05 - mae: 0.0033 - val_loss: 3.4565e-05 - val_mae: 0.0046\n",
      "Epoch 448/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.9872e-05 - mae: 0.0036\n",
      "Epoch 448: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.9585e-05 - mae: 0.0035 - val_loss: 3.4693e-05 - val_mae: 0.0047\n",
      "Epoch 449/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7191e-05 - mae: 0.0033\n",
      "Epoch 449: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7191e-05 - mae: 0.0033 - val_loss: 3.9614e-05 - val_mae: 0.0052\n",
      "Epoch 450/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8294e-05 - mae: 0.0034\n",
      "Epoch 450: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8070e-05 - mae: 0.0034 - val_loss: 3.4921e-05 - val_mae: 0.0048\n",
      "Epoch 451/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7417e-05 - mae: 0.0033\n",
      "Epoch 451: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7417e-05 - mae: 0.0033 - val_loss: 3.5301e-05 - val_mae: 0.0046\n",
      "Epoch 452/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5524e-05 - mae: 0.0032\n",
      "Epoch 452: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9103e-05 - mae: 0.0035 - val_loss: 3.4608e-05 - val_mae: 0.0047\n",
      "Epoch 453/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8511e-05 - mae: 0.0035\n",
      "Epoch 453: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1426e-05 - mae: 0.0036 - val_loss: 7.7820e-05 - val_mae: 0.0075\n",
      "Epoch 454/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.9464e-05 - mae: 0.0035\n",
      "Epoch 454: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 2.0653e-05 - mae: 0.0036 - val_loss: 4.1593e-05 - val_mae: 0.0050\n",
      "Epoch 455/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0905e-05 - mae: 0.0036\n",
      "Epoch 455: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.0700e-05 - mae: 0.0036 - val_loss: 3.8765e-05 - val_mae: 0.0051\n",
      "Epoch 456/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9343e-05 - mae: 0.0035\n",
      "Epoch 456: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8068e-05 - mae: 0.0034 - val_loss: 3.9363e-05 - val_mae: 0.0052\n",
      "Epoch 457/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6070e-05 - mae: 0.0032\n",
      "Epoch 457: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7130e-05 - mae: 0.0033 - val_loss: 3.4343e-05 - val_mae: 0.0045\n",
      "Epoch 458/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6399e-05 - mae: 0.0032\n",
      "Epoch 458: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9865e-05 - mae: 0.0035 - val_loss: 3.7112e-05 - val_mae: 0.0050\n",
      "Epoch 459/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8510e-05 - mae: 0.0034\n",
      "Epoch 459: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8604e-05 - mae: 0.0034 - val_loss: 4.0434e-05 - val_mae: 0.0049\n",
      "Epoch 460/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.8589e-05 - mae: 0.0034\n",
      "Epoch 460: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.9094e-05 - mae: 0.0035 - val_loss: 4.3302e-05 - val_mae: 0.0051\n",
      "Epoch 461/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9930e-05 - mae: 0.0036\n",
      "Epoch 461: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9755e-05 - mae: 0.0036 - val_loss: 3.4395e-05 - val_mae: 0.0047\n",
      "Epoch 462/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6489e-05 - mae: 0.0032\n",
      "Epoch 462: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8640e-05 - mae: 0.0034 - val_loss: 3.3567e-05 - val_mae: 0.0046\n",
      "Epoch 463/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7594e-05 - mae: 0.0034\n",
      "Epoch 463: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8138e-05 - mae: 0.0034 - val_loss: 3.3973e-05 - val_mae: 0.0046\n",
      "Epoch 464/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9171e-05 - mae: 0.0035\n",
      "Epoch 464: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8213e-05 - mae: 0.0034 - val_loss: 3.3374e-05 - val_mae: 0.0045\n",
      "Epoch 465/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7611e-05 - mae: 0.0033\n",
      "Epoch 465: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8951e-05 - mae: 0.0035 - val_loss: 3.5099e-05 - val_mae: 0.0048\n",
      "Epoch 466/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5275e-05 - mae: 0.0031\n",
      "Epoch 466: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7208e-05 - mae: 0.0033 - val_loss: 4.3968e-05 - val_mae: 0.0055\n",
      "Epoch 467/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6779e-05 - mae: 0.0033\n",
      "Epoch 467: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7632e-05 - mae: 0.0034 - val_loss: 3.5869e-05 - val_mae: 0.0048\n",
      "Epoch 468/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.6648e-05 - mae: 0.0033\n",
      "Epoch 468: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.6648e-05 - mae: 0.0033 - val_loss: 3.5139e-05 - val_mae: 0.0048\n",
      "Epoch 469/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0446e-05 - mae: 0.0036\n",
      "Epoch 469: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9875e-05 - mae: 0.0036 - val_loss: 3.8780e-05 - val_mae: 0.0051\n",
      "Epoch 470/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7636e-05 - mae: 0.0034\n",
      "Epoch 470: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7310e-05 - mae: 0.0033 - val_loss: 3.3693e-05 - val_mae: 0.0045\n",
      "Epoch 471/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8252e-05 - mae: 0.0034\n",
      "Epoch 471: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9834e-05 - mae: 0.0036 - val_loss: 3.3099e-05 - val_mae: 0.0045\n",
      "Epoch 472/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6216e-05 - mae: 0.0032\n",
      "Epoch 472: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7428e-05 - mae: 0.0033 - val_loss: 3.3138e-05 - val_mae: 0.0046\n",
      "Epoch 473/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8580e-05 - mae: 0.0034\n",
      "Epoch 473: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8095e-05 - mae: 0.0034 - val_loss: 3.4582e-05 - val_mae: 0.0046\n",
      "Epoch 474/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8235e-05 - mae: 0.0034\n",
      "Epoch 474: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.2236e-05 - mae: 0.0038 - val_loss: 3.8127e-05 - val_mae: 0.0050\n",
      "Epoch 475/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.3679e-05 - mae: 0.0039\n",
      "Epoch 475: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.4065e-05 - mae: 0.0039 - val_loss: 3.6650e-05 - val_mae: 0.0047\n",
      "Epoch 476/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7344e-05 - mae: 0.0033\n",
      "Epoch 476: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.8401e-05 - mae: 0.0034 - val_loss: 3.9104e-05 - val_mae: 0.0049\n",
      "Epoch 477/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8718e-05 - mae: 0.0034\n",
      "Epoch 477: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.6954e-05 - mae: 0.0033 - val_loss: 3.3328e-05 - val_mae: 0.0046\n",
      "Epoch 478/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9851e-05 - mae: 0.0036\n",
      "Epoch 478: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9317e-05 - mae: 0.0035 - val_loss: 3.8872e-05 - val_mae: 0.0051\n",
      "Epoch 479/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9634e-05 - mae: 0.0035\n",
      "Epoch 479: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8565e-05 - mae: 0.0034 - val_loss: 3.5184e-05 - val_mae: 0.0048\n",
      "Epoch 480/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 2.3157e-05 - mae: 0.0039\n",
      "Epoch 480: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 2.3096e-05 - mae: 0.0039 - val_loss: 3.3183e-05 - val_mae: 0.0046\n",
      "Epoch 481/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7816e-05 - mae: 0.0034\n",
      "Epoch 481: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 2.0233e-05 - mae: 0.0036 - val_loss: 3.3973e-05 - val_mae: 0.0045\n",
      "Epoch 482/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8171e-05 - mae: 0.0034\n",
      "Epoch 482: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8171e-05 - mae: 0.0034 - val_loss: 3.4015e-05 - val_mae: 0.0045\n",
      "Epoch 483/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8978e-05 - mae: 0.0034\n",
      "Epoch 483: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7747e-05 - mae: 0.0034 - val_loss: 3.3661e-05 - val_mae: 0.0045\n",
      "Epoch 484/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7391e-05 - mae: 0.0033\n",
      "Epoch 484: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8322e-05 - mae: 0.0034 - val_loss: 3.5968e-05 - val_mae: 0.0046\n",
      "Epoch 485/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8909e-05 - mae: 0.0035\n",
      "Epoch 485: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.9428e-05 - mae: 0.0036 - val_loss: 3.3961e-05 - val_mae: 0.0046\n",
      "Epoch 486/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8654e-05 - mae: 0.0035\n",
      "Epoch 486: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9092e-05 - mae: 0.0036 - val_loss: 3.2956e-05 - val_mae: 0.0045\n",
      "Epoch 487/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7424e-05 - mae: 0.0033\n",
      "Epoch 487: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8556e-05 - mae: 0.0034 - val_loss: 4.0743e-05 - val_mae: 0.0052\n",
      "Epoch 488/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7131e-05 - mae: 0.0032\n",
      "Epoch 488: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7428e-05 - mae: 0.0033 - val_loss: 4.0467e-05 - val_mae: 0.0049\n",
      "Epoch 489/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8034e-05 - mae: 0.0034\n",
      "Epoch 489: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8169e-05 - mae: 0.0034 - val_loss: 3.3481e-05 - val_mae: 0.0046\n",
      "Epoch 490/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6718e-05 - mae: 0.0033\n",
      "Epoch 490: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7903e-05 - mae: 0.0034 - val_loss: 3.6955e-05 - val_mae: 0.0049\n",
      "Epoch 491/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9864e-05 - mae: 0.0035\n",
      "Epoch 491: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9543e-05 - mae: 0.0035 - val_loss: 3.3461e-05 - val_mae: 0.0045\n",
      "Epoch 492/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8541e-05 - mae: 0.0034\n",
      "Epoch 492: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7934e-05 - mae: 0.0034 - val_loss: 3.6549e-05 - val_mae: 0.0049\n",
      "Epoch 493/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7349e-05 - mae: 0.0033\n",
      "Epoch 493: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7276e-05 - mae: 0.0033 - val_loss: 3.3993e-05 - val_mae: 0.0046\n",
      "Epoch 494/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7269e-05 - mae: 0.0033\n",
      "Epoch 494: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7460e-05 - mae: 0.0033 - val_loss: 3.3408e-05 - val_mae: 0.0045\n",
      "Epoch 495/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9623e-05 - mae: 0.0035\n",
      "Epoch 495: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0806e-05 - mae: 0.0037 - val_loss: 3.3361e-05 - val_mae: 0.0045\n",
      "Epoch 496/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9020e-05 - mae: 0.0035\n",
      "Epoch 496: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.8645e-05 - mae: 0.0034 - val_loss: 3.4381e-05 - val_mae: 0.0047\n",
      "Epoch 497/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6549e-05 - mae: 0.0033\n",
      "Epoch 497: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7633e-05 - mae: 0.0033 - val_loss: 3.3696e-05 - val_mae: 0.0046\n",
      "Epoch 498/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6970e-05 - mae: 0.0033\n",
      "Epoch 498: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7940e-05 - mae: 0.0034 - val_loss: 3.3914e-05 - val_mae: 0.0046\n",
      "Epoch 499/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8215e-05 - mae: 0.0034\n",
      "Epoch 499: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9778e-05 - mae: 0.0035 - val_loss: 3.3313e-05 - val_mae: 0.0045\n",
      "Epoch 500/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6395e-05 - mae: 0.0033\n",
      "Epoch 500: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8288e-05 - mae: 0.0034 - val_loss: 3.6267e-05 - val_mae: 0.0047\n",
      "Epoch 501/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8867e-05 - mae: 0.0034\n",
      "Epoch 501: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8867e-05 - mae: 0.0034 - val_loss: 3.7096e-05 - val_mae: 0.0049\n",
      "Epoch 502/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9758e-05 - mae: 0.0035\n",
      "Epoch 502: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8049e-05 - mae: 0.0034 - val_loss: 3.3285e-05 - val_mae: 0.0045\n",
      "Epoch 503/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5511e-05 - mae: 0.0031\n",
      "Epoch 503: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7444e-05 - mae: 0.0033 - val_loss: 3.5924e-05 - val_mae: 0.0046\n",
      "Epoch 504/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7293e-05 - mae: 0.0033\n",
      "Epoch 504: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7386e-05 - mae: 0.0033 - val_loss: 3.3946e-05 - val_mae: 0.0046\n",
      "Epoch 505/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7814e-05 - mae: 0.0033\n",
      "Epoch 505: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7710e-05 - mae: 0.0033 - val_loss: 3.7721e-05 - val_mae: 0.0050\n",
      "Epoch 506/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5210e-05 - mae: 0.0031\n",
      "Epoch 506: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9029e-05 - mae: 0.0034 - val_loss: 3.9297e-05 - val_mae: 0.0049\n",
      "Epoch 507/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7203e-05 - mae: 0.0033\n",
      "Epoch 507: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7043e-05 - mae: 0.0033 - val_loss: 3.6652e-05 - val_mae: 0.0049\n",
      "Epoch 508/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8577e-05 - mae: 0.0035\n",
      "Epoch 508: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8322e-05 - mae: 0.0035 - val_loss: 3.3202e-05 - val_mae: 0.0045\n",
      "Epoch 509/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6118e-05 - mae: 0.0032\n",
      "Epoch 509: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8395e-05 - mae: 0.0034 - val_loss: 3.4049e-05 - val_mae: 0.0045\n",
      "Epoch 510/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.8612e-05 - mae: 0.0034\n",
      "Epoch 510: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.8025e-05 - mae: 0.0034 - val_loss: 3.4723e-05 - val_mae: 0.0047\n",
      "Epoch 511/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.6887e-05 - mae: 0.0033\n",
      "Epoch 511: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.8458e-05 - mae: 0.0034 - val_loss: 3.6558e-05 - val_mae: 0.0049\n",
      "Epoch 512/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7923e-05 - mae: 0.0033\n",
      "Epoch 512: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7546e-05 - mae: 0.0033 - val_loss: 3.7141e-05 - val_mae: 0.0049\n",
      "Epoch 513/1000\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.7541e-05 - mae: 0.0034\n",
      "Epoch 513: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.7594e-05 - mae: 0.0034 - val_loss: 4.0191e-05 - val_mae: 0.0049\n",
      "Epoch 514/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570/800 [====================>.........] - ETA: 0s - loss: 1.9740e-05 - mae: 0.0035\n",
      "Epoch 514: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.8873e-05 - mae: 0.0034 - val_loss: 3.6824e-05 - val_mae: 0.0049\n",
      "Epoch 515/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7127e-05 - mae: 0.0033\n",
      "Epoch 515: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.8098e-05 - mae: 0.0034 - val_loss: 3.7188e-05 - val_mae: 0.0047\n",
      "Epoch 516/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7932e-05 - mae: 0.0033\n",
      "Epoch 516: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7130e-05 - mae: 0.0033 - val_loss: 3.4153e-05 - val_mae: 0.0046\n",
      "Epoch 517/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7437e-05 - mae: 0.0033\n",
      "Epoch 517: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.7901e-05 - mae: 0.0034 - val_loss: 4.1475e-05 - val_mae: 0.0053\n",
      "Epoch 518/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8793e-05 - mae: 0.0035\n",
      "Epoch 518: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.8903e-05 - mae: 0.0034 - val_loss: 4.6362e-05 - val_mae: 0.0056\n",
      "Epoch 519/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 2.1983e-05 - mae: 0.0038\n",
      "Epoch 519: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 2.1047e-05 - mae: 0.0037 - val_loss: 4.0839e-05 - val_mae: 0.0052\n",
      "Epoch 520/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6040e-05 - mae: 0.0032\n",
      "Epoch 520: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.7629e-05 - mae: 0.0033 - val_loss: 3.9000e-05 - val_mae: 0.0051\n",
      "Epoch 521/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.9727e-05 - mae: 0.0035\n",
      "Epoch 521: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.8913e-05 - mae: 0.0035 - val_loss: 3.3754e-05 - val_mae: 0.0046\n",
      "Epoch 522/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7849e-05 - mae: 0.0034\n",
      "Epoch 522: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 2.0230e-05 - mae: 0.0036 - val_loss: 3.8283e-05 - val_mae: 0.0048\n",
      "Epoch 523/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7908e-05 - mae: 0.0034\n",
      "Epoch 523: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.7870e-05 - mae: 0.0034 - val_loss: 3.5896e-05 - val_mae: 0.0046\n",
      "Epoch 524/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.6589e-05 - mae: 0.0032\n",
      "Epoch 524: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.7769e-05 - mae: 0.0034 - val_loss: 3.3683e-05 - val_mae: 0.0046\n",
      "Epoch 525/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8564e-05 - mae: 0.0034\n",
      "Epoch 525: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7802e-05 - mae: 0.0034 - val_loss: 3.4378e-05 - val_mae: 0.0047\n",
      "Epoch 526/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7952e-05 - mae: 0.0033\n",
      "Epoch 526: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7265e-05 - mae: 0.0033 - val_loss: 3.5782e-05 - val_mae: 0.0046\n",
      "Epoch 527/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5723e-05 - mae: 0.0032\n",
      "Epoch 527: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7805e-05 - mae: 0.0034 - val_loss: 3.3300e-05 - val_mae: 0.0045\n",
      "Epoch 528/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8435e-05 - mae: 0.0034\n",
      "Epoch 528: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 2.1356e-05 - mae: 0.0037 - val_loss: 3.4099e-05 - val_mae: 0.0046\n",
      "Epoch 529/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6108e-05 - mae: 0.0032\n",
      "Epoch 529: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.6919e-05 - mae: 0.0033 - val_loss: 3.3917e-05 - val_mae: 0.0046\n",
      "Epoch 530/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7120e-05 - mae: 0.0033\n",
      "Epoch 530: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.1104e-05 - mae: 0.0037 - val_loss: 3.3869e-05 - val_mae: 0.0046\n",
      "Epoch 531/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 2.0184e-05 - mae: 0.0036\n",
      "Epoch 531: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.8861e-05 - mae: 0.0035 - val_loss: 3.4217e-05 - val_mae: 0.0045\n",
      "Epoch 532/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9858e-05 - mae: 0.0035\n",
      "Epoch 532: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 2.0026e-05 - mae: 0.0036 - val_loss: 3.5311e-05 - val_mae: 0.0048\n",
      "Epoch 533/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8620e-05 - mae: 0.0035\n",
      "Epoch 533: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8888e-05 - mae: 0.0034 - val_loss: 3.3873e-05 - val_mae: 0.0046\n",
      "Epoch 534/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9180e-05 - mae: 0.0034\n",
      "Epoch 534: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.9458e-05 - mae: 0.0035 - val_loss: 3.3427e-05 - val_mae: 0.0046\n",
      "Epoch 535/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6667e-05 - mae: 0.0033\n",
      "Epoch 535: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.7475e-05 - mae: 0.0033 - val_loss: 3.5077e-05 - val_mae: 0.0048\n",
      "Epoch 536/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.8320e-05 - mae: 0.0034\n",
      "Epoch 536: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7785e-05 - mae: 0.0034 - val_loss: 3.3400e-05 - val_mae: 0.0045\n",
      "Epoch 537/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7941e-05 - mae: 0.0033\n",
      "Epoch 537: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.9765e-05 - mae: 0.0035 - val_loss: 4.2469e-05 - val_mae: 0.0054\n",
      "Epoch 538/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8293e-05 - mae: 0.0034\n",
      "Epoch 538: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.7795e-05 - mae: 0.0034 - val_loss: 3.4391e-05 - val_mae: 0.0047\n",
      "Epoch 539/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6398e-05 - mae: 0.0032\n",
      "Epoch 539: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.7409e-05 - mae: 0.0033 - val_loss: 3.7354e-05 - val_mae: 0.0050\n",
      "Epoch 540/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6687e-05 - mae: 0.0032\n",
      "Epoch 540: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8095e-05 - mae: 0.0034 - val_loss: 3.3753e-05 - val_mae: 0.0045\n",
      "Epoch 541/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8031e-05 - mae: 0.0033\n",
      "Epoch 541: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8167e-05 - mae: 0.0034 - val_loss: 3.6503e-05 - val_mae: 0.0049\n",
      "Epoch 542/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6637e-05 - mae: 0.0032\n",
      "Epoch 542: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0422e-05 - mae: 0.0036 - val_loss: 3.7054e-05 - val_mae: 0.0049\n",
      "Epoch 543/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7466e-05 - mae: 0.0033\n",
      "Epoch 543: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7774e-05 - mae: 0.0034 - val_loss: 3.3613e-05 - val_mae: 0.0045\n",
      "Epoch 544/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5634e-05 - mae: 0.0032\n",
      "Epoch 544: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8022e-05 - mae: 0.0034 - val_loss: 3.3450e-05 - val_mae: 0.0046\n",
      "Epoch 545/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6245e-05 - mae: 0.0032\n",
      "Epoch 545: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7705e-05 - mae: 0.0034 - val_loss: 3.3122e-05 - val_mae: 0.0045\n",
      "Epoch 546/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9896e-05 - mae: 0.0036\n",
      "Epoch 546: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 2.1322e-05 - mae: 0.0037 - val_loss: 3.7268e-05 - val_mae: 0.0050\n",
      "Epoch 547/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7245e-05 - mae: 0.0034\n",
      "Epoch 547: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8706e-05 - mae: 0.0035 - val_loss: 5.4827e-05 - val_mae: 0.0061\n",
      "Epoch 548/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.0385e-05 - mae: 0.0037\n",
      "Epoch 548: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0800e-05 - mae: 0.0037 - val_loss: 3.3353e-05 - val_mae: 0.0045\n",
      "Epoch 549/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9458e-05 - mae: 0.0036\n",
      "Epoch 549: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 2.2047e-05 - mae: 0.0038 - val_loss: 3.6492e-05 - val_mae: 0.0047\n",
      "Epoch 550/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1279e-05 - mae: 0.0037\n",
      "Epoch 550: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.9289e-05 - mae: 0.0035 - val_loss: 3.9545e-05 - val_mae: 0.0049\n",
      "Epoch 551/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7993e-05 - mae: 0.0034\n",
      "Epoch 551: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8887e-05 - mae: 0.0035 - val_loss: 3.5710e-05 - val_mae: 0.0048\n",
      "Epoch 552/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8842e-05 - mae: 0.0035\n",
      "Epoch 552: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8494e-05 - mae: 0.0035 - val_loss: 3.6952e-05 - val_mae: 0.0047\n",
      "Epoch 553/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8647e-05 - mae: 0.0035\n",
      "Epoch 553: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8513e-05 - mae: 0.0035 - val_loss: 3.6202e-05 - val_mae: 0.0046\n",
      "Epoch 554/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7510e-05 - mae: 0.0033\n",
      "Epoch 554: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.6933e-05 - mae: 0.0033 - val_loss: 3.3442e-05 - val_mae: 0.0046\n",
      "Epoch 555/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6840e-05 - mae: 0.0032\n",
      "Epoch 555: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9366e-05 - mae: 0.0035 - val_loss: 3.4390e-05 - val_mae: 0.0047\n",
      "Epoch 556/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5341e-05 - mae: 0.0031\n",
      "Epoch 556: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7401e-05 - mae: 0.0033 - val_loss: 3.5318e-05 - val_mae: 0.0048\n",
      "Epoch 557/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8847e-05 - mae: 0.0035\n",
      "Epoch 557: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8684e-05 - mae: 0.0035 - val_loss: 3.3286e-05 - val_mae: 0.0045\n",
      "Epoch 558/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.9065e-05 - mae: 0.0035\n",
      "Epoch 558: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.9001e-05 - mae: 0.0035 - val_loss: 4.0231e-05 - val_mae: 0.0052\n",
      "Epoch 559/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7791e-05 - mae: 0.0034\n",
      "Epoch 559: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8531e-05 - mae: 0.0034 - val_loss: 3.5367e-05 - val_mae: 0.0048\n",
      "Epoch 560/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8376e-05 - mae: 0.0034\n",
      "Epoch 560: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7345e-05 - mae: 0.0033 - val_loss: 3.2936e-05 - val_mae: 0.0045\n",
      "Epoch 561/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0451e-05 - mae: 0.0036\n",
      "Epoch 561: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8473e-05 - mae: 0.0034 - val_loss: 4.2222e-05 - val_mae: 0.0054\n",
      "Epoch 562/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8276e-05 - mae: 0.0034\n",
      "Epoch 562: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8127e-05 - mae: 0.0034 - val_loss: 3.3694e-05 - val_mae: 0.0046\n",
      "Epoch 563/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9016e-05 - mae: 0.0035\n",
      "Epoch 563: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8975e-05 - mae: 0.0034 - val_loss: 3.5692e-05 - val_mae: 0.0048\n",
      "Epoch 564/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6131e-05 - mae: 0.0032\n",
      "Epoch 564: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6728e-05 - mae: 0.0033 - val_loss: 3.3448e-05 - val_mae: 0.0045\n",
      "Epoch 565/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6164e-05 - mae: 0.0032\n",
      "Epoch 565: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7258e-05 - mae: 0.0033 - val_loss: 3.4278e-05 - val_mae: 0.0045\n",
      "Epoch 566/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7223e-05 - mae: 0.0033\n",
      "Epoch 566: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8073e-05 - mae: 0.0034 - val_loss: 3.3449e-05 - val_mae: 0.0045\n",
      "Epoch 567/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6875e-05 - mae: 0.0032\n",
      "Epoch 567: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.7397e-05 - mae: 0.0033 - val_loss: 3.7085e-05 - val_mae: 0.0049\n",
      "Epoch 568/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7044e-05 - mae: 0.0033\n",
      "Epoch 568: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8434e-05 - mae: 0.0034 - val_loss: 3.6188e-05 - val_mae: 0.0049\n",
      "Epoch 569/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7909e-05 - mae: 0.0034\n",
      "Epoch 569: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.9087e-05 - mae: 0.0035 - val_loss: 3.3715e-05 - val_mae: 0.0045\n",
      "Epoch 570/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6731e-05 - mae: 0.0033\n",
      "Epoch 570: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8289e-05 - mae: 0.0034 - val_loss: 3.3350e-05 - val_mae: 0.0045\n",
      "Epoch 571/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9756e-05 - mae: 0.0035\n",
      "Epoch 571: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.8622e-05 - mae: 0.0034 - val_loss: 3.3341e-05 - val_mae: 0.0045\n",
      "Epoch 572/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7209e-05 - mae: 0.0033\n",
      "Epoch 572: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.7688e-05 - mae: 0.0033 - val_loss: 3.3853e-05 - val_mae: 0.0046\n",
      "Epoch 573/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.5875e-05 - mae: 0.0032\n",
      "Epoch 573: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.6427e-05 - mae: 0.0032 - val_loss: 4.1504e-05 - val_mae: 0.0053\n",
      "Epoch 574/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7002e-05 - mae: 0.0033\n",
      "Epoch 574: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.7059e-05 - mae: 0.0033 - val_loss: 3.3477e-05 - val_mae: 0.0045\n",
      "Epoch 575/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.7335e-05 - mae: 0.0033\n",
      "Epoch 575: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8839e-05 - mae: 0.0035 - val_loss: 3.5322e-05 - val_mae: 0.0048\n",
      "Epoch 576/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6414e-05 - mae: 0.0032\n",
      "Epoch 576: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.6943e-05 - mae: 0.0033 - val_loss: 3.3546e-05 - val_mae: 0.0045\n",
      "Epoch 577/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7823e-05 - mae: 0.0033\n",
      "Epoch 577: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7877e-05 - mae: 0.0033 - val_loss: 3.9262e-05 - val_mae: 0.0048\n",
      "Epoch 578/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 2.0162e-05 - mae: 0.0035\n",
      "Epoch 578: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0237e-05 - mae: 0.0035 - val_loss: 4.9130e-05 - val_mae: 0.0055\n",
      "Epoch 579/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9282e-05 - mae: 0.0035\n",
      "Epoch 579: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.2128e-05 - mae: 0.0038 - val_loss: 3.4291e-05 - val_mae: 0.0047\n",
      "Epoch 580/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5966e-05 - mae: 0.0032\n",
      "Epoch 580: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7549e-05 - mae: 0.0033 - val_loss: 3.6628e-05 - val_mae: 0.0049\n",
      "Epoch 581/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2463e-05 - mae: 0.0038\n",
      "Epoch 581: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 2.0288e-05 - mae: 0.0036 - val_loss: 3.5812e-05 - val_mae: 0.0046\n",
      "Epoch 582/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7540e-05 - mae: 0.0034\n",
      "Epoch 582: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.7696e-05 - mae: 0.0034 - val_loss: 3.4075e-05 - val_mae: 0.0045\n",
      "Epoch 583/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.4774e-05 - mae: 0.0031\n",
      "Epoch 583: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8435e-05 - mae: 0.0034 - val_loss: 3.9958e-05 - val_mae: 0.0052\n",
      "Epoch 584/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6649e-05 - mae: 0.0033\n",
      "Epoch 584: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.8102e-05 - mae: 0.0034 - val_loss: 3.6046e-05 - val_mae: 0.0049\n",
      "Epoch 585/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9186e-05 - mae: 0.0035\n",
      "Epoch 585: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8792e-05 - mae: 0.0035 - val_loss: 3.3568e-05 - val_mae: 0.0046\n",
      "Epoch 586/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.6778e-05 - mae: 0.0033\n",
      "Epoch 586: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 1.8682e-05 - mae: 0.0034 - val_loss: 6.0820e-05 - val_mae: 0.0065\n",
      "Epoch 587/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1145e-05 - mae: 0.0036\n",
      "Epoch 587: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9666e-05 - mae: 0.0035 - val_loss: 4.0896e-05 - val_mae: 0.0050\n",
      "Epoch 588/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8122e-05 - mae: 0.0034\n",
      "Epoch 588: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.9394e-05 - mae: 0.0035 - val_loss: 3.5509e-05 - val_mae: 0.0048\n",
      "Epoch 589/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8680e-05 - mae: 0.0034\n",
      "Epoch 589: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 131us/sample - loss: 1.8236e-05 - mae: 0.0034 - val_loss: 3.5701e-05 - val_mae: 0.0046\n",
      "Epoch 590/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6921e-05 - mae: 0.0033\n",
      "Epoch 590: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6808e-05 - mae: 0.0033 - val_loss: 3.5200e-05 - val_mae: 0.0046\n",
      "Epoch 591/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8471e-05 - mae: 0.0034\n",
      "Epoch 591: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8513e-05 - mae: 0.0034 - val_loss: 3.6820e-05 - val_mae: 0.0047\n",
      "Epoch 592/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7643e-05 - mae: 0.0033\n",
      "Epoch 592: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7775e-05 - mae: 0.0033 - val_loss: 3.5072e-05 - val_mae: 0.0046\n",
      "Epoch 593/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5553e-05 - mae: 0.0031\n",
      "Epoch 593: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8397e-05 - mae: 0.0034 - val_loss: 3.9162e-05 - val_mae: 0.0051\n",
      "Epoch 594/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9787e-05 - mae: 0.0035\n",
      "Epoch 594: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.9787e-05 - mae: 0.0035 - val_loss: 5.4601e-05 - val_mae: 0.0059\n",
      "Epoch 595/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.0616e-05 - mae: 0.0036\n",
      "Epoch 595: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 2.0616e-05 - mae: 0.0036 - val_loss: 4.1660e-05 - val_mae: 0.0053\n",
      "Epoch 596/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7761e-05 - mae: 0.0034\n",
      "Epoch 596: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7812e-05 - mae: 0.0034 - val_loss: 3.6209e-05 - val_mae: 0.0049\n",
      "Epoch 597/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0976e-05 - mae: 0.0036\n",
      "Epoch 597: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9766e-05 - mae: 0.0035 - val_loss: 4.1593e-05 - val_mae: 0.0053\n",
      "Epoch 598/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8331e-05 - mae: 0.0035\n",
      "Epoch 598: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.8825e-05 - mae: 0.0035 - val_loss: 3.3131e-05 - val_mae: 0.0045\n",
      "Epoch 599/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9470e-05 - mae: 0.0035\n",
      "Epoch 599: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7898e-05 - mae: 0.0033 - val_loss: 3.4137e-05 - val_mae: 0.0045\n",
      "Epoch 600/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1552e-05 - mae: 0.0037\n",
      "Epoch 600: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9823e-05 - mae: 0.0036 - val_loss: 3.3639e-05 - val_mae: 0.0046\n",
      "Epoch 601/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7134e-05 - mae: 0.0033\n",
      "Epoch 601: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 2.1499e-05 - mae: 0.0037 - val_loss: 3.5989e-05 - val_mae: 0.0049\n",
      "Epoch 602/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 2.0719e-05 - mae: 0.0036\n",
      "Epoch 602: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 2.0509e-05 - mae: 0.0036 - val_loss: 3.4483e-05 - val_mae: 0.0047\n",
      "Epoch 603/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.1012e-05 - mae: 0.0037\n",
      "Epoch 603: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8611e-05 - mae: 0.0035 - val_loss: 3.7467e-05 - val_mae: 0.0047\n",
      "Epoch 604/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6031e-05 - mae: 0.0032\n",
      "Epoch 604: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.6610e-05 - mae: 0.0032 - val_loss: 3.4456e-05 - val_mae: 0.0045\n",
      "Epoch 605/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8943e-05 - mae: 0.0035\n",
      "Epoch 605: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9113e-05 - mae: 0.0035 - val_loss: 3.6097e-05 - val_mae: 0.0046\n",
      "Epoch 606/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7433e-05 - mae: 0.0034\n",
      "Epoch 606: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7366e-05 - mae: 0.0033 - val_loss: 3.3304e-05 - val_mae: 0.0045\n",
      "Epoch 607/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6534e-05 - mae: 0.0033\n",
      "Epoch 607: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7351e-05 - mae: 0.0033 - val_loss: 3.4057e-05 - val_mae: 0.0045\n",
      "Epoch 608/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.2980e-05 - mae: 0.0038\n",
      "Epoch 608: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 2.0459e-05 - mae: 0.0036 - val_loss: 3.3162e-05 - val_mae: 0.0045\n",
      "Epoch 609/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7694e-05 - mae: 0.0033\n",
      "Epoch 609: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7924e-05 - mae: 0.0033 - val_loss: 3.3873e-05 - val_mae: 0.0045\n",
      "Epoch 610/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.8654e-05 - mae: 0.0034\n",
      "Epoch 610: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7314e-05 - mae: 0.0033 - val_loss: 3.7744e-05 - val_mae: 0.0050\n",
      "Epoch 611/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8065e-05 - mae: 0.0034\n",
      "Epoch 611: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8003e-05 - mae: 0.0034 - val_loss: 3.3217e-05 - val_mae: 0.0045\n",
      "Epoch 612/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.2210e-05 - mae: 0.0038\n",
      "Epoch 612: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9345e-05 - mae: 0.0035 - val_loss: 4.4418e-05 - val_mae: 0.0055\n",
      "Epoch 613/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.1752e-05 - mae: 0.0038\n",
      "Epoch 613: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9408e-05 - mae: 0.0035 - val_loss: 3.3290e-05 - val_mae: 0.0045\n",
      "Epoch 614/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8027e-05 - mae: 0.0034\n",
      "Epoch 614: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8027e-05 - mae: 0.0034 - val_loss: 3.4048e-05 - val_mae: 0.0046\n",
      "Epoch 615/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8404e-05 - mae: 0.0034\n",
      "Epoch 615: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8259e-05 - mae: 0.0034 - val_loss: 3.3796e-05 - val_mae: 0.0045\n",
      "Epoch 616/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7563e-05 - mae: 0.0033\n",
      "Epoch 616: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7677e-05 - mae: 0.0034 - val_loss: 3.3498e-05 - val_mae: 0.0046\n",
      "Epoch 617/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8800e-05 - mae: 0.0035\n",
      "Epoch 617: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8800e-05 - mae: 0.0035 - val_loss: 3.3537e-05 - val_mae: 0.0046\n",
      "Epoch 618/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.8778e-05 - mae: 0.0034\n",
      "Epoch 618: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.9547e-05 - mae: 0.0035 - val_loss: 3.5993e-05 - val_mae: 0.0048\n",
      "Epoch 619/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7270e-05 - mae: 0.0033\n",
      "Epoch 619: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.9272e-05 - mae: 0.0035 - val_loss: 4.0913e-05 - val_mae: 0.0053\n",
      "Epoch 620/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9022e-05 - mae: 0.0035\n",
      "Epoch 620: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8285e-05 - mae: 0.0034 - val_loss: 3.3644e-05 - val_mae: 0.0046\n",
      "Epoch 621/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0467e-05 - mae: 0.0036\n",
      "Epoch 621: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.0537e-05 - mae: 0.0036 - val_loss: 3.7054e-05 - val_mae: 0.0049\n",
      "Epoch 622/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5929e-05 - mae: 0.0031\n",
      "Epoch 622: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.7224e-05 - mae: 0.0033 - val_loss: 3.3886e-05 - val_mae: 0.0045\n",
      "Epoch 623/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7045e-05 - mae: 0.0033\n",
      "Epoch 623: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.7646e-05 - mae: 0.0034 - val_loss: 4.1541e-05 - val_mae: 0.0050\n",
      "Epoch 624/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 2.1061e-05 - mae: 0.0037\n",
      "Epoch 624: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 2.0990e-05 - mae: 0.0036 - val_loss: 3.4245e-05 - val_mae: 0.0047\n",
      "Epoch 625/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9075e-05 - mae: 0.0035\n",
      "Epoch 625: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8811e-05 - mae: 0.0035 - val_loss: 3.3241e-05 - val_mae: 0.0045\n",
      "Epoch 626/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7028e-05 - mae: 0.0032\n",
      "Epoch 626: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7220e-05 - mae: 0.0032 - val_loss: 3.3400e-05 - val_mae: 0.0046\n",
      "Epoch 627/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6990e-05 - mae: 0.0033\n",
      "Epoch 627: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.6827e-05 - mae: 0.0033 - val_loss: 4.1154e-05 - val_mae: 0.0053\n",
      "Epoch 628/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8116e-05 - mae: 0.0034\n",
      "Epoch 628: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7659e-05 - mae: 0.0034 - val_loss: 3.4495e-05 - val_mae: 0.0047\n",
      "Epoch 629/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0062e-05 - mae: 0.0036\n",
      "Epoch 629: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9703e-05 - mae: 0.0035 - val_loss: 3.3329e-05 - val_mae: 0.0046\n",
      "Epoch 630/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7786e-05 - mae: 0.0034\n",
      "Epoch 630: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.8106e-05 - mae: 0.0034 - val_loss: 3.3398e-05 - val_mae: 0.0045\n",
      "Epoch 631/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7311e-05 - mae: 0.0033\n",
      "Epoch 631: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.7171e-05 - mae: 0.0033 - val_loss: 3.3841e-05 - val_mae: 0.0046\n",
      "Epoch 632/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8083e-05 - mae: 0.0034\n",
      "Epoch 632: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7423e-05 - mae: 0.0034 - val_loss: 3.3916e-05 - val_mae: 0.0045\n",
      "Epoch 633/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7711e-05 - mae: 0.0034\n",
      "Epoch 633: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7673e-05 - mae: 0.0034 - val_loss: 3.6236e-05 - val_mae: 0.0049\n",
      "Epoch 634/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9426e-05 - mae: 0.0035\n",
      "Epoch 634: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8425e-05 - mae: 0.0035 - val_loss: 3.3399e-05 - val_mae: 0.0046\n",
      "Epoch 635/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.9337e-05 - mae: 0.0035\n",
      "Epoch 635: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.9370e-05 - mae: 0.0035 - val_loss: 3.9809e-05 - val_mae: 0.0052\n",
      "Epoch 636/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8678e-05 - mae: 0.0035\n",
      "Epoch 636: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.1226e-05 - mae: 0.0037 - val_loss: 3.9936e-05 - val_mae: 0.0052\n",
      "Epoch 637/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7059e-05 - mae: 0.0033\n",
      "Epoch 637: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7644e-05 - mae: 0.0033 - val_loss: 3.3742e-05 - val_mae: 0.0046\n",
      "Epoch 638/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6381e-05 - mae: 0.0032\n",
      "Epoch 638: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.6349e-05 - mae: 0.0032 - val_loss: 3.3942e-05 - val_mae: 0.0045\n",
      "Epoch 639/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 2.0074e-05 - mae: 0.0036\n",
      "Epoch 639: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.9078e-05 - mae: 0.0035 - val_loss: 3.6051e-05 - val_mae: 0.0048\n",
      "Epoch 640/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9430e-05 - mae: 0.0035\n",
      "Epoch 640: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.0517e-05 - mae: 0.0036 - val_loss: 3.4163e-05 - val_mae: 0.0045\n",
      "Epoch 641/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8839e-05 - mae: 0.0035\n",
      "Epoch 641: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9857e-05 - mae: 0.0035 - val_loss: 3.9819e-05 - val_mae: 0.0049\n",
      "Epoch 642/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.9253e-05 - mae: 0.0035\n",
      "Epoch 642: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.9992e-05 - mae: 0.0036 - val_loss: 3.3840e-05 - val_mae: 0.0045\n",
      "Epoch 643/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9059e-05 - mae: 0.0035\n",
      "Epoch 643: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7994e-05 - mae: 0.0034 - val_loss: 3.4203e-05 - val_mae: 0.0047\n",
      "Epoch 644/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7920e-05 - mae: 0.0034\n",
      "Epoch 644: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8866e-05 - mae: 0.0035 - val_loss: 3.6708e-05 - val_mae: 0.0047\n",
      "Epoch 645/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7262e-05 - mae: 0.0034\n",
      "Epoch 645: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7807e-05 - mae: 0.0034 - val_loss: 3.4848e-05 - val_mae: 0.0047\n",
      "Epoch 646/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.5410e-05 - mae: 0.0032\n",
      "Epoch 646: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7891e-05 - mae: 0.0034 - val_loss: 4.0086e-05 - val_mae: 0.0052\n",
      "Epoch 647/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.5916e-05 - mae: 0.0032\n",
      "Epoch 647: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7024e-05 - mae: 0.0033 - val_loss: 3.3558e-05 - val_mae: 0.0045\n",
      "Epoch 648/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7196e-05 - mae: 0.0033\n",
      "Epoch 648: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6761e-05 - mae: 0.0033 - val_loss: 3.3837e-05 - val_mae: 0.0045\n",
      "Epoch 649/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7247e-05 - mae: 0.0033\n",
      "Epoch 649: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9627e-05 - mae: 0.0036 - val_loss: 3.3593e-05 - val_mae: 0.0045\n",
      "Epoch 650/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6513e-05 - mae: 0.0032\n",
      "Epoch 650: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8237e-05 - mae: 0.0034 - val_loss: 3.3607e-05 - val_mae: 0.0046\n",
      "Epoch 651/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7672e-05 - mae: 0.0032\n",
      "Epoch 651: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8351e-05 - mae: 0.0034 - val_loss: 3.4080e-05 - val_mae: 0.0047\n",
      "Epoch 652/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.6853e-05 - mae: 0.0033\n",
      "Epoch 652: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7184e-05 - mae: 0.0033 - val_loss: 3.3328e-05 - val_mae: 0.0046\n",
      "Epoch 653/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5746e-05 - mae: 0.0031\n",
      "Epoch 653: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8700e-05 - mae: 0.0034 - val_loss: 3.4908e-05 - val_mae: 0.0046\n",
      "Epoch 654/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6300e-05 - mae: 0.0033\n",
      "Epoch 654: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8033e-05 - mae: 0.0034 - val_loss: 3.3181e-05 - val_mae: 0.0045\n",
      "Epoch 655/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7011e-05 - mae: 0.0033\n",
      "Epoch 655: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7011e-05 - mae: 0.0033 - val_loss: 3.4113e-05 - val_mae: 0.0045\n",
      "Epoch 656/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.2018e-05 - mae: 0.0037\n",
      "Epoch 656: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.0927e-05 - mae: 0.0036 - val_loss: 3.3461e-05 - val_mae: 0.0046\n",
      "Epoch 657/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7919e-05 - mae: 0.0034\n",
      "Epoch 657: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7491e-05 - mae: 0.0034 - val_loss: 3.3903e-05 - val_mae: 0.0045\n",
      "Epoch 658/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0656e-05 - mae: 0.0037\n",
      "Epoch 658: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8768e-05 - mae: 0.0035 - val_loss: 3.3976e-05 - val_mae: 0.0045\n",
      "Epoch 659/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.7689e-05 - mae: 0.0033\n",
      "Epoch 659: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7713e-05 - mae: 0.0033 - val_loss: 3.6237e-05 - val_mae: 0.0046\n",
      "Epoch 660/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.7535e-05 - mae: 0.0034\n",
      "Epoch 660: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.8168e-05 - mae: 0.0034 - val_loss: 4.2169e-05 - val_mae: 0.0051\n",
      "Epoch 661/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8018e-05 - mae: 0.0034\n",
      "Epoch 661: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8111e-05 - mae: 0.0034 - val_loss: 3.4219e-05 - val_mae: 0.0045\n",
      "Epoch 662/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.9091e-05 - mae: 0.0034\n",
      "Epoch 662: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.9674e-05 - mae: 0.0035 - val_loss: 4.5190e-05 - val_mae: 0.0056\n",
      "Epoch 663/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7566e-05 - mae: 0.0033\n",
      "Epoch 663: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7756e-05 - mae: 0.0033 - val_loss: 3.4098e-05 - val_mae: 0.0047\n",
      "Epoch 664/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8028e-05 - mae: 0.0033\n",
      "Epoch 664: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8195e-05 - mae: 0.0033 - val_loss: 3.5952e-05 - val_mae: 0.0049\n",
      "Epoch 665/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8786e-05 - mae: 0.0035\n",
      "Epoch 665: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9375e-05 - mae: 0.0035 - val_loss: 3.4596e-05 - val_mae: 0.0045\n",
      "Epoch 666/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8686e-05 - mae: 0.0035\n",
      "Epoch 666: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9846e-05 - mae: 0.0036 - val_loss: 4.4598e-05 - val_mae: 0.0055\n",
      "Epoch 667/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8690e-05 - mae: 0.0034\n",
      "Epoch 667: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8774e-05 - mae: 0.0035 - val_loss: 3.3432e-05 - val_mae: 0.0046\n",
      "Epoch 668/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7734e-05 - mae: 0.0034\n",
      "Epoch 668: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7057e-05 - mae: 0.0033 - val_loss: 3.9550e-05 - val_mae: 0.0049\n",
      "Epoch 669/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8499e-05 - mae: 0.0034\n",
      "Epoch 669: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.8345e-05 - mae: 0.0034 - val_loss: 3.4871e-05 - val_mae: 0.0047\n",
      "Epoch 670/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1262e-05 - mae: 0.0038\n",
      "Epoch 670: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.9754e-05 - mae: 0.0036 - val_loss: 3.3604e-05 - val_mae: 0.0045\n",
      "Epoch 671/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7403e-05 - mae: 0.0033\n",
      "Epoch 671: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7201e-05 - mae: 0.0033 - val_loss: 3.6501e-05 - val_mae: 0.0049\n",
      "Epoch 672/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8723e-05 - mae: 0.0035\n",
      "Epoch 672: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8716e-05 - mae: 0.0035 - val_loss: 3.4629e-05 - val_mae: 0.0047\n",
      "Epoch 673/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9650e-05 - mae: 0.0035\n",
      "Epoch 673: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9266e-05 - mae: 0.0035 - val_loss: 3.3541e-05 - val_mae: 0.0046\n",
      "Epoch 674/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7853e-05 - mae: 0.0034\n",
      "Epoch 674: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7090e-05 - mae: 0.0033 - val_loss: 3.4424e-05 - val_mae: 0.0047\n",
      "Epoch 675/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8735e-05 - mae: 0.0035\n",
      "Epoch 675: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8875e-05 - mae: 0.0035 - val_loss: 3.3553e-05 - val_mae: 0.0046\n",
      "Epoch 676/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5916e-05 - mae: 0.0032\n",
      "Epoch 676: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7139e-05 - mae: 0.0033 - val_loss: 3.8078e-05 - val_mae: 0.0050\n",
      "Epoch 677/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7581e-05 - mae: 0.0033\n",
      "Epoch 677: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7357e-05 - mae: 0.0033 - val_loss: 3.4957e-05 - val_mae: 0.0046\n",
      "Epoch 678/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7186e-05 - mae: 0.0033\n",
      "Epoch 678: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.9213e-05 - mae: 0.0035 - val_loss: 3.3285e-05 - val_mae: 0.0045\n",
      "Epoch 679/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9582e-05 - mae: 0.0035\n",
      "Epoch 679: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8980e-05 - mae: 0.0035 - val_loss: 3.3988e-05 - val_mae: 0.0045\n",
      "Epoch 680/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5779e-05 - mae: 0.0032\n",
      "Epoch 680: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.7779e-05 - mae: 0.0034 - val_loss: 3.4954e-05 - val_mae: 0.0046\n",
      "Epoch 681/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.9187e-05 - mae: 0.0035\n",
      "Epoch 681: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8055e-05 - mae: 0.0034 - val_loss: 3.7901e-05 - val_mae: 0.0050\n",
      "Epoch 682/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7779e-05 - mae: 0.0034\n",
      "Epoch 682: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8719e-05 - mae: 0.0035 - val_loss: 4.2243e-05 - val_mae: 0.0053\n",
      "Epoch 683/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.2073e-05 - mae: 0.0037\n",
      "Epoch 683: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9890e-05 - mae: 0.0035 - val_loss: 3.3250e-05 - val_mae: 0.0045\n",
      "Epoch 684/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 2.0023e-05 - mae: 0.0036\n",
      "Epoch 684: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9710e-05 - mae: 0.0036 - val_loss: 3.8133e-05 - val_mae: 0.0050\n",
      "Epoch 685/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9534e-05 - mae: 0.0036\n",
      "Epoch 685: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.9983e-05 - mae: 0.0036 - val_loss: 3.9560e-05 - val_mae: 0.0049\n",
      "Epoch 686/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0480e-05 - mae: 0.0037\n",
      "Epoch 686: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9004e-05 - mae: 0.0035 - val_loss: 3.9575e-05 - val_mae: 0.0049\n",
      "Epoch 687/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7042e-05 - mae: 0.0033\n",
      "Epoch 687: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7673e-05 - mae: 0.0034 - val_loss: 3.3370e-05 - val_mae: 0.0046\n",
      "Epoch 688/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380/800 [=============>................] - ETA: 0s - loss: 1.8726e-05 - mae: 0.0035\n",
      "Epoch 688: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8115e-05 - mae: 0.0034 - val_loss: 3.4372e-05 - val_mae: 0.0047\n",
      "Epoch 689/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6879e-05 - mae: 0.0032\n",
      "Epoch 689: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7540e-05 - mae: 0.0033 - val_loss: 3.3578e-05 - val_mae: 0.0046\n",
      "Epoch 690/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7849e-05 - mae: 0.0034\n",
      "Epoch 690: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8132e-05 - mae: 0.0034 - val_loss: 3.4516e-05 - val_mae: 0.0045\n",
      "Epoch 691/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8578e-05 - mae: 0.0034\n",
      "Epoch 691: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8578e-05 - mae: 0.0034 - val_loss: 5.3085e-05 - val_mae: 0.0060\n",
      "Epoch 692/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8901e-05 - mae: 0.0035\n",
      "Epoch 692: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8769e-05 - mae: 0.0035 - val_loss: 3.4628e-05 - val_mae: 0.0047\n",
      "Epoch 693/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7104e-05 - mae: 0.0033\n",
      "Epoch 693: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7104e-05 - mae: 0.0033 - val_loss: 3.3175e-05 - val_mae: 0.0045\n",
      "Epoch 694/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8622e-05 - mae: 0.0035\n",
      "Epoch 694: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8622e-05 - mae: 0.0035 - val_loss: 3.4709e-05 - val_mae: 0.0047\n",
      "Epoch 695/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8129e-05 - mae: 0.0034\n",
      "Epoch 695: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8025e-05 - mae: 0.0034 - val_loss: 3.3371e-05 - val_mae: 0.0046\n",
      "Epoch 696/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6536e-05 - mae: 0.0032\n",
      "Epoch 696: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6941e-05 - mae: 0.0033 - val_loss: 3.4684e-05 - val_mae: 0.0047\n",
      "Epoch 697/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7544e-05 - mae: 0.0033\n",
      "Epoch 697: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7552e-05 - mae: 0.0033 - val_loss: 3.7039e-05 - val_mae: 0.0047\n",
      "Epoch 698/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7259e-05 - mae: 0.0034\n",
      "Epoch 698: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.7494e-05 - mae: 0.0034 - val_loss: 3.3671e-05 - val_mae: 0.0046\n",
      "Epoch 699/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8686e-05 - mae: 0.0034\n",
      "Epoch 699: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8493e-05 - mae: 0.0034 - val_loss: 3.3392e-05 - val_mae: 0.0045\n",
      "Epoch 700/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5230e-05 - mae: 0.0031\n",
      "Epoch 700: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.8208e-05 - mae: 0.0034 - val_loss: 6.5638e-05 - val_mae: 0.0068\n",
      "Epoch 701/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.1100e-05 - mae: 0.0036\n",
      "Epoch 701: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8930e-05 - mae: 0.0035 - val_loss: 3.4399e-05 - val_mae: 0.0047\n",
      "Epoch 702/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6940e-05 - mae: 0.0033\n",
      "Epoch 702: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6905e-05 - mae: 0.0033 - val_loss: 3.3497e-05 - val_mae: 0.0046\n",
      "Epoch 703/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6432e-05 - mae: 0.0032\n",
      "Epoch 703: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8113e-05 - mae: 0.0034 - val_loss: 3.5903e-05 - val_mae: 0.0048\n",
      "Epoch 704/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7022e-05 - mae: 0.0033\n",
      "Epoch 704: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8227e-05 - mae: 0.0034 - val_loss: 3.3182e-05 - val_mae: 0.0045\n",
      "Epoch 705/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.0918e-05 - mae: 0.0036\n",
      "Epoch 705: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9846e-05 - mae: 0.0036 - val_loss: 3.2952e-05 - val_mae: 0.0045\n",
      "Epoch 706/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6885e-05 - mae: 0.0033\n",
      "Epoch 706: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7288e-05 - mae: 0.0034 - val_loss: 3.7375e-05 - val_mae: 0.0050\n",
      "Epoch 707/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 2.3079e-05 - mae: 0.0038\n",
      "Epoch 707: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 2.4169e-05 - mae: 0.0039 - val_loss: 3.7971e-05 - val_mae: 0.0050\n",
      "Epoch 708/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8917e-05 - mae: 0.0035\n",
      "Epoch 708: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.8983e-05 - mae: 0.0035 - val_loss: 3.3491e-05 - val_mae: 0.0046\n",
      "Epoch 709/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7672e-05 - mae: 0.0034\n",
      "Epoch 709: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0115e-05 - mae: 0.0036 - val_loss: 3.3156e-05 - val_mae: 0.0045\n",
      "Epoch 710/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 2.0277e-05 - mae: 0.0036\n",
      "Epoch 710: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 2.0275e-05 - mae: 0.0036 - val_loss: 3.3927e-05 - val_mae: 0.0045\n",
      "Epoch 711/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8352e-05 - mae: 0.0034\n",
      "Epoch 711: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7160e-05 - mae: 0.0033 - val_loss: 3.3982e-05 - val_mae: 0.0046\n",
      "Epoch 712/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7033e-05 - mae: 0.0033\n",
      "Epoch 712: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.8164e-05 - mae: 0.0034 - val_loss: 3.9603e-05 - val_mae: 0.0052\n",
      "Epoch 713/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9633e-05 - mae: 0.0035\n",
      "Epoch 713: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.9594e-05 - mae: 0.0035 - val_loss: 3.3689e-05 - val_mae: 0.0046\n",
      "Epoch 714/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9126e-05 - mae: 0.0035\n",
      "Epoch 714: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9126e-05 - mae: 0.0035 - val_loss: 3.3065e-05 - val_mae: 0.0045\n",
      "Epoch 715/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6710e-05 - mae: 0.0032\n",
      "Epoch 715: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6612e-05 - mae: 0.0032 - val_loss: 3.3525e-05 - val_mae: 0.0046\n",
      "Epoch 716/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7204e-05 - mae: 0.0033\n",
      "Epoch 716: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7290e-05 - mae: 0.0033 - val_loss: 3.8314e-05 - val_mae: 0.0051\n",
      "Epoch 717/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7523e-05 - mae: 0.0033\n",
      "Epoch 717: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8775e-05 - mae: 0.0034 - val_loss: 3.6561e-05 - val_mae: 0.0049\n",
      "Epoch 718/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7292e-05 - mae: 0.0034\n",
      "Epoch 718: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7292e-05 - mae: 0.0034 - val_loss: 3.8197e-05 - val_mae: 0.0048\n",
      "Epoch 719/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6582e-05 - mae: 0.0032\n",
      "Epoch 719: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.7180e-05 - mae: 0.0033 - val_loss: 3.3185e-05 - val_mae: 0.0046\n",
      "Epoch 720/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7468e-05 - mae: 0.0033\n",
      "Epoch 720: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7042e-05 - mae: 0.0033 - val_loss: 3.3279e-05 - val_mae: 0.0046\n",
      "Epoch 721/1000\n",
      "380/800 [=============>................] - ETA: 0s - loss: 1.5598e-05 - mae: 0.0031\n",
      "Epoch 721: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7648e-05 - mae: 0.0034 - val_loss: 3.3193e-05 - val_mae: 0.0045\n",
      "Epoch 722/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8430e-05 - mae: 0.0034\n",
      "Epoch 722: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 1.8392e-05 - mae: 0.0034 - val_loss: 3.3394e-05 - val_mae: 0.0045\n",
      "Epoch 723/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7567e-05 - mae: 0.0033\n",
      "Epoch 723: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8836e-05 - mae: 0.0035 - val_loss: 3.3082e-05 - val_mae: 0.0045\n",
      "Epoch 724/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.6960e-05 - mae: 0.0033\n",
      "Epoch 724: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.8051e-05 - mae: 0.0034 - val_loss: 3.4477e-05 - val_mae: 0.0046\n",
      "Epoch 725/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8734e-05 - mae: 0.0035\n",
      "Epoch 725: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 2.0731e-05 - mae: 0.0036 - val_loss: 4.2175e-05 - val_mae: 0.0053\n",
      "Epoch 726/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.7622e-05 - mae: 0.0034\n",
      "Epoch 726: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.9037e-05 - mae: 0.0035 - val_loss: 3.6593e-05 - val_mae: 0.0049\n",
      "Epoch 727/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8912e-05 - mae: 0.0035\n",
      "Epoch 727: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.9116e-05 - mae: 0.0035 - val_loss: 4.2786e-05 - val_mae: 0.0054\n",
      "Epoch 728/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7422e-05 - mae: 0.0034\n",
      "Epoch 728: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8243e-05 - mae: 0.0034 - val_loss: 3.3459e-05 - val_mae: 0.0045\n",
      "Epoch 729/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.7329e-05 - mae: 0.0033\n",
      "Epoch 729: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 1.8912e-05 - mae: 0.0035 - val_loss: 3.7161e-05 - val_mae: 0.0047\n",
      "Epoch 730/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.9322e-05 - mae: 0.0035\n",
      "Epoch 730: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.9930e-05 - mae: 0.0035 - val_loss: 3.6490e-05 - val_mae: 0.0049\n",
      "Epoch 731/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7601e-05 - mae: 0.0033\n",
      "Epoch 731: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.7893e-05 - mae: 0.0034 - val_loss: 3.3556e-05 - val_mae: 0.0045\n",
      "Epoch 732/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7951e-05 - mae: 0.0034\n",
      "Epoch 732: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.8136e-05 - mae: 0.0034 - val_loss: 3.3943e-05 - val_mae: 0.0045\n",
      "Epoch 733/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7406e-05 - mae: 0.0033\n",
      "Epoch 733: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.7680e-05 - mae: 0.0034 - val_loss: 3.6868e-05 - val_mae: 0.0047\n",
      "Epoch 734/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.8630e-05 - mae: 0.0034\n",
      "Epoch 734: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.7888e-05 - mae: 0.0033 - val_loss: 3.7320e-05 - val_mae: 0.0050\n",
      "Epoch 735/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.0260e-05 - mae: 0.0036\n",
      "Epoch 735: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.8847e-05 - mae: 0.0035 - val_loss: 3.4607e-05 - val_mae: 0.0047\n",
      "Epoch 736/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6886e-05 - mae: 0.0033\n",
      "Epoch 736: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.7275e-05 - mae: 0.0033 - val_loss: 3.6017e-05 - val_mae: 0.0049\n",
      "Epoch 737/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.8160e-05 - mae: 0.0034\n",
      "Epoch 737: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 1.7961e-05 - mae: 0.0033 - val_loss: 3.3243e-05 - val_mae: 0.0045\n",
      "Epoch 738/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.6667e-05 - mae: 0.0032\n",
      "Epoch 738: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7559e-05 - mae: 0.0033 - val_loss: 3.3549e-05 - val_mae: 0.0045\n",
      "Epoch 739/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.6981e-05 - mae: 0.0033\n",
      "Epoch 739: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6981e-05 - mae: 0.0033 - val_loss: 4.4296e-05 - val_mae: 0.0055\n",
      "Epoch 740/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6251e-05 - mae: 0.0031\n",
      "Epoch 740: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7537e-05 - mae: 0.0034 - val_loss: 4.8335e-05 - val_mae: 0.0057\n",
      "Epoch 741/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8815e-05 - mae: 0.0035\n",
      "Epoch 741: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8922e-05 - mae: 0.0035 - val_loss: 3.4713e-05 - val_mae: 0.0047\n",
      "Epoch 742/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8145e-05 - mae: 0.0033\n",
      "Epoch 742: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8415e-05 - mae: 0.0034 - val_loss: 3.3883e-05 - val_mae: 0.0045\n",
      "Epoch 743/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9008e-05 - mae: 0.0034\n",
      "Epoch 743: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 2.0779e-05 - mae: 0.0036 - val_loss: 3.3367e-05 - val_mae: 0.0045\n",
      "Epoch 744/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.9262e-05 - mae: 0.0035\n",
      "Epoch 744: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8872e-05 - mae: 0.0035 - val_loss: 3.3715e-05 - val_mae: 0.0046\n",
      "Epoch 745/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7854e-05 - mae: 0.0033\n",
      "Epoch 745: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8555e-05 - mae: 0.0034 - val_loss: 3.3742e-05 - val_mae: 0.0045\n",
      "Epoch 746/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8690e-05 - mae: 0.0035\n",
      "Epoch 746: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0426e-05 - mae: 0.0036 - val_loss: 4.7162e-05 - val_mae: 0.0057\n",
      "Epoch 747/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8778e-05 - mae: 0.0034\n",
      "Epoch 747: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8307e-05 - mae: 0.0034 - val_loss: 3.8418e-05 - val_mae: 0.0051\n",
      "Epoch 748/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9161e-05 - mae: 0.0035\n",
      "Epoch 748: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8011e-05 - mae: 0.0034 - val_loss: 4.0618e-05 - val_mae: 0.0049\n",
      "Epoch 749/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8069e-05 - mae: 0.0034\n",
      "Epoch 749: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8125e-05 - mae: 0.0034 - val_loss: 4.1470e-05 - val_mae: 0.0050\n",
      "Epoch 750/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.9282e-05 - mae: 0.0035\n",
      "Epoch 750: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.9043e-05 - mae: 0.0035 - val_loss: 3.3125e-05 - val_mae: 0.0045\n",
      "Epoch 751/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7688e-05 - mae: 0.0033\n",
      "Epoch 751: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7626e-05 - mae: 0.0033 - val_loss: 3.4672e-05 - val_mae: 0.0045\n",
      "Epoch 752/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9062e-05 - mae: 0.0035\n",
      "Epoch 752: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.9604e-05 - mae: 0.0035 - val_loss: 3.5343e-05 - val_mae: 0.0046\n",
      "Epoch 753/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7634e-05 - mae: 0.0034\n",
      "Epoch 753: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.7605e-05 - mae: 0.0034 - val_loss: 3.3812e-05 - val_mae: 0.0045\n",
      "Epoch 754/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1315e-05 - mae: 0.0037\n",
      "Epoch 754: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9107e-05 - mae: 0.0035 - val_loss: 3.3194e-05 - val_mae: 0.0045\n",
      "Epoch 755/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.5148e-05 - mae: 0.0031\n",
      "Epoch 755: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.6635e-05 - mae: 0.0033 - val_loss: 3.9906e-05 - val_mae: 0.0052\n",
      "Epoch 756/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5321e-05 - mae: 0.0031\n",
      "Epoch 756: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7216e-05 - mae: 0.0033 - val_loss: 3.4168e-05 - val_mae: 0.0047\n",
      "Epoch 757/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.8929e-05 - mae: 0.0034\n",
      "Epoch 757: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9350e-05 - mae: 0.0035 - val_loss: 3.3661e-05 - val_mae: 0.0045\n",
      "Epoch 758/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9616e-05 - mae: 0.0035\n",
      "Epoch 758: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.9634e-05 - mae: 0.0035 - val_loss: 4.0332e-05 - val_mae: 0.0052\n",
      "Epoch 759/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0864e-05 - mae: 0.0036\n",
      "Epoch 759: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 2.0482e-05 - mae: 0.0036 - val_loss: 3.4732e-05 - val_mae: 0.0045\n",
      "Epoch 760/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8748e-05 - mae: 0.0035\n",
      "Epoch 760: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.8601e-05 - mae: 0.0035 - val_loss: 3.3285e-05 - val_mae: 0.0045\n",
      "Epoch 761/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7335e-05 - mae: 0.0034\n",
      "Epoch 761: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7485e-05 - mae: 0.0034 - val_loss: 3.4007e-05 - val_mae: 0.0045\n",
      "Epoch 762/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5428e-05 - mae: 0.0031\n",
      "Epoch 762: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7354e-05 - mae: 0.0033 - val_loss: 4.2585e-05 - val_mae: 0.0051\n",
      "Epoch 763/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0261e-05 - mae: 0.0037\n",
      "Epoch 763: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 2.0525e-05 - mae: 0.0037 - val_loss: 4.0083e-05 - val_mae: 0.0052\n",
      "Epoch 764/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6929e-05 - mae: 0.0033\n",
      "Epoch 764: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7629e-05 - mae: 0.0033 - val_loss: 3.3480e-05 - val_mae: 0.0045\n",
      "Epoch 765/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7605e-05 - mae: 0.0034\n",
      "Epoch 765: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7655e-05 - mae: 0.0034 - val_loss: 3.4410e-05 - val_mae: 0.0047\n",
      "Epoch 766/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7343e-05 - mae: 0.0033\n",
      "Epoch 766: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7343e-05 - mae: 0.0033 - val_loss: 4.9558e-05 - val_mae: 0.0058\n",
      "Epoch 767/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.8540e-05 - mae: 0.0034\n",
      "Epoch 767: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.8452e-05 - mae: 0.0034 - val_loss: 3.5130e-05 - val_mae: 0.0046\n",
      "Epoch 768/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 2.2725e-05 - mae: 0.0039\n",
      "Epoch 768: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 2.3819e-05 - mae: 0.0039 - val_loss: 3.6695e-05 - val_mae: 0.0047\n",
      "Epoch 769/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.7966e-05 - mae: 0.0042\n",
      "Epoch 769: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 2.4134e-05 - mae: 0.0039 - val_loss: 3.4252e-05 - val_mae: 0.0046\n",
      "Epoch 770/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7286e-05 - mae: 0.0034\n",
      "Epoch 770: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7873e-05 - mae: 0.0034 - val_loss: 4.6679e-05 - val_mae: 0.0054\n",
      "Epoch 771/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6978e-05 - mae: 0.0033\n",
      "Epoch 771: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7152e-05 - mae: 0.0033 - val_loss: 3.3540e-05 - val_mae: 0.0046\n",
      "Epoch 772/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.8562e-05 - mae: 0.0034\n",
      "Epoch 772: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8552e-05 - mae: 0.0034 - val_loss: 3.5750e-05 - val_mae: 0.0048\n",
      "Epoch 773/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7274e-05 - mae: 0.0033\n",
      "Epoch 773: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.8445e-05 - mae: 0.0034 - val_loss: 3.3596e-05 - val_mae: 0.0046\n",
      "Epoch 774/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7096e-05 - mae: 0.0033\n",
      "Epoch 774: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7024e-05 - mae: 0.0033 - val_loss: 3.7309e-05 - val_mae: 0.0050\n",
      "Epoch 775/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/800 [============================>.] - ETA: 0s - loss: 1.7779e-05 - mae: 0.0034\n",
      "Epoch 775: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8201e-05 - mae: 0.0034 - val_loss: 4.2742e-05 - val_mae: 0.0054\n",
      "Epoch 776/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7644e-05 - mae: 0.0034\n",
      "Epoch 776: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7869e-05 - mae: 0.0034 - val_loss: 4.0567e-05 - val_mae: 0.0052\n",
      "Epoch 777/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7449e-05 - mae: 0.0033\n",
      "Epoch 777: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7536e-05 - mae: 0.0034 - val_loss: 3.4308e-05 - val_mae: 0.0047\n",
      "Epoch 778/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7839e-05 - mae: 0.0034\n",
      "Epoch 778: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7839e-05 - mae: 0.0034 - val_loss: 3.3452e-05 - val_mae: 0.0045\n",
      "Epoch 779/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8514e-05 - mae: 0.0035\n",
      "Epoch 779: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.8883e-05 - mae: 0.0035 - val_loss: 3.7087e-05 - val_mae: 0.0049\n",
      "Epoch 780/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6886e-05 - mae: 0.0033\n",
      "Epoch 780: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7030e-05 - mae: 0.0033 - val_loss: 3.3743e-05 - val_mae: 0.0046\n",
      "Epoch 781/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9762e-05 - mae: 0.0036\n",
      "Epoch 781: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9519e-05 - mae: 0.0035 - val_loss: 4.7690e-05 - val_mae: 0.0057\n",
      "Epoch 782/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7762e-05 - mae: 0.0034\n",
      "Epoch 782: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.8105e-05 - mae: 0.0034 - val_loss: 3.3873e-05 - val_mae: 0.0045\n",
      "Epoch 783/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.8769e-05 - mae: 0.0035\n",
      "Epoch 783: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.8473e-05 - mae: 0.0034 - val_loss: 5.1495e-05 - val_mae: 0.0059\n",
      "Epoch 784/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.1286e-05 - mae: 0.0037\n",
      "Epoch 784: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 2.1286e-05 - mae: 0.0037 - val_loss: 4.2141e-05 - val_mae: 0.0053\n",
      "Epoch 785/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9496e-05 - mae: 0.0035\n",
      "Epoch 785: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 2.0448e-05 - mae: 0.0036 - val_loss: 3.3474e-05 - val_mae: 0.0045\n",
      "Epoch 786/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6752e-05 - mae: 0.0032\n",
      "Epoch 786: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.6607e-05 - mae: 0.0032 - val_loss: 3.8158e-05 - val_mae: 0.0050\n",
      "Epoch 787/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.8664e-05 - mae: 0.0034\n",
      "Epoch 787: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8959e-05 - mae: 0.0035 - val_loss: 3.4851e-05 - val_mae: 0.0046\n",
      "Epoch 788/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9826e-05 - mae: 0.0035\n",
      "Epoch 788: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.9826e-05 - mae: 0.0035 - val_loss: 3.7296e-05 - val_mae: 0.0050\n",
      "Epoch 789/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8942e-05 - mae: 0.0035\n",
      "Epoch 789: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8870e-05 - mae: 0.0035 - val_loss: 3.9865e-05 - val_mae: 0.0052\n",
      "Epoch 790/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7060e-05 - mae: 0.0033\n",
      "Epoch 790: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 1.7397e-05 - mae: 0.0033 - val_loss: 3.3808e-05 - val_mae: 0.0046\n",
      "Epoch 791/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.9442e-05 - mae: 0.0035\n",
      "Epoch 791: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 1.9173e-05 - mae: 0.0035 - val_loss: 3.3674e-05 - val_mae: 0.0046\n",
      "Epoch 792/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6168e-05 - mae: 0.0032\n",
      "Epoch 792: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7023e-05 - mae: 0.0033 - val_loss: 3.3546e-05 - val_mae: 0.0046\n",
      "Epoch 793/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7510e-05 - mae: 0.0034\n",
      "Epoch 793: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7031e-05 - mae: 0.0033 - val_loss: 3.6276e-05 - val_mae: 0.0049\n",
      "Epoch 794/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7240e-05 - mae: 0.0033\n",
      "Epoch 794: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7240e-05 - mae: 0.0033 - val_loss: 3.4080e-05 - val_mae: 0.0046\n",
      "Epoch 795/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9350e-05 - mae: 0.0035\n",
      "Epoch 795: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9350e-05 - mae: 0.0035 - val_loss: 4.1014e-05 - val_mae: 0.0053\n",
      "Epoch 796/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6841e-05 - mae: 0.0032\n",
      "Epoch 796: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.6784e-05 - mae: 0.0032 - val_loss: 3.3450e-05 - val_mae: 0.0046\n",
      "Epoch 797/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9297e-05 - mae: 0.0035\n",
      "Epoch 797: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.9076e-05 - mae: 0.0035 - val_loss: 3.3152e-05 - val_mae: 0.0045\n",
      "Epoch 798/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5292e-05 - mae: 0.0031\n",
      "Epoch 798: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.6696e-05 - mae: 0.0032 - val_loss: 3.9451e-05 - val_mae: 0.0051\n",
      "Epoch 799/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6989e-05 - mae: 0.0033\n",
      "Epoch 799: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.6903e-05 - mae: 0.0033 - val_loss: 3.9681e-05 - val_mae: 0.0049\n",
      "Epoch 800/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8572e-05 - mae: 0.0035\n",
      "Epoch 800: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8482e-05 - mae: 0.0034 - val_loss: 3.3660e-05 - val_mae: 0.0046\n",
      "Epoch 801/1000\n",
      "470/800 [================>.............] - ETA: 0s - loss: 1.9005e-05 - mae: 0.0035\n",
      "Epoch 801: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 1.8669e-05 - mae: 0.0034 - val_loss: 3.3939e-05 - val_mae: 0.0045\n",
      "Epoch 802/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.7484e-05 - mae: 0.0033\n",
      "Epoch 802: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7803e-05 - mae: 0.0034 - val_loss: 3.4596e-05 - val_mae: 0.0047\n",
      "Epoch 803/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7422e-05 - mae: 0.0034\n",
      "Epoch 803: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.8046e-05 - mae: 0.0034 - val_loss: 3.5826e-05 - val_mae: 0.0046\n",
      "Epoch 804/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/800 [=================>............] - ETA: 0s - loss: 1.9781e-05 - mae: 0.0035\n",
      "Epoch 804: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.8611e-05 - mae: 0.0034 - val_loss: 3.6593e-05 - val_mae: 0.0047\n",
      "Epoch 805/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.6966e-05 - mae: 0.0033\n",
      "Epoch 805: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.6842e-05 - mae: 0.0033 - val_loss: 3.3450e-05 - val_mae: 0.0045\n",
      "Epoch 806/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.5523e-05 - mae: 0.0032\n",
      "Epoch 806: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.7507e-05 - mae: 0.0034 - val_loss: 3.6276e-05 - val_mae: 0.0049\n",
      "Epoch 807/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.7306e-05 - mae: 0.0033\n",
      "Epoch 807: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 2.0199e-05 - mae: 0.0036 - val_loss: 3.3556e-05 - val_mae: 0.0046\n",
      "Epoch 808/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.7331e-05 - mae: 0.0033\n",
      "Epoch 808: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.7096e-05 - mae: 0.0033 - val_loss: 4.1808e-05 - val_mae: 0.0053\n",
      "Epoch 809/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8255e-05 - mae: 0.0034\n",
      "Epoch 809: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.7398e-05 - mae: 0.0033 - val_loss: 3.3607e-05 - val_mae: 0.0046\n",
      "Epoch 810/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8994e-05 - mae: 0.0035\n",
      "Epoch 810: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.9994e-05 - mae: 0.0035 - val_loss: 3.3648e-05 - val_mae: 0.0046\n",
      "Epoch 811/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.0661e-05 - mae: 0.0036\n",
      "Epoch 811: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 2.0946e-05 - mae: 0.0037 - val_loss: 6.7958e-05 - val_mae: 0.0067\n",
      "Epoch 812/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.3971e-05 - mae: 0.0039\n",
      "Epoch 812: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 2.4286e-05 - mae: 0.0040 - val_loss: 3.3795e-05 - val_mae: 0.0045\n",
      "Epoch 813/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6023e-05 - mae: 0.0032\n",
      "Epoch 813: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.8837e-05 - mae: 0.0034 - val_loss: 3.3880e-05 - val_mae: 0.0046\n",
      "Epoch 814/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 2.1374e-05 - mae: 0.0037\n",
      "Epoch 814: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.9661e-05 - mae: 0.0035 - val_loss: 3.4522e-05 - val_mae: 0.0047\n",
      "Epoch 815/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.8982e-05 - mae: 0.0034\n",
      "Epoch 815: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.8094e-05 - mae: 0.0033 - val_loss: 3.3765e-05 - val_mae: 0.0046\n",
      "Epoch 816/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 1.8267e-05 - mae: 0.0034\n",
      "Epoch 816: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.7274e-05 - mae: 0.0033 - val_loss: 3.4367e-05 - val_mae: 0.0045\n",
      "Epoch 817/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7919e-05 - mae: 0.0034\n",
      "Epoch 817: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.8048e-05 - mae: 0.0034 - val_loss: 3.4754e-05 - val_mae: 0.0047\n",
      "Epoch 818/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.7501e-05 - mae: 0.0034\n",
      "Epoch 818: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.7250e-05 - mae: 0.0033 - val_loss: 3.4044e-05 - val_mae: 0.0046\n",
      "Epoch 819/1000\n",
      "520/800 [==================>...........] - ETA: 0s - loss: 1.9415e-05 - mae: 0.0035\n",
      "Epoch 819: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.9348e-05 - mae: 0.0035 - val_loss: 3.3542e-05 - val_mae: 0.0045\n",
      "Epoch 820/1000\n",
      "550/800 [===================>..........] - ETA: 0s - loss: 2.0259e-05 - mae: 0.0035\n",
      "Epoch 820: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 2.1211e-05 - mae: 0.0037 - val_loss: 3.3846e-05 - val_mae: 0.0046\n",
      "Epoch 821/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.6928e-05 - mae: 0.0033\n",
      "Epoch 821: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.8492e-05 - mae: 0.0035 - val_loss: 5.4140e-05 - val_mae: 0.0061\n",
      "Epoch 822/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8980e-05 - mae: 0.0035\n",
      "Epoch 822: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.8989e-05 - mae: 0.0035 - val_loss: 3.5648e-05 - val_mae: 0.0048\n",
      "Epoch 823/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 2.0586e-05 - mae: 0.0037\n",
      "Epoch 823: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.9664e-05 - mae: 0.0035 - val_loss: 3.3726e-05 - val_mae: 0.0045\n",
      "Epoch 824/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.6684e-05 - mae: 0.0033\n",
      "Epoch 824: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.6743e-05 - mae: 0.0033 - val_loss: 3.8149e-05 - val_mae: 0.0050\n",
      "Epoch 825/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6064e-05 - mae: 0.0032\n",
      "Epoch 825: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6467e-05 - mae: 0.0032 - val_loss: 3.4453e-05 - val_mae: 0.0047\n",
      "Epoch 826/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6348e-05 - mae: 0.0032\n",
      "Epoch 826: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7789e-05 - mae: 0.0033 - val_loss: 3.5202e-05 - val_mae: 0.0048\n",
      "Epoch 827/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6969e-05 - mae: 0.0032\n",
      "Epoch 827: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7774e-05 - mae: 0.0034 - val_loss: 3.4916e-05 - val_mae: 0.0046\n",
      "Epoch 828/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.6316e-05 - mae: 0.0032\n",
      "Epoch 828: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.8556e-05 - mae: 0.0034 - val_loss: 3.6944e-05 - val_mae: 0.0049\n",
      "Epoch 829/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5713e-05 - mae: 0.0032\n",
      "Epoch 829: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6735e-05 - mae: 0.0032 - val_loss: 3.3606e-05 - val_mae: 0.0045\n",
      "Epoch 830/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7190e-05 - mae: 0.0033\n",
      "Epoch 830: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 1.7741e-05 - mae: 0.0034 - val_loss: 4.0092e-05 - val_mae: 0.0052\n",
      "Epoch 831/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5183e-05 - mae: 0.0031\n",
      "Epoch 831: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.6447e-05 - mae: 0.0032 - val_loss: 3.3372e-05 - val_mae: 0.0046\n",
      "Epoch 832/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.7203e-05 - mae: 0.0033\n",
      "Epoch 832: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.6849e-05 - mae: 0.0032 - val_loss: 3.7095e-05 - val_mae: 0.0047\n",
      "Epoch 833/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/800 [=================>............] - ETA: 0s - loss: 1.8567e-05 - mae: 0.0035\n",
      "Epoch 833: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.7489e-05 - mae: 0.0033 - val_loss: 3.3487e-05 - val_mae: 0.0045\n",
      "Epoch 834/1000\n",
      "480/800 [=================>............] - ETA: 0s - loss: 1.9314e-05 - mae: 0.0036\n",
      "Epoch 834: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 1.9071e-05 - mae: 0.0035 - val_loss: 3.3920e-05 - val_mae: 0.0047\n",
      "Epoch 835/1000\n",
      "500/800 [=================>............] - ETA: 0s - loss: 1.6341e-05 - mae: 0.0032\n",
      "Epoch 835: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.6805e-05 - mae: 0.0033 - val_loss: 3.3543e-05 - val_mae: 0.0046\n",
      "Epoch 836/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.6926e-05 - mae: 0.0032\n",
      "Epoch 836: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.8563e-05 - mae: 0.0034 - val_loss: 3.3414e-05 - val_mae: 0.0045\n",
      "Epoch 837/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.7448e-05 - mae: 0.0033\n",
      "Epoch 837: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.7235e-05 - mae: 0.0033 - val_loss: 3.3580e-05 - val_mae: 0.0046\n",
      "Epoch 838/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.5421e-05 - mae: 0.0031\n",
      "Epoch 838: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7691e-05 - mae: 0.0033 - val_loss: 3.5979e-05 - val_mae: 0.0048\n",
      "Epoch 839/1000\n",
      "530/800 [==================>...........] - ETA: 0s - loss: 1.5937e-05 - mae: 0.0032\n",
      "Epoch 839: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.7007e-05 - mae: 0.0033 - val_loss: 3.4996e-05 - val_mae: 0.0046\n",
      "Epoch 840/1000\n",
      "510/800 [==================>...........] - ETA: 0s - loss: 1.8436e-05 - mae: 0.0034\n",
      "Epoch 840: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.7166e-05 - mae: 0.0033 - val_loss: 3.4931e-05 - val_mae: 0.0046\n",
      "Epoch 841/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.9011e-05 - mae: 0.0035\n",
      "Epoch 841: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 1.8471e-05 - mae: 0.0035 - val_loss: 3.3910e-05 - val_mae: 0.0046\n",
      "Epoch 842/1000\n",
      "490/800 [=================>............] - ETA: 0s - loss: 1.6568e-05 - mae: 0.0033\n",
      "Epoch 842: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.7823e-05 - mae: 0.0034 - val_loss: 3.3973e-05 - val_mae: 0.0045\n",
      "Epoch 843/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6794e-05 - mae: 0.0032\n",
      "Epoch 843: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.9604e-05 - mae: 0.0035 - val_loss: 4.7736e-05 - val_mae: 0.0054\n",
      "Epoch 844/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8160e-05 - mae: 0.0034\n",
      "Epoch 844: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8115e-05 - mae: 0.0034 - val_loss: 3.4770e-05 - val_mae: 0.0046\n",
      "Epoch 845/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5861e-05 - mae: 0.0031\n",
      "Epoch 845: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7012e-05 - mae: 0.0033 - val_loss: 4.3659e-05 - val_mae: 0.0054\n",
      "Epoch 846/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 2.1934e-05 - mae: 0.0037\n",
      "Epoch 846: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 2.1278e-05 - mae: 0.0037 - val_loss: 3.3753e-05 - val_mae: 0.0046\n",
      "Epoch 847/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7221e-05 - mae: 0.0033\n",
      "Epoch 847: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.7990e-05 - mae: 0.0034 - val_loss: 4.6206e-05 - val_mae: 0.0056\n",
      "Epoch 848/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8488e-05 - mae: 0.0034\n",
      "Epoch 848: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8914e-05 - mae: 0.0035 - val_loss: 3.4881e-05 - val_mae: 0.0047\n",
      "Epoch 849/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 2.0239e-05 - mae: 0.0036\n",
      "Epoch 849: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.9346e-05 - mae: 0.0035 - val_loss: 3.4687e-05 - val_mae: 0.0046\n",
      "Epoch 850/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6815e-05 - mae: 0.0033\n",
      "Epoch 850: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.7093e-05 - mae: 0.0033 - val_loss: 3.3794e-05 - val_mae: 0.0046\n",
      "Epoch 851/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7192e-05 - mae: 0.0033\n",
      "Epoch 851: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.6910e-05 - mae: 0.0033 - val_loss: 3.4034e-05 - val_mae: 0.0046\n",
      "Epoch 852/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.6851e-05 - mae: 0.0033\n",
      "Epoch 852: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.6808e-05 - mae: 0.0032 - val_loss: 3.3585e-05 - val_mae: 0.0045\n",
      "Epoch 853/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7687e-05 - mae: 0.0034\n",
      "Epoch 853: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7779e-05 - mae: 0.0034 - val_loss: 3.3567e-05 - val_mae: 0.0046\n",
      "Epoch 854/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.6892e-05 - mae: 0.0033\n",
      "Epoch 854: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.7377e-05 - mae: 0.0033 - val_loss: 3.3352e-05 - val_mae: 0.0045\n",
      "Epoch 855/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8005e-05 - mae: 0.0034\n",
      "Epoch 855: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.7923e-05 - mae: 0.0034 - val_loss: 3.6908e-05 - val_mae: 0.0047\n",
      "Epoch 856/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7600e-05 - mae: 0.0033\n",
      "Epoch 856: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.7675e-05 - mae: 0.0034 - val_loss: 3.3390e-05 - val_mae: 0.0045\n",
      "Epoch 857/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.6251e-05 - mae: 0.0032\n",
      "Epoch 857: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.6880e-05 - mae: 0.0033 - val_loss: 3.4078e-05 - val_mae: 0.0047\n",
      "Epoch 858/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8951e-05 - mae: 0.0035\n",
      "Epoch 858: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8893e-05 - mae: 0.0034 - val_loss: 3.3452e-05 - val_mae: 0.0046\n",
      "Epoch 859/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6309e-05 - mae: 0.0032\n",
      "Epoch 859: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.6465e-05 - mae: 0.0032 - val_loss: 3.5073e-05 - val_mae: 0.0046\n",
      "Epoch 860/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.8785e-05 - mae: 0.0034\n",
      "Epoch 860: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.8379e-05 - mae: 0.0034 - val_loss: 3.7271e-05 - val_mae: 0.0047\n",
      "Epoch 861/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9395e-05 - mae: 0.0035\n",
      "Epoch 861: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.9692e-05 - mae: 0.0035 - val_loss: 3.4565e-05 - val_mae: 0.0047\n",
      "Epoch 862/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6890e-05 - mae: 0.0033\n",
      "Epoch 862: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.6945e-05 - mae: 0.0033 - val_loss: 3.3801e-05 - val_mae: 0.0046\n",
      "Epoch 863/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7630e-05 - mae: 0.0034\n",
      "Epoch 863: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.7788e-05 - mae: 0.0034 - val_loss: 3.6037e-05 - val_mae: 0.0046\n",
      "Epoch 864/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.7914e-05 - mae: 0.0034\n",
      "Epoch 864: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 1.7862e-05 - mae: 0.0033 - val_loss: 4.0408e-05 - val_mae: 0.0049\n",
      "Epoch 865/1000\n",
      "610/800 [=====================>........] - ETA: 0s - loss: 1.8169e-05 - mae: 0.0034\n",
      "Epoch 865: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 199us/sample - loss: 1.8451e-05 - mae: 0.0034 - val_loss: 3.8074e-05 - val_mae: 0.0050\n",
      "Epoch 866/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.6712e-05 - mae: 0.0032\n",
      "Epoch 866: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.7151e-05 - mae: 0.0033 - val_loss: 3.5299e-05 - val_mae: 0.0046\n",
      "Epoch 867/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.8113e-05 - mae: 0.0034\n",
      "Epoch 867: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.9111e-05 - mae: 0.0035 - val_loss: 3.3391e-05 - val_mae: 0.0045\n",
      "Epoch 868/1000\n",
      "540/800 [===================>..........] - ETA: 0s - loss: 1.8832e-05 - mae: 0.0035\n",
      "Epoch 868: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 215us/sample - loss: 1.7730e-05 - mae: 0.0034 - val_loss: 3.5160e-05 - val_mae: 0.0048\n",
      "Epoch 869/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7260e-05 - mae: 0.0033\n",
      "Epoch 869: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.8068e-05 - mae: 0.0034 - val_loss: 4.0625e-05 - val_mae: 0.0052\n",
      "Epoch 870/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.7922e-05 - mae: 0.0034\n",
      "Epoch 870: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 1.8384e-05 - mae: 0.0034 - val_loss: 3.9850e-05 - val_mae: 0.0052\n",
      "Epoch 871/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6762e-05 - mae: 0.0032\n",
      "Epoch 871: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7076e-05 - mae: 0.0033 - val_loss: 3.3178e-05 - val_mae: 0.0045\n",
      "Epoch 872/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7279e-05 - mae: 0.0033\n",
      "Epoch 872: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.8434e-05 - mae: 0.0034 - val_loss: 3.3228e-05 - val_mae: 0.0045\n",
      "Epoch 873/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8374e-05 - mae: 0.0034\n",
      "Epoch 873: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.8490e-05 - mae: 0.0034 - val_loss: 3.3174e-05 - val_mae: 0.0045\n",
      "Epoch 874/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7161e-05 - mae: 0.0033\n",
      "Epoch 874: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.7630e-05 - mae: 0.0033 - val_loss: 3.3628e-05 - val_mae: 0.0046\n",
      "Epoch 875/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7848e-05 - mae: 0.0033\n",
      "Epoch 875: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.7873e-05 - mae: 0.0034 - val_loss: 3.3140e-05 - val_mae: 0.0045\n",
      "Epoch 876/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5770e-05 - mae: 0.0032\n",
      "Epoch 876: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6948e-05 - mae: 0.0033 - val_loss: 3.9628e-05 - val_mae: 0.0052\n",
      "Epoch 877/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.8849e-05 - mae: 0.0035\n",
      "Epoch 877: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.7353e-05 - mae: 0.0034 - val_loss: 3.4021e-05 - val_mae: 0.0046\n",
      "Epoch 878/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.7705e-05 - mae: 0.0034\n",
      "Epoch 878: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 1.7563e-05 - mae: 0.0034 - val_loss: 3.5030e-05 - val_mae: 0.0046\n",
      "Epoch 879/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5947e-05 - mae: 0.0032\n",
      "Epoch 879: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7359e-05 - mae: 0.0033 - val_loss: 3.6646e-05 - val_mae: 0.0049\n",
      "Epoch 880/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.1861e-05 - mae: 0.0037\n",
      "Epoch 880: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9300e-05 - mae: 0.0035 - val_loss: 3.5460e-05 - val_mae: 0.0046\n",
      "Epoch 881/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.5831e-05 - mae: 0.0032\n",
      "Epoch 881: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6470e-05 - mae: 0.0032 - val_loss: 3.4598e-05 - val_mae: 0.0047\n",
      "Epoch 882/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9306e-05 - mae: 0.0035\n",
      "Epoch 882: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.8031e-05 - mae: 0.0034 - val_loss: 3.3546e-05 - val_mae: 0.0045\n",
      "Epoch 883/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6072e-05 - mae: 0.0032\n",
      "Epoch 883: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.6762e-05 - mae: 0.0033 - val_loss: 3.3596e-05 - val_mae: 0.0045\n",
      "Epoch 884/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7746e-05 - mae: 0.0034\n",
      "Epoch 884: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.7463e-05 - mae: 0.0033 - val_loss: 3.3975e-05 - val_mae: 0.0046\n",
      "Epoch 885/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0903e-05 - mae: 0.0037\n",
      "Epoch 885: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9900e-05 - mae: 0.0036 - val_loss: 3.4306e-05 - val_mae: 0.0046\n",
      "Epoch 886/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6971e-05 - mae: 0.0033\n",
      "Epoch 886: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7197e-05 - mae: 0.0033 - val_loss: 3.4035e-05 - val_mae: 0.0046\n",
      "Epoch 887/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7045e-05 - mae: 0.0033\n",
      "Epoch 887: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7301e-05 - mae: 0.0033 - val_loss: 3.3646e-05 - val_mae: 0.0046\n",
      "Epoch 888/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 2.1504e-05 - mae: 0.0037\n",
      "Epoch 888: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 2.0182e-05 - mae: 0.0036 - val_loss: 3.3979e-05 - val_mae: 0.0046\n",
      "Epoch 889/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.7131e-05 - mae: 0.0033\n",
      "Epoch 889: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.7406e-05 - mae: 0.0033 - val_loss: 3.6256e-05 - val_mae: 0.0049\n",
      "Epoch 890/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.8743e-05 - mae: 0.0035\n",
      "Epoch 890: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 169us/sample - loss: 1.8596e-05 - mae: 0.0035 - val_loss: 3.4224e-05 - val_mae: 0.0047\n",
      "Epoch 891/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/800 [========================>.....] - ETA: 0s - loss: 1.9097e-05 - mae: 0.0035\n",
      "Epoch 891: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.9368e-05 - mae: 0.0035 - val_loss: 3.7447e-05 - val_mae: 0.0050\n",
      "Epoch 892/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.7184e-05 - mae: 0.0033\n",
      "Epoch 892: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.7149e-05 - mae: 0.0033 - val_loss: 3.3990e-05 - val_mae: 0.0045\n",
      "Epoch 893/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7366e-05 - mae: 0.0033\n",
      "Epoch 893: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.7478e-05 - mae: 0.0033 - val_loss: 4.0980e-05 - val_mae: 0.0053\n",
      "Epoch 894/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.7825e-05 - mae: 0.0034\n",
      "Epoch 894: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 185us/sample - loss: 1.7615e-05 - mae: 0.0033 - val_loss: 3.3821e-05 - val_mae: 0.0046\n",
      "Epoch 895/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.8663e-05 - mae: 0.0034\n",
      "Epoch 895: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.8275e-05 - mae: 0.0034 - val_loss: 3.3955e-05 - val_mae: 0.0046\n",
      "Epoch 896/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 2.2174e-05 - mae: 0.0038\n",
      "Epoch 896: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 2.2174e-05 - mae: 0.0038 - val_loss: 4.4478e-05 - val_mae: 0.0052\n",
      "Epoch 897/1000\n",
      "580/800 [====================>.........] - ETA: 0s - loss: 1.7878e-05 - mae: 0.0034\n",
      "Epoch 897: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 205us/sample - loss: 1.7750e-05 - mae: 0.0034 - val_loss: 3.5178e-05 - val_mae: 0.0046\n",
      "Epoch 898/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7298e-05 - mae: 0.0033\n",
      "Epoch 898: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.8646e-05 - mae: 0.0034 - val_loss: 3.4528e-05 - val_mae: 0.0046\n",
      "Epoch 899/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7265e-05 - mae: 0.0033\n",
      "Epoch 899: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 178us/sample - loss: 1.7190e-05 - mae: 0.0033 - val_loss: 3.3907e-05 - val_mae: 0.0046\n",
      "Epoch 900/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.6884e-05 - mae: 0.0033\n",
      "Epoch 900: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 191us/sample - loss: 1.7409e-05 - mae: 0.0034 - val_loss: 3.4142e-05 - val_mae: 0.0045\n",
      "Epoch 901/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8879e-05 - mae: 0.0035\n",
      "Epoch 901: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.8685e-05 - mae: 0.0035 - val_loss: 3.8921e-05 - val_mae: 0.0051\n",
      "Epoch 902/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7803e-05 - mae: 0.0034\n",
      "Epoch 902: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.7803e-05 - mae: 0.0034 - val_loss: 3.3819e-05 - val_mae: 0.0046\n",
      "Epoch 903/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7262e-05 - mae: 0.0033\n",
      "Epoch 903: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 1.6977e-05 - mae: 0.0033 - val_loss: 3.3304e-05 - val_mae: 0.0045\n",
      "Epoch 904/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.7960e-05 - mae: 0.0034\n",
      "Epoch 904: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.8124e-05 - mae: 0.0034 - val_loss: 3.3236e-05 - val_mae: 0.0045\n",
      "Epoch 905/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7717e-05 - mae: 0.0033\n",
      "Epoch 905: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 1.7978e-05 - mae: 0.0033 - val_loss: 3.3332e-05 - val_mae: 0.0045\n",
      "Epoch 906/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7994e-05 - mae: 0.0034\n",
      "Epoch 906: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8368e-05 - mae: 0.0035 - val_loss: 3.3715e-05 - val_mae: 0.0045\n",
      "Epoch 907/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6733e-05 - mae: 0.0032\n",
      "Epoch 907: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7896e-05 - mae: 0.0034 - val_loss: 3.7789e-05 - val_mae: 0.0050\n",
      "Epoch 908/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.7174e-05 - mae: 0.0033\n",
      "Epoch 908: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7233e-05 - mae: 0.0033 - val_loss: 3.3646e-05 - val_mae: 0.0045\n",
      "Epoch 909/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7052e-05 - mae: 0.0033\n",
      "Epoch 909: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.6930e-05 - mae: 0.0033 - val_loss: 3.3217e-05 - val_mae: 0.0045\n",
      "Epoch 910/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7376e-05 - mae: 0.0033\n",
      "Epoch 910: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 1.7319e-05 - mae: 0.0033 - val_loss: 3.3925e-05 - val_mae: 0.0046\n",
      "Epoch 911/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7851e-05 - mae: 0.0034\n",
      "Epoch 911: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 200us/sample - loss: 1.8059e-05 - mae: 0.0034 - val_loss: 3.3429e-05 - val_mae: 0.0045\n",
      "Epoch 912/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.9791e-05 - mae: 0.0036\n",
      "Epoch 912: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.9646e-05 - mae: 0.0035 - val_loss: 3.4619e-05 - val_mae: 0.0047\n",
      "Epoch 913/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.6677e-05 - mae: 0.0032\n",
      "Epoch 913: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.6677e-05 - mae: 0.0032 - val_loss: 3.3656e-05 - val_mae: 0.0045\n",
      "Epoch 914/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.9562e-05 - mae: 0.0035\n",
      "Epoch 914: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 2.0061e-05 - mae: 0.0036 - val_loss: 3.9475e-05 - val_mae: 0.0051\n",
      "Epoch 915/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.8586e-05 - mae: 0.0034\n",
      "Epoch 915: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 176us/sample - loss: 1.7840e-05 - mae: 0.0034 - val_loss: 4.1022e-05 - val_mae: 0.0053\n",
      "Epoch 916/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.7578e-05 - mae: 0.0033\n",
      "Epoch 916: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.7206e-05 - mae: 0.0033 - val_loss: 3.6516e-05 - val_mae: 0.0049\n",
      "Epoch 917/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.7094e-05 - mae: 0.0033\n",
      "Epoch 917: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.6586e-05 - mae: 0.0032 - val_loss: 3.3654e-05 - val_mae: 0.0046\n",
      "Epoch 918/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5740e-05 - mae: 0.0032\n",
      "Epoch 918: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6447e-05 - mae: 0.0032 - val_loss: 3.5250e-05 - val_mae: 0.0048\n",
      "Epoch 919/1000\n",
      "780/800 [============================>.] - ETA: 0s - loss: 1.8936e-05 - mae: 0.0035\n",
      "Epoch 919: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.8620e-05 - mae: 0.0034 - val_loss: 3.6811e-05 - val_mae: 0.0049\n",
      "Epoch 920/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/800 [===========================>..] - ETA: 0s - loss: 1.9116e-05 - mae: 0.0035\n",
      "Epoch 920: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.8994e-05 - mae: 0.0035 - val_loss: 3.5090e-05 - val_mae: 0.0047\n",
      "Epoch 921/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8328e-05 - mae: 0.0034\n",
      "Epoch 921: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 1.7635e-05 - mae: 0.0033 - val_loss: 3.3738e-05 - val_mae: 0.0045\n",
      "Epoch 922/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.9118e-05 - mae: 0.0035\n",
      "Epoch 922: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.9118e-05 - mae: 0.0035 - val_loss: 3.9719e-05 - val_mae: 0.0052\n",
      "Epoch 923/1000\n",
      "660/800 [=======================>......] - ETA: 0s - loss: 1.7947e-05 - mae: 0.0034\n",
      "Epoch 923: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 183us/sample - loss: 1.8140e-05 - mae: 0.0034 - val_loss: 4.3061e-05 - val_mae: 0.0054\n",
      "Epoch 924/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.8362e-05 - mae: 0.0034\n",
      "Epoch 924: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.8147e-05 - mae: 0.0034 - val_loss: 3.3672e-05 - val_mae: 0.0046\n",
      "Epoch 925/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8681e-05 - mae: 0.0034\n",
      "Epoch 925: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 170us/sample - loss: 1.8980e-05 - mae: 0.0034 - val_loss: 4.3140e-05 - val_mae: 0.0054\n",
      "Epoch 926/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 2.1215e-05 - mae: 0.0037\n",
      "Epoch 926: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 2.0973e-05 - mae: 0.0037 - val_loss: 3.3938e-05 - val_mae: 0.0046\n",
      "Epoch 927/1000\n",
      "680/800 [========================>.....] - ETA: 0s - loss: 1.6626e-05 - mae: 0.0033\n",
      "Epoch 927: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 173us/sample - loss: 1.6666e-05 - mae: 0.0033 - val_loss: 3.3800e-05 - val_mae: 0.0046\n",
      "Epoch 928/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.5908e-05 - mae: 0.0032\n",
      "Epoch 928: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6438e-05 - mae: 0.0032 - val_loss: 3.7078e-05 - val_mae: 0.0049\n",
      "Epoch 929/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.7887e-05 - mae: 0.0034\n",
      "Epoch 929: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.7730e-05 - mae: 0.0034 - val_loss: 3.3508e-05 - val_mae: 0.0046\n",
      "Epoch 930/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.5981e-05 - mae: 0.0032\n",
      "Epoch 930: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6223e-05 - mae: 0.0032 - val_loss: 3.3685e-05 - val_mae: 0.0045\n",
      "Epoch 931/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7661e-05 - mae: 0.0034\n",
      "Epoch 931: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7590e-05 - mae: 0.0033 - val_loss: 3.3570e-05 - val_mae: 0.0046\n",
      "Epoch 932/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7097e-05 - mae: 0.0033\n",
      "Epoch 932: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.7369e-05 - mae: 0.0033 - val_loss: 3.7659e-05 - val_mae: 0.0048\n",
      "Epoch 933/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.9701e-05 - mae: 0.0035\n",
      "Epoch 933: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.9619e-05 - mae: 0.0035 - val_loss: 3.5354e-05 - val_mae: 0.0048\n",
      "Epoch 934/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.8130e-05 - mae: 0.0034\n",
      "Epoch 934: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.8851e-05 - mae: 0.0035 - val_loss: 5.6700e-05 - val_mae: 0.0062\n",
      "Epoch 935/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0210e-05 - mae: 0.0036\n",
      "Epoch 935: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.9726e-05 - mae: 0.0035 - val_loss: 3.4167e-05 - val_mae: 0.0045\n",
      "Epoch 936/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6790e-05 - mae: 0.0033\n",
      "Epoch 936: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6862e-05 - mae: 0.0033 - val_loss: 3.7316e-05 - val_mae: 0.0050\n",
      "Epoch 937/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8421e-05 - mae: 0.0034\n",
      "Epoch 937: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.8540e-05 - mae: 0.0034 - val_loss: 3.3390e-05 - val_mae: 0.0045\n",
      "Epoch 938/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.7144e-05 - mae: 0.0033\n",
      "Epoch 938: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 1.6861e-05 - mae: 0.0032 - val_loss: 3.4693e-05 - val_mae: 0.0046\n",
      "Epoch 939/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7474e-05 - mae: 0.0033\n",
      "Epoch 939: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.8753e-05 - mae: 0.0034 - val_loss: 4.0450e-05 - val_mae: 0.0052\n",
      "Epoch 940/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.7011e-05 - mae: 0.0033\n",
      "Epoch 940: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8362e-05 - mae: 0.0034 - val_loss: 4.3477e-05 - val_mae: 0.0054\n",
      "Epoch 941/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6202e-05 - mae: 0.0032\n",
      "Epoch 941: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.6361e-05 - mae: 0.0032 - val_loss: 3.7082e-05 - val_mae: 0.0047\n",
      "Epoch 942/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 2.0562e-05 - mae: 0.0037\n",
      "Epoch 942: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9925e-05 - mae: 0.0036 - val_loss: 3.7194e-05 - val_mae: 0.0047\n",
      "Epoch 943/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.9214e-05 - mae: 0.0035\n",
      "Epoch 943: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.8979e-05 - mae: 0.0035 - val_loss: 3.3268e-05 - val_mae: 0.0045\n",
      "Epoch 944/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.6433e-05 - mae: 0.0033\n",
      "Epoch 944: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.6586e-05 - mae: 0.0032 - val_loss: 3.4888e-05 - val_mae: 0.0047\n",
      "Epoch 945/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8847e-05 - mae: 0.0034\n",
      "Epoch 945: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.8011e-05 - mae: 0.0034 - val_loss: 3.4207e-05 - val_mae: 0.0045\n",
      "Epoch 946/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 2.0575e-05 - mae: 0.0036\n",
      "Epoch 946: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 2.0840e-05 - mae: 0.0036 - val_loss: 3.3557e-05 - val_mae: 0.0046\n",
      "Epoch 947/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.6659e-05 - mae: 0.0032\n",
      "Epoch 947: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 148us/sample - loss: 1.7258e-05 - mae: 0.0033 - val_loss: 3.4412e-05 - val_mae: 0.0047\n",
      "Epoch 948/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6977e-05 - mae: 0.0033\n",
      "Epoch 948: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.6947e-05 - mae: 0.0033 - val_loss: 3.3287e-05 - val_mae: 0.0045\n",
      "Epoch 949/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5533e-05 - mae: 0.0031\n",
      "Epoch 949: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.6562e-05 - mae: 0.0032 - val_loss: 3.3327e-05 - val_mae: 0.0045\n",
      "Epoch 950/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.6786e-05 - mae: 0.0032\n",
      "Epoch 950: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 1.7603e-05 - mae: 0.0033 - val_loss: 3.6409e-05 - val_mae: 0.0049\n",
      "Epoch 951/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8073e-05 - mae: 0.0034\n",
      "Epoch 951: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 171us/sample - loss: 1.8574e-05 - mae: 0.0035 - val_loss: 3.7748e-05 - val_mae: 0.0050\n",
      "Epoch 952/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.8635e-05 - mae: 0.0035\n",
      "Epoch 952: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.8681e-05 - mae: 0.0035 - val_loss: 3.7591e-05 - val_mae: 0.0050\n",
      "Epoch 953/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7422e-05 - mae: 0.0034\n",
      "Epoch 953: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 1.9523e-05 - mae: 0.0036 - val_loss: 4.3569e-05 - val_mae: 0.0054\n",
      "Epoch 954/1000\n",
      "670/800 [========================>.....] - ETA: 0s - loss: 1.7182e-05 - mae: 0.0033\n",
      "Epoch 954: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.7206e-05 - mae: 0.0033 - val_loss: 3.3353e-05 - val_mae: 0.0045\n",
      "Epoch 955/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.8116e-05 - mae: 0.0034\n",
      "Epoch 955: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 172us/sample - loss: 1.7844e-05 - mae: 0.0034 - val_loss: 3.3228e-05 - val_mae: 0.0045\n",
      "Epoch 956/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7868e-05 - mae: 0.0033\n",
      "Epoch 956: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 177us/sample - loss: 1.7657e-05 - mae: 0.0033 - val_loss: 3.3308e-05 - val_mae: 0.0045\n",
      "Epoch 957/1000\n",
      "640/800 [=======================>......] - ETA: 0s - loss: 1.9601e-05 - mae: 0.0035\n",
      "Epoch 957: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 181us/sample - loss: 1.9116e-05 - mae: 0.0035 - val_loss: 3.3229e-05 - val_mae: 0.0045\n",
      "Epoch 958/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.6649e-05 - mae: 0.0033\n",
      "Epoch 958: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 1.6772e-05 - mae: 0.0033 - val_loss: 3.3086e-05 - val_mae: 0.0045\n",
      "Epoch 959/1000\n",
      "740/800 [==========================>...] - ETA: 0s - loss: 1.6607e-05 - mae: 0.0032\n",
      "Epoch 959: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.6763e-05 - mae: 0.0032 - val_loss: 3.2952e-05 - val_mae: 0.0045\n",
      "Epoch 960/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.8082e-05 - mae: 0.0033\n",
      "Epoch 960: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8211e-05 - mae: 0.0034 - val_loss: 3.3600e-05 - val_mae: 0.0046\n",
      "Epoch 961/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.7877e-05 - mae: 0.0034\n",
      "Epoch 961: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7787e-05 - mae: 0.0034 - val_loss: 3.2781e-05 - val_mae: 0.0045\n",
      "Epoch 962/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6744e-05 - mae: 0.0033\n",
      "Epoch 962: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.7672e-05 - mae: 0.0033 - val_loss: 3.2957e-05 - val_mae: 0.0045\n",
      "Epoch 963/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.9368e-05 - mae: 0.0035\n",
      "Epoch 963: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8368e-05 - mae: 0.0034 - val_loss: 3.4229e-05 - val_mae: 0.0045\n",
      "Epoch 964/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 2.0232e-05 - mae: 0.0036\n",
      "Epoch 964: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.9941e-05 - mae: 0.0036 - val_loss: 3.5071e-05 - val_mae: 0.0048\n",
      "Epoch 965/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.6817e-05 - mae: 0.0033\n",
      "Epoch 965: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.6999e-05 - mae: 0.0033 - val_loss: 3.3206e-05 - val_mae: 0.0046\n",
      "Epoch 966/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8637e-05 - mae: 0.0034\n",
      "Epoch 966: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 1.8637e-05 - mae: 0.0034 - val_loss: 3.5810e-05 - val_mae: 0.0049\n",
      "Epoch 967/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.8371e-05 - mae: 0.0034\n",
      "Epoch 967: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.8371e-05 - mae: 0.0034 - val_loss: 3.9419e-05 - val_mae: 0.0052\n",
      "Epoch 968/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.9946e-05 - mae: 0.0036\n",
      "Epoch 968: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.8231e-05 - mae: 0.0034 - val_loss: 3.3721e-05 - val_mae: 0.0045\n",
      "Epoch 969/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.6009e-05 - mae: 0.0031\n",
      "Epoch 969: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.6728e-05 - mae: 0.0032 - val_loss: 3.3430e-05 - val_mae: 0.0045\n",
      "Epoch 970/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6927e-05 - mae: 0.0033\n",
      "Epoch 970: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.6630e-05 - mae: 0.0032 - val_loss: 3.3921e-05 - val_mae: 0.0046\n",
      "Epoch 971/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7490e-05 - mae: 0.0034\n",
      "Epoch 971: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 160us/sample - loss: 1.7670e-05 - mae: 0.0034 - val_loss: 3.3525e-05 - val_mae: 0.0046\n",
      "Epoch 972/1000\n",
      "730/800 [==========================>...] - ETA: 0s - loss: 1.8134e-05 - mae: 0.0034\n",
      "Epoch 972: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 167us/sample - loss: 1.7808e-05 - mae: 0.0034 - val_loss: 3.3359e-05 - val_mae: 0.0046\n",
      "Epoch 973/1000\n",
      "750/800 [===========================>..] - ETA: 0s - loss: 1.7274e-05 - mae: 0.0033\n",
      "Epoch 973: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 164us/sample - loss: 1.7355e-05 - mae: 0.0033 - val_loss: 3.3330e-05 - val_mae: 0.0046\n",
      "Epoch 974/1000\n",
      "720/800 [==========================>...] - ETA: 0s - loss: 1.6235e-05 - mae: 0.0032\n",
      "Epoch 974: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.6567e-05 - mae: 0.0032 - val_loss: 3.4463e-05 - val_mae: 0.0047\n",
      "Epoch 975/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7122e-05 - mae: 0.0033\n",
      "Epoch 975: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 1.7096e-05 - mae: 0.0033 - val_loss: 3.8958e-05 - val_mae: 0.0051\n",
      "Epoch 976/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7447e-05 - mae: 0.0033\n",
      "Epoch 976: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7340e-05 - mae: 0.0033 - val_loss: 3.3574e-05 - val_mae: 0.0046\n",
      "Epoch 977/1000\n",
      "770/800 [===========================>..] - ETA: 0s - loss: 1.7602e-05 - mae: 0.0033\n",
      "Epoch 977: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7836e-05 - mae: 0.0033 - val_loss: 3.4116e-05 - val_mae: 0.0045\n",
      "Epoch 978/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/800 [============================>.] - ETA: 0s - loss: 1.7087e-05 - mae: 0.0033\n",
      "Epoch 978: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 157us/sample - loss: 1.7038e-05 - mae: 0.0033 - val_loss: 3.9019e-05 - val_mae: 0.0051\n",
      "Epoch 979/1000\n",
      "710/800 [=========================>....] - ETA: 0s - loss: 1.7550e-05 - mae: 0.0033\n",
      "Epoch 979: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.7272e-05 - mae: 0.0033 - val_loss: 3.3469e-05 - val_mae: 0.0045\n",
      "Epoch 980/1000\n",
      "760/800 [===========================>..] - ETA: 0s - loss: 1.6955e-05 - mae: 0.0033\n",
      "Epoch 980: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 1.7230e-05 - mae: 0.0033 - val_loss: 3.9718e-05 - val_mae: 0.0052\n",
      "Epoch 981/1000\n",
      "650/800 [=======================>......] - ETA: 0s - loss: 1.7234e-05 - mae: 0.0033\n",
      "Epoch 981: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 189us/sample - loss: 1.7949e-05 - mae: 0.0034 - val_loss: 4.3703e-05 - val_mae: 0.0054\n",
      "Epoch 982/1000\n",
      "690/800 [========================>.....] - ETA: 0s - loss: 1.8399e-05 - mae: 0.0034\n",
      "Epoch 982: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 175us/sample - loss: 1.8359e-05 - mae: 0.0034 - val_loss: 3.3481e-05 - val_mae: 0.0046\n",
      "Epoch 983/1000\n",
      "390/800 [=============>................] - ETA: 0s - loss: 1.5336e-05 - mae: 0.0032\n",
      "Epoch 983: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.7677e-05 - mae: 0.0033 - val_loss: 3.3542e-05 - val_mae: 0.0045\n",
      "Epoch 984/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.5884e-05 - mae: 0.0032\n",
      "Epoch 984: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.7526e-05 - mae: 0.0033 - val_loss: 3.6351e-05 - val_mae: 0.0046\n",
      "Epoch 985/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 1.7884e-05 - mae: 0.0034\n",
      "Epoch 985: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.8184e-05 - mae: 0.0034 - val_loss: 3.7569e-05 - val_mae: 0.0050\n",
      "Epoch 986/1000\n",
      "430/800 [===============>..............] - ETA: 0s - loss: 1.8325e-05 - mae: 0.0035\n",
      "Epoch 986: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.7801e-05 - mae: 0.0034 - val_loss: 3.6087e-05 - val_mae: 0.0046\n",
      "Epoch 987/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.7220e-05 - mae: 0.0033\n",
      "Epoch 987: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.7422e-05 - mae: 0.0033 - val_loss: 3.4681e-05 - val_mae: 0.0047\n",
      "Epoch 988/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7764e-05 - mae: 0.0034\n",
      "Epoch 988: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.7968e-05 - mae: 0.0034 - val_loss: 3.7609e-05 - val_mae: 0.0050\n",
      "Epoch 989/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.7455e-05 - mae: 0.0033\n",
      "Epoch 989: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 1.7393e-05 - mae: 0.0033 - val_loss: 3.4273e-05 - val_mae: 0.0047\n",
      "Epoch 990/1000\n",
      "700/800 [=========================>....] - ETA: 0s - loss: 1.6390e-05 - mae: 0.0032\n",
      "Epoch 990: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 168us/sample - loss: 1.6738e-05 - mae: 0.0033 - val_loss: 3.9578e-05 - val_mae: 0.0052\n",
      "Epoch 991/1000\n",
      "790/800 [============================>.] - ETA: 0s - loss: 1.8230e-05 - mae: 0.0034\n",
      "Epoch 991: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 153us/sample - loss: 1.8122e-05 - mae: 0.0034 - val_loss: 3.4349e-05 - val_mae: 0.0047\n",
      "Epoch 992/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.7545e-05 - mae: 0.0033\n",
      "Epoch 992: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 146us/sample - loss: 1.7936e-05 - mae: 0.0034 - val_loss: 3.5204e-05 - val_mae: 0.0048\n",
      "Epoch 993/1000\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.9126e-05 - mae: 0.0035\n",
      "Epoch 993: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.9251e-05 - mae: 0.0035 - val_loss: 3.8108e-05 - val_mae: 0.0048\n",
      "Epoch 994/1000\n",
      "450/800 [===============>..............] - ETA: 0s - loss: 1.5629e-05 - mae: 0.0032\n",
      "Epoch 994: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 1.6856e-05 - mae: 0.0033 - val_loss: 3.5649e-05 - val_mae: 0.0048\n",
      "Epoch 995/1000\n",
      "460/800 [================>.............] - ETA: 0s - loss: 1.9301e-05 - mae: 0.0036\n",
      "Epoch 995: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.9765e-05 - mae: 0.0036 - val_loss: 3.7425e-05 - val_mae: 0.0050\n",
      "Epoch 996/1000\n",
      "440/800 [===============>..............] - ETA: 0s - loss: 1.7565e-05 - mae: 0.0033\n",
      "Epoch 996: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.8024e-05 - mae: 0.0034 - val_loss: 3.3272e-05 - val_mae: 0.0045\n",
      "Epoch 997/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7282e-05 - mae: 0.0033\n",
      "Epoch 997: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 1.7282e-05 - mae: 0.0033 - val_loss: 3.4026e-05 - val_mae: 0.0045\n",
      "Epoch 998/1000\n",
      "410/800 [==============>...............] - ETA: 0s - loss: 1.6129e-05 - mae: 0.0032\n",
      "Epoch 998: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 1.6789e-05 - mae: 0.0033 - val_loss: 3.8770e-05 - val_mae: 0.0051\n",
      "Epoch 999/1000\n",
      "420/800 [==============>...............] - ETA: 0s - loss: 2.0322e-05 - mae: 0.0036\n",
      "Epoch 999: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 1.9482e-05 - mae: 0.0036 - val_loss: 3.7701e-05 - val_mae: 0.0050\n",
      "Epoch 1000/1000\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.7977e-05 - mae: 0.0034\n",
      "Epoch 1000: val_loss did not improve from 0.00003\n",
      "800/800 [==============================] - 0s 149us/sample - loss: 1.7977e-05 - mae: 0.0034 - val_loss: 3.4456e-05 - val_mae: 0.0046\n"
     ]
    }
   ],
   "source": [
    "### Try one model with non-zero last layer bias\n",
    "\n",
    "Layers = [{'size': nx+1, 'activation': None    , 'use_bias': None},\n",
    "          {'size': 10 , 'activation': 'relu'  , 'use_bias': True},\n",
    "          {'size': 1  , 'activation': 'linear', 'use_bias': True}]\n",
    "Losses = [{'kind': 'mse', 'weight': 1.0}]\n",
    "\n",
    "K = TrainFullyConnected(M_samples, H_samples, \n",
    "                    Layers, Losses,\n",
    "                    'adam', ['mae'], \n",
    "                    10, 1000, 0.2, \n",
    "                    'model', os.path.abspath(''))\n",
    "\n",
    "best_model = K.quickTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0602507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 - y: 0.06248438773822334\r",
      "iteration 2 - y: 0.06233181154500988\r",
      "iteration 3 - y: 0.06217923535179641\r",
      "iteration 4 - y: 0.062026659158582936\r",
      "iteration 5 - y: 0.061874082965369476\r",
      "iteration 6 - y: 0.061721506772156015\r",
      "iteration 7 - y: 0.06156893057894254\r",
      "iteration 8 - y: 0.06141635438572908\r",
      "iteration 9 - y: 0.06126377819251562\r",
      "iteration 10 - y: 0.061111201999302145\r",
      "iteration 11 - y: 0.060958625806088684\r",
      "iteration 12 - y: 0.06080604961287522\r",
      "iteration 13 - y: 0.060653473419661756\r",
      "iteration 14 - y: 0.060500897226448296\r",
      "iteration 15 - y: 0.06034832103323482\r",
      "iteration 16 - y: 0.060195744840021354\r",
      "iteration 17 - y: 0.06004316864680789\r",
      "iteration 18 - y: 0.05989059245359443\r",
      "iteration 19 - y: 0.05973801626038096\r",
      "iteration 20 - y: 0.0595854400671675\r",
      "iteration 21 - y: 0.05943286387395404\r",
      "iteration 22 - y: 0.059280287680740576\r",
      "iteration 23 - y: 0.05912771148752711\r",
      "iteration 24 - y: 0.05897513529431364\r",
      "iteration 25 - y: 0.05882255910110018\r",
      "iteration 26 - y: 0.05866998290788672\r",
      "iteration 27 - y: 0.05851740671467325\r",
      "iteration 28 - y: 0.058364830521459785\r",
      "iteration 29 - y: 0.058212254328246324\r",
      "iteration 30 - y: 0.05805967813503286\r",
      "iteration 31 - y: 0.05790710194181939\r",
      "iteration 32 - y: 0.05775452574860593\r",
      "iteration 33 - y: 0.05760194955539247\r",
      "iteration 34 - y: 0.05744937336217899\r",
      "iteration 35 - y: 0.05729679716896554\r",
      "iteration 36 - y: 0.057144220975752065\r",
      "iteration 37 - y: 0.056991644782538604\r",
      "iteration 38 - y: 0.056839068589325144\r",
      "iteration 39 - y: 0.056686492396111676\r",
      "iteration 40 - y: 0.05653391620289821\r",
      "iteration 41 - y: 0.05638134000968475\r",
      "iteration 42 - y: 0.05622876381647128\r",
      "iteration 43 - y: 0.05607618762325782\r",
      "iteration 44 - y: 0.05592361143004436\r",
      "iteration 45 - y: 0.055771035236830885\r",
      "iteration 46 - y: 0.055618459043617424\r",
      "iteration 47 - y: 0.055465882850403964\r",
      "iteration 48 - y: 0.055313306657190496\r",
      "iteration 49 - y: 0.05516073046397703\r",
      "iteration 50 - y: 0.05500815427076357\r",
      "iteration 51 - y: 0.05485557807755011\r",
      "iteration 52 - y: 0.05470300188433663\r",
      "iteration 53 - y: 0.05455042569112318\r",
      "iteration 54 - y: 0.05439784949790971\r",
      "iteration 55 - y: 0.05424527330469625\r",
      "iteration 56 - y: 0.05409269711148279\r",
      "iteration 57 - y: 0.053940120918269316\r",
      "iteration 58 - y: 0.053787544725055855\r",
      "iteration 59 - y: 0.053634968531842395\r",
      "iteration 60 - y: 0.05348239233862892\r",
      "iteration 61 - y: 0.05332981614541546\r",
      "iteration 62 - y: 0.05317723995220199\r",
      "iteration 63 - y: 0.05302466375898853\r",
      "iteration 64 - y: 0.052872087565775064\r",
      "iteration 65 - y: 0.0527195113725616\r",
      "iteration 66 - y: 0.052566935179348136\r",
      "iteration 67 - y: 0.052414358986134675\r",
      "iteration 68 - y: 0.052261782792921214\r",
      "iteration 69 - y: 0.05210920659970775\r",
      "iteration 70 - y: 0.05195663040649427\r",
      "iteration 71 - y: 0.05180405421328081\r",
      "iteration 72 - y: 0.05165147802006735\r",
      "iteration 73 - y: 0.05149890182685389\r",
      "iteration 74 - y: 0.05134632563364042\r",
      "iteration 75 - y: 0.051193749440426956\r",
      "iteration 76 - y: 0.051041173247213495\r",
      "iteration 77 - y: 0.05088859705400003\r",
      "iteration 78 - y: 0.05073602086078656\r",
      "iteration 79 - y: 0.050583444667573106\r",
      "iteration 80 - y: 0.05043086847435964\r",
      "iteration 81 - y: 0.050278292281146164\r",
      "iteration 82 - y: 0.050125716087932704\r",
      "iteration 83 - y: 0.04997313989471924\r",
      "iteration 84 - y: 0.04982056370150578\r",
      "iteration 85 - y: 0.049667987508292315\r",
      "iteration 86 - y: 0.04951541131507885\r",
      "iteration 87 - y: 0.04936283512186538\r",
      "iteration 88 - y: 0.04921025892865192\r",
      "iteration 89 - y: 0.04905768273543846\r",
      "iteration 90 - y: 0.04890510654222499\r",
      "iteration 91 - y: 0.04875253034901153\r",
      "iteration 92 - y: 0.04859995415579806\r",
      "iteration 93 - y: 0.048447377962584595\r",
      "iteration 94 - y: 0.048294801769371135\r",
      "iteration 95 - y: 0.048142225576157674\r",
      "iteration 96 - y: 0.0479896493829442\r",
      "iteration 97 - y: 0.04783707318973074\r",
      "iteration 98 - y: 0.04768449699651727\r",
      "iteration 99 - y: 0.047531920803303804\r",
      "iteration 100 - y: 0.04737934461009034\r",
      "iteration 101 - y: 0.04722676841687688\r",
      "iteration 102 - y: 0.047074192223663415\r",
      "iteration 103 - y: 0.046921616030449954\r",
      "iteration 104 - y: 0.04676903983723649\r",
      "iteration 105 - y: 0.046616463644023026\r",
      "iteration 106 - y: 0.04646388745080956\r",
      "iteration 107 - y: 0.04631131125759609\r",
      "iteration 108 - y: 0.04615873506438263\r",
      "iteration 109 - y: 0.04600615887116917\r",
      "iteration 110 - y: 0.0458535826779557\r",
      "iteration 111 - y: 0.045701006484742235\r",
      "iteration 112 - y: 0.04554843029152877\r",
      "iteration 113 - y: 0.04539585409831531\r",
      "iteration 114 - y: 0.04524327790510184\r",
      "iteration 115 - y: 0.045090701711888385\r",
      "iteration 116 - y: 0.04493812551867492\r",
      "iteration 117 - y: 0.04478554932546145\r",
      "iteration 118 - y: 0.04463297313224799\r",
      "iteration 119 - y: 0.04448039693903452\r",
      "iteration 120 - y: 0.04432782074582106\r",
      "iteration 121 - y: 0.044175244552607594\r",
      "iteration 122 - y: 0.044022668359394126\r",
      "iteration 123 - y: 0.04387009216618066\r",
      "iteration 124 - y: 0.0437175159729672\r",
      "iteration 125 - y: 0.04356493977975374\r",
      "iteration 126 - y: 0.04341236358654027\r",
      "iteration 127 - y: 0.04325978739332681\r",
      "iteration 128 - y: 0.04310721120011334\r",
      "iteration 129 - y: 0.042954635006899874\r",
      "iteration 130 - y: 0.042802058813686414\r",
      "iteration 131 - y: 0.042649482620472946\r",
      "iteration 132 - y: 0.04249690642725948\r",
      "iteration 133 - y: 0.04234433023404602\r",
      "iteration 134 - y: 0.04219175404083256\r",
      "iteration 135 - y: 0.04203917784761909\r",
      "iteration 136 - y: 0.04188660165440562\r",
      "iteration 137 - y: 0.04173402546119216\r",
      "iteration 138 - y: 0.0415814492679787\r",
      "iteration 139 - y: 0.04142887307476523\r",
      "iteration 140 - y: 0.041276296881551766\r",
      "iteration 141 - y: 0.041123720688338306\r",
      "iteration 142 - y: 0.04097114449512484\r",
      "iteration 143 - y: 0.04081856830191138\r",
      "iteration 144 - y: 0.04066599210869791\r",
      "iteration 145 - y: 0.04051341591548445\r",
      "iteration 146 - y: 0.04036083972227098\r",
      "iteration 147 - y: 0.040208263529057514\r",
      "iteration 148 - y: 0.04005568733584405\r",
      "iteration 149 - y: 0.039903111142630586\r",
      "iteration 150 - y: 0.039750534949417125\r",
      "iteration 151 - y: 0.03959795875620366\r",
      "iteration 152 - y: 0.0394453825629902\r",
      "iteration 153 - y: 0.03929280636977673\r",
      "iteration 154 - y: 0.03914023017656326\r",
      "iteration 155 - y: 0.0389876539833498\r",
      "iteration 156 - y: 0.03883507779013634\r",
      "iteration 157 - y: 0.03868250159692287\r",
      "iteration 158 - y: 0.038529925403709406\r",
      "iteration 159 - y: 0.038377349210495945\r",
      "iteration 160 - y: 0.03822477301728248\r",
      "iteration 161 - y: 0.03807219682406901\r",
      "iteration 162 - y: 0.03791962063085555\r",
      "iteration 163 - y: 0.03776704443764208\r",
      "iteration 164 - y: 0.03761446824442862\r",
      "iteration 165 - y: 0.037461892051215154\r",
      "iteration 166 - y: 0.03730931585800169\r",
      "iteration 167 - y: 0.037156739664788226\r",
      "iteration 168 - y: 0.037004163471574765\r",
      "iteration 169 - y: 0.0368515872783613\r",
      "iteration 170 - y: 0.03669901108514784\r",
      "iteration 171 - y: 0.03654643489193437\r",
      "iteration 172 - y: 0.0363938586987209\r",
      "iteration 173 - y: 0.03624128250550744\r",
      "iteration 174 - y: 0.036088706312293974\r",
      "iteration 175 - y: 0.03593613011908051\r",
      "iteration 176 - y: 0.035783553925867045\r",
      "iteration 177 - y: 0.035630977732653585\r",
      "iteration 178 - y: 0.035478401539440124\r",
      "iteration 179 - y: 0.03532582534622665\r",
      "iteration 180 - y: 0.03517324915301319\r",
      "iteration 181 - y: 0.03502067295979973\r",
      "iteration 182 - y: 0.03486809676658626\r",
      "iteration 183 - y: 0.03471552057337279\r",
      "iteration 184 - y: 0.03456294438015933\r",
      "iteration 185 - y: 0.03441036818694587\r",
      "iteration 186 - y: 0.034257791993732405\r",
      "iteration 187 - y: 0.03410521580051894\r",
      "iteration 188 - y: 0.033952639607305476\r",
      "iteration 189 - y: 0.03380006341409201\r",
      "iteration 190 - y: 0.03364748722087855\r",
      "iteration 191 - y: 0.03349491102766508\r",
      "iteration 192 - y: 0.03334233483445161\r",
      "iteration 193 - y: 0.03318975864123815\r",
      "iteration 194 - y: 0.033037182448024685\r",
      "iteration 195 - y: 0.032884606254811224\r",
      "iteration 196 - y: 0.032732030061597764\r",
      "iteration 197 - y: 0.03257945386838429\r",
      "iteration 198 - y: 0.03242687767517083\r",
      "iteration 199 - y: 0.03227430148195737\r",
      "iteration 200 - y: 0.0321217252887439\r",
      "iteration 201 - y: 0.03196914909553044\r",
      "iteration 202 - y: 0.03181657290231697\r",
      "iteration 203 - y: 0.031663996709103505\r",
      "iteration 204 - y: 0.031511420515890044\r",
      "iteration 205 - y: 0.03135884432267658\r",
      "iteration 206 - y: 0.031206268129463116\r",
      "iteration 207 - y: 0.031053691936249656\r",
      "iteration 208 - y: 0.03090111574303619\r",
      "iteration 209 - y: 0.030748539549822727\r",
      "iteration 210 - y: 0.030595963356609267\r",
      "iteration 211 - y: 0.0304433871633958\r",
      "iteration 212 - y: 0.030290810970182346\r",
      "iteration 213 - y: 0.03013823477696888\r",
      "iteration 214 - y: 0.029985658583755417\r",
      "iteration 215 - y: 0.029833082390541957\r",
      "iteration 216 - y: 0.029680506197328496\r",
      "iteration 217 - y: 0.029527930004115036\r",
      "iteration 218 - y: 0.029375353810901568\r",
      "iteration 219 - y: 0.02922277761768811\r",
      "iteration 220 - y: 0.029070201424474647\r",
      "iteration 221 - y: 0.028917625231261183\r",
      "iteration 222 - y: 0.028765049038047726\r",
      "iteration 223 - y: 0.028612472844834258\r",
      "iteration 224 - y: 0.028459896651620797\r",
      "iteration 225 - y: 0.028307320458407337\r",
      "iteration 226 - y: 0.028154744265193873\r",
      "iteration 227 - y: 0.02800216807198041\r",
      "iteration 228 - y: 0.027849591878766948\r",
      "iteration 229 - y: 0.027697015685553487\r",
      "iteration 230 - y: 0.027544439492340027\r",
      "iteration 231 - y: 0.027391863299126563\r",
      "iteration 232 - y: 0.0272392871059131\r",
      "iteration 233 - y: 0.027086710912699638\r",
      "iteration 234 - y: 0.026934134719486177\r",
      "iteration 235 - y: 0.026781558526272713\r",
      "iteration 236 - y: 0.02662898233305925\r",
      "iteration 237 - y: 0.026476406139845785\r",
      "iteration 238 - y: 0.026323829946632328\r",
      "iteration 239 - y: 0.02617125375341886\r",
      "iteration 240 - y: 0.0260186775602054\r",
      "iteration 241 - y: 0.02586610136699194\r",
      "iteration 242 - y: 0.025713525173778475\r",
      "iteration 243 - y: 0.025560948980565015\r",
      "iteration 244 - y: 0.02540837278735155\r",
      "iteration 245 - y: 0.02525579659413809\r",
      "iteration 246 - y: 0.02510322040092463\r",
      "iteration 247 - y: 0.024950644207711165\r",
      "iteration 248 - y: 0.0247980680144977\r",
      "iteration 249 - y: 0.02464549182128424\r",
      "iteration 250 - y: 0.02449291562807078\r",
      "iteration 251 - y: 0.024340339434857312\r",
      "iteration 252 - y: 0.024187763241643852\r",
      "iteration 253 - y: 0.02403518704843039\r",
      "iteration 254 - y: 0.023882610855216927\r",
      "iteration 255 - y: 0.023730034662003463\r",
      "iteration 256 - y: 0.023577458468790002\r",
      "iteration 257 - y: 0.023424882275576535\r",
      "iteration 258 - y: 0.023272306082363074\r",
      "iteration 259 - y: 0.023119729889149614\r",
      "iteration 260 - y: 0.022967153695936153\r",
      "iteration 261 - y: 0.022814577502722685\r",
      "iteration 262 - y: 0.022662001309509225\r",
      "iteration 263 - y: 0.022509425116295764\r",
      "iteration 264 - y: 0.022356848923082304\r",
      "iteration 265 - y: 0.022204272729868836\r",
      "iteration 266 - y: 0.022051696536655375\r",
      "iteration 267 - y: 0.021899120343441915\r",
      "iteration 268 - y: 0.021746544150228454\r",
      "iteration 269 - y: 0.02159396795701499\r",
      "iteration 270 - y: 0.021441391763801526\r",
      "iteration 271 - y: 0.021288815570588066\r",
      "iteration 272 - y: 0.0211362393773746\r",
      "iteration 273 - y: 0.02098366318416114\r",
      "iteration 274 - y: 0.020831086990947677\r",
      "iteration 275 - y: 0.020678510797734216\r",
      "iteration 276 - y: 0.020525934604520752\r",
      "iteration 277 - y: 0.020373358411307288\r",
      "iteration 278 - y: 0.020220782218093827\r",
      "iteration 279 - y: 0.020068206024880367\r",
      "iteration 280 - y: 0.019915629831666903\r",
      "iteration 281 - y: 0.01976305363845344\r",
      "iteration 282 - y: 0.019610477445239978\r",
      "iteration 283 - y: 0.019457901252026517\r",
      "iteration 284 - y: 0.019305325058813053\r",
      "iteration 285 - y: 0.01915274886559959\r",
      "iteration 286 - y: 0.01900017267238613\r",
      "iteration 287 - y: 0.018847596479172665\r",
      "iteration 288 - y: 0.018695020285959204\r",
      "iteration 289 - y: 0.01854244409274574\r",
      "iteration 290 - y: 0.01838986789953228\r",
      "iteration 291 - y: 0.01823729170631882\r",
      "iteration 292 - y: 0.018084715513105355\r",
      "iteration 293 - y: 0.01793213931989189\r",
      "iteration 294 - y: 0.01777956312667843\r",
      "iteration 295 - y: 0.01762698693346497\r",
      "iteration 296 - y: 0.017474410740251505\r",
      "iteration 297 - y: 0.01732183454703804\r",
      "iteration 298 - y: 0.01716925835382458\r",
      "iteration 299 - y: 0.017016682160611116\r",
      "iteration 300 - y: 0.016909826852528377\r",
      "iteration 301 - y: 0.016804855565873907\r",
      "iteration 302 - y: 0.016699884279219434\r",
      "iteration 303 - y: 0.016594912992564964\r",
      "iteration 304 - y: 0.01648994170591049\r",
      "iteration 305 - y: 0.016384970419256017\r",
      "iteration 306 - y: 0.016279999132601544\r",
      "iteration 307 - y: 0.016175027845947074\r",
      "iteration 308 - y: 0.016070056559292605\r",
      "iteration 309 - y: 0.01596508527263813\r",
      "iteration 310 - y: 0.015860113985983658\r",
      "iteration 311 - y: 0.015755142699329185\r",
      "iteration 312 - y: 0.015650171412674715\r",
      "iteration 313 - y: 0.015545200126020245\r",
      "iteration 314 - y: 0.015440228839365772\r",
      "iteration 315 - y: 0.015335257552711298\r",
      "iteration 316 - y: 0.015230286266056827\r",
      "iteration 317 - y: 0.015125314979402355\r",
      "iteration 318 - y: 0.015020343692747885\r",
      "iteration 319 - y: 0.014915372406093412\r",
      "iteration 320 - y: 0.01481040111943894\r",
      "iteration 321 - y: 0.014705429832784469\r",
      "iteration 322 - y: 0.014600458546129996\r",
      "iteration 323 - y: 0.014495487259475526\r",
      "iteration 324 - y: 0.014390515972821052\r",
      "iteration 325 - y: 0.014285544686166579\r",
      "iteration 326 - y: 0.01418057339951211\r",
      "iteration 327 - y: 0.014079267722663508\r",
      "iteration 328 - y: 0.01400596201151334\r",
      "iteration 329 - y: 0.01393265630036317\r",
      "iteration 330 - y: 0.013859350589213\r",
      "iteration 331 - y: 0.013786044878062832\r",
      "iteration 332 - y: 0.013712739166912662\r",
      "iteration 333 - y: 0.013639433455762495\r",
      "iteration 334 - y: 0.013566127744612326\r",
      "iteration 335 - y: 0.013492822033462156\r",
      "iteration 336 - y: 0.013419516322311987\r",
      "iteration 337 - y: 0.01334621061116182\r",
      "iteration 338 - y: 0.01327290490001165\r",
      "iteration 339 - y: 0.013199599188861481\r",
      "iteration 340 - y: 0.013126293477711312\r",
      "iteration 341 - y: 0.013052987766561143\r",
      "iteration 342 - y: 0.012979682055410974\r",
      "iteration 343 - y: 0.012906376344260805\r",
      "iteration 344 - y: 0.012833070633110635\r",
      "iteration 345 - y: 0.012759764921960468\r",
      "iteration 346 - y: 0.012686459210810297\r",
      "iteration 347 - y: 0.01261315349966013\r",
      "iteration 348 - y: 0.012539847788509962\r",
      "iteration 349 - y: 0.012466542077359791\r",
      "iteration 350 - y: 0.012393236366209624\r",
      "iteration 351 - y: 0.012319930655059454\r",
      "iteration 352 - y: 0.012246624943909285\r",
      "iteration 353 - y: 0.012173319232759116\r",
      "iteration 354 - y: 0.012100013521608947\r",
      "iteration 355 - y: 0.012026707810458778\r",
      "iteration 356 - y: 0.011953402099308608\r",
      "iteration 357 - y: 0.011880096388158439\r",
      "iteration 358 - y: 0.011806790677008272\r",
      "iteration 359 - y: 0.011733484965858102\r",
      "iteration 360 - y: 0.011660179254707933\r",
      "iteration 361 - y: 0.011586873543557764\r",
      "iteration 362 - y: 0.011513567832407597\r",
      "iteration 363 - y: 0.011440262121257427\r",
      "iteration 364 - y: 0.011366956410107258\r",
      "iteration 365 - y: 0.011293650698957089\r",
      "iteration 366 - y: 0.01122034498780692\r",
      "iteration 367 - y: 0.01114703927665675\r",
      "iteration 368 - y: 0.011073733565506581\r",
      "iteration 369 - y: 0.011000427854356412\r",
      "iteration 370 - y: 0.010927122143206245\r",
      "iteration 371 - y: 0.010853816432056074\r",
      "iteration 372 - y: 0.010780510720905906\r",
      "iteration 373 - y: 0.010707205009755737\r",
      "iteration 374 - y: 0.010633899298605568\r",
      "iteration 375 - y: 0.0105605935874554\r",
      "iteration 376 - y: 0.010487287876305231\r",
      "iteration 377 - y: 0.010413982165155062\r",
      "iteration 378 - y: 0.010340676454004893\r",
      "iteration 379 - y: 0.010267370742854724\r",
      "iteration 380 - y: 0.010194065031704554\r",
      "iteration 381 - y: 0.010120759320554385\r",
      "iteration 382 - y: 0.010047453609404216\r",
      "iteration 383 - y: 0.009974147898254047\r",
      "iteration 384 - y: 0.00990084218710388\r",
      "iteration 385 - y: 0.00982753647595371\r",
      "iteration 386 - y: 0.00975423076480354\r",
      "iteration 387 - y: 0.009680925053653373\r",
      "iteration 388 - y: 0.009607619342503204\r",
      "iteration 389 - y: 0.009534313631353035\r",
      "iteration 390 - y: 0.009461007920202866\r",
      "iteration 391 - y: 0.009387702209052697\r",
      "iteration 392 - y: 0.009314396497902527\r",
      "iteration 393 - y: 0.009241090786752358\r",
      "iteration 394 - y: 0.009167785075602189\r",
      "iteration 395 - y: 0.009094479364452021\r",
      "iteration 396 - y: 0.00902117365330185\r",
      "iteration 397 - y: 0.008947867942151683\r",
      "iteration 398 - y: 0.008874562231001514\r",
      "iteration 399 - y: 0.008801256519851346\r",
      "iteration 400 - y: 0.008727950808701177\r",
      "iteration 401 - y: 0.008654645097551008\r",
      "iteration 402 - y: 0.008581339386400839\r",
      "iteration 403 - y: 0.00850803367525067\r",
      "iteration 404 - y: 0.0084347279641005\r",
      "iteration 405 - y: 0.008361422252950331\r",
      "iteration 406 - y: 0.008288116541800162\r",
      "iteration 407 - y: 0.008214810830649994\r",
      "iteration 408 - y: 0.008141505119499824\r",
      "iteration 409 - y: 0.008068199408349656\r",
      "iteration 410 - y: 0.007994893697199487\r",
      "iteration 411 - y: 0.007921587986049318\r",
      "iteration 412 - y: 0.00784828227489915\r",
      "iteration 413 - y: 0.007774976563748979\r",
      "iteration 414 - y: 0.007701670852598812\r",
      "iteration 415 - y: 0.0076283651414486425\r",
      "iteration 416 - y: 0.007555059430298473\r",
      "iteration 417 - y: 0.007481753719148304\r",
      "iteration 418 - y: 0.007408448007998135\r",
      "iteration 419 - y: 0.007335142296847967\r",
      "iteration 420 - y: 0.007261836585697797\r",
      "iteration 421 - y: 0.007188530874547629\r",
      "iteration 422 - y: 0.00711522516339746\r",
      "iteration 423 - y: 0.007041919452247291\r",
      "iteration 424 - y: 0.006968613741097121\r",
      "iteration 425 - y: 0.006895308029946953\r",
      "iteration 426 - y: 0.006822002318796784\r",
      "iteration 427 - y: 0.006748696607646615\r",
      "iteration 428 - y: 0.006675390896496446\r",
      "iteration 429 - y: 0.006602085185346277\r",
      "iteration 430 - y: 0.006528779474196108\r",
      "iteration 431 - y: 0.006455473763045939\r",
      "iteration 432 - y: 0.00638216805189577\r",
      "iteration 433 - y: 0.006308862340745602\r",
      "iteration 434 - y: 0.006235556629595433\r",
      "iteration 435 - y: 0.006162250918445264\r",
      "iteration 436 - y: 0.006088945207295094\r",
      "iteration 437 - y: 0.006015639496144925\r",
      "iteration 438 - y: 0.005942333784994757\r",
      "iteration 439 - y: 0.005869028073844588\r",
      "iteration 440 - y: 0.005795722362694418\r",
      "iteration 441 - y: 0.005722416651544249\r",
      "iteration 442 - y: 0.005649110940394079\r",
      "iteration 443 - y: 0.005575805229243911\r",
      "iteration 444 - y: 0.005502499518093741\r",
      "iteration 445 - y: 0.0054291938069435716\r",
      "iteration 446 - y: 0.005355888095793402\r",
      "iteration 447 - y: 0.005282582384643232\r",
      "iteration 448 - y: 0.005209276673493062\r",
      "iteration 449 - y: 0.005135970962342893\r",
      "iteration 450 - y: 0.005062665251192724\r",
      "iteration 451 - y: 0.004989359540042555\r",
      "iteration 452 - y: 0.0049160538288923845\r",
      "iteration 453 - y: 0.0048427481177422144\r",
      "iteration 454 - y: 0.004769442406592045\r",
      "iteration 455 - y: 0.004696136695441876\r",
      "iteration 456 - y: 0.004622830984291707\r",
      "iteration 457 - y: 0.004549525273141536\r",
      "iteration 458 - y: 0.004476219561991367\r",
      "iteration 459 - y: 0.0044029138508411975\r",
      "iteration 460 - y: 0.004329608139691028\r",
      "iteration 461 - y: 0.004256302428540858\r",
      "iteration 462 - y: 0.004182996717390688\r",
      "iteration 463 - y: 0.004109691006240519\r",
      "iteration 464 - y: 0.00403638529509035\r",
      "iteration 465 - y: 0.00396307958394018\r",
      "iteration 466 - y: 0.0038897738727900104\r",
      "iteration 467 - y: 0.0038164681616398408\r",
      "iteration 468 - y: 0.003743162450489671\r",
      "iteration 469 - y: 0.003669856739339502\r",
      "iteration 470 - y: 0.0035965510281893323\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 117 - y: 0.00036861478682004547\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.00309859]], dtype=float32), array([[0.0030961]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = K.loadBestModel()\n",
    "normalizeDict = {'bool_':True, 'kind': 'MaxAbs'}\n",
    "kwargs = {'y_ref': 0.0004}\n",
    "X = XAIR(best_model, 'lrp.z', 'classic', M_samples[:10], normalizeDict, **kwargs)\n",
    "X.check_sample(M_samples[0]), X.check_sample(M_samples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a104b5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad0e7e184f0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, stats  = X.quick_analyze()\n",
    "plt.plot(a[0])\n",
    "plt.plot(a[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
