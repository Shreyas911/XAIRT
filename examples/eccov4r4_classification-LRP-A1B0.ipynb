{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5950c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 10:12:24.326256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 10:12:45.040101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 459 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-01-26 10:12:45.042803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37648 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-01-26 10:12:45.045168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37648 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    }
   ],
   "source": [
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import metrics\n",
    "import innvestigate\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "import xarray as xr\n",
    "import xmitgcm\n",
    "from xmitgcm import open_mdsdataset\n",
    "import ecco_v4_py as ecco\n",
    "\n",
    "import random\n",
    "\n",
    "# See if GPUs are available\n",
    "from keras import backend as K\n",
    "if bool(K._get_available_gpus()):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Append to sys.path the absolute path to src/XAIRT\n",
    "path_list = os.path.abspath('').split('/')\n",
    "path_src_XAIRT = ''\n",
    "for link in path_list[:-1]:\n",
    "    path_src_XAIRT = path_src_XAIRT+link+'/'\n",
    "sys.path.append(path_src_XAIRT+'/src')\n",
    "\n",
    "# Now import module XAIRT\n",
    "from XAIRT import *\n",
    "\n",
    "### https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed ###\n",
    "### https://keras.io/examples/keras_recipes/reproducibility_recipes/ ###\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7d3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sverdrup \n",
    "# mainDir  = '/scratch2/pillarh/eccov4r4'\n",
    "# gridDir  = mainDir + '/GRID'\n",
    "# thetaDir = mainDir + '/eccov4r4_nctiles_daily/THETA'\n",
    "\n",
    "### Lonestar6\n",
    "mainDir  = '/work/07665/shrey911/ls6/LRP_eccov4r4_data'\n",
    "gridDir  = mainDir\n",
    "thetaDir = mainDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e54e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sverdrup  - /scratch2/shreyas/LRP_eccov4r4_data\n",
    "### Lonestar6 - /work/07665/shrey911/ls6/LRP_eccov4r4_data\n",
    "\n",
    "ds = xr.open_dataset(f'/work/07665/shrey911/ls6/LRP_eccov4r4_data/thetaSurfECCOv4r4.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1456a104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_binary_array: loading file /work/07665/shrey911/ls6/LRP_eccov4r4_data/hFacC.data\n",
      "load_binary_array: data array shape  (1170, 90)\n",
      "load_binary_array: data array type  >f4\n",
      "llc_compact_to_faces: dims, llc  (1170, 90) 90\n",
      "llc_compact_to_faces: data_compact array type  >f4\n",
      "llc_faces_to_tiles: data_tiles shape  (13, 90, 90)\n",
      "llc_faces_to_tiles: data_tiles dtype  >f4\n"
     ]
    }
   ],
   "source": [
    "hFacC = ecco.read_llc_to_tiles(gridDir, 'hFacC.data')\n",
    "hFacC_mask = hFacC > 0\n",
    "hFacC_mask = hFacC_mask.astype(float)\n",
    "\n",
    "XC = ds['XC'].data\n",
    "YC = ds['YC'].data\n",
    "\n",
    "latMask = YC > -20.0\n",
    "latMask = latMask.astype(float)\n",
    "\n",
    "maskFinal = hFacC_mask * latMask\n",
    "NaNmaskFinal = np.copy(maskFinal)\n",
    "NaNmaskFinal[NaNmaskFinal == 0] = np.nan\n",
    "\n",
    "da_hFacC_mask = xr.DataArray(\n",
    "    data=hFacC_mask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds['tile'].data,\n",
    "        j    = ds['j'].data,\n",
    "        i    = ds['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"hFacC mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_latMask = xr.DataArray(\n",
    "    data=latMask,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds['tile'].data,\n",
    "        j    = ds['j'].data,\n",
    "        i    = ds['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Latitude Mask 1 if > -20, else 0\"),\n",
    ")\n",
    "\n",
    "da_maskFinal = xr.DataArray(\n",
    "    data=maskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds['tile'].data,\n",
    "        j    = ds['j'].data,\n",
    "        i    = ds['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D 1 if > 0, else 0\"),\n",
    ")\n",
    "\n",
    "da_NaNmaskFinal = xr.DataArray(\n",
    "    data=NaNmaskFinal,\n",
    "    dims=[\"tile\", \"j\", \"i\"],\n",
    "    coords=dict(\n",
    "        tile = ds['tile'].data,\n",
    "        j    = ds['j'].data,\n",
    "        i    = ds['i'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"Mask 2D True if > 0, else NaN\"),\n",
    ")\n",
    "\n",
    "wetpoints = np.nonzero(maskFinal.data)\n",
    "da_wetpoints = xr.DataArray(\n",
    "    data=np.asarray(wetpoints),\n",
    "    dims=[\"wetpoints_dim\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        wetpoints_dim = np.arange(np.asarray(wetpoints).shape[0], dtype = int),\n",
    "        num_wetpoints = np.arange(np.asarray(wetpoints).shape[1], dtype = int),\n",
    "    ),\n",
    "    attrs=dict(description=\"indices of wetpoints in the order (tile, j, i) in the three rows\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(hFacC_mask   = da_hFacC_mask,\n",
    "          latMask      = da_latMask,\n",
    "          maskFinal    = da_maskFinal,\n",
    "          NaNmaskFinal = da_NaNmaskFinal,\n",
    "          wetpoints    = da_wetpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1956165-f2a7-49ba-b111-104c1db8ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize(field):\n",
    "    \n",
    "    leap_yr_offsets_jan_feb   = [0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7]\n",
    "    leap_yr_offsets_after_feb = [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7]\n",
    "\n",
    "    if len(field.shape) > 1:\n",
    "        seasonal_trend = np.zeros((366, field.shape[1]))\n",
    "    else:\n",
    "        seasonal_trend = np.zeros((366,))\n",
    "\n",
    "    #### Calculate seasonal trend\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        for year in range(26):\n",
    "            seasonal_trend[d] += field[d+365*year+leap_yr_offsets_jan_feb[year]]\n",
    "    seasonal_trend[:59] = seasonal_trend[:59] / 26.0\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indefielding\n",
    "    n = 0\n",
    "    for year in range(0,26,4):\n",
    "        seasonal_trend[59] += field[365*year+n+59]\n",
    "        n = n+1\n",
    "    seasonal_trend[59] = seasonal_trend[59] / 7.0\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        for year in range(26):\n",
    "            seasonal_trend[d] += field[d-1+365*year+leap_yr_offsets_after_feb[year]]\n",
    "    seasonal_trend[60:] = seasonal_trend[60:] / 26.0\n",
    "    \n",
    "    #### Deseason data\n",
    "    \n",
    "    # Jan 1 - Feb 28\n",
    "    for d in range(59):\n",
    "        for year in range(26):\n",
    "            field[d+365*year+leap_yr_offsets_jan_feb[year]] = field[d+365*year+leap_yr_offsets_jan_feb[year]] \\\n",
    "                                                        - seasonal_trend[d]\n",
    "    \n",
    "    # Feb 29 starting 1996, so year 2 in 0-indexing\n",
    "    n = 0\n",
    "    for year in range(0,26,4):\n",
    "        field[365*year+n+59] = field[365*year+n+59] - seasonal_trend[59]\n",
    "        n = n+1\n",
    "            \n",
    "    # Mar 1 - Dec 31\n",
    "    for d in range(60,366):\n",
    "        for year in range(26):\n",
    "            field[d-1+365*year+leap_yr_offsets_after_feb[year]] = field[d-1+365*year+leap_yr_offsets_after_feb[year]] \\\n",
    "                                                            - seasonal_trend[d]\n",
    "\n",
    "    scipy.signal.detrend(field, axis=0, type='linear', bp=0, overwrite_data=True)\n",
    "    field = field - np.mean(field, axis = 0)\n",
    "\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c42bb6-4d8c-4aa0-956b-4873ba71147e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = ds['THETA'].data[:,wetpoints[0],wetpoints[1],wetpoints[2]].copy()\n",
    "X = anomalize(X)\n",
    "X_full = X.copy()\n",
    "X = X[30:-30]\n",
    "\n",
    "y = ds['THETA'].isel(tile = 10, j = 1, i = 43).data.copy()\n",
    "y = anomalize(y)\n",
    "# https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "y = np.convolve(y, np.ones(61)/61, mode='valid')\n",
    "oneHotCost = np.zeros((y.shape[0], 2), dtype = int)\n",
    "oneHotCost[:,0] = y >= 0.0\n",
    "oneHotCost[:,1] = y <  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbaf2d3-8602-4ca4-9bd6-737099d4f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_X = xr.DataArray(\n",
    "    data=X,\n",
    "    dims=[\"time_allData\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data as matrix X; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_y = xr.DataArray(\n",
    "    data=y,\n",
    "    dims=[\"time_allData\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function y; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_X_full = xr.DataArray(\n",
    "    data=X_full,\n",
    "    dims=[\"time\", \"num_wetpoints\"],\n",
    "    coords=dict(\n",
    "        time          = ds['time'],\n",
    "        num_wetpoints = ds['num_wetpoints'].data,\n",
    "    ),\n",
    "    attrs=dict(description=\"All data without accounting for conv filter as matrix X_full; deseasoned, delinearized and mean removed.\"),\n",
    ")\n",
    "\n",
    "da_oneHotCost = xr.DataArray(\n",
    "    data=oneHotCost,\n",
    "    dims=[\"time_allData\", \"NN_output_dim\"],\n",
    "    coords=dict(\n",
    "        time_allData  = ds['time'].data[30:-30],\n",
    "        NN_output_dim = np.array([0,1]),\n",
    "    ),\n",
    "    attrs=dict(description=\"All cost function as one-hot vector.\"),\n",
    ")\n",
    "\n",
    "ds = ds.assign(X          = da_X,\n",
    "               y          = da_y,\n",
    "               X_full     = da_X_full,\n",
    "               oneHotCost = da_oneHotCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb797f76-7914-4d02-9ab5-f47c788dde7b",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b017273-5b15-4d62-a7ae-16381c5513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP(model, model_wo_softmax, X, y_true, lagSteps, lrp_methods, \n",
    "        suffix, normalizeDict, **kwargs):\n",
    "\n",
    "    result = {}\n",
    "    cost_NN = model.predict(X)\n",
    "    result[f'score_{suffix}'] = model.evaluate(X, y_true, verbose=0)\n",
    "\n",
    "    pred_NN = cost_NN.copy()\n",
    "    pred_NN[:,0] = pred_NN[:,0] > 0.5\n",
    "    pred_NN[:,1] = pred_NN[:,1] > 0.5\n",
    "\n",
    "    idx_NN_pos = []\n",
    "    idx_NN_neg = []\n",
    "\n",
    "    if lagSteps >= 0:\n",
    "        for i in range(len(y_true[lagSteps:,0])):\n",
    "            if y_true[lagSteps+i,0] == 1 and pred_NN[i,0] == 1:\n",
    "                idx_NN_pos.append(i)\n",
    "            if y_true[lagSteps+i,1] == 1 and pred_NN[i,1] == 1:\n",
    "                idx_NN_neg.append(i)\n",
    "    else:\n",
    "        for i in range(len(y_true[:lagSteps,0])):\n",
    "            if y_true[i,0] == 1 and pred_NN[i-lagSteps,0] == 1:\n",
    "                idx_NN_pos.append(i-lagSteps)\n",
    "            if y_true[i,1] == 1 and pred_NN[i-lagSteps,1] == 1:\n",
    "                idx_NN_neg.append(i-lagSteps)    \n",
    "\n",
    "    rel = np.zeros((len(idx_NN_pos), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_pos]\n",
    "    result[f'samples_correct_pos_{suffix}'] = rel\n",
    "\n",
    "    rel = np.zeros((len(idx_NN_neg), 13, 90, 90))\n",
    "    rel[:,:,:,:] = np.nan\n",
    "    rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = X[idx_NN_neg]\n",
    "    result[f'samples_correct_neg_{suffix}'] = rel\n",
    "    \n",
    "    for method in lrp_methods:\n",
    "\n",
    "        title = method['title']\n",
    "        \n",
    "        print(f'Analyze using {title} for {suffix} data')\n",
    "        \n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_pos], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_pos_{suffix}'] = rel\n",
    "\n",
    "        Xplain = XAIR(model_wo_softmax, method, 'classic', X[idx_NN_neg], \n",
    "                      normalizeDict, **kwargs)\n",
    "        a, _  = Xplain.quick_analyze()\n",
    "        \n",
    "        rel = np.zeros((a.shape[0], 13, 90, 90))\n",
    "        rel[:,:,:,:] = np.nan\n",
    "        rel[:,wetpoints[0],wetpoints[1],wetpoints[2]] = a\n",
    "        result[method['title']+f'_neg_{suffix}'] = rel\n",
    "\n",
    "    return result\n",
    "\n",
    "def quickSetup(X, y,\n",
    "               test_split_frac,\n",
    "               lrp_methods,\n",
    "               lagSteps,\n",
    "               decay_func = None,\n",
    "               **NNkwargs):\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    idx = int(X.shape[0]*(1-test_split_frac))\n",
    "    X_train = X[:idx]\n",
    "    oneHotCost_train = oneHotCost[:idx]\n",
    "    X_test = X[idx:]\n",
    "    oneHotCost_test = oneHotCost[idx:]\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    NNkwargs['optimizer'] = sgd\n",
    "    \n",
    "    if lagSteps > 0:\n",
    "        K = TrainFullyConnectedNN(X_train[:-lagSteps], oneHotCost_train[lagSteps:], **NNkwargs)\n",
    "    elif lagSteps == 0:\n",
    "        K = TrainFullyConnectedNN(X_train, oneHotCost_train, **NNkwargs)\n",
    "    else:\n",
    "        K = TrainFullyConnectedNN(X_train[-lagSteps:], oneHotCost_train[:lagSteps], **NNkwargs)\n",
    "        \n",
    "    best_model = K.quickTrain(decay_func)\n",
    "    best_model_wo_softmax = innvestigate.model_wo_softmax(best_model)\n",
    "                   \n",
    "    result['cost_predict_NN'] = best_model.predict(X)\n",
    "                   \n",
    "    normalizeDict = {'bool_': True, 'kind': 'MaxAbs'}\n",
    "    kwargs = {'y_ref': 0.00}\n",
    "    \n",
    "    # LRP for training data\n",
    "    result_train = LRP(best_model, best_model_wo_softmax, \n",
    "                       X_train, oneHotCost_train, lagSteps, lrp_methods, \n",
    "                       suffix = 'train', normalizeDict = normalizeDict, **kwargs)\n",
    "    result.update(result_train)\n",
    "\n",
    "    # LRP for test data\n",
    "    result_test = LRP(best_model, best_model_wo_softmax, \n",
    "                       X_test, oneHotCost_test, lagSteps, lrp_methods, \n",
    "                       suffix = 'test', normalizeDict = normalizeDict, **kwargs)\n",
    "    result.update(result_test)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2171702a-a06c-4148-b84d-680a368626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = [{'size': X.shape[1], 'activation': None     , 'use_bias': None},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, 'l2_w_reg': 10  , 'l2_b_reg': 10},\n",
    "          {'size': 8         , 'activation': 'relu'   , 'use_bias': True, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01},\n",
    "          {'size': 2         , 'activation': 'softmax', 'use_bias': True, 'l2_w_reg': 0.01, 'l2_b_reg': 0.01}]\n",
    "Losses = [{'kind': 'categorical_crossentropy', 'weight': 1.0}]\n",
    "\n",
    "LRPDict_theta = {}\n",
    "lagStepsList = [-60,-30,0,30,60,90,120,150,180]\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 25\n",
    "    lrate = initial_lrate * drop**np.floor((1+epoch)/epochs_drop)\n",
    "    return lrate\n",
    "\n",
    "methods = [dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0', optParams = {}),\n",
    "           dict(name='lrp.alpha_1_beta_0', title = 'LRP-A1B0-W2', optParams = {'input_layer_rule':'WSquare'})]\n",
    "\n",
    "cmaps = ['RdBu_r', 'RdBu_r']\n",
    "color_ranges = [[-0.5,0.5], [-0.3,0.3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f6386a6-f5f7-4784-a556-83efa68ecfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag: -60 days, for Theta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 10:13:18.281447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 459 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-01-26 10:13:18.283545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37648 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-01-26 10:13:18.285561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37648 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n",
      "2024-01-26 10:13:18.299543: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-01-26 10:13:34.846988: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2024-01-26 10:13:34.847068: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2024-01-26 10:13:34.847139: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
     ]
    },
    {
     "ename": "type",
     "evalue": "2 root error(s) found.\n  (0) INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node dense/MatMul}}]]\n\t [[loss/AddN_1/_65]]\n  (1) INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node dense/MatMul}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLag: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlagStepsList[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days, for Theta\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m NNkwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m'\u001b[39m: Losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: [metrics\u001b[38;5;241m.\u001b[39mcategorical_accuracy],\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m128\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlagStepsList[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirname\u001b[39m\u001b[38;5;124m'\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_nn_seed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m}\n\u001b[0;32m---> 10\u001b[0m LRPDict_theta[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLRP\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlagStepsList[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mquickSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_split_frac\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlrp_methods\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlagSteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlagStepsList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdecay_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstep_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mNNkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 90\u001b[0m, in \u001b[0;36mquickSetup\u001b[0;34m(X, y, test_split_frac, lrp_methods, lagSteps, decay_func, **NNkwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     K \u001b[38;5;241m=\u001b[39m TrainFullyConnectedNN(X_train[\u001b[38;5;241m-\u001b[39mlagSteps:], oneHotCost_train[:lagSteps], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mNNkwargs)\n\u001b[0;32m---> 90\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquickTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecay_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m best_model_wo_softmax \u001b[38;5;241m=\u001b[39m innvestigate\u001b[38;5;241m.\u001b[39mmodel_wo_softmax(best_model)\n\u001b[1;32m     93\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost_predict_NN\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m~/XAIRT//src/XAIRT/model/Trainer.py:256\u001b[0m, in \u001b[0;36mTrainFullyConnectedNN.quickTrain\u001b[0;34m(self, decay_func)\u001b[0m\n\u001b[1;32m    253\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lrateSchedule(decay_func)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createCheckpoint()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloadBestModel()\n",
      "File \u001b[0;32m~/XAIRT//src/XAIRT/model/Trainer.py:226\u001b[0m, in \u001b[0;36mTrainFullyConnectedNN._trainModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \t\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_trainModel\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m            \t\t\t\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m            \t\t\t\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m            \t\t\t\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \t\t\t\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \t\t\t\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\t\t\t\t\t\t\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m    234\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_state\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/keras/engine/training_v1.py:776\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    775\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:641\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    638\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    639\u001b[0m   val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:377\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    379\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/keras/backend.py:4284\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   4280\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   4281\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[1;32m   4282\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4284\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4285\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[1;32m   4287\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4289\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[1;32m   4290\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/work/07665/shrey911/ls6/mambaforge/envs/py310_LRP/lib/python3.10/site-packages/tensorflow/python/client/session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node dense/MatMul}}]]\n\t [[loss/AddN_1/_65]]\n  (1) INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node dense/MatMul}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "for i in range(len(lagStepsList)):\n",
    "\n",
    "    print(f'Lag: {lagStepsList[i]} days, for Theta')\n",
    "    \n",
    "    NNkwargs = {'losses': Losses, 'metrics': [metrics.categorical_accuracy],\n",
    "                'batch_size': 128, 'epochs': 100, 'validation_split': 0.2,\n",
    "                'filename': f'model{lagStepsList[i]}', 'dirname': os.path.abspath(''),\n",
    "                'random_nn_seed': 42}\n",
    "    \n",
    "    LRPDict_theta[f'LRP{lagStepsList[i]}'] = quickSetup(X, y, test_split_frac = 1.0/3.0,\n",
    "                                                        lrp_methods = methods,\n",
    "                                                        lagSteps = lagStepsList[i],\n",
    "                                                        decay_func = step_decay,\n",
    "                                                        layers = Layers, **NNkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebc6a6-95a8-4cf0-9bf1-264466266f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP_normalize(field):\n",
    "    field[np.where(field < 0)] = 0.0\n",
    "    field = np.nanmean(field, axis=0)\n",
    "    field = field / np.nanmax(np.abs(field))\n",
    "    \n",
    "    return xr.DataArray(field)\n",
    "\n",
    "lrp_minus60_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_minus60_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_minus60_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_minus60_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_minus60_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_minus60_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_minus60_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_minus60_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP-60']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_minus30_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_minus30_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_minus30_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_minus30_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_minus30_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_minus30_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_minus30_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_minus30_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP-30']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_0_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_0_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_0_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_0_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_0_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_0_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_0_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_0_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP0']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_30_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_30_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_30_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_30_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_30_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_30_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_30_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_30_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP30']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_60_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_60_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_60_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_60_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_60_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_60_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_60_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_60_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP60']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_90_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_90_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_90_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_90_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_90_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_90_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_90_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_90_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP90']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_120_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_120_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_120_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_120_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_120_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_120_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_120_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_120_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP120']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_150_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_150_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_150_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_150_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_150_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_150_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_150_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_150_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP150']['LRP-A1B0_neg_test'].copy())\n",
    "\n",
    "lrp_180_a1b0_w2_pos_train_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-W2_pos_train'].copy())\n",
    "lrp_180_a1b0_w2_neg_train_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-W2_neg_train'].copy())\n",
    "lrp_180_a1b0_w2_pos_test_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-W2_pos_test'].copy())\n",
    "lrp_180_a1b0_w2_neg_test_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0-W2_neg_test'].copy())\n",
    "lrp_180_a1b0_pos_train_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0_pos_train'].copy())\n",
    "lrp_180_a1b0_neg_train_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0_neg_train'].copy())\n",
    "lrp_180_a1b0_pos_test_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0_pos_test'].copy())\n",
    "lrp_180_a1b0_neg_test_da = LRP_normalize(LRPDict_theta['LRP180']['LRP-A1B0_neg_test'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbb893-2206-43fc-8141-8e3e72c675a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lrp = xr.Dataset()\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_minus60_a1b0_w2_pos_train = lrp_minus60_a1b0_w2_pos_train_da,\n",
    "                       lrp_minus60_a1b0_w2_neg_train = lrp_minus60_a1b0_w2_neg_train_da,\n",
    "                       lrp_minus60_a1b0_w2_pos_test = lrp_minus60_a1b0_w2_pos_test_da,\n",
    "                       lrp_minus60_a1b0_w2_neg_test = lrp_minus60_a1b0_w2_neg_test_da,\n",
    "                       lrp_minus60_a1b0_pos_train = lrp_minus60_a1b0_pos_train_da,\n",
    "                       lrp_minus60_a1b0_neg_train = lrp_minus60_a1b0_neg_train_da,\n",
    "                       lrp_minus60_a1b0_pos_test = lrp_minus60_a1b0_pos_test_da,\n",
    "                       lrp_minus60_a1b0_neg_test = lrp_minus60_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_minus30_a1b0_w2_pos_train = lrp_minus30_a1b0_w2_pos_train_da,\n",
    "                       lrp_minus30_a1b0_w2_neg_train = lrp_minus30_a1b0_w2_neg_train_da,\n",
    "                       lrp_minus30_a1b0_w2_pos_test = lrp_minus30_a1b0_w2_pos_test_da,\n",
    "                       lrp_minus30_a1b0_w2_neg_test = lrp_minus30_a1b0_w2_neg_test_da,\n",
    "                       lrp_minus30_a1b0_pos_train = lrp_minus30_a1b0_pos_train_da,\n",
    "                       lrp_minus30_a1b0_neg_train = lrp_minus30_a1b0_neg_train_da,\n",
    "                       lrp_minus30_a1b0_pos_test = lrp_minus30_a1b0_pos_test_da,\n",
    "                       lrp_minus30_a1b0_neg_test = lrp_minus30_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_0_a1b0_w2_pos_train = lrp_0_a1b0_w2_pos_train_da,\n",
    "                       lrp_0_a1b0_w2_neg_train = lrp_0_a1b0_w2_neg_train_da,\n",
    "                       lrp_0_a1b0_w2_pos_test = lrp_0_a1b0_w2_pos_test_da,\n",
    "                       lrp_0_a1b0_w2_neg_test = lrp_0_a1b0_w2_neg_test_da,\n",
    "                       lrp_0_a1b0_pos_train = lrp_0_a1b0_pos_train_da,\n",
    "                       lrp_0_a1b0_neg_train = lrp_0_a1b0_neg_train_da,\n",
    "                       lrp_0_a1b0_pos_test = lrp_0_a1b0_pos_test_da,\n",
    "                       lrp_0_a1b0_neg_test = lrp_0_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_30_a1b0_w2_pos_train = lrp_30_a1b0_w2_pos_train_da,\n",
    "                       lrp_30_a1b0_w2_neg_train = lrp_30_a1b0_w2_neg_train_da,\n",
    "                       lrp_30_a1b0_w2_pos_test = lrp_30_a1b0_w2_pos_test_da,\n",
    "                       lrp_30_a1b0_w2_neg_test = lrp_30_a1b0_w2_neg_test_da,\n",
    "                       lrp_30_a1b0_pos_train = lrp_30_a1b0_pos_train_da,\n",
    "                       lrp_30_a1b0_neg_train = lrp_30_a1b0_neg_train_da,\n",
    "                       lrp_30_a1b0_pos_test = lrp_30_a1b0_pos_test_da,\n",
    "                       lrp_30_a1b0_neg_test = lrp_30_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_60_a1b0_w2_pos_train = lrp_60_a1b0_w2_pos_train_da,\n",
    "                       lrp_60_a1b0_w2_neg_train = lrp_60_a1b0_w2_neg_train_da,\n",
    "                       lrp_60_a1b0_w2_pos_test = lrp_60_a1b0_w2_pos_test_da,\n",
    "                       lrp_60_a1b0_w2_neg_test = lrp_60_a1b0_w2_neg_test_da,\n",
    "                       lrp_60_a1b0_pos_train = lrp_60_a1b0_pos_train_da,\n",
    "                       lrp_60_a1b0_neg_train = lrp_60_a1b0_neg_train_da,\n",
    "                       lrp_60_a1b0_pos_test = lrp_60_a1b0_pos_test_da,\n",
    "                       lrp_60_a1b0_neg_test = lrp_60_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_90_a1b0_w2_pos_train = lrp_90_a1b0_w2_pos_train_da,\n",
    "                       lrp_90_a1b0_w2_neg_train = lrp_90_a1b0_w2_neg_train_da,\n",
    "                       lrp_90_a1b0_w2_pos_test = lrp_90_a1b0_w2_pos_test_da,\n",
    "                       lrp_90_a1b0_w2_neg_test = lrp_90_a1b0_w2_neg_test_da,\n",
    "                       lrp_90_a1b0_pos_train = lrp_90_a1b0_pos_train_da,\n",
    "                       lrp_90_a1b0_neg_train = lrp_90_a1b0_neg_train_da,\n",
    "                       lrp_90_a1b0_pos_test = lrp_90_a1b0_pos_test_da,\n",
    "                       lrp_90_a1b0_neg_test = lrp_90_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_120_a1b0_w2_pos_train = lrp_120_a1b0_w2_pos_train_da,\n",
    "                       lrp_120_a1b0_w2_neg_train = lrp_120_a1b0_w2_neg_train_da,\n",
    "                       lrp_120_a1b0_w2_pos_test = lrp_120_a1b0_w2_pos_test_da,\n",
    "                       lrp_120_a1b0_w2_neg_test = lrp_120_a1b0_w2_neg_test_da,\n",
    "                       lrp_120_a1b0_pos_train = lrp_120_a1b0_pos_train_da,\n",
    "                       lrp_120_a1b0_neg_train = lrp_120_a1b0_neg_train_da,\n",
    "                       lrp_120_a1b0_pos_test = lrp_120_a1b0_pos_test_da,\n",
    "                       lrp_120_a1b0_neg_test = lrp_120_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_150_a1b0_w2_pos_train = lrp_150_a1b0_w2_pos_train_da,\n",
    "                       lrp_150_a1b0_w2_neg_train = lrp_150_a1b0_w2_neg_train_da,\n",
    "                       lrp_150_a1b0_w2_pos_test = lrp_150_a1b0_w2_pos_test_da,\n",
    "                       lrp_150_a1b0_w2_neg_test = lrp_150_a1b0_w2_neg_test_da,\n",
    "                       lrp_150_a1b0_pos_train = lrp_150_a1b0_pos_train_da,\n",
    "                       lrp_150_a1b0_neg_train = lrp_150_a1b0_neg_train_da,\n",
    "                       lrp_150_a1b0_pos_test = lrp_150_a1b0_pos_test_da,\n",
    "                       lrp_150_a1b0_neg_test = lrp_150_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp = ds_lrp.assign(lrp_180_a1b0_w2_pos_train = lrp_180_a1b0_w2_pos_train_da,\n",
    "                       lrp_180_a1b0_w2_neg_train = lrp_180_a1b0_w2_neg_train_da,\n",
    "                       lrp_180_a1b0_w2_pos_test = lrp_180_a1b0_w2_pos_test_da,\n",
    "                       lrp_180_a1b0_w2_neg_test = lrp_180_a1b0_w2_neg_test_da,\n",
    "                       lrp_180_a1b0_pos_train = lrp_180_a1b0_pos_train_da,\n",
    "                       lrp_180_a1b0_neg_train = lrp_180_a1b0_neg_train_da,\n",
    "                       lrp_180_a1b0_pos_test = lrp_180_a1b0_pos_test_da,\n",
    "                       lrp_180_a1b0_neg_test = lrp_180_a1b0_neg_test_da)\n",
    "\n",
    "ds_lrp.to_netcdf('LRP_output_forHelen/LRP_A1B0.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc13f6-ef66-4c0f-ad71-4e63f4bf33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_pos_train'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -1, cmax = 1,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (pos_train) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (positive_train)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb1965-1e27-4716-9cbc-6dcc77836282",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_neg_train'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -1, cmax = 1,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (neg_train) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (negative_train)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc297c5-e82e-414e-9dc9-5f14167dbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_pos_train'].copy()\n",
    "        field[np.where(field < 0)] = 0.0\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        field = field / np.nanmax(np.abs(field))\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True, \n",
    "                                      cmap = cmaps[color_idx], \n",
    "                                      cmin = color_ranges[color_idx][0], \n",
    "                                      cmax = color_ranges[color_idx][1],\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "        P[1].set_title(f\"{method['title']}_train lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "\n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_train for pos QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb38225-1241-4923-bf3b-43a3fdd2e908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_neg_train'].copy()\n",
    "        field[np.where(field < 0)] = 0.0\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        field = field / np.nanmax(np.abs(field))\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True,\n",
    "                                      cmap = cmaps[color_idx],\n",
    "                                      cmin = color_ranges[color_idx][0], \n",
    "                                      cmax = color_ranges[color_idx][1],\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "        \n",
    "        P[1].set_title(f\"{method['title']}_train lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "        \n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_train for neg QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02e3ef-9ebb-44a3-b1ed-a80e4e7f18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_pos_test'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -2, cmax = 2,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (pos_test) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (positive_test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835e4cb-90aa-48f6-9f34-c9e74877c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "for lag in lagStepsList:\n",
    "\n",
    "    field = np.nanmean(LRPDict_theta[f'LRP{lag}']['samples_correct_neg_test'], axis=0)\n",
    "    \n",
    "    P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar=True, cmap=cmocean.cm.balance, \n",
    "                                      cmin = -2, cmax = 2,\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "    P[1].set_title(f\"Composite observations (neg_test) lag {lag} days\")\n",
    "    subplot_idx += 1\n",
    "\n",
    "# plt.title(\"Composite observations (negative_test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ae7bf-a133-4930-9834-5815b4a1143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_pos_test'].copy()\n",
    "        field[np.where(field < 0)] = 0.0\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        field = field / np.nanmax(np.abs(field))\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True,\n",
    "                                      cmap = cmaps[color_idx], \n",
    "                                      cmin = color_ranges[color_idx][0], \n",
    "                                      cmax = color_ranges[color_idx][1],\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "\n",
    "        P[1].set_title(f\"{method['title']}_test lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "        \n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_test for pos QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbedfe39-a3d2-49f1-b28b-ae3c88865aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_idx = 1\n",
    "color_idx = 0\n",
    "\n",
    "for method in methods:\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    \n",
    "    for lag in lagStepsList:\n",
    "\n",
    "        field = LRPDict_theta[f'LRP{lag}'][method['title']+'_neg_test'].copy()\n",
    "        field[np.where(field < 0)] = 0.0\n",
    "        field = np.nanmean(field, axis=0)\n",
    "        field = field / np.nanmax(np.abs(field))\n",
    "        \n",
    "        P = ecco.plot_proj_to_latlon_grid(ds.XC, ds.YC,\n",
    "                                      field,\n",
    "                                      plot_type = 'contourf',\n",
    "                                      show_colorbar = True,\n",
    "                                      cmap = cmaps[color_idx], \n",
    "                                      cmin = color_ranges[color_idx][0], \n",
    "                                      cmax = color_ranges[color_idx][1],\n",
    "                                      user_lon_0 = -150,\n",
    "                                      dx=2, dy=2, projection_type = 'robin',\n",
    "                                      less_output = True, subplot_grid = [3,3,subplot_idx])\n",
    "        \n",
    "        P[1].set_title(f\"{method['title']}_test lag {lag} days\")\n",
    "        subplot_idx += 1\n",
    "        \n",
    "    subplot_idx = 1\n",
    "    color_idx += 1\n",
    "    # plt.title(f\"{method['title']}_test for neg QoI' w/o neg relevances\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_LRP",
   "language": "python",
   "name": "py310_lrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
